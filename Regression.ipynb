{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "# debug\n",
    "import pdb\n",
    "from loguru import logger\n",
    "\n",
    "# custom\n",
    "from parser import work_parser, author_parser, venue_parser, institution_parser\n",
    "from scraper import oa_work_scraper, oa_author_scraper\n",
    "\n",
    "\n",
    "# basic\n",
    "import json\n",
    "import pdb\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "# regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore', category=DeprecationWarning,  message='`np.bool` is a deprecated alias')\n",
    "filterwarnings(action='ignore', category=DeprecationWarning,  message='`np.int` is a deprecated alias' )\n",
    "filterwarnings(action='ignore', category=DeprecationWarning,  message='`np.object` is a deprecated alias')\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from collections import namedtuple\n",
    "from nltk.tokenize import word_tokenize\n",
    "import time\n",
    "\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 823/823 [02:16<00:00,  6.02it/s]\n",
      "100%|██████████| 5653/5653 [03:20<00:00, 28.19it/s]\n",
      "100%|██████████| 241/241 [00:14<00:00, 16.58it/s]\n",
      "100%|██████████| 35/35 [00:01<00:00, 18.34it/s]\n"
     ]
    }
   ],
   "source": [
    "input_types = ['works', 'authors', 'insts', 'venues']\n",
    "\n",
    "data = {}\n",
    "for input_type in input_types:\n",
    "    DATA_FILE = './data/data.' + input_type + '.2012.v1.txt'\n",
    "    with open(DATA_FILE, 'r') as f:\n",
    "        input_json = [ast.literal_eval(work) for work in tqdm(f.readlines())]\n",
    "        input_dict = [j for i in input_json for j in i]\n",
    "        data[input_type] = pd.DataFrame.from_dict(input_dict).set_index('id')\n",
    "#         print(data[input_type].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "works\n",
      "Index(['doi', 'title', 'type', 'publication_date', 'host_venue',\n",
      "       'open_access_is_oa', 'open_access_oa_status', 'authorships',\n",
      "       'page_count', 'cited_by_count', 'concepts', 'referenced_works',\n",
      "       'abstract', 'counts_by_year'],\n",
      "      dtype='object')\n",
      "=====================\n",
      "authors\n",
      "Index(['orchid', 'display_name', 'works_count', 'cited_by_count',\n",
      "       'created_date', 'concepts', 'counts_by_year'],\n",
      "      dtype='object')\n",
      "=====================\n",
      "insts\n",
      "Index(['display_name', 'country_code', 'type', 'homepage_url', 'works_count',\n",
      "       'cited_by_count', 'associated_institutions', 'concepts',\n",
      "       'counts_by_year', 'created_date'],\n",
      "      dtype='object')\n",
      "=====================\n",
      "venues\n",
      "Index(['display_name', 'works_count', 'cited_by_count', 'is_oa', 'type',\n",
      "       'created_date', 'concepts', 'counts_by_year'],\n",
      "      dtype='object')\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for k in data:\n",
    "    print(k)\n",
    "    print(data[k].columns)\n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aarushi/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/aarushi/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data['works'])\n",
    "# df=df[:1000]\n",
    "df['no_of_authors'] = df['authorships'].map(lambda x: len(x))\n",
    "df['no_of_referenced_works'] = df['referenced_works'].map(len)\n",
    "# FEATURES\n",
    "# df['yearly_citation_count']=pd.DataFrame(df['cited_by_count']/(pd.to_datetime(df['publication_date']).rsub(pd.to_datetime('2022-10-31')).dt.days/365.25)).round()\n",
    "\n",
    "#(2) Journal and Publisher relevant features\n",
    "venues_data = data['venues']\n",
    "venue_significance =defaultdict(lambda: 0,  (venues_data['cited_by_count']/venues_data['works_count']).to_dict())\n",
    "df['venue_significance'] = df['host_venue'].map(lambda x: venue_significance[x])\n",
    "# venue_works =venues_data['works_count'].to_dict()\n",
    "venue_works =defaultdict(lambda: 0,venues_data['works_count'].to_dict())\n",
    "df['venue_works'] = df['host_venue'].map(lambda x: venue_works[x])\n",
    "# venue_citations =venues_data['cited_by_count'].to_dict()\n",
    "venue_citations =defaultdict(lambda: 0,venues_data['cited_by_count'].to_dict())\n",
    "df['venue_citations'] = df['host_venue'].map(lambda x: venue_citations[x])\n",
    "\n",
    "# (3)Author-specific Features\n",
    "# barcc = df['no_of_referenced_works'].mean()\n",
    "thresh_author_citation_prominent = data['authors']['cited_by_count'].mean()\n",
    "# author_citation = data['authors'][\"cited_by_count\"].to_dict()\n",
    "author_citation = defaultdict(lambda: 0, data['authors'][\"cited_by_count\"].to_dict())\n",
    "df['author_prominency'] = df['authorships'].map(lambda x: 1 if max([author_citation[i[0]] for i in x])-thresh_author_citation_prominent>=0 else 0)\n",
    "df['authors_mean_citations'] = df['authorships'].map(lambda x: np.mean([author_citation[i[0]] for i in x]))\n",
    "\n",
    "# author_work_count = data['authors'][\"works_count\"].to_dict()\n",
    "author_work_count = defaultdict(lambda: 0, data['authors'][\"works_count\"].to_dict())\n",
    "df['authors_mean_works'] = df['authorships'].map(lambda x: np.mean([author_work_count[i[0]] for i in x]))\n",
    "\n",
    "\n",
    "# (7)Insti-specific Features\n",
    "# insts_citation = data['insts'][\"cited_by_count\"].to_dict()\n",
    "insts_citation = defaultdict(lambda: 0, data['insts'][\"cited_by_count\"].to_dict())\n",
    "df['insts_mean_citations'] = df['authorships'].map(lambda x: np.mean([insts_citation[i[1][0]] for i in x if len(i[1])>0]))\n",
    "\n",
    "# insts_work_count = data['insts'][\"works_count\"].to_dict()\n",
    "insts_work_count = defaultdict(lambda: 0, data['insts'][\"works_count\"].to_dict())\n",
    "df['insts_mean_works'] = df['authorships'].map(lambda x: np.mean([insts_work_count[i[1][0]] for i in x if len(i[1])>0]))\n",
    "\n",
    "\n",
    "#(4)Page_count AA: a lot are -1 in the datase. The feature might be not so relevant\n",
    "df['page_count']=df['page_count'].map(lambda x: x if isinstance(x, int ) else x[0])\n",
    "\n",
    "#(5) Publication month\n",
    "df['publication_month'] = df['publication_date'].map(lambda x: int(x.split('-')[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1179.4758884906769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aarushi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/aarushi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/aarushi/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "849.8859283924103\n",
      "Index(['open_access_is_oa', 'page_count', 'cited_by_count', 'abstract',\n",
      "       'no_of_authors', 'no_of_referenced_works', 'venue_significance',\n",
      "       'venue_works', 'venue_citations', 'author_prominency',\n",
      "       'authors_mean_citations', 'authors_mean_works', 'insts_mean_citations',\n",
      "       'insts_mean_works', 'publication_month', 'abstract_vector',\n",
      "       'abstract_feature_1', 'abstract_feature_2', 'abstract_feature_3',\n",
      "       'abstract_feature_4', 'abstract_feature_5', 'abstract_feature_6',\n",
      "       'abstract_feature_7', 'abstract_feature_8', 'abstract_feature_9',\n",
      "       'abstract_feature_10', 'abstract_feature_11', 'abstract_feature_12',\n",
      "       'abstract_feature_13', 'abstract_feature_14', 'abstract_feature_15',\n",
      "       'abstract_feature_16', 'abstract_feature_17', 'abstract_feature_18',\n",
      "       'abstract_feature_19', 'abstract_feature_20'],\n",
      "      dtype='object')\n",
      "(102477, 33)\n",
      "(102477,)\n",
      "(25620, 33)\n",
      "(25620,)\n"
     ]
    }
   ],
   "source": [
    "df_new = df.drop(columns=['open_access_oa_status','doi', 'title', 'type','publication_date','host_venue','authorships', 'referenced_works', 'concepts','counts_by_year'])\n",
    "df_new = df_new.fillna(0)\n",
    "X_train, X_test = train_test_split(df_new, test_size=0.2)\n",
    "np.save(open('train_id.npy', 'wb'), np.array(list(X_train.index)))\n",
    "np.save(open('test_id.npy', 'wb'), np.array(list(X_test.index)))\n",
    "# aa=np.load('test.npy')#, 'r')\n",
    "\n",
    "# (6) Textual features\n",
    "t=time.time()\n",
    "docs = list(X_train['abstract'])\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(docs)]\n",
    "model = Doc2Vec(tagged_data, vector_size = 20, window = 3, min_count = 1, epochs = 10, workers = 4)\n",
    "model.save(\"Abstract_doc2Vec\")\n",
    "# model2 = model.load(\"Abstract_doc2Vec\")\n",
    "print(time.time()-t)\n",
    "t=time.time()\n",
    "X_train[\"abstract_vector\"] = X_train[\"abstract\"].map(lambda x: model.infer_vector(word_tokenize(x)))\n",
    "X_test[\"abstract_vector\"] = X_test[\"abstract\"].map(lambda x: model.infer_vector(word_tokenize(x)))\n",
    "columns = [\"abstract_feature_\"+str(i) for i in range(1,21)]\n",
    "X_train[columns] = pd.DataFrame(X_train[\"abstract_vector\"].tolist(), index= X_train.index)\n",
    "X_test[columns] = pd.DataFrame(X_test[\"abstract_vector\"].tolist(), index= X_test.index)\n",
    "print(time.time()-t)\n",
    "\n",
    "print(X_train.columns)\n",
    "\n",
    "# target variable\n",
    "y_train=X_train['cited_by_count']\n",
    "y_test = X_test['cited_by_count']\n",
    "X_train = X_train.drop(columns=['abstract', 'abstract_vector'])\n",
    "X_test = X_test.drop(columns=['abstract','abstract_vector'])\n",
    "X_train.to_csv(\"Training_data.csv\")#, index=False)\n",
    "X_test.to_csv(\"Test_data.csv\")#, index=False)\n",
    "X_train = X_train.drop(columns=['cited_by_count'])\n",
    "X_test = X_test.drop(columns=['cited_by_count'])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:===========\n",
      "  - Train RMSE: 25.755 +/- 0.0\n",
      "  - Test RMSE: 47.069 +/- 0.0\n",
      "Train Mean absolute error:  7.867497940751983\n",
      "Test Mean absolute error:  8.27016541579083\n",
      "[ 4.22616493e+00 -8.73679535e-01  3.39464316e-01  1.64043378e-01\n",
      "  8.87633418e-01  1.63614624e-05 -5.79082800e-06  2.85417181e+00\n",
      "  1.35737100e-03 -2.62724760e-02  3.79743895e-07 -2.33656427e-05\n",
      " -4.82785240e-02  4.38302174e-01  2.53482784e-01  2.08403721e+00\n",
      " -1.07956290e+00 -2.37552127e+00 -1.95784353e-01  7.94044047e-01\n",
      " -2.03907194e+00 -2.02551605e+00 -2.83709809e+00 -1.44459208e+00\n",
      " -3.44047451e-01 -6.51918879e+00 -1.37184051e+00  9.03161866e-01\n",
      " -9.30952728e-02 -1.21290830e+00  2.35497603e+00  1.12841701e+00\n",
      "  2.81649928e+00]\n",
      "{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'normalize': False}\n",
      "0.1364806726872041\n",
      "0.0591526263990334\n",
      "GradientBoostingRegressor:===========\n",
      "  - Train RMSE: 22.442 +/- 0.0\n",
      "  - Test RMSE: 46.805 +/- 0.0\n",
      "Train Mean absolute error:  7.096495132209163\n",
      "Test Mean absolute error:  7.728794565962442\n",
      "{'alpha': 0.9, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'ls', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'presort': 'auto', 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "0.3443558120352336\n",
      "0.06965922730792884\n",
      "MLPRegressor:===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aarushi/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Train RMSE: 28.097 +/- 0.0\n",
      "  - Test RMSE: 48.786 +/- 0.0\n",
      "Train Mean absolute error:  7.122628015130462\n",
      "Test Mean absolute error:  7.511123345029008\n",
      "{'activation': 'relu', 'alpha': 0, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': 1, 'learning_rate': 'constant', 'learning_rate_init': 0.0001, 'max_iter': 100, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 0, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "-0.027736915769789094\n",
      "-0.010736355797493546\n",
      "XGBRegressor:===========\n",
      "  - Train RMSE: 12.542 +/- 0.0\n",
      "  - Test RMSE: 47.548 +/- 0.0\n",
      "Train Mean absolute error:  5.6441358425818375\n",
      "Test Mean absolute error:  7.839350211244021\n",
      "{'objective': 'reg:squarederror', 'base_score': 0.5, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'gamma': 0, 'gpu_id': -1, 'grow_policy': 'depthwise', 'importance_type': None, 'interaction_constraints': '', 'learning_rate': 0.300000012, 'max_bin': 256, 'max_cat_to_onehot': 4, 'max_delta_step': 0, 'max_depth': 6, 'max_leaves': 0, 'min_child_weight': 1, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 100, 'n_jobs': 0, 'num_parallel_tree': 1, 'predictor': 'auto', 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 1, 'sampling_method': 'uniform', 'scale_pos_weight': 1, 'subsample': 1, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': None}\n",
      "0.7952058943544078\n",
      "0.03992167120103929\n"
     ]
    }
   ],
   "source": [
    "# regression analysis\n",
    "# X_train = X_train.drop(columns=['open_access_oa_status'])\n",
    "# X_test = X_test.drop(columns=['open_access_oa_status'])\n",
    "# X_train = X_train.drop(columns=['abstract_vector'])\n",
    "# X_test = X_test.drop(columns=['abstract_vector'])\n",
    "\n",
    "\n",
    "def print_metric(metric_name, metric_list):\n",
    "    mean, std = np.mean(metric_list), np.std(metric_list)\n",
    "    print (f\"  - {metric_name}: {np.round(mean, 3)} +/- {np.round(std, 3)}\")\n",
    "    return\n",
    "\n",
    "reg_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
    "#     'SGDRegressor': SGDRegressor(),\n",
    "    'MLPRegressor': MLPRegressor(alpha=0, random_state=SEED, hidden_layer_sizes=1,max_iter=100, learning_rate_init =0.0001),\n",
    "    'XGBRegressor': XGBRegressor(),\n",
    "}\n",
    "\n",
    "for model_name, model in reg_models.items():\n",
    "#     kf = KFold(n_splits=5, random_state=2, shuffle=True)\n",
    "        train_rmse, test_rmse = [], []\n",
    "        print (f\"{model_name}:===========\")\n",
    "    \n",
    "#     for train_index, test_index in kf.split(X):\n",
    "#         X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#         y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "#         # normalize\n",
    "#         scaler = StandardScaler()\n",
    "#         X_train = scaler.fit_transform(X_train)\n",
    "#         X_test = scaler.transform(X_test)\n",
    "\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred = model.predict(X_train)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true=y_train, y_pred=y_pred))\n",
    "#         rmse = mean_squared_error(y_true=y_train, y_pred=y_pred, squared=False)\n",
    "        train_rmse.append(rmse)\n",
    "        train_abs_error = mean_absolute_error(y_train, y_pred)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true=y_test, y_pred=y_pred))\n",
    "#         rmse = mean_squared_error(y_true=y_test, y_pred=y_pred, squared=False)\n",
    "        test_rmse.append(rmse)\n",
    "        test_abs_error = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "        print_metric('Train RMSE', train_rmse)\n",
    "        print_metric('Test RMSE', test_rmse)\n",
    "        print(\"Train Mean absolute error: \", train_abs_error)\n",
    "        print(\"Test Mean absolute error: \", test_abs_error)\n",
    "        if model_name[:5]==\"Linea\":\n",
    "            print(model.coef_)\n",
    "            print(model.get_params())\n",
    "            print(model.score(X_train,y_train))\n",
    "            print(model.score(X_test,y_test))\n",
    "        elif model_name[:3]==\"SGD\" :\n",
    "            print(model.coef_)\n",
    "            print(model.get_params())\n",
    "            print(model.score(X_train,y_train))\n",
    "            print(model.score(X_test,y_test))\n",
    "        elif model_name[:4]==\"Grad\":#\n",
    "            print(model.get_params())\n",
    "            print(model.score(X_train,y_train))\n",
    "            print(model.score(X_test,y_test))\n",
    "        elif model_name[:3]==\"XGB\":\n",
    "            print(model.get_params())\n",
    "            print(model.score(X_train,y_train))\n",
    "            print(model.score(X_test,y_test))\n",
    "        else:\n",
    "            print(model.get_params())\n",
    "            print(model.score(X_train,y_train))\n",
    "            print(model.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
