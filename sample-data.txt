[{'id': 'W2194775991', 'doi': 'https://doi.org/10.1109/cvpr.2016.90', 'title': 'Deep Residual Learning for Image Recognition', 'type': 'proceedings-article', 'publication_date': '2016-06-27', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2164292938', ['I4210164937']], ['A2499063207', ['I4210164937']], ['A2119543935', ['I4210164937']], ['A2200192130', ['I4210164937']]], 'cited_by_count': 82989, 'concepts': [['C155512373', '0.9166446'], ['C41008148', '0.779227'], ['C154945302', '0.730741'], ['C21780288', '0.72122216'], ['C2776151529', '0.6045929']], 'referenced_works': ['W1536680647', 'W1677182931', 'W1903029394', 'W1932847118', 'W1976921161', 'W1984309565', 'W1997542937', 'W2031489346', 'W2064675550', 'W2097117768', 'W2102605133', 'W2107878631', 'W2117812871', 'W2124509324', 'W2147238549', 'W2147800946', 'W2159979951', 'W2964103341', 'W4212915314', 'W4231990273', 'W4238404964', 'W4242212377'], 'abstract': 'Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.', 'counts_by_year': [[2022, 11945], [2021, 23636], [2020, 20114], [2019, 15164], [2018, 8056], [2017, 3197], [2016, 754], [2015, 17], [2014, 9], [2012, 6]]}, {'id': 'W2311203695', 'doi': 'https://doi.org/10.1093/molbev/msw054', 'title': 'MEGA7: Molecular Evolutionary Genetics Analysis Version 7.0 for Bigger Datasets', 'type': 'journal-article', 'publication_date': '2016-03-22', 'host_venue': 'V57552105', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2149898553', ['I185163786']], ['A2307130762', ['I84392919']], ['A2164450181', ['I69740276']]], 'cited_by_count': 27959, 'concepts': [['C2776784076', '0.7593913'], ['C41008148', '0.63809496'], ['C70343354', '0.62603956'], ['C37789001', '0.6124724'], ['C2777904410', '0.5609492']], 'referenced_works': ['W1519266993', 'W1992566665', 'W2034285706', 'W2103296919', 'W2107886421', 'W2117260150', 'W2121552166', 'W2124105487', 'W2152207030'], 'abstract': 'Abstract We present the latest version of the Molecular Evolutionary Genetics Analysis (M ega ) software, which contains many sophisticated methods and tools for phylogenomics and phylomedicine. In this major upgrade, M ega has been optimized for use on 64-bit computing systems for analyzing larger datasets. Researchers can now explore and analyze tens of thousands of sequences in M ega . The new version also provides an advanced wizard for building timetrees and includes a new functionality to automatically predict gene duplication events in gene family trees. The 64-bit M ega is made available in two interfaces: graphical and command line. The graphical user interface (GUI) is a native Microsoft Windows application that can also be used on Mac OS X. The command line M ega is available as native applications for Windows, Linux, and Mac OS X. They are intended for use in high-throughput and scripted analysis. Both versions are available from www.megasoftware.net free of charge.', 'counts_by_year': [[2022, 3469], [2021, 5252], [2020, 6079], [2019, 5965], [2018, 4423], [2017, 2360], [2016, 369], [2015, 4]]}, {'id': 'W2963341956', 'doi': 'https://doi.org/10.18653/v1/n19-1423', 'title': None, 'type': 'proceedings-article', 'publication_date': '2018-10-11', 'host_venue': 'V4306420633', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2153797812', ['I1291425158']], ['A2142444093', ['I1291425158']], ['A2552119883', ['I1291425158']], ['A1997665198', ['I1291425158']]], 'cited_by_count': 20600, 'concepts': [['C41008148', '0.785668'], ['C137293760', '0.6452642'], ['C44291984', '0.6449243'], ['C66322947', '0.6372907'], ['C2776359362', '0.562268']], 'referenced_works': ['W131533222', 'W1486649854', 'W1566289585', 'W1599016936', 'W1840435438', 'W2025768430', 'W2108598243', 'W2117130368', 'W2121227244', 'W2130903752', 'W2131462252', 'W2131744502', 'W2144578941', 'W2149933564', 'W2153579005', 'W2158108973', 'W2158139315', 'W2170973209', 'W2250539671', 'W2251939518', 'W2270070752', 'W2396767181', 'W2413794162', 'W2462831000', 'W2507974895', 'W2551396370', 'W2610858497', 'W2784823820', 'W2880875857', 'W2888329843', 'W2891602716', 'W2897076808', 'W2951714314', 'W2962718483', 'W2962739339', 'W2962808855', 'W2963026768', 'W2963088785', 'W2963159690', 'W2963339397', 'W2963403868', 'W2963563735', 'W2963564796', 'W2963644595', 'W2963748441', 'W2963756346', 'W2963804993', 'W2963846996', 'W2963918774', 'W2978670439', 'W3098057198', 'W3104033643'], 'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).', 'counts_by_year': [[2022, 736], [2021, 9119], [2020, 7596], [2019, 3063], [2018, 78], [2017, 6]]}, {'id': 'W2799524357', 'doi': 'https://doi.org/10.1093/molbev/msy096', 'title': 'MEGA X: Molecular Evolutionary Genetics Analysis across Computing Platforms', 'type': 'journal-article', 'publication_date': '2018-06-01', 'host_venue': 'V57552105', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2149898553', ['I185163786', 'I84392919']], ['A2307130762', ['I84392919']], ['A2782248470', ['I84392919']], ['A2799579685', ['I84392919']], ['A2164450181', ['I69740276']]], 'cited_by_count': 16432, 'concepts': [['C2781078984', '0.95899737'], ['C149810388', '0.7217384'], ['C86803240', '0.67771566'], ['C2777904410', '0.62332606'], ['C513985346', '0.5981139']], 'referenced_works': ['W2097403532', 'W2121552166', 'W2156434383', 'W2311203695'], 'abstract': 'The Molecular Evolutionary Genetics Analysis (Mega) software implements many analytical methods and tools for phylogenomics and phylomedicine. Here, we report a transformation of Mega to enable cross-platform use on Microsoft Windows and Linux operating systems. Mega X does not require virtualization or emulation software and provides a uniform user experience across platforms. Mega X has additionally been upgraded to use multiple computing cores for many molecular evolutionary analyses. Mega X is available in two interfaces (graphical and command line) and can be downloaded from www.megasoftware.net free of charge.', 'counts_by_year': [[2022, 4753], [2021, 6184], [2020, 3982], [2019, 1399], [2018, 91]]}, {'id': 'W2963446712', 'doi': 'https://doi.org/10.1109/cvpr.2017.243', 'title': 'Densely Connected Convolutional Networks', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2114281204', ['I205783295']], ['A2566736780', ['I99065089']], ['A731054299', ['I2252078561']], ['A2003907699', ['I205783295']]], 'cited_by_count': 15939, 'concepts': [['C41008148', '0.8167397'], ['C185798385', '0.80665535'], ['C2776401178', '0.78979075'], ['C2779227376', '0.7632209'], ['C81363708', '0.6724046']], 'referenced_works': ['W1677182931', 'W1903029394', 'W2097117768', 'W2102013737', 'W2112796928', 'W2117539524', 'W2147800946', 'W2162741153', 'W2183341477', 'W2194775991', 'W2302255633', 'W2331143823'], 'abstract': 'Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .', 'counts_by_year': [[2022, 2581], [2021, 4523], [2020, 4265], [2019, 3016], [2018, 1366], [2017, 150], [2016, 6], [2015, 1], [2012, 1]]}, {'id': 'W639708223', 'doi': 'https://doi.org/10.1109/tpami.2016.2577031', 'title': 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks', 'type': 'journal-article', 'publication_date': '2017-06-01', 'host_venue': 'V199944782', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2119543935', ['I126520041']], ['A2164292938', ['I4210164937']], ['A2473549963', ['I2252078561']], ['A2200192130', ['I4210164937']]], 'cited_by_count': 12873, 'concepts': [['C41008148', '0.8494384'], ['C81363708', '0.75703406'], ['C2776151529', '0.7233534'], ['C75608658', '0.703492'], ['C2780513914', '0.66441274']], 'referenced_works': ['W639708223', 'W1536680647', 'W1903029394', 'W1923115158', 'W1958328135', 'W1991367009', 'W2031489346', 'W2046382188', 'W2066624635', 'W2068730032', 'W2088049833', 'W2097117768', 'W2102605133', 'W2117539524', 'W2147800946', 'W2155893237', 'W2168356304', 'W2618530766', 'W2963690996'], 'abstract': "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.", 'counts_by_year': [[2022, 3046], [2021, 3936], [2020, 2561], [2019, 1957], [2018, 1033], [2017, 261], [2016, 54], [2015, 6], [2012, 2]]}, {'id': 'W2963037989', 'doi': 'https://doi.org/10.1109/cvpr.2016.91', 'title': 'You Only Look Once: Unified, Real-Time Object Detection', 'type': 'proceedings-article', 'publication_date': '2016-06-27', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2392241600', ['I201448701']], ['A2310010008', ['I2945602774']], ['A2473549963', ['I2252078561']], ['A1988090614', ['I2945602774']]], 'cited_by_count': 12865, 'concepts': [['C2776151529', '0.885805'], ['C41008148', '0.7814618'], ['C63584917', '0.7625847'], ['C154945302', '0.7612001'], ['C64869954', '0.6373813']], 'referenced_works': ['W7746136', 'W95258188', 'W124653583', 'W1507506748', 'W1525954826', 'W1536680647', 'W1832500336', 'W1932624639', 'W2037227137', 'W2056025798', 'W2068730032', 'W2088049833', 'W2102605133', 'W2107634464', 'W2117539524', 'W2122146326', 'W2124386111', 'W2168356304', 'W2535410496', 'W3097096317'], 'abstract': 'We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.', 'counts_by_year': [[2022, 2281], [2021, 3449], [2020, 3132], [2019, 2422], [2018, 1123], [2017, 408], [2016, 41], [2015, 3]]}, {'id': 'W2183341477', 'doi': 'https://doi.org/10.1109/cvpr.2016.308', 'title': 'Rethinking the Inception Architecture for Computer Vision', 'type': 'proceedings-article', 'publication_date': '2016-06-27', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A331124168', ['I1291425158']], ['A2022840042', ['I1291425158']], ['A2272624714', ['I1291425158']], ['A2343055381', ['I1291425158']], ['A2557947390', ['I45129253']]], 'cited_by_count': 12553, 'concepts': [['C41008148', '0.82277274'], ['C185798385', '0.5734806'], ['C2776214188', '0.55791986'], ['C45374587', '0.52368534'], ['C119857082', '0.5158228']], 'referenced_works': ['W54257720', 'W1677182931', 'W1903029394', 'W1944396096', 'W2016053056', 'W2068730032', 'W2097117768', 'W2102605133', 'W2111979893', 'W2113325037', 'W3099206234'], 'abstract': 'Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.', 'counts_by_year': [[2022, 1772], [2021, 3449], [2020, 3151], [2019, 2388], [2018, 1341], [2017, 382], [2016, 52], [2015, 2], [2014, 1]]}, {'id': 'W2618530766', 'doi': 'https://doi.org/10.1145/3065386', 'title': 'ImageNet classification with deep convolutional neural networks', 'type': 'journal-article', 'publication_date': '2017-05-24', 'host_venue': 'V103482838', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A1171453863', ['I1291425158']], ['A215131072', ['I1291425158']], ['A563069026', ['I2747134083']]], 'cited_by_count': 11497, 'concepts': [['C188441871', '0.8576871'], ['C41008148', '0.82062376'], ['C22019652', '0.7917407'], ['C81363708', '0.75154054'], ['C70437156', '0.64878106']], 'referenced_works': ['W2015861736', 'W2018435387', 'W2026942141', 'W2053229256', 'W2061212083', 'W2097117768', 'W2101926813', 'W2110764733', 'W2130325614', 'W2144161366', 'W2166049352', 'W2169805405', 'W2546302380', 'W2911964244'], 'abstract': 'We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.', 'counts_by_year': [[2022, 2816], [2021, 3953], [2020, 2233], [2019, 1297], [2018, 577], [2017, 338], [2016, 152], [2015, 45], [2014, 34], [2013, 24]]}, {'id': 'W2963150697', 'doi': 'https://doi.org/10.1109/iccv.2017.322', 'title': 'Mask R-CNN', 'type': 'proceedings-article', 'publication_date': '2017-03-20', 'host_venue': 'V4306402512', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2164292938', ['I4210114444']], ['A102740216', ['I4210114444']], ['A1944499404', ['I216852786']], ['A2473549963', ['I216852786']]], 'cited_by_count': 11067, 'concepts': [['C41008148', '0.60304093'], ['C154945302', '0.4118134']], 'referenced_works': ['W1536680647', 'W1903029394', 'W1923115158', 'W1960289438', 'W1991367009', 'W2080873731', 'W2088049833', 'W2102605133', 'W2147800946', 'W2194775991', 'W2216125271', 'W2288122362', 'W2317851288', 'W2549139847', 'W2555182955', 'W2578797046', 'W2963516811', 'W2964304707'], 'abstract': 'We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.', 'counts_by_year': [[2022, 1721], [2021, 3369], [2020, 3040], [2019, 2237], [2018, 650], [2017, 40]]}, {'id': 'W3102476541', 'doi': 'https://doi.org/10.1145/2939672.2939785', 'title': 'XGBoost', 'type': 'proceedings-article', 'publication_date': '2016-03-09', 'host_venue': 'V2597173376', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2126135973', ['I201448701']], ['A1988556028', ['I201448701']]], 'cited_by_count': 10405, 'concepts': [['C46686674', '0.92228204'], ['C41008148', '0.79534125'], ['C48044578', '0.7413219'], ['C119857082', '0.63006496'], ['C154945302', '0.5558268']], 'referenced_works': ['W1678356000', 'W1987356990', 'W2008183828', 'W2024046085', 'W2070493638', 'W2076618162', 'W2108214384', 'W2112452856', 'W2125816831', 'W2911964244', 'W2997591727', 'W3001645704'], 'abstract': 'Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.', 'counts_by_year': [[2022, 2650], [2021, 3114], [2020, 2280], [2019, 1311], [2018, 718], [2017, 266], [2016, 46], [2015, 1], [2012, 1]]}, {'id': 'W2401404581', 'doi': 'https://doi.org/10.1038/nmeth.3869', 'title': 'DADA2: High-resolution sample inference from Illumina amplicon data', 'type': 'journal-article', 'publication_date': '2016-07-01', 'host_venue': 'V127827428', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2110608429', ['I97018004']], ['A1829572977', ['I4210133866']], ['A2602181027', ['I97018004']], ['A2167032357', ['I4210133866']], ['A3015736222', ['I4210133866']], ['A2150170722', ['I97018004']]], 'cited_by_count': 10246, 'concepts': [['C8185291', '0.68745255'], ['C97256817', '0.4661046'], ['C70721500', '0.4386518'], ['C86803240', '0.43731374'], ['C198531522', '0.43085322']], 'referenced_works': ['W1565875270', 'W1624769014', 'W1973207456', 'W1983047294', 'W1987816906', 'W1991486768', 'W2013371150', 'W2072970694', 'W2073548709', 'W2079520182', 'W2084791457', 'W2088833470', 'W2098234692', 'W2105368763', 'W2106308757', 'W2108718991', 'W2117457769', 'W2124608356', 'W2128769815', 'W2136879569', 'W2162667876', 'W2164762627', 'W2166171121'], 'abstract': 'We present the open-source software package DADA2 for modeling and correcting Illumina-sequenced amplicon errors (https://github.com/benjjneb/dada2). DADA2 infers sample sequences exactly and resolves differences of as little as 1 nucleotide. In several mock communities, DADA2 identified more real variants and output fewer spurious sequences than other methods. We applied DADA2 to vaginal samples from a cohort of pregnant women, revealing a diversity of previously undetected Lactobacillus crispatus variants.', 'counts_by_year': [[2022, 3205], [2021, 3303], [2020, 2165], [2019, 1039], [2018, 383], [2017, 87], [2016, 18]]}, {'id': 'W2257979135', 'doi': 'https://doi.org/10.1038/nature16961', 'title': 'Mastering the game of Go with deep neural networks and tree search', 'type': 'journal-article', 'publication_date': '2016-01-28', 'host_venue': 'V137773608', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2593774290', ['I1291425158']], ['A2497967435', ['I1291425158']], ['A2190280414', ['I1291425158']], ['A2015134264', ['I1291425158']], ['A2697048878', ['I1291425158']], ['A2600316320', ['I1291425158']], ['A2512603362', ['I1291425158']], ['A3124694539', ['I1291425158']], ['A2516928172', ['I1291425158']], ['A2107049243', ['I1291425158']], ['A2020844714', ['I1291425158']], ['A2170953427', ['I1291425158']], ['A2508317249', ['I1291425158']], ['A223268448', ['I1291425158']], ['A215131072', ['I1291425158']], ['A2952465156', ['I1291425158']], ['A2511611111', ['I1291425158']], ['A1111049960', ['I1291425158']], ['A2032008572', ['I1291425158']], ['A4302276', ['I1291425158']]], 'cited_by_count': 9542, 'concepts': [['C46149586', '0.86482334'], ['C41008148', '0.71358466'], ['C2780465443', '0.6778748'], ['C50644808', '0.6620959'], ['C97541855', '0.6127386']], 'referenced_works': ['W50617830', 'W202421935', 'W1497290640', 'W1500868819', 'W1501727541', 'W1509593372', 'W1510972507', 'W1528097685', 'W1542941925', 'W1551466210', 'W1563895031', 'W1574724904', 'W1587022413', 'W1589775371', 'W1625390266', 'W1714211023', 'W1863869622', 'W1977989560', 'W1989923401', 'W1994685255', 'W1997840820', 'W2020135152', 'W2029572069', 'W2041367235', 'W2056740053', 'W2099001564', 'W2101101673', 'W2110630796', 'W2126316555', 'W2128020951', 'W2134400955', 'W2135129960', 'W2144354855', 'W2145339207', 'W2153039919', 'W2154291838', 'W2157803532', 'W2161608691', 'W2342614577', 'W2535938846', 'W2911296969', 'W2913764096', 'W2919115771', 'W4214717370'], 'abstract': "The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.", 'counts_by_year': [[2022, 950], [2021, 1946], [2020, 2052], [2019, 1886], [2018, 1411], [2017, 873], [2016, 401], [2015, 6], [2012, 2]]}, {'id': 'W2962793481', 'doi': 'https://doi.org/10.1109/iccv.2017.244', 'title': 'Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks', 'type': 'proceedings-article', 'publication_date': '2017-10-01', 'host_venue': 'V4306419272', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2112232458', ['I95457486']], ['A2604325483', ['I95457486']], ['A2077136294', ['I95457486']], ['A2088536091', ['I95457486']]], 'cited_by_count': 9291, 'concepts': [['C2779757391', '0.87979054'], ['C115961682', '0.6613763'], ['C149364088', '0.6438298'], ['C41008148', '0.63700366'], ['C154945302', '0.63232315']], 'referenced_works': ['W845365781', 'W1530781137', 'W1896934482', 'W1903029394', 'W1905829557', 'W2019969451', 'W2058017216', 'W2083366168', 'W2100495367', 'W2117539524', 'W2118557299', 'W2157575844', 'W2194775991', 'W2292976057', 'W2326925005', 'W2340897893', 'W2474531669', 'W2475287302', 'W2520707372', 'W2560481159', 'W2963420272', 'W2963444790', 'W3142024233', 'W4230371388'], 'abstract': 'Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to push F(G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.', 'counts_by_year': [[2022, 1209], [2021, 2658], [2020, 2615], [2019, 1942], [2018, 768], [2017, 85], [2012, 2]]}, {'id': 'W2963073614', 'doi': 'https://doi.org/10.1109/cvpr.2017.632', 'title': 'Image-to-Image Translation with Conditional Adversarial Networks', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2077136294', ['I95457486']], ['A2112232458', ['I95457486']], ['A2120864060', ['I95457486']], ['A2088536091', ['I95457486']]], 'cited_by_count': 9104, 'concepts': [['C2779757391', '0.80383134'], ['C115961682', '0.7355706'], ['C41008148', '0.7344664'], ['C37736160', '0.6980449'], ['C149364088', '0.6660076']], 'referenced_works': ['W845365781', 'W1903029394', 'W1905829557', 'W1972420097', 'W1999360130', 'W2019969451', 'W2026019603', 'W2083366168', 'W2093848332', 'W2097073572', 'W2100495367', 'W2117539524', 'W2129112648', 'W2133665775', 'W2275363859', 'W2292976057', 'W2340897893', 'W2461158874', 'W2475287302', 'W2514262209', 'W2963420272', 'W2964345931', 'W4214815024'], 'abstract': 'We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.', 'counts_by_year': [[2022, 1118], [2021, 2501], [2020, 2440], [2019, 1984], [2018, 896], [2017, 149], [2016, 1]]}, {'id': 'W2412782625', 'doi': 'https://doi.org/10.1109/tpami.2017.2699184', 'title': 'DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs', 'type': 'journal-article', 'publication_date': '2018-04-01', 'host_venue': 'V199944782', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2127898042', ['I1291425158']], ['A2466861874', ['I1291425158']], ['A2259861871', ['I45129253']], ['A2425489219', ['I1291425158']], ['A2065104188', ['I145311948']]], 'cited_by_count': 9042, 'concepts': [['C152565575', '0.81482506'], ['C154945302', '0.7277061'], ['C41008148', '0.6866145'], ['C153180895', '0.6659558'], ['C110384440', '0.65841407']], 'referenced_works': ['W1495267108', 'W1536680647', 'W1539790486', 'W1610060839', 'W1745334888', 'W1783315696', 'W1903029394', 'W1903370114', 'W1915250530', 'W1938976761', 'W1948751323', 'W1949049686', 'W1964772475', 'W1971410590', 'W1991367009', 'W1994259434', 'W2019904315', 'W2020278873', 'W2022508996', 'W2046382188', 'W2054279472', 'W2068730032', 'W2088049833', 'W2092985495', 'W2093112233', 'W2100588357', 'W2102605133', 'W2104408738', 'W2108548932', 'W2112796928', 'W2116877738', 'W2116988482', 'W2118984444', 'W2124592697', 'W2125215748', 'W2129259959', 'W2144794286', 'W2153820018', 'W2155893237', 'W2158427031', 'W2158842374', 'W2162915993', 'W2299400169', 'W2302255633', 'W2309415944', 'W2317851288', 'W2333621733', 'W2340897893', 'W2341555367', 'W2473131906', 'W2508741746', 'W2535516436', 'W2545985378', 'W2962872526', 'W2962891704', 'W2963108253', 'W2963935758', 'W3214386716', 'W4248635988'], 'abstract': 'In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or \'atrous convolution\', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.', 'counts_by_year': [[2022, 1653], [2021, 2581], [2020, 2155], [2019, 1720], [2018, 721], [2017, 172], [2016, 24]]}, {'id': 'W2327037637', 'doi': 'https://doi.org/10.1016/j.jcm.2016.02.012', 'title': 'A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for Reliability Research', 'type': 'journal-article', 'publication_date': '2016-06-01', 'host_venue': 'V67905653', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2130721857', ['I79559882']], ['A2435338696', ['I79559882']]], 'cited_by_count': 8980, 'concepts': [['C104709138', '0.8910669'], ['C43214815', '0.76637167'], ['C2780182762', '0.7448585'], ['C61863361', '0.53995585'], ['C71924100', '0.49432445']], 'referenced_works': ['W1967797264', 'W1971704380', 'W2001074566', 'W2011808739', 'W2012076689', 'W2015795623', 'W2064454539', 'W2067724039', 'W2083448810', 'W2096632664', 'W2108546303', 'W2136148671', 'W2136962717', 'W2141403362', 'W2142021158', 'W2150637635', 'W2152282295', 'W4234180827'], 'abstract': 'Intraclass correlation coefficient (ICC) is a widely used reliability index in test-retest, intrarater, and interrater reliability analyses. This article introduces the basic concept of ICC in the content of reliability analysis.There are 10 forms of ICCs. Because each form involves distinct assumptions in their calculation and will lead to different interpretations, researchers should explicitly specify the ICC form they used in their calculation. A thorough review of the research design is needed in selecting the appropriate form of ICC to evaluate reliability. The best practice of reporting ICC should include software information, "model," "type," and "definition" selections.When coming across an article that includes ICC, readers should first check whether information about the ICC form has been reported and if an appropriate ICC form was used. Based on the 95% confident interval of the ICC estimate, values less than 0.5, between 0.5 and 0.75, between 0.75 and 0.9, and greater than 0.90 are indicative of poor, moderate, good, and excellent reliability, respectively.This article provides a practical guideline for clinical researchers to choose the correct form of ICC and suggests the best practice of reporting ICC parameters in scientific publications. This article also gives readers an appreciation for what to look for when coming across ICC while reading an article.', 'counts_by_year': [[2022, 2374], [2021, 2753], [2020, 1989], [2019, 1211], [2018, 561], [2017, 80], [2016, 5]]}, {'id': 'W2963881378', 'doi': 'https://doi.org/10.1109/tpami.2016.2644615', 'title': 'SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation', 'type': 'journal-article', 'publication_date': '2017-12-01', 'host_venue': 'V199944782', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2070256948', ['I241749']], ['A2117656881', ['I241749']]], 'cited_by_count': 8326, 'concepts': [['C41008148', '0.7827778'], ['C154945302', '0.724313'], ['C2776401178', '0.6398804'], ['C118505674', '0.6353753'], ['C110384440', '0.6320176']], 'referenced_works': ['W114517082', 'W129501612', 'W181701252', 'W1610707153', 'W1677182931', 'W1745334888', 'W1903029394', 'W1905829557', 'W1923184257', 'W1938976761', 'W1948751323', 'W2022508996', 'W2027327099', 'W2033979122', 'W2037227137', 'W2047226863', 'W2067912884', 'W2083597815', 'W2091695913', 'W2097117768', 'W2100588357', 'W2110764733', 'W2117539524', 'W2119823327', 'W2124592697', 'W2125215748', 'W2139427956', 'W2150066425', 'W2153423793', 'W2155893237', 'W2171943915', 'W2293078015', 'W2340897893', 'W2536208356', 'W2546302380', 'W2963108253', 'W4244914727'], 'abstract': 'We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet.', 'counts_by_year': [[2022, 1625], [2021, 2258], [2020, 1934], [2019, 1488], [2018, 759], [2017, 202], [2016, 38]]}, {'id': 'W2900569176', 'doi': 'https://doi.org/10.1093/nar/gky1131', 'title': 'STRING v11: protein–protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets', 'type': 'journal-article', 'publication_date': '2019-01-08', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A1964902353', ['I12708293']], ['A2790062422', ['I12708293']], ['A2984214236', ['I12708293']], ['A2169747193', ['I2801134892']], ['A1441202580', ['I12708293']], ['A1424552278', ['I4210106474']], ['A1993364599', ['I12708293']], ['A1992942017', ['I2801134892']], ['A2612353087', ['I180670191']], ['A2068107358', ['I4210153543']], ['A2122923977', ['I2801134892']], ['A71748352', ['I12708293']]], 'cited_by_count': 7927, 'concepts': [['C152724338', '0.70312065'], ['C157486923', '0.6932156'], ['C86803240', '0.6400099'], ['C70721500', '0.6196621'], ['C41008148', '0.47805005']], 'referenced_works': ['W1966327575', 'W1973951655', 'W1983534198', 'W1989277387', 'W1990223774', 'W1995683190', 'W2043764521', 'W2052968292', 'W2057113047', 'W2064295826', 'W2079818352', 'W2084533385', 'W2095387858', 'W2096173332', 'W2098427571', 'W2105924489', 'W2107644337', 'W2107717979', 'W2110336366', 'W2118258530', 'W2120973253', 'W2125118217', 'W2128601422', 'W2130687290', 'W2137986227', 'W2145506897', 'W2153349010', 'W2154915765', 'W2154947819', 'W2155653061', 'W2162151166', 'W2163681976', 'W2164813401', 'W2223119478', 'W2223752691', 'W2224812643', 'W2293697267', 'W2294516783', 'W2313677581', 'W2465613542', 'W2509717357', 'W2525035644', 'W2537623931', 'W2559588208', 'W2580902218', 'W2604808360', 'W2624282460', 'W2733397209', 'W2762538579', 'W2768151985', 'W2785461156', 'W2792450944', 'W2793021023', 'W2794479004', 'W2809000676', 'W2915536738', 'W2951679282', 'W2953067954', 'W4210702584', 'W4239055561'], 'abstract': 'Proteins and their functional interactions form the backbone of the cellular machinery. Their connectivity network needs to be considered for the full understanding of biological phenomena, but the available information on protein-protein associations is incomplete and exhibits varying levels of annotation granularity and reliability. The STRING database aims to collect, score and integrate all publicly available sources of protein-protein interaction information, and to complement these with computational predictions. Its goal is to achieve a comprehensive and objective global network, including direct (physical) as well as indirect (functional) interactions. The latest version of STRING (11.0) more than doubles the number of organisms it covers, to 5090. The most important new feature is an option to upload entire, genome-wide datasets as input, allowing users to visualize subsets as interaction networks and to perform gene-set enrichment analysis on the entire input. For the enrichment analysis, STRING implements well-known classification systems such as Gene Ontology and KEGG, but also offers additional, new classification systems based on high-throughput text-mining as well as on a hierarchical clustering of the association network itself. The STRING resource is available online at https://string-db.org/.', 'counts_by_year': [[2022, 2414], [2021, 3212], [2020, 1932], [2019, 357], [2018, 5], [2017, 2]]}, {'id': 'W2963351448', 'doi': 'https://doi.org/10.1109/iccv.2017.324', 'title': 'Focal Loss for Dense Object Detection', 'type': 'proceedings-article', 'publication_date': '2017-08-07', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2618037599', ['I205783295']], ['A2642611022', ['I2252078561']], ['A2473549963', ['I2252078561']], ['A2164292938', ['I2252078561']], ['A1944499404', ['I2252078561']]], 'cited_by_count': 7781, 'concepts': [['C94915269', '0.8226304'], ['C41008148', '0.777915'], ['C2776151529', '0.574716'], ['C154945302', '0.56151795'], ['C95623464', '0.53615636']], 'referenced_works': ['W1536680647', 'W1903029394', 'W2031489346', 'W2036989445', 'W2056695679', 'W2068730032', 'W2088049833', 'W2102605133', 'W2147800946', 'W2159386181', 'W2161969291', 'W2164598857', 'W2194775991', 'W2288122362', 'W2963037989', 'W2963150697', 'W2963516811'], 'abstract': 'The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.', 'counts_by_year': [[2022, 1531], [2021, 2797], [2020, 2044], [2019, 1132], [2018, 258], [2017, 8], [2012, 1]]}, {'id': 'W3006659024', 'doi': 'https://doi.org/10.1016/s0140-6736(20)30460-8', 'title': 'The psychological impact of quarantine and how to reduce it: rapid review of the evidence', 'type': 'journal-article', 'publication_date': '2020-03-14', 'host_venue': 'V49861241', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2169932928', ['I183935753']], ['A2606856606', ['I183935753']], ['A2516095213', ['I183935753']], ['A2720130028', ['I183935753']], ['A2042757892', ['I183935753']], ['A2583750672', ['I183935753']], ['A2154241586', ['I183935753']]], 'cited_by_count': 7767, 'concepts': [['C2781402358', '0.8976271'], ['C2779302386', '0.5287751'], ['C71924100', '0.45916802'], ['C125370674', '0.4591389'], ['C2777589236', '0.45843488']], 'referenced_works': ['W1586661029', 'W1971131248', 'W1973132254', 'W1980500731', 'W1986605986', 'W1990329824', 'W1997995919', 'W2015590916', 'W2028368011', 'W2036410023', 'W2038105839', 'W2039371348', 'W2051329386', 'W2057197096', 'W2065757920', 'W2079929038', 'W2085395707', 'W2087193196', 'W2088378896', 'W2090482984', 'W2092611456', 'W2109525459', 'W2117801927', 'W2118137912', 'W2138367718', 'W2148806795', 'W2150483277', 'W2169337111', 'W2170812681', 'W2192952483', 'W2494500655', 'W2548791536', 'W2552325832', 'W2586817597', 'W2740368168', 'W2775241253', 'W2784999760', 'W2789955342', 'W3003379403'], 'abstract': 'The December, 2019 coronavirus disease outbreak has seen many countries ask people who have potentially come into contact with the infection to isolate themselves at home or in a dedicated quarantine facility. Decisions on how to apply quarantine should be based on the best available evidence. We did a Review of the psychological impact of quarantine using three electronic databases. Of 3166 papers found, 24 are included in this Review. Most reviewed studies reported negative psychological effects including post-traumatic stress symptoms, confusion, and anger. Stressors included longer quarantine duration, infection fears, frustration, boredom, inadequate supplies, inadequate information, financial loss, and stigma. Some researchers have suggested long-lasting effects. In situations where quarantine is deemed necessary, officials should quarantine individuals for no longer than required, provide clear rationale for quarantine and information about protocols, and ensure sufficient supplies are provided. Appeals to altruism by reminding the public about the benefits of quarantine to wider society can be favourable.', 'counts_by_year': [[2022, 1989], [2021, 3558], [2020, 2212], [2019, 3]]}, {'id': 'W2252795400', 'doi': 'https://doi.org/10.1103/physrevlett.116.061102', 'title': 'Observation of Gravitational Waves from a Binary Black Hole Merger', 'type': 'journal-article', 'publication_date': '2016-02-11', 'host_venue': 'V24807848', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A3178754429', ['I122411786']], ['A2931258442', ['I122411786']], ['A2898869343', ['I121820613']], ['A2984081854', ['I122411786']], ['A1989600986', ['I160013858']], ['A2102662472', ['I33213144']], ['A3130519413', []], ['A2209450666', ['I1294671590']], ['A2420842909', ['I131729948']], ['A2120816080', ['I122411786']], ['A2643273335', ['I149899117']], ['A3169239100', ['I149899117']], ['A2063771739', []], ['A2196045220', []], ['A2149021947', ['I63966007']], ['A2603756616', ['I80849659']], ['A2013867193', ['I160013858']], ['A3188632010', ['I59781447']], ['A2590089022', ['I11947397']], ['A3131221433', ['I114112103']], ['A3187237540', ['I160013858']], ['A1977554423', ['I118347636']], ['A2690326216', ['I122411786']], ['A2582221634', ['I43579087']], ['A2730442849', ['I122411786']], ['A2111483339', ['I33213144']], ['A3192504475', ['I122411786']], ['A1984389928', ['I368840534']], ['A3187769929', ['I142934699']], ['A2304354506', ['I102197404']], ['A2259678854', ['I19149307']], ['A2947746710', ['I116067653']], ['A2113463528', ['I43439940']], ['A1989490620', ['I159176309']], ['A2031868467', []], ['A2022335298', ['I160013858']], ['A1986532585', ['I149899117']], ['A354908839', ['I149899117']], ['A2624902964', ['I149899117']], ['A2776483237', ['I169173203']], ['A2980672605', []], ['A3037946954', ['I23732399']], ['A404213974', ['I160013858']], ['A2079318338', []], ['A2431706484', ['I70983195']], ['A3192154922', ['I122411786']], ['A2596778389', ['I7882870']], ['A2951079984', ['I122411786']], ['A2161639735', []], ['A2136366644', ['I160013858']], ['A2580889767', ['I7882870']], ['A2041338530', ['I63966007']], ['A2862977381', ['I169173203']], ['A3190637119', []], ['A2106758119', []], ['A2127911369', []], ['A2061148210', ['I78577930']], ['A2030123867', ['I97018004']], ['A2440901623', ['I160013858']], ['A2137004321', []], ['A3125242436', ['I149899117']], ['A2065817332', []], ['A2463697456', ['I160013858']], ['A3022873432', ['I149899117']], ['A2943352481', []], ['A2110592976', ['I4654613']], ['A2561527376', ['I7882870']], ['A3212413394', ['I7882870']], ['A3149347399', ['I122411786']], ['A2590148452', []], ['A3188724226', ['I149899117']], ['A2167365184', ['I79619799']], ['A416185457', ['I160013858']], ['A2902171395', []], ['A3187809538', []], ['A2294285147', ['I70983195']], ['A2222729917', []], ['A2305878762', ['I19880235']], ['A2592016208', ['I122411786']], ['A2332769670', []], ['A2916176969', ['I868834043']], ['A252456976', ['I149899117']], ['A1685694851', ['I63966007']], ['A2242844166', []], ['A1977286476', []], ['A2784445771', ['I70983195']], ['A2191991431', ['I102197404']], ['A2146764893', ['I122411786']], ['A2573281761', ['I177877127']], ['A2110062665', ['I177877127']], ['A2479864622', []], ['A3036941851', ['I145872427']], ['A2471586637', ['I149899117']], ['A1652255037', ['I63966007']], ['A2510367362', ['I1294671590']], ['A3187225073', ['I1294671590']], ['A2180469661', ['I149899117']], ['A2429888154', ['I149899117']], ['A411019632', ['I106118109']], ['A2542447142', ['I79619799']], ['A2656910661', []], ['A401219682', ['I1294671590']], ['A2340744523', []], ['A2245142053', ['I122411786']], ['A3123957502', ['I160013858']], ['A2752961157', ['I72951846']], ['A3188004614', ['I169173203']], ['A2161572305', []], ['A3187368144', ['I160013858']], ['A2133056318', ['I43579087']], ['A2579247217', ['I19880235']], ['A3213734362', ['I160013858']], ['A2590013777', ['I181233156']], ['A2762264107', []], ['A3213443375', ['I1294671590']], ['A2987127593', ['I149899117']], ['A1664647747', ['I102197404']], ['A2266672070', ['I43579087']], ['A2461687777', ['I122411786']], ['A2271853632', ['I70983195']], ['A2828582320', ['I79619799']], ['A2262935243', ['I63966007']], ['A2188286533', ['I121820613']], ['A1920643890', ['I63966007']], ['A2049455156', ['I4654613']], ['A1908833969', ['I865915315']], ['A1949674659', ['I66946132']], ['A2955044138', ['I1294671590']], ['A2989870371', ['I169173203']], ['A2047849238', ['I97018004']], ['A2816657102', ['I149899117']], ['A1985904051', ['I130701444']], ['A2595927603', ['I100532134']], ['A1971896208', ['I122411786']], ['A3011224335', ['I50441567']], ['A3161920399', ['I122411786']], ['A2284239319', ['I71267560']], ['A2107701862', ['I1306266525']], ['A2946402245', []], ['A2119394733', ['I99065089']], ['A3208452685', ['I149899117']], ['A2633311535', ['I169173203']], ['A2005944322', []], ['A2958567712', ['I12315562']], ['A2253635596', ['I102197404']], ['A3183896882', ['I116067653']], ['A2853512116', ['I43579087']], ['A2428212893', ['I368840534']], ['A2215987423', ['I102197404']], ['A2134872530', []], ['A3207900275', ['I160013858']], ['A2593215993', ['I122411786']], ['A2267267924', ['I160013858']], ['A2963834723', ['I160013858']], ['A2993381879', ['I116067653']], ['A2058101756', ['I122411786']], ['A361432891', ['I122411786']], ['A2552948661', ['I130769515']], ['A3081522446', ['I7882870']], ['A2293492264', ['I25846049']], ['A2158094367', ['I153230381']], ['A2355490837', ['I169173203']], ['A3187385361', ['I40347166']], ['A3175291849', ['I122411786']], ['A2096700839', ['I25846049']], ['A1966111217', ['I160013858']], ['A2026320856', []], ['A2439091654', ['I878022262']], ['A2267524564', ['I66946132']], ['A1996499331', ['I118347636']], ['A2165216277', ['I188497080']], ['A2302273048', ['I177877127']], ['A2133767007', []], ['A2252018950', ['I177877127']], ['A2471669862', ['I33213144']], ['A2113263115', []], ['A2104149412', ['I130701444']], ['A3192959517', ['I1294671590']], ['A2793250799', ['I116067653']], ['A3212972421', []], ['A2050847768', ['I861853513']], ['A1993090682', ['I908309457']], ['A2946504086', ['I158011677']], ['A2091580292', ['I80849659']], ['A2509097593', ['I861853513']], ['A2345183096', ['I160013858']], ['A2169864109', []], ['A2466503393', ['I121820613']], ['A2343198265', ['I23732399']], ['A2104489708', ['I12315562']], ['A1997433525', []], ['A2984511297', ['I80849659']], ['A2794293094', ['I188497080']], ['A3088771138', ['I111979921']], ['A3182150140', ['I1294671590']], ['A2973073773', ['I78577930']], ['A3204824529', ['I122411786']], ['A2252855434', ['I130701444']], ['A2008373628', ['I177877127']], ['A2017395160', []], ['A2578295048', ['I122411786']], ['A2868493386', ['I12315562']], ['A2574034277', ['I7882870']], ['A2277296262', ['I43579087']], ['A3129173276', ['I118430337']], ['A2324162412', ['I121820613']], ['A2976007553', ['I2800403580']], ['A361377382', ['I79619799']], ['A2144671710', ['I7882870']], ['A2577600451', ['I7882870']], ['A2002723579', []], ['A2044934989', ['I149899117']], ['A3197011930', ['I7882870']], ['A2042635623', ['I160013858']], ['A1979152311', []], ['A2254161542', ['I165779595']], ['A2516309085', ['I33213144']], ['A2146738652', []], ['A2153454514', []], ['A1973370420', ['I118430337']], ['A2168664669', ['I102197404']], ['A3083064634', ['I7882870']], ['A2015763309', ['I91136226']], ['A2141759182', []], ['A2828709917', ['I70983195']], ['A1988922277', ['I97018004']], ['A2462136909', []], ['A3010691662', ['I100532134']], ['A2304156027', ['I71267560']], ['A1418452008', []], ['A2116577483', ['I79619799']], ['A2975633334', []], ['A2569824804', ['I149899117']], ['A2121531058', ['I1294671590']], ['A2789344678', ['I122411786']], ['A2595328712', []], ['A2141040907', ['I71267560']], ['A2880828452', ['I160013858']], ['A678187721', ['I59781447']], ['A2950279434', ['I118430337']], ['A3212049868', ['I160013858']], ['A2267448327', ['I861853513']], ['A3011099301', ['I160013858']], ['A2265221633', ['I861853513']], ['A3212506750', ['I149899117']], ['A2942996779', ['I160013858']], ['A1852580484', ['I166088655']], ['A2797796199', ['I100532134']], ['A2147618017', ['I63966007']], ['A2598058840', ['I368840534']], ['A1744413271', ['I149899117']], ['A2424897147', ['I7882870']], ['A2305700984', ['I43579087']], ['A2241179355', ['I160013858']], ['A3106418577', ['I122411786']], ['A2598542268', []], ['A2230265955', ['I99065089']], ['A2761346687', ['I1294671590']], ['A2107317751', []], ['A2317069749', ['I91136226']], ['A2565748458', ['I188497080']], ['A2063698960', []], ['A2778232883', ['I149899117']], ['A2056008936', ['I122411786']], ['A3119048600', ['I33213144']], ['A3016124112', ['I33213144']], ['A2309452646', ['I122411786']], ['A2467688294', ['I63966007']], ['A2601060555', ['I122411786']], ['A2917506697', ['I63966007']], ['A2948178927', []], ['A2317536582', ['I130769515']], ['A1806296415', ['I78577930']], ['A2914472670', ['I116067653']], ['A2592053980', ['I70983195']], ['A2307450509', ['I79510175']], ['A2147865693', ['I99065089']], ['A2136363211', ['I177877127']], ['A3207155494', ['I160013858']], ['A2641626098', ['I40347166']], ['A2641626098', ['I79619799']], ['A2037446865', ['I166088655']], ['A1999419386', ['I79510175']], ['A2773897234', ['I149899117']], ['A2149221475', ['I97018004']], ['A2136043414', ['I33213144']], ['A1833794549', ['I160013858']], ['A3175940303', ['I80849659']], ['A1998320771', []], ['A1965779292', ['I160013858']], ['A2171646649', ['I130769515']], ['A3066893051', []], ['A3103283984', ['I169173203']], ['A3152383050', ['I70983195']], ['A2792463995', []], ['A2681436121', ['I7882870']], ['A2552322958', []], ['A2591496726', ['I1294671590']], ['A2925099713', ['I102197404']], ['A2139140847', ['I861853513']], ['A420234118', ['I160013858']], ['A2223968577', ['I149899117']], ['A1924538289', ['I106118109']], ['A2026851826', ['I79619799']], ['A2133367798', ['I181233156']], ['A2580419259', ['I102197404']], ['A2957692551', ['I149899117']], ['A2112263148', ['I63966007']], ['A2893006435', []], ['A3173498059', ['I33213144']], ['A438297632', []], ['A3000397202', ['I368840534']], ['A2340803116', ['I98677209']], ['A2608772347', ['I160013858']], ['A2215402173', ['I59781447']], ['A2848021311', ['I71267560']], ['A2146705618', ['I169173203']], ['A3214056200', []], ['A2616884440', ['I1306266525']], ['A3211664842', ['I160013858']], ['A2762625586', ['I1294671590']], ['A2953778706', []], ['A3184262436', ['I160013858']], ['A3196982953', []], ['A2436983654', ['I227486990']], ['A2472445730', ['I1294671590']], ['A2224979328', ['I11947397']], ['A2196472791', ['I11947397']], ['A3210093540', ['I145872427']], ['A3212527609', []], ['A1637814842', []], ['A1931051767', ['I160013858']], ['A2159304583', ['I84475105']], ['A2265051445', ['I7882870']], ['A2317186014', ['I33213144']], ['A2588028427', ['I27837315']], ['A2120371615', ['I33213144']], ['A1598052457', ['I106118109']], ['A2128477673', ['I121820613']], ['A3181227588', ['I160013858']], ['A2021276475', ['I11947397']], ['A2524013334', ['I7882870']], ['A1968023185', ['I19880235']], ['A2116344843', ['I122411786']], ['A2430134528', []], ['A3193155114', ['I1294671590']], ['A2616057256', ['I7882870']], ['A3184238689', ['I66946132']], ['A1859510812', ['I100532134']], ['A2137278808', ['I7882870']], ['A2951105902', ['I63966007']], ['A2765051707', []], ['A3132460754', ['I160013858']], ['A2486920711', ['I79619799']], ['A2473293121', ['I1286704778']], ['A2134513389', ['I145872427']], ['A3018296462', ['I149899117']], ['A2895613899', ['I149899117']], ['A2975350928', ['I160013858']], ['A2488799353', ['I99065089']], ['A2580259573', ['I59781447']], ['A3037040773', []], ['A2082107602', ['I122411786']], ['A2018645323', ['I122411786']], ['A2592154046', ['I27837315']], ['A2579983090', ['I142934699']], ['A2302614570', ['I72951846']], ['A2946415084', ['I122411786']], ['A2420107187', ['I7882870']], ['A3164587974', ['I11947397']], ['A2806435661', ['I149899117']], ['A2275643125', []], ['A2776268936', ['I130769515']], ['A2332457619', ['I79510175']], ['A2765542636', []], ['A1977732146', ['I121820613']], ['A2157477235', ['I160013858']], ['A2503627946', ['I181401687']], ['A2042136083', ['I149899117']], ['A2190268344', ['I7882870']], ['A2035800947', ['I33213144']], ['A2973622029', ['I79619799']], ['A1836536504', ['I7882870']], ['A2135192745', ['I155173764']], ['A2056194163', ['I122411786']], ['A2236585722', []], ['A1980042119', []], ['A2016231509', ['I149899117']], ['A2344571799', ['I1294671590']], ['A3116516404', ['I102197404']], ['A2151255333', []], ['A2303437377', ['I7882870']], ['A2083397653', ['I7882870']], ['A2287281526', ['I7882870']], ['A396076215', ['I122411786']], ['A2042955919', []], ['A2046589782', ['I7882870']], ['A2138917923', ['I24603500']], ['A2324446455', ['I122411786']], ['A2013597997', ['I100532134']], ['A1947667200', ['I5681781']], ['A2517767075', []], ['A2067057196', ['I40347166']], ['A2149005230', ['I79510175']], ['A2582736864', ['I5681781']], ['A1920053454', ['I7882870']], ['A2138217739', ['I7882870']], ['A2595902692', ['I177877127']], ['A2966114616', ['I7882870']], ['A2628746823', ['I25846049']], ['A2771330512', ['I12097938']], ['A2761304767', ['I102197404']], ['A2846634780', ['I84475105']], ['A2053798579', ['I50441567']], ['A2580008531', ['I7882870']], ['A1994649786', []], ['A374118689', ['I130769515']], ['A2882041359', ['I149899117']], ['A2157686660', []], ['A1581467037', ['I12315562']], ['A2334165722', ['I7882870']], ['A2273340091', []], ['A2763511771', ['I122411786']], ['A2157559798', ['I142934699']], ['A3204584411', ['I63966007']], ['A1997196459', ['I11947397']], ['A2651934103', []], ['A2432416562', ['I122411786']], ['A2260744060', []], ['A2112631292', ['I878022262']], ['A2112891699', ['I130701444']], ['A2075323785', ['I108511484']], ['A2096689809', ['I181647926']], ['A371636951', ['I50441567']], ['A2893095224', ['I121820613']], ['A470399586', ['I11947397']], ['A2129196187', ['I43439940']], ['A2468505319', ['I7882870']], ['A1995530748', []], ['A2137027242', ['I177877127']], ['A3211500195', ['I122996671']], ['A2474110466', ['I79510175']], ['A2998692115', ['I111979921']], ['A2567819330', ['I368840534']], ['A2150621295', ['I878022262']], ['A2974510088', ['I122411786']], ['A2957347368', ['I181233156']], ['A2763762051', []], ['A2951704019', ['I63966007']], ['A3014997762', []], ['A2169656381', []], ['A2718779615', ['I177877127']], ['A2946076134', []], ['A3021219501', []], ['A2144846099', ['I1294671590']], ['A2271062393', []], ['A2375511350', ['I50441567']], ['A2596826241', ['I70983195']], ['A707522805', ['I122411786']], ['A2332964699', ['I91136226']], ['A2466450850', ['I149899117']], ['A3213303994', ['I118430337']], ['A2337736266', ['I149899117']], ['A2150745972', ['I19880235']], ['A2963947177', ['I160013858']], ['A2949655747', ['I79510175']], ['A2117989385', []], ['A2973371795', ['I1313323035']], ['A2255034731', []], ['A2943678237', ['I878022262']], ['A2992126523', ['I4921948']], ['A2896102104', ['I4575257']], ['A2948020601', ['I878022262']], ['A2600239339', ['I97018004']], ['A3037258224', ['I4921948']], ['A2158194427', ['I5681781']], ['A2980724654', []], ['A1977234752', []], ['A2778226645', []], ['A2257335508', ['I159176309']], ['A2923035099', ['I33213144']], ['A2779389584', ['I149899117']], ['A3212701660', ['I121820613']], ['A3190285517', []], ['A2114391012', ['I122411786']], ['A2431304288', ['I63966007']], ['A2080575653', ['I43579087']], ['A2002141085', ['I159176309']], ['A2586345805', ['I122411786']], ['A1685051018', ['I4654613']], ['A2590008495', ['I122411786']], ['A2838408147', ['I149899117']], ['A2198090973', ['I149899117']], ['A2426512568', []], ['A2166327144', []], ['A2780020188', ['I149899117']], ['A2234325469', []], ['A2949671386', ['I7882870']], ['A2254396243', ['I25846049']], ['A3175777026', []], ['A3198241234', ['I149899117']], ['A2834120332', ['I70983195']], ['A2605199855', []], ['A2791156957', ['I155173764']], ['A2306815261', ['I97018004']], ['A1973573521', ['I56590836']], ['A2408956840', ['I122411786']], ['A1826541738', ['I130701444']], ['A2021249184', ['I861853513']], ['A2549372245', ['I7882870']], ['A2511818535', ['I99065089']], ['A2809015571', ['I4921948']], ['A2990015612', ['I4575257']], ['A2134863343', ['I139264467']], ['A3001018922', ['I7882870']], ['A3121789463', ['I70983195']], ['A3044922546', ['I160013858']], ['A3102563777', ['I149899117']], ['A2546432156', ['I102197404']], ['A3131819562', ['I1294671590']], ['A2795157789', ['I56590836']], ['A2484846178', []], ['A2884995774', ['I122411786']], ['A2223239043', ['I63966007']], ['A2682071947', ['I82495205']], ['A425630834', ['I181647926']], ['A2471465842', ['I7882870']], ['A2318381896', ['I24603500']], ['A2027578991', ['I79510175']], ['A2626309292', ['I70983195']], ['A2983987425', ['I160013858']], ['A2953771372', ['I1294671590']], ['A1978127905', []], ['A2031087335', ['I160013858']], ['A3101521172', []], ['A2615368805', ['I155173764']], ['A2547743495', ['I142934699']], ['A2849484998', []], ['A2826849062', ['I149899117']], ['A2762249900', ['I188497080']], ['A2984096928', ['I63966007']], ['A2131331663', ['I177877127']], ['A2591453750', ['I97018004']], ['A357350526', ['I149899117']], ['A2063177140', ['I63966007']], ['A2442311525', ['I121820613']], ['A1499064879', ['I70983195']], ['A3164034524', ['I72951846']], ['A394073259', ['I122411786']], ['A1954864400', ['I160013858']], ['A3190282054', ['I1294671590']], ['A2215735041', ['I116067653']], ['A2764146561', ['I1294671590']], ['A2426567721', ['I79619799']], ['A2530083674', ['I2800403580']], ['A3170207227', ['I7882870']], ['A2973259274', ['I118347636']], ['A2440179721', ['I43579087']], ['A2209475002', []], ['A2072248821', []], ['A2788508077', ['I1294671590']], ['A3217471248', ['I78577930']], ['A3165376653', ['I78577930']], ['A2770679456', ['I97018004']], ['A379122203', ['I122411786']], ['A2888361824', ['I160013858']], ['A201038113', ['I1294671590']], ['A2102079205', ['I7882870']], ['A2309715647', ['I33213144']], ['A2163983367', ['I122411786']], ['A2576349919', ['I122411786']], ['A2986016436', ['I63966007']], ['A3188738125', ['I1294671590']], ['A3172795602', ['I70983195']], ['A2263492593', ['I7882870']], ['A1663768879', ['I63966007']], ['A2203868986', ['I78577930']], ['A2469800559', ['I63966007']], ['A3183609872', ['I72951846']], ['A2032351783', ['I149899117']], ['A2465526937', []], ['A1973105240', ['I118347636']], ['A2125448681', []], ['A2949277257', ['I31746571']], ['A2580450533', ['I122411786']], ['A2121893416', ['I122411786']], ['A2607098049', ['I118347636']], ['A2650871170', ['I12097938']], ['A2976632078', ['I130769515']], ['A2975777987', ['I149899117']], ['A406107958', []], ['A2001351290', ['I165779595']], ['A2218083907', []], ['A2826050039', ['I149899117']], ['A2948494050', ['I43579087']], ['A2270695881', []], ['A3170098099', ['I1294671590']], ['A2574097560', ['I122411786']], ['A2468204009', ['I7882870']], ['A2428639699', ['I130769515']], ['A2304030444', ['I2800403580']], ['A356943668', ['I861853513']], ['A2798531098', ['I79619799']], ['A2761792207', ['I100532134']], ['A2910186728', ['I79619799']], ['A2070676417', ['I16285277']], ['A3088851510', ['I71267560']], ['A2634403777', ['I63966007']], ['A2780469136', ['I23732399']], ['A2947192273', ['I160013858']], ['A2842074435', ['I149899117']], ['A2853966690', ['I1303069342']], ['A2420206282', ['I11947397']], ['A2118856850', ['I59781447']], ['A2136639717', ['I19880235']], ['A2899302788', ['I33213144']], ['A2123327683', ['I63966007']], ['A2973274746', ['I160013858']], ['A2099520814', []], ['A3211526149', ['I63966007']], ['A3133318729', ['I160013858']], ['A2481071036', ['I166088655']], ['A3014210212', ['I241749']], ['A2012427454', []], ['A2784806291', []], ['A2082008669', ['I118430337']], ['A2148346173', ['I149899117']], ['A2952436053', ['I1294671590']], ['A2893928364', ['I79619799']], ['A2614920720', ['I33213144']], ['A2108725442', ['I33213144']], ['A2763198395', ['I79510175']], ['A3185592983', ['I11947397']], ['A2595464485', ['I43579087']], ['A3135606165', ['I118430337']], ['A2266688588', ['I59781447']], ['A2919488214', []], ['A1982961642', ['I5681781']], ['A2340083009', ['I78577930']], ['A2818447739', ['I7882870']], ['A2469540581', ['I33213144']], ['A2957965052', ['I116067653']], ['A2005208586', ['I861853513']], ['A2599763636', ['I127439422']], ['A1876941671', ['I33213144']], ['A1989082292', ['I24603500']], ['A2439563356', ['I145872427']], ['A2946507550', ['I160013858']], ['A2547973123', ['I27837315']], ['A2162966607', ['I7882870']], ['A2171044953', ['I118347636']], ['A2781815556', ['I149899117']], ['A2272353480', ['I145872427']], ['A2023299128', ['I149899117']], ['A2073685276', []], ['A2097564241', []], ['A3121491794', ['I118430337']], ['A2871160851', ['I70983195']], ['A2272207797', []], ['A1973156962', ['I43579087']], ['A2553195835', ['I1286704778']], ['A2469545255', ['I63966007']], ['A2849436941', ['I138943879']], ['A2305605118', []], ['A3121867565', []], ['A3101351426', ['I79510175']], ['A2975609283', ['I50441567']], ['A2985611607', ['I149899117']], ['A2140161256', []], ['A2579244377', []], ['A2108716544', ['I155173764']], ['A2794032221', ['I122411786']], ['A2579844043', ['I5681781']], ['A2579214900', ['I33213144']], ['A2056767313', []], ['A2104041286', ['I12315562']], ['A2976108310', ['I122996671']], ['A2780831007', []], ['A2258687281', ['I181233156']], ['A2055008422', ['I1313323035']], ['A3165668364', ['I160013858']], ['A2840184033', ['I159176309']], ['A2127597866', ['I25846049']], ['A2798661100', ['I66946132']], ['A2521298114', ['I111979921']], ['A1967288566', ['I79510175']], ['A2427216197', []], ['A2024848060', []], ['A2316050742', []], ['A2766819460', ['I149899117']], ['A2777411521', ['I97018004']], ['A3039043906', []], ['A2253053690', ['I7882870']], ['A388307725', []], ['A3168259381', ['I160013858']], ['A3173202239', ['I160013858']], ['A2964026637', ['I160013858']], ['A2605182216', ['I97018004']], ['A2269450469', ['I7882870']], ['A2587175082', ['I122411786']], ['A1975865521', ['I100532134']], ['A2084820644', ['I70983195']], ['A2142834634', []], ['A2070179702', ['I143104139']], ['A2954703215', ['I122411786']], ['A2155309926', []], ['A2833209380', ['I7882870']], ['A2272538144', ['I861853513']], ['A3187223402', ['I1294671590']], ['A2994390685', ['I149899117']], ['A415854352', ['I160013858']], ['A2943874771', ['I160013858']], ['A403887412', []], ['A3189702922', ['I100532134']], ['A2091317358', ['I160013858']], ['A2147448641', ['I7882870']], ['A2835683065', ['I149899117']], ['A395586697', ['I160013858']], ['A1654852912', []], ['A2975120789', ['I149899117']], ['A2140290691', ['I7882870']], ['A2554014841', ['I59781447']], ['A1915066629', ['I79510175']], ['A1283032547', ['I56590836']], ['A3029355136', ['I2800403580']], ['A2304879075', ['I122411786']], ['A2075222951', []], ['A2575409521', ['I160013858']], ['A3103146582', ['I149899117']], ['A1975441365', ['I149899117']], ['A2985418656', ['I160013858']], ['A1839436355', ['I19880235']], ['A3100080145', ['I149899117']], ['A1856306342', ['I160013858']], ['A2469000994', ['I160013858']], ['A2976081679', ['I149899117']], ['A2983684962', ['I43579087']], ['A2920643673', ['I177877127']], ['A3197230867', ['I118430337']], ['A2118448810', ['I122411786']], ['A1963716262', ['I181233156']], ['A2921152201', []], ['A2614875211', ['I118347636']], ['A355872337', []], ['A2306114148', ['I106118109']], ['A2134809853', []], ['A3003397437', ['I118430337']], ['A2052564495', []], ['A362084126', ['I861853513']], ['A2918218855', ['I149899117']], ['A1884058422', ['I160013858']], ['A3214583590', ['I116067653']], ['A3204309051', ['I142934699']], ['A2118507658', []], ['A3207885158', ['I1294671590']], ['A2098395136', ['I160013858']], ['A2462241501', ['I868834043']], ['A2089513116', ['I33213144']], ['A2077388583', ['I16285277']], ['A2223535313', ['I70983195']], ['A2461163460', ['I861853513']], ['A2941682954', ['I27837315']], ['A2805807800', ['I7882870']], ['A2108733834', ['I7882870']], ['A3188567202', ['I102197404']], ['A2985097369', ['I160013858']], ['A2878446963', ['I1294671590']], ['A2089289871', ['I122411786']], ['A2100500661', ['I181233156']], ['A2517624868', ['I118430337']], ['A2100129471', ['I160013858']], ['A2563662130', ['I16285277']], ['A2777681260', []], ['A2792340923', ['I46305939']], ['A2469241872', ['I7882870']], ['A2497839463', ['I149899117']], ['A354099228', []], ['A2038817311', []], ['A2773400098', ['I122411786']], ['A372378086', []], ['A1454296234', ['I43579087']], ['A2271697176', []], ['A2957078916', ['I122996671']], ['A3134223752', ['I149899117']], ['A3135876474', ['I127439422']], ['A2030525815', ['I56590836']], ['A2143863161', ['I111979921']], ['A2596842744', ['I122411786']], ['A2793876488', []], ['A2268325361', ['I111979921']], ['A2115187843', ['I122411786']], ['A2924289912', ['I27837315']], ['A3094318201', ['I100532134']], ['A2917981810', ['I79510175']], ['A1994600236', ['I70983195']], ['A2923680840', ['I27837315']], ['A2528509270', []], ['A2075208945', []], ['A2256275616', ['I181233156']], ['A2131426749', ['I149899117']], ['A2170197708', ['I149899117']], ['A2263080307', ['I122411786']], ['A2132247024', ['I159176309']], ['A2262095931', ['I181233156']], ['A1928098798', ['I159176309']], ['A2780652161', ['I149899117']], ['A2777934897', []], ['A2170966916', ['I79510175']], ['A2473231171', ['I7882870']], ['A3002678630', ['I118347636']], ['A2304026211', []], ['A2868184314', ['I64295750']], ['A2837016479', []], ['A1908375455', ['I116067653']], ['A2954896656', ['I1313323035']], ['A2550237247', ['I142934699']], ['A2267468254', ['I145872427']], ['A2004046723', []], ['A2311189936', ['I118347636']], ['A2610678376', []], ['A2499132062', ['I145872427']], ['A2169968594', ['I111979921']], ['A2866031454', ['I149899117']], ['A2526710642', ['I122411786']], ['A2576581198', ['I97018004']], ['A3132031979', ['I66946132']], ['A2143397848', ['I43579087']], ['A2791879238', ['I63966007']], ['A2997064189', ['I130701444']], ['A3147198915', ['I130701444']], ['A2590111591', ['I43579087']], ['A2683727804', []], ['A2575681443', ['I80849659']], ['A2817604252', ['I149899117']], ['A2287315714', ['I122411786']], ['A3042780887', ['I1306266525']], ['A2123349806', ['I149899117']], ['A3205612424', ['I121820613']], ['A3188774669', ['I160013858']], ['A3165812064', ['I50441567']], ['A2059989788', ['I118347636']], ['A3159832477', ['I142934699']], ['A2772774340', ['I122411786']], ['A3021249316', ['I122411786']], ['A2973327313', ['I122411786']], ['A2311932885', []], ['A2038388809', ['I7882870']], ['A2924118733', ['I160013858']], ['A2029544047', ['I59781447']], ['A2225816550', []], ['A2597221171', ['I78577930']], ['A2976094966', ['I149899117']], ['A1541186076', ['I7882870']], ['A2976816135', ['I7882870']], ['A2778070955', []], ['A2428665566', ['I43579087']], ['A2591107197', ['I79619799']], ['A3083537935', ['I118430337']], ['A2580845160', ['I7882870']], ['A2762826624', ['I100532134']], ['A400490453', ['I160013858']], ['A2202548565', ['I188497080']], ['A2065534013', ['I19880235']], ['A3211902094', ['I1303069342']], ['A2017918644', []], ['A2851724123', ['I102298084']], ['A2789987291', ['I165779595']], ['A2121561893', ['I79510175']], ['A2042879894', []], ['A2762642778', ['I84475105']], ['A3191806085', ['I169173203']], ['A2816315007', ['I181233156']], ['A2040393559', ['I33213144']], ['A1983407470', ['I227486990']], ['A2305489452', ['I149899117']], ['A2995618979', ['I149899117']], ['A3207592459', ['I122411786']], ['A2975623048', ['I149899117']], ['A3214199245', ['I122411786']], ['A2164075407', ['I79619799']], ['A2395485183', []], ['A2781019202', []], ['A2774501621', []], ['A1417201204', ['I122411786']], ['A2109240833', ['I56590836']], ['A3107409158', ['I160013858']], ['A2953014576', ['I79510175']], ['A443913643', ['I181647926']], ['A2594138872', ['I91136226']], ['A2988752235', ['I160013858']], ['A2129842365', ['I118430337']], ['A3139912005', ['I122411786']], ['A2256758043', ['I79619799']], ['A2478539643', ['I160013858']], ['A2087966639', []], ['A2209246135', ['I368840534']], ['A3212781388', ['I160013858']], ['A2271925699', ['I102064193']], ['A3128012180', ['I63966007']], ['A3211986598', ['I1294671590']], ['A2853962417', ['I118430337']], ['A735303264', ['I60060512']], ['A355478588', ['I11947397']], ['A3212031215', ['I43579087']], ['A3118211531', ['I70983195']], ['A2087571514', []], ['A420791329', ['I122411786']], ['A2310694307', ['I118430337']], ['A2072883853', ['I122411786']], ['A2358827619', []], ['A2159263755', []], ['A2139670350', ['I865915315']], ['A2164731926', []], ['A2248622543', ['I70983195']], ['A2258435089', []], ['A2122667816', []], ['A2013806816', ['I7882870']], ['A3198178892', ['I160013858']], ['A2589417994', ['I122411786']], ['A1976376432', []], ['A1972270540', ['I63966007']], ['A2232612787', ['I79619799']], ['A2874945874', ['I160013858']], ['A2617753731', ['I79619799']], ['A2422158808', ['I5681781']], ['A2049394008', ['I201448701']], ['A3190535392', ['I1294671590']], ['A2975438246', ['I160013858']], ['A2962926893', ['I160013858']], ['A3206956357', ['I79619799']], ['A2779629177', ['I868834043']], ['A3198816670', ['I1294671590']], ['A2794769002', ['I63966007']], ['A2829398367', ['I70983195']], ['A1990234647', ['I160013858']], ['A1968203033', []], ['A2596783131', ['I33213144']], ['A1825043761', ['I79619799']], ['A2554150283', ['I19880235']], ['A2190794208', ['I118347636']], ['A2869745026', ['I166972335']], ['A2163347761', ['I166972335']], ['A2131286536', ['I63966007']], ['A3198547128', ['I121820613']], ['A2563614108', ['I122411786']], ['A2997733971', ['I149899117']], ['A2763702278', ['I160013858']], ['A2306205450', ['I79619799']], ['A3160097155', ['I79619799']], ['A2154798004', ['I99065089']], ['A3208430161', ['I177877127']], ['A2112136592', ['I7882870']], ['A2195821694', ['I118347636']], ['A2163404454', []], ['A2806399133', ['I1294671590']], ['A2763069523', []], ['A2763611573', ['I114983960']], ['A2779871521', ['I149899117']], ['A3197332458', ['I122411786']], ['A2405613006', ['I63966007']], ['A2121270612', []], ['A2993470130', ['I177877127']], ['A2780408220', ['I149899117']], ['A2472787669', ['I149899117']], ['A2014299767', ['I149899117']], ['A2097961538', ['I155173764']], ['A2561376831', ['I122411786']], ['A2474202316', ['I91136226']], ['A2809141130', ['I149899117']], ['A2839317138', []], ['A2578197286', ['I122411786']], ['A2527206488', ['I33213144']], ['A2175238087', ['I122411786']], ['A2942710400', ['I79510175']], ['A2346180621', ['I60205797']], ['A2893230900', []], ['A2697219847', []], ['A2819723302', ['I149899117']], ['A2157982768', ['I149899117']], ['A2590603314', ['I122411786']], ['A2581410174', ['I43579087']], ['A2090992818', []], ['A1914572088', ['I7882870']], ['A2122129026', []], ['A2588464670', ['I7882870']], ['A2328863242', []], ['A2467072921', ['I111979921']], ['A621246381', []], ['A2250629068', ['I63966007']], ['A2810030146', ['I122411786']], ['A2161109715', ['I66946132']], ['A2605415677', ['I118347636']], ['A2563109565', ['I63966007']], ['A2762480273', ['I1294671590']], ['A2076993918', []], ['A2089872507', ['I160013858']], ['A2211178064', ['I84475105']], ['A2029428687', ['I160013858']], ['A3103749501', ['I111979921']], ['A3025601730', ['I63966007']], ['A3125218237', ['I122411786']], ['A2776586902', ['I16285277']], ['A2560496473', ['I155173764']], ['A2773266029', ['I177877127']], ['A2610356264', ['I111979921']], ['A2419807468', ['I111979921']], ['A2852550122', ['I177877127']], ['A2622566261', ['I63966007']], ['A2115911080', ['I24603500']], ['A2078501612', ['I122411786']]], 'cited_by_count': 7532, 'concepts': [['C121332964', '0.9353533'], ['C190330329', '0.8456547'], ['C50341732', '0.7136749'], ['C44870925', '0.6502539'], ['C107195408', '0.5751232']], 'referenced_works': ['W1134494461', 'W1930959531', 'W1965516292', 'W1966442216', 'W1969362644', 'W1971068715', 'W1978916504', 'W1983439542', 'W1989170879', 'W1989374967', 'W1990660834', 'W1990978371', 'W1994870116', 'W1995114597', 'W1996004845', 'W1997367763', 'W2004614917', 'W2007877891', 'W2014804681', 'W2014958866', 'W2018289223', 'W2019709661', 'W2034066066', 'W2036334887', 'W2037245193', 'W2037306691', 'W2037939815', 'W2039911906', 'W2041689890', 'W2044542193', 'W2048638122', 'W2049007728', 'W2050092976', 'W2051449498', 'W2051539854', 'W2051754610', 'W2052074642', 'W2052438420', 'W2055524364', 'W2055664909', 'W2060745302', 'W2061930234', 'W2069557347', 'W2070717869', 'W2070806579', 'W2071126256', 'W2073862476', 'W2074237027', 'W2077003551', 'W2084808542', 'W2095105526', 'W2097082581', 'W2097501174', 'W2102006113', 'W2104863352', 'W2108294549', 'W2115305599', 'W2115462982', 'W2116108818', 'W2121098569', 'W2127244041', 'W2147286576', 'W2148994792', 'W2153135464', 'W2158293236', 'W2158710569', 'W2172269227', 'W2214545089', 'W2245922284', 'W2262661619', 'W2263465265', 'W2272494806', 'W2565472021', 'W2764789329', 'W2951135371', 'W3102004940', 'W3103070168', 'W3103271996', 'W3105158216', 'W3105874581', 'W3106331539', 'W3106384049', 'W3106401843', 'W3124935603', 'W4234856688', 'W4253829505'], 'abstract': 'On September 14, 2015 at 09:50:45 UTC the two detectors of the Laser Interferometer Gravitational-Wave Observatory simultaneously observed a transient gravitational-wave signal. The signal sweeps upwards in frequency from 35 to 250 Hz with a peak gravitational-wave strain of $1.0 \\times 10^{-21}$. It matches the waveform predicted by general relativity for the inspiral and merger of a pair of black holes and the ringdown of the resulting single black hole. The signal was observed with a matched-filter signal-to-noise ratio of 24 and a false alarm rate estimated to be less than 1 event per 203 000 years, equivalent to a significance greater than 5.1 {\\sigma}. The source lies at a luminosity distance of $410^{+160}_{-180}$ Mpc corresponding to a redshift $z = 0.09^{+0.03}_{-0.04}$. In the source frame, the initial black hole masses are $36^{+5}_{-4} M_\\odot$ and $29^{+4}_{-4} M_\\odot$, and the final black hole mass is $62^{+4}_{-4} M_\\odot$, with $3.0^{+0.5}_{-0.5} M_\\odot c^2$ radiated in gravitational waves. All uncertainties define 90% credible intervals.These observations demonstrate the existence of binary stellar-mass black hole systems. This is the first direct detection of gravitational waves and the first observation of a binary black hole merger.', 'counts_by_year': [[2022, 769], [2021, 1199], [2020, 1117], [2019, 1115], [2018, 1242], [2017, 1208], [2016, 872], [2015, 4], [2014, 1], [2013, 1], [2012, 1]]}, {'id': 'W2891378911', 'doi': 'https://doi.org/10.7326/m18-0850', 'title': 'PRISMA Extension for Scoping Reviews (PRISMA-ScR): Checklist and Explanation', 'type': 'journal-article', 'publication_date': '2018-10-02', 'host_venue': 'V119722071', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2309799991', ['I1297363086']], ['A2617452959', ['I1297363086']], ['A2019992113', ['I1297363086']], ['A2159267241', ['I185261750']], ['A2304163474', ['I185261750']], ['A2089694106', ['I12912129']], ['A204361474', ['I114257568']], ['A2149778953', ['I5681781']], ['A2599590025', ['I92389990']], ['A2150589240', ['I1281319737']], ['A3047107023', ['I1309849503']], ['A1817415791', ['I98635879']], ['A2096225926', ['I82994568']], ['A2170002401', ['I153718931']], ['A2146381240', ['I52099693']], ['A2242835398', ['I154425047']], ['A2889823622', ['I71178462']], ['A2141406167', ['I98251732']], ['A146910989', ['I114257568']], ['A2107305861', ['I10947320']], ['A2100142044', ['I204722609']], ['A2170148757', ['I129902397']], ['A2037624892', ['I4210105654']], ['A2435108014', ['I72054575']], ['A2168257647', ['I183935753']], ['A2620421172', ['I1281319737']], ['A2578148585', ['I4210105654']], ['A2093298369', ['I1297363086']]], 'cited_by_count': 7501, 'concepts': [['C2779356329', '0.8808917'], ['C189708586', '0.791667'], ['C547195049', '0.6444638'], ['C71924100', '0.5868179'], ['C2780182762', '0.52717364']], 'referenced_works': ['W179517975', 'W1572319397', 'W1911361640', 'W1965875998', 'W2002933873', 'W2012107441', 'W2013134958', 'W2045153381', 'W2072019641', 'W2075950485', 'W2084154288', 'W2091538045', 'W2093038967', 'W2105321265', 'W2107638293', 'W2112145696', 'W2114832664', 'W2139989820', 'W2144873669', 'W2152062489', 'W2192816303', 'W2253148421', 'W2261406464', 'W2299177375', 'W2501240337', 'W2559690575', 'W2562241822', 'W2574028069', 'W2604925503', 'W2734874570', 'W2750634401', 'W2752395180', 'W2755185356', 'W2768738777', 'W2769378808', 'W2790437056', 'W2801546261'], 'abstract': 'Scoping reviews, a type of knowledge synthesis, follow a systematic approach to map evidence on a topic and identify main concepts, theories, sources, and knowledge gaps. Although more scoping reviews are being done, their methodological and reporting quality need improvement. This document presents the PRISMA-ScR (Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews) checklist and explanation. The checklist was developed by a 24-member expert panel and 2 research leads following published guidance from the EQUATOR (Enhancing the QUAlity and Transparency Of health Research) Network. The final checklist contains 20 essential reporting items and 2 optional items. The authors provide a rationale and an example of good reporting for each item. The intent of the PRISMA-ScR is to help readers (including researchers, publishers, commissioners, policymakers, health care providers, guideline developers, and patients or consumers) develop a greater understanding of relevant terminology, core concepts, and key items to report for scoping reviews.', 'counts_by_year': [[2022, 3012], [2021, 2671], [2020, 1347], [2019, 444], [2018, 12], [2017, 1]]}, {'id': 'W3008443627', 'doi': 'https://doi.org/10.1016/s1473-3099(20)30120-1', 'title': 'An interactive web-based dashboard to track COVID-19 in real time', 'type': 'journal-article', 'publication_date': '2020-05-01', 'host_venue': 'V23772524', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A3007071319', ['I145311948']], ['A3006928334', ['I145311948']], ['A2133668590', ['I145311948']]], 'cited_by_count': 6649, 'concepts': [['C3008058167', '0.7632141'], ['C33499554', '0.7532301'], ['C116675565', '0.74938536'], ['C107029721', '0.7015785'], ['C3007834351', '0.6342529']], 'referenced_works': ['W2406220407'], 'abstract': 'In December, 2019, a local outbreak of pneumonia of initially unknown cause was detected in Wuhan (Hubei, China), and was quickly determined to be caused by a novel coronavirus,1 namely severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The outbreak has since spread to every province of mainland China as well as 27 other countries and regions, with more than 70 000 confirmed cases as of Feb 17, 2020.2 In response to this ongoing public health emergency, we developed an online interactive dashboard, hosted by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University, Baltimore, MD, USA, to visualise and track reported cases of coronavirus disease 2019 (COVID-19) in real time.', 'counts_by_year': [[2022, 1521], [2021, 2901], [2020, 2211], [2019, 3]]}, {'id': 'W2963163009', 'doi': 'https://doi.org/10.1109/cvpr.2018.00474', 'title': 'MobileNetV2: Inverted Residuals and Linear Bottlenecks', 'type': 'proceedings-article', 'publication_date': '2018-06-18', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A1973675100', ['I1291425158']], ['A2612173839', ['I1291425158']], ['A2277241447', ['I1291425158']], ['A623170103', ['I1291425158']], ['A2127898042', ['I1291425158']]], 'cited_by_count': 6585, 'concepts': [['C41008148', '0.78811723'], ['C155512373', '0.64714736'], ['C2776151529', '0.5761917'], ['C89600930', '0.575485'], ['C2780513914', '0.5168558']], 'referenced_works': ['W2117539524'], 'abstract': 'In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters', 'counts_by_year': [[2022, 1419], [2021, 2458], [2020, 1786], [2019, 842], [2018, 67], [2017, 3], [2012, 2]]}, {'id': 'W2962739339', 'doi': 'https://doi.org/10.18653/v1/n18-1202', 'title': 'Deep Contextualized Word Representations', 'type': 'proceedings-article', 'publication_date': '2018-02-15', 'host_venue': 'V4306420633', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2502758396', ['I2945602774']], ['A2723434371', ['I2945602774']], ['A2068391019', ['I24603500']], ['A2133268498', ['I2945602774']], ['A2568940999', ['I2945602774']], ['A2552119883', ['I1291425158']], ['A334758317', ['I201448701']]], 'cited_by_count': 6514, 'concepts': [['C41008148', '0.82800496'], ['C204321447', '0.7671748'], ['C2780276568', '0.76706696'], ['C154945302', '0.7350334'], ['C90805587', '0.6364709']], 'referenced_works': ['W6908809', 'W581956982', 'W1026270304', 'W1632114991', 'W1840435438', 'W1938755728', 'W2035717317', 'W2064675550', 'W2095705004', 'W2103076621', 'W2144578941', 'W2147880316', 'W2153579005', 'W2155069789', 'W2158139315', 'W2158847908', 'W2158899491', 'W2162456950', 'W2250539671', 'W2251035762', 'W2251599843', 'W2251939518', 'W2259472270', 'W2296283641', 'W2463895987', 'W2493916176', 'W2507974895', 'W2516255829', 'W2518202280', 'W2551396370', 'W2556468274', 'W2605717780', 'W2608787653', 'W2611669587', 'W2740747242', 'W2740765036', 'W2740782137', 'W2757205734', 'W2962718483', 'W2962832505', 'W2963167649', 'W2963266340', 'W2963563735', 'W2963625095', 'W2963695529', 'W2963719234', 'W2963748441', 'W2963748792', 'W2963756346', 'W2963769536', 'W2964091467', 'W2964121744', 'W2964199361', 'W2964222246', 'W3016169217'], 'abstract': 'We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.', 'counts_by_year': [[2022, 625], [2021, 1741], [2020, 2136], [2019, 1780], [2018, 225], [2017, 2], [2013, 1], [2012, 1]]}, {'id': 'W2592929672', 'doi': 'https://doi.org/10.1016/j.media.2017.07.005', 'title': 'A survey on deep learning in medical image analysis', 'type': 'journal-article', 'publication_date': '2017-12-01', 'host_venue': 'V116571295', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2031367385', ['I145872427']], ['A2439833746', ['I145872427']], ['A2125607297', ['I145872427']], ['A712907143', ['I145872427']], ['A2147145211', ['I145872427']], ['A2326503246', ['I145872427']], ['A1495806369', ['I145872427']], ['A1994494219', ['I145872427']], ['A2098514387', ['I145872427']]], 'cited_by_count': 6403, 'concepts': [['C108583219', '0.8507552'], ['C154945302', '0.7430622'], ['C41008148', '0.6909767'], ['C9652623', '0.608575'], ['C89600930', '0.5920236']], 'referenced_works': ['W1457602677', 'W1582640985', 'W1871050032', 'W1884191083', 'W1897657709', 'W1903029394', 'W1972498024', 'W1974874858', 'W1986649315', 'W1988452762', 'W1995460724', 'W2022508996', 'W2036109700', 'W2059272842', 'W2061715187', 'W2064675550', 'W2082526668', 'W2084369411', 'W2084783417', 'W2097117768', 'W2100495367', 'W2101926813', 'W2103061399', 'W2107878631', 'W2112467442', 'W2112796928', 'W2126598020', 'W2136922672', 'W2145283323', 'W2157331557', 'W2161113826', 'W2163922914', 'W2169624977', 'W2175951243', 'W2194775991', 'W2195388612', 'W2238108400', 'W2248620004', 'W2253429366', 'W2264887978', 'W2266464013', 'W2274227799', 'W2280351290', 'W2282915343', 'W2284198383', 'W2284539364', 'W2292862470', 'W2301358467', 'W2302541206', 'W2310992461', 'W2312404985', 'W2317789088', 'W2318872361', 'W2321283863', 'W2322371438', 'W2323200062', 'W2323929895', 'W2326062726', 'W2328247767', 'W2334763311', 'W2335197832', 'W2338271170', 'W2339885376', 'W2341106171', 'W2342591535', 'W2343172899', 'W2343973580', 'W2344328023', 'W2344858100', 'W2344912502', 'W2345003174', 'W2345010043', 'W2346062110', 'W2357815549', 'W2378154190', 'W2383601426', 'W2396197374', 'W2396622801', 'W2401520370', 'W2404618390', 'W2408733084', 'W2413582275', 'W2473830868', 'W2493683088', 'W2504150216', 'W2504220184', 'W2509685700', 'W2510224130', 'W2512138407', 'W2513367050', 'W2515708490', 'W2515717359', 'W2517898202', 'W2518674481', 'W2520016695', 'W2524399695', 'W2529153069', 'W2529609428', 'W2530279937', 'W2531444579', 'W2532750509', 'W2533800772', 'W2538673204', 'W2546410677', 'W2547055581', 'W2547944663', 'W2550380503', 'W2550409828', 'W2551562422', 'W2553191729', 'W2556177465', 'W2557738935', 'W2559553341', 'W2559794190', 'W2559881210', 'W2560014990', 'W2560476520', 'W2561022822', 'W2561981131', 'W2564782580', 'W2566613935', 'W2572512422', 'W2577893174', 'W2578452911', 'W2581082771', 'W2582220857', 'W2750983821', 'W2962807789', 'W2962914239', 'W2963200485', 'W2963565427', 'W3104085283', 'W3104258355'], 'abstract': 'Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks and provide concise overviews of studies per application area. Open challenges and directions for future research are discussed.', 'counts_by_year': [[2022, 1086], [2021, 1736], [2020, 1595], [2019, 1311], [2018, 589], [2017, 72]]}, {'id': 'W2581082771', 'doi': 'https://doi.org/10.1038/nature21056', 'title': 'Dermatologist-level classification of skin cancer with deep neural networks', 'type': 'journal-article', 'publication_date': '2017-02-02', 'host_venue': 'V137773608', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2116717815', ['I97018004']], ['A2560836568', ['I97018004']], ['A2609276108', ['I97018004']], ['A2596902436', ['I97018004']], ['A27856879', ['I97018004', 'I204866599']], ['A2110618111', ['I99484792']], ['A2075956027', ['I97018004']]], 'cited_by_count': 6237, 'concepts': [['C2777789703', '0.5654452'], ['C16005928', '0.5007267'], ['C50644808', '0.5003662'], ['C154945302', '0.43671942'], ['C71924100', '0.36202353']], 'referenced_works': ['W1998451602', 'W2035580331', 'W2040600853', 'W2068879084', 'W2073556702', 'W2083574998', 'W2091798718', 'W2097117768', 'W2105993323', 'W2108598243', 'W2117539524', 'W2145339207', 'W2164273268', 'W2165698076', 'W2180648740', 'W2183341477', 'W2194775991', 'W2257979135', 'W2919115771'], 'abstract': 'Skin cancer, the most common human malignancy, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs) show potential for general and highly variable tasks across many fine-grained object categories. Here we demonstrate classification of skin lesions using a single CNN, trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images-two orders of magnitude larger than previous datasets-consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 (ref. 13) and can therefore potentially provide low-cost universal access to vital diagnostic care.', 'counts_by_year': [[2022, 947], [2021, 1560], [2020, 1509], [2019, 1217], [2018, 779], [2017, 214], [2016, 1]]}, {'id': 'W2282821441', 'doi': 'https://doi.org/10.1145/2939672.2939778', 'title': '"Why Should I Trust You?"', 'type': 'proceedings-article', 'publication_date': '2016-08-13', 'host_venue': 'V4306420424', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2986741849', ['I201448701']], ['A2279876130', ['I201448701']], ['A1988556028', ['I201448701']]], 'cited_by_count': 5839, 'concepts': [['C41008148', '0.7643446'], ['C154945302', '0.66171265'], ['C119857082', '0.6614214'], ['C95623464', '0.6475982'], ['C169258074', '0.4490085']], 'referenced_works': ['W260550315', 'W1905882502', 'W1991671938', 'W1996796871', 'W2010158189', 'W2036779186', 'W2044879407', 'W2053075547', 'W2063978378', 'W2097117768', 'W2108816886', 'W2134584261', 'W2137406659', 'W2143996311', 'W2164878629', 'W2165254944'], 'abstract': 'Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.', 'counts_by_year': [[2022, 985], [2021, 1818], [2020, 1455], [2019, 895], [2018, 495], [2017, 154], [2016, 27], [2015, 1], [2012, 1]]}, {'id': 'W2794480084', 'doi': 'https://doi.org/10.1038/nbt.4096', 'title': 'Integrating single-cell transcriptomic data across different conditions, technologies, and species', 'type': 'journal-article', 'publication_date': '2018-04-02', 'host_venue': 'V106963461', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2890264039', ['I4210151462']], ['A2582164442', ['I4210151462']], ['A1969604979', ['I4210151462']], ['A2072861437', ['I57206974']], ['A2026615643', ['I57206974']]], 'cited_by_count': 5810, 'concepts': [['C70721500', '0.62967944'], ['C86803240', '0.5181114'], ['C187191949', '0.45617342'], ['C162317418', '0.45151734'], ['C127716648', '0.4408308']], 'referenced_works': ['W1966626540', 'W1966701961', 'W1967327758', 'W1975077471', 'W1977361006', 'W1977544680', 'W1982130774', 'W1984689597', 'W1984883254', 'W1993287310', 'W1998712640', 'W2015497428', 'W2025341678', 'W2027470770', 'W2032321721', 'W2036010272', 'W2068767807', 'W2069089843', 'W2082253757', 'W2096086497', 'W2096663965', 'W2097369455', 'W2098290597', 'W2100235303', 'W2101445704', 'W2102212449', 'W2107092366', 'W2107665951', 'W2109890799', 'W2117113028', 'W2124368774', 'W2125336256', 'W2135937351', 'W2146512944', 'W2151013106', 'W2154977230', 'W2177432730', 'W2190545194', 'W2236822143', 'W2286142055', 'W2332292689', 'W2345356016', 'W2407916594', 'W2443466595', 'W2460637143', 'W2465316128', 'W2465917013', 'W2471513101', 'W2471536144', 'W2510746232', 'W2523620612', 'W2556548281', 'W2564176045', 'W2591733518', 'W2598326928', 'W2605810679', 'W2774307122', 'W2791731597', 'W2950420695', 'W2951506174', 'W2951890476', 'W2952303649', 'W2953251392', 'W4213108508', 'W4244203357', 'W4251002338'], 'abstract': "Computational single-cell RNA-seq (scRNA-seq) methods have been successfully applied to experiments representing a single condition, technology, or species to discover and define cellular phenotypes. However, identifying subpopulations of cells that are present across multiple data sets remains challenging. Here, we introduce an analytical strategy for integrating scRNA-seq data sets based on common sources of variation, enabling the identification of shared populations across data sets and downstream comparative analysis. We apply this approach, implemented in our R toolkit Seurat (http://satijalab.org/seurat/), to align scRNA-seq data sets of peripheral blood mononuclear cells under resting and stimulated conditions, hematopoietic progenitors sequenced using two profiling technologies, and pancreatic cell 'atlases' generated from human and mouse islets. In each case, we learn distinct or transitional cell states jointly across data sets, while boosting statistical power through integrated analysis. Our approach facilitates general comparisons of scRNA-seq data sets, potentially deepening our understanding of how distinct cell states respond to perturbation, disease, and evolution.", 'counts_by_year': [[2022, 1253], [2021, 1882], [2020, 1545], [2019, 897], [2018, 224], [2017, 3]]}, {'id': 'W2949177718', 'doi': 'https://doi.org/10.1016/j.cell.2019.05.031', 'title': 'Comprehensive Integration of Single-Cell Data', 'type': 'journal-article', 'publication_date': '2019-06-13', 'host_venue': 'V110447773', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2302481518', ['I4210151462']], ['A2890264039', ['I57206974']], ['A2582164442', ['I4210151462']], ['A347231821', ['I4210151462']], ['A2072861437', ['I57206974']], ['A2578834099', ['I57206974']], ['A2600934986', ['I57206974']], ['A146166719', ['I4210151462']], ['A1969604979', ['I4210151462']], ['A2026615643', ['I57206974']]], 'cited_by_count': 5749, 'concepts': [['C86803240', '0.8493032'], ['C70721500', '0.6485213'], ['C83640560', '0.4559723'], ['C58041806', '0.44635615'], ['C2779903281', '0.44564146']], 'referenced_works': ['W1526397263', 'W1631320694', 'W1665894543', 'W1964895626', 'W1964951991', 'W1965863160', 'W1966327575', 'W1966626540', 'W1966701961', 'W1969153299', 'W1977544680', 'W1987971958', 'W1993287310', 'W2014677321', 'W2029891497', 'W2032518018', 'W2050676604', 'W2069089843', 'W2097065948', 'W2098290597', 'W2102619694', 'W2108234281', 'W2124985265', 'W2138255346', 'W2144463184', 'W2151409320', 'W2161546116', 'W2168271199', 'W2169456326', 'W2179438025', 'W2231058499', 'W2341539131', 'W2411397641', 'W2472063172', 'W2510746232', 'W2514448720', 'W2516455020', 'W2523369352', 'W2523620612', 'W2526262591', 'W2540833380', 'W2551194178', 'W2561754210', 'W2572662684', 'W2580989000', 'W2582946425', 'W2585175142', 'W2591733518', 'W2605624585', 'W2739492614', 'W2747877289', 'W2752426001', 'W2753908609', 'W2763574125', 'W2766140892', 'W2766437506', 'W2785085486', 'W2788837643', 'W2790768464', 'W2791731597', 'W2794478094', 'W2794480084', 'W2794521141', 'W2795331645', 'W2796170779', 'W2796538159', 'W2805619986', 'W2809368942', 'W2817838603', 'W2885927965', 'W2887694579', 'W2887910040', 'W2889236955', 'W2890156321', 'W2894687190', 'W2899224111', 'W2901677030', 'W2911108160', 'W2912390237', 'W2942880491', 'W2948469692', 'W2949238013', 'W2950909085', 'W2950976066', 'W2951232102', 'W2951506174', 'W2952383043', 'W2952602120', 'W4213108508'], 'abstract': 'Single-cell transcriptomics has transformed our ability to characterize cell states, but deep biological understanding requires more than a taxonomic listing of clusters. As new methods arise to measure distinct cellular modalities, a key analytical challenge is to integrate these datasets to better understand cellular identity and function. Here, we develop a strategy to "anchor" diverse datasets together, enabling us to integrate single-cell measurements not only across scRNA-seq technologies, but also across different modalities. After demonstrating improvement over existing methods for integrating scRNA-seq data, we anchor scRNA-seq experiments with scATAC-seq to explore chromatin differences in closely related interneuron subsets and project protein expression measurements onto a bone marrow atlas to characterize lymphocyte populations. Lastly, we harmonize in situ gene expression and scRNA-seq datasets, allowing transcriptome-wide imputation of spatial gene expression patterns. Our work presents a strategy for the assembly of harmonized references and transfer of information across datasets.', 'counts_by_year': [[2022, 1848], [2021, 2344], [2020, 1342], [2019, 196], [2018, 8]]}, {'id': 'W2944434778', 'doi': 'https://doi.org/10.1016/j.jbi.2019.103208', 'title': 'The REDCap consortium: Building an international community of software platform partners', 'type': 'journal-article', 'publication_date': '2019-05-09', 'host_venue': 'V11622463', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2155253363', ['I200719446', 'I901861585']], ['A2786863363', ['I901861585']], ['A2282713179', ['I901861585']], ['A2944556014', ['I901861585']], ['A2944723506', ['I901861585']], ['A2944490161', ['I901861585']], ['A2944635108', ['I901861585']], ['A2943999714', ['I901861585']], ['A2944629579', ['I901861585']], ['A2598642466', ['I901861585']], ['A2128472415', ['I901861585']]], 'cited_by_count': 5717, 'concepts': [['C91790935', '0.6331141'], ['C2779965156', '0.4656001'], ['C101780184', '0.45993996'], ['C2777904410', '0.4343732'], ['C41008148', '0.41983634']], 'referenced_works': ['W1508644103', 'W1517472723', 'W1526182200', 'W1553628562', 'W1573344485', 'W1575328666', 'W1579673523', 'W1639164366', 'W1811656171', 'W1833822692', 'W1835259194', 'W1885178597', 'W1964487839', 'W1981090471', 'W1989750541', 'W1995298382', 'W2008244476', 'W2016017240', 'W2019193813', 'W2024585719', 'W2029784122', 'W2037272442', 'W2040151576', 'W2043160805', 'W2048136803', 'W2056633185', 'W2078792829', 'W2093274439', 'W2114897044', 'W2123152121', 'W2128681611', 'W2129880493', 'W2138278593', 'W2146490267', 'W2151868700', 'W2159739284', 'W2162935122', 'W2166831763', 'W2169416010', 'W2171350235', 'W2172119044', 'W2209139941', 'W2277376353', 'W2340727448', 'W2399492262', 'W2410556404', 'W2416583220', 'W2419333550', 'W4236442784', 'W4292447049'], 'abstract': '• The Research Electronic Data Capture (REDCap) data platform launched in 2004. • The REDCap Consortium is the community of REDCap administrators and organizations. • The REDCap Consortium grew from local to international impact in six phases. • REDCap partners include 3207 organizations in 128 countries as of December 2018. • Our consortium-building lessons should help the research informatics community. The Research Electronic Data Capture (REDCap) data management platform was developed in 2004 to address an institutional need at Vanderbilt University, then shared with a limited number of adopting sites beginning in 2006. Given bi-directional benefit in early sharing experiments, we created a broader consortium sharing and support model for any academic, non-profit, or government partner wishing to adopt the software. Our sharing framework and consortium-based support model have evolved over time along with the size of the consortium (currently more than 3200 REDCap partners across 128 countries). While the “REDCap Consortium” model represents only one example of how to build and disseminate a software platform, lessons learned from our approach may assist other research institutions seeking to build and disseminate innovative technologies.', 'counts_by_year': [[2022, 2274], [2021, 2349], [2020, 1033], [2019, 53]]}, {'id': 'W2963276645', 'doi': 'https://doi.org/10.1038/s41587-019-0209-9', 'title': 'Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2', 'type': 'journal-article', 'publication_date': '2019-08-01', 'host_venue': 'V106963461', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2154510224', ['I203172682']], ['A2108227264', ['I203172682']], ['A2750890337', ['I203172682']], ['A2016358918', ['I203172682']], ['A1085528843', ['I1299303238']], ['A2040343731', ['I130238516']], ['A2183796229', ['I66958751']], ['A2105418373', ['I63966007']], ['A2126972980', ['I124055696']], ['A2402787733', ['I193223587']], ['A2339371595', ['I19820366']], ['A2620085994', ['I180670191']], ['A2118349020', ['I79576946']], ['A2055791678', ['I124055696']], ['A2121351140', ['I142606810']], ['A2141220529', ['I84218800']], ['A2110608429', ['I137902535']], ['A2176181809', ['I36258959']], ['A2234703548', ['I203172682']], ['A1994187118', ['I203172682']], ['A2104388772', ['I36258959']], ['A2137958068', ['I3148997608']], ['A1478187186', ['I36258959']], ['A2689701037', ['I129902397']], ['A2109436966', ['I141945490']], ['A2527499111', ['I63966007']], ['A2076188023', []], ['A2596192884', ['I36258959']], ['A2461162126', ['I141945490']], ['A289368394', ['I51713134']], ['A2423507785', ['I36258959']], ['A2117678214', ['I201448701']], ['A2282227485', ['I141945490']], ['A2332172545', ['I36258959']], ['A2896972339', ['I203172682']], ['A2629657521', ['I87216513']], ['A2511040296', ['I130238516']], ['A2150170722', ['I97018004']], ['A2891822733', ['I36258959']], ['A1531017582', ['I107606265']], ['A1843286711', ['I118347636']], ['A2294815279', ['I44260953']], ['A2027271794', ['I36258959']], ['A2131040418', ['I36258959']], ['A2483460619', ['I31746571']], ['A2118151953', ['I31766871']], ['A2896223338', ['I203172682']], ['A2139140981', ['I203172682']], ['A2166393535', ['I26538001']], ['A2100954238', ['I130238516']], ['A2545922287', ['I36258959']], ['A2016716163', ['I36258959']], ['A2896603438', ['I203172682']], ['A1999382879', ['I129902397']], ['A2267639195', ['I1344073410']], ['A2153749140', ['I149899117']], ['A2643802224', []], ['A2272045644', ['I1299303238']], ['A271023826', ['I51713134']], ['A2534688598', ['I36258959']], ['A2686851693', ['I36258959']], ['A2649179817', ['I201448701']], ['A2497045332', ['I36258959']], ['A2426431315', ['I107606265']], ['A2464102483', ['I36258959']], ['A2056512017', ['I92446798']], ['A2312250890', ['I141945490']], ['A2782738262', ['I36258959']], ['A2897083891', ['I203172682']], ['A2096129940', ['I36258959']], ['A2325193365', ['I36258959']], ['A2889798203', []], ['A2105161595', ['I203172682']], ['A2895843760', ['I70983195']], ['A2133700266', ['I36258959']], ['A2297605235', ['I97098773']], ['A257200781', ['I51713134']], ['A2897206683', ['I124055696']], ['A2337692599', ['I1336096307']], ['A2099838456', ['I79620101']], ['A2895929255', ['I97098773']], ['A160904506', ['I193223587']], ['A2720913967', ['I51713134']], ['A2533681889', ['I203172682']], ['A2155161042', ['I1299303238']], ['A2780729044', ['I36258959']], ['A2188271139', ['I167576493']], ['A2791445208', []], ['A2740660874', ['I44854399']], ['A2520438218', ['I26538001']], ['A2321137388', ['I201448701']], ['A2751562266', ['I36258959']], ['A242678278', ['I180670191']], ['A2801528326', ['I156087764']], ['A2044513248', ['I913481162']], ['A2708679373', []], ['A1642874537', ['I36258959']], ['A2144616630', ['I1299303238']], ['A2897172342', ['I138006243']], ['A2102259889', ['I149899117']], ['A2136580313', ['I1299303238']], ['A2121048590', ['I36258959']], ['A2897262315', ['I184903800']], ['A2896872799', ['I33213144']], ['A2432964053', ['I203172682']], ['A2131709232', ['I201448701']], ['A2311815666', ['I36258959']], ['A2413028446', ['I201448701']], ['A2305925485', ['I1285764155']], ['A2944796044', ['I36258959']], ['A2159013505', ['I36258959']], ['A1995678064', ['I203172682']]], 'cited_by_count': 5682, 'concepts': [['C143121216', '0.7692013'], ['C48044578', '0.6612159'], ['C32833848', '0.6170314'], ['C41008148', '0.5810271'], ['C70721500', '0.53567606']], 'referenced_works': ['W1703384511', 'W1973094248', 'W2056279562', 'W2097478615', 'W2108718991', 'W2110300022', 'W2130731460', 'W2175093233', 'W2190487303', 'W2401404581', 'W2504691963', 'W2539403003', 'W2565309149', 'W2741520323', 'W2762306894', 'W2766903152', 'W2793643488', 'W2800461749', 'W2801297226', 'W2807194798', 'W2858264627', 'W2892206041', 'W2897611961', 'W2901568755', 'W2951076599', 'W2953236953'], 'abstract': 'QIIME 2 development was primarily funded by NSF Awards 1565100 to J.G.C. and 1565057 to R.K. Partial support was also provided by the following: grants NIH U54CA143925 (J.G.C. and T.P.) and U54MD012388 (J.G.C. and T.P.); grants from the Alfred P. Sloan Foundation (J.G.C. and R.K.); ERCSTG project MetaPG (N.S.); the Strategic Priority Research Program of the Chinese Academy of Sciences QYZDB-SSW-SMC021 (Y.B.); the Australian National Health and Medical Research Council APP1085372 (G.A.H., J.G.C., Von Bing Yap and R.K.); the Natural Sciences and Engineering Research Council (NSERC) to D.L.G.; and the State of Arizona Technology and Research Initiative Fund (TRIF), administered by the Arizona Board of Regents, through Northern Arizona University. All NCI coauthors were supported by the Intramural Research Program of the National Cancer Institute. S.M.G. and C. Diener were supported by the Washington Research Foundation Distinguished Investigator Award.', 'counts_by_year': [[2022, 2302], [2021, 2173], [2020, 1082], [2019, 101]]}, {'id': 'W2804822363', 'doi': 'https://doi.org/10.1093/nar/gky427', 'title': 'SWISS-MODEL: homology modelling of protein structures and complexes', 'type': 'journal-article', 'publication_date': '2018-07-02', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A3163533158', ['I1850255', 'I12708293']], ['A2136801193', ['I1850255', 'I12708293']], ['A2055822103', ['I1850255', 'I12708293']], ['A2016874237', ['I1850255', 'I12708293']], ['A2026147096', ['I1850255', 'I12708293']], ['A1994304541', ['I1850255', 'I12708293']], ['A1963888660', ['I1850255', 'I12708293']], ['A2146965024', ['I1850255', 'I12708293']], ['A2803918910', ['I1850255', 'I12708293']], ['A2052081008', ['I1850255', 'I12708293']], ['A2339424813', ['I1850255', 'I12708293']], ['A1860503938', ['I1850255', 'I12708293']]], 'cited_by_count': 5600, 'concepts': [['C169627665', '0.71115434'], ['C177212765', '0.66373855'], ['C86803240', '0.6441239'], ['C165525559', '0.6399512'], ['C70721500', '0.5692506']], 'referenced_works': ['W12382117', 'W1588645461', 'W1757046366', 'W1843054844', 'W1869944834', 'W1903986304', 'W1967293793', 'W1981264193', 'W1982679430', 'W1993951624', 'W1995510302', 'W2011658408', 'W2012966511', 'W2013580208', 'W2015642465', 'W2018296380', 'W2020669016', 'W2021703339', 'W2038555943', 'W2039063819', 'W2047544230', 'W2050333027', 'W2051210555', 'W2054291744', 'W2055395356', 'W2060809301', 'W2065283382', 'W2069152576', 'W2069596790', 'W2074713036', 'W2081598613', 'W2101078720', 'W2106648157', 'W2108862316', 'W2115167726', 'W2115540209', 'W2120836664', 'W2124463526', 'W2128484254', 'W2137986227', 'W2140673705', 'W2142678478', 'W2144954642', 'W2146084306', 'W2149525061', 'W2149605074', 'W2152301430', 'W2155212694', 'W2158714788', 'W2159614853', 'W2163614321', 'W2164813401', 'W2167795220', 'W2171419407', 'W2285587551', 'W2405504822', 'W2558272290', 'W2573340705', 'W2590125722', 'W2610302091', 'W2746313716', 'W2750193626', 'W2751260413', 'W2766157941', 'W2766188130', 'W2770647690', 'W2783728269', 'W2949223833', 'W4210702584', 'W4234714728', 'W4239761798', 'W4240895940'], 'abstract': 'Homology modelling has matured into an important technique in structural biology, significantly contributing to narrowing the gap between known protein sequences and experimentally determined structures. Fully automated workflows and servers simplify and streamline the homology modelling process, also allowing users without a specific computational expertise to generate reliable protein models and have easy access to modelling results, their visualization and interpretation. Here, we present an update to the SWISS-MODEL server, which pioneered the field of automated modelling 25 years ago and been continuously further developed. Recently, its functionality has been extended to the modelling of homo- and heteromeric complexes. Starting from the amino acid sequences of the interacting proteins, both the stoichiometry and the overall structure of the complex are inferred by homology modelling. Other major improvements include the implementation of a new modelling engine, ProMod3 and the introduction a new local model quality estimation method, QMEANDisCo. SWISS-MODEL is freely available at https://swissmodel.expasy.org.', 'counts_by_year': [[2022, 1453], [2021, 1840], [2020, 1476], [2019, 730], [2018, 90], [2017, 1]]}, {'id': 'W2962756421', 'doi': 'https://doi.org/10.1145/2939672.2939754', 'title': 'node2vec', 'type': 'proceedings-article', 'publication_date': '2016-08-13', 'host_venue': 'V4306420424', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2008277149', ['I97018004']], ['A1878631932', ['I97018004']]], 'cited_by_count': 5598, 'concepts': [['C41008148', '0.73863006'], ['C59404180', '0.6727011'], ['C48044578', '0.6047878'], ['C2780598303', '0.54999614'], ['C154945302', '0.54823375']], 'referenced_works': ['W1996430422', 'W2001141328', 'W2001325956', 'W2010457001', 'W2011986160', 'W2037933327', 'W2048586760', 'W2053186076', 'W2057685268', 'W2060616833', 'W2090891622', 'W2107569009', 'W2117486996', 'W2136040699', 'W2149490995', 'W2154454189', 'W2163922914', 'W2168627253', 'W2202083088', 'W2250539671', 'W2782823532', 'W2917431650', 'W3104097132', 'W3105705953'], 'abstract': "Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.", 'counts_by_year': [[2022, 843], [2021, 1397], [2020, 1353], [2019, 1110], [2018, 637], [2017, 225], [2016, 26], [2015, 1], [2012, 1]]}, {'id': 'W2319902168', 'doi': 'https://doi.org/10.1107/s2052520616003954', 'title': 'The Cambridge Structural Database', 'type': 'journal-article', 'publication_date': '2016-04-01', 'host_venue': 'V4210168249', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2036719881', ['I4210098980']], ['A2073376466', ['I4210098980']], ['A2555412638', ['I4210098980']], ['A3211412814', ['I4210098980']]], 'cited_by_count': 5553, 'concepts': [['C154504017', '0.8451867'], ['C41008148', '0.6743726'], ['C113843644', '0.53592205'], ['C2779965156', '0.53302693'], ['C77088390', '0.5205215']], 'referenced_works': ['W1508604947', 'W1533988144', 'W1601495365', 'W1819141267', 'W1924295507', 'W1964838468', 'W1965749350', 'W1967445304', 'W1969310000', 'W1969360633', 'W1976892175', 'W1979368277', 'W1999732134', 'W2009213936', 'W2011703027', 'W2012580598', 'W2014637311', 'W2017702218', 'W2032842297', 'W2057709509', 'W2059205366', 'W2068568813', 'W2070942956', 'W2084014179', 'W2086945873', 'W2093135499', 'W2102810281', 'W2104044165', 'W2104979868', 'W2108187752', 'W2112845989', 'W2119233780', 'W2126967581', 'W2130479394', 'W2145538332', 'W2149133581', 'W2153675433', 'W2156118208', 'W2163363362', 'W2227255012', 'W2315318154', 'W4236377166'], 'abstract': 'The Cambridge Structural Database (CSD) contains a complete record of all published organic and metal–organic small-molecule crystal structures. The database has been in operation for over 50 years and continues to be the primary means of sharing structural chemistry data and knowledge across disciplines. As well as structures that are made public to support scientific articles, it includes many structures published directly as CSD Communications. All structures are processed both computationally and by expert structural chemistry editors prior to entering the database. A key component of this processing is the reliable association of the chemical identity of the structure studied with the experimental data. This important step helps ensure that data is widely discoverable and readily reusable. Content is further enriched through selective inclusion of additional experimental data. Entries are available to anyone through free CSD community web services. Linking services developed and maintained by the CCDC, combined with the use of standard identifiers, facilitate discovery from other resources. Data can also be accessed through CCDC and third party software applications and through an application programming interface.', 'counts_by_year': [[2022, 757], [2021, 975], [2020, 1036], [2019, 950], [2018, 864], [2017, 698], [2016, 269], [2015, 1]]}, {'id': 'W2302501749', 'doi': 'https://doi.org/10.1038/sdata.2016.18', 'title': 'The FAIR Guiding Principles for scientific data management and stewardship', 'type': 'journal-article', 'publication_date': '2016-03-15', 'host_venue': 'V2607323502', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2953904972', ['I88060688']], ['A2035111836', ['I97018004']], ['A2116600581', ['I1318003438']], ['A2578375354', ['I1318003438']], ['A3042743586', []], ['A2566318653', []], ['A2117093475', []], ['A2233558105', []], ['A2097162788', ['I865915315']], ['A2151520125', ['I1299303238']], ['A2056351025', []], ['A2019532136', ['I153648349']], ['A2569294699', ['I136199984']], ['A906928525', ['I136199984']], ['A1592820567', []], ['A1283367657', ['I1318003438']], ['A1976589559', ['I100135526']], ['A2098532862', ['I34352273']], ['A2140136256', ['I913481162']], ['A2070649601', ['I40120149']], ['A2126464511', ['I32062511']], ['A2249402561', ['I1318003438']], ['A2122310831', ['I28407311']], ['A1966303417', ['I36258959']], ['A2105280663', []], ['A2092462184', ['I2800006345']], ['A2166951043', []], ['A2194150621', ['I865915315']], ['A2343285426', []], ['A2140016166', ['I121797337']], ['A2810414909', []], ['A665904529', ['I36258959']], ['A2311282906', []], ['A2069198049', ['I88273585']], ['A2008193581', ['I2800139495']], ['A396324943', ['I40120149']], ['A2918148363', ['I2800006345']], ['A3174594295', ['I67348948']], ['A1907586488', ['I40120149']], ['A1226035095', ['I2800006345']], ['A2051890214', ['I12708293']], ['A2924106006', ['I155867693']], ['A2912873105', []], ['A1416178009', ['I1334415907']], ['A2950505616', ['I2800006345']], ['A2019296152', ['I913958620']], ['A214717124', ['I913958620']], ['A1969486554', []], ['A2009985931', []], ['A2921079473', []], ['A1882554364', ['I121797337']], ['A2298185606', ['I40120149']], ['A1803236419', ['I913958620']]], 'cited_by_count': 5491, 'concepts': [['C2777950569', '0.76005065'], ['C206588197', '0.7025972'], ['C137981799', '0.6799203'], ['C26713055', '0.64093393'], ['C41008148', '0.5510547']], 'referenced_works': ['W1482374180', 'W1515149315', 'W1533059251', 'W1572313459', 'W1857123336', 'W1957272438', 'W1969142598', 'W1995808589', 'W2019481684', 'W2020152108', 'W2036318837', 'W2088723712', 'W2122344857', 'W2130479394', 'W2141297584', 'W2145789830', 'W2157122545', 'W2159721641', 'W2282471829', 'W2739999456', 'W4214851230', 'W4234914623', 'W4242729757'], 'abstract': 'There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders-representing academia, industry, funding agencies, and scholarly publishers-have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.', 'counts_by_year': [[2022, 1358], [2021, 1458], [2020, 1167], [2019, 763], [2018, 445], [2017, 242], [2016, 50], [2012, 1]]}, {'id': 'W2965373594', 'doi': 'https://doi.org/10.48550/arxiv.1907.11692', 'title': 'RoBERTa: A Robustly Optimized BERT Pretraining Approach', 'type': 'posted-content', 'publication_date': '2019-07-26', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2920923173', []], ['A2166285282', []], ['A2516589705', []], ['A2939035485', []], ['A2703745726', []], ['A2104844286', []], ['A2250897584', []], ['A3208918517', []], ['A334758317', []], ['A2130709233', []]], 'cited_by_count': 5473, 'concepts': [['C8642999', '0.90885496'], ['C41008148', '0.7416245'], ['C12590798', '0.6950581'], ['C2776760102', '0.6199603'], ['C26517878', '0.6113754']], 'referenced_works': ['W131533222', 'W1566289585', 'W1599016936', 'W1840435438', 'W2130158090', 'W2170973209', 'W2251939518', 'W2396767181', 'W2462831000', 'W2525127255', 'W2784121710', 'W2793353489', 'W2805206884', 'W2899663614', 'W2899771611', 'W2914120296', 'W2914526845', 'W2920812691', 'W2930786691', 'W2933138175', 'W2937297214', 'W2938830017', 'W2944815030', 'W2945260553', 'W2945785363', 'W2947813521', 'W2948629866', 'W2950501607', 'W2962753370', 'W2962784628', 'W2963026768', 'W2963112338', 'W2963310665', 'W2963323070', 'W2963341956', 'W2963403868', 'W2963748441', 'W2963756346', 'W2963807318', 'W2963846996', 'W2964121744', 'W2970597249', 'W2978670439', 'W2990704537'], 'abstract': 'Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.', 'counts_by_year': [[2022, 139], [2021, 3025], [2020, 2068], [2019, 239], [2018, 2]]}, {'id': 'W2493916176', 'doi': 'https://doi.org/10.1162/tacl_a_00051', 'title': 'Enriching Word Vectors with Subword Information', 'type': 'journal-article', 'publication_date': '2017-06-12', 'host_venue': 'V2729999759', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A1882694979', ['I2252078561']], ['A2114720862', ['I2252078561']], ['A2512114774', ['I2252078561']], ['A292626543', ['I2252078561']]], 'cited_by_count': 5449, 'concepts': [['C41008148', '0.8651931'], ['C90805587', '0.8462911'], ['C204321447', '0.7666453'], ['C154945302', '0.6857027'], ['C2780861071', '0.6730859']], 'referenced_works': ['W1662133657', 'W1981617416', 'W2078483536', 'W2128870637', 'W2147152072', 'W2882319491'], 'abstract': 'Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.', 'counts_by_year': [[2022, 550], [2021, 1439], [2020, 1541], [2019, 1230], [2018, 560], [2017, 113], [2016, 8], [2012, 1]]}, {'id': 'W2531409750', 'doi': 'https://doi.org/10.1109/cvpr.2017.195', 'title': 'Xception: Deep Learning with Depthwise Separable Convolutions', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2892644763', ['I1291425158']]], 'cited_by_count': 5438, 'concepts': [['C45347329', '0.8886862'], ['C2777984123', '0.8742049'], ['C70710897', '0.847232'], ['C81363708', '0.7379879'], ['C41008148', '0.72657245']], 'referenced_works': ['W1849277567', 'W2086161653', 'W2097117768', 'W2167383966'], 'abstract': 'We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.', 'counts_by_year': [[2022, 1097], [2021, 1721], [2020, 1401], [2019, 922], [2018, 261], [2017, 22], [2012, 1]]}, {'id': 'W2614081736', 'doi': 'https://doi.org/10.1038/nmeth.4285', 'title': 'ModelFinder: fast model selection for accurate phylogenetic estimates', 'type': 'journal-article', 'publication_date': '2017-06-01', 'host_venue': 'V127827428', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A287611975', ['I185491429', 'I4210161554']], ['A2141006206', ['I2802916955']], ['A2198298410', ['I4210161554', 'I118347636']], ['A261659319', ['I129774422', 'I2802916955']], ['A2030260630', ['I4210161554', 'I118347636']]], 'cited_by_count': 5319, 'concepts': [['C193252679', '0.80881584'], ['C81917197', '0.6715551'], ['C2779343474', '0.6416013'], ['C93959086', '0.5937533'], ['C90132467', '0.5058193']], 'referenced_works': ['W1887892324', 'W1951091454', 'W1957469265', 'W1968783265', 'W1969042907', 'W1972245556', 'W2012220164', 'W2026062398', 'W2036501702', 'W2048513925', 'W2060425093', 'W2082928585', 'W2091232725', 'W2099181176', 'W2111211467', 'W2111647009', 'W2117059491', 'W2118288463', 'W2120611093', 'W2124790653', 'W2133870991', 'W2139134537', 'W2144654387', 'W2148013742', 'W2151723210', 'W2157552136', 'W2162393839', 'W2163354046', 'W2163627198', 'W2167455933', 'W2168830571', 'W2502655619', 'W2601127770', 'W4211177544'], 'abstract': 'Model-based molecular phylogenetics plays an important role in comparisons of genomic data, and model selection is a key step in all such analyses. We present ModelFinder, a fast model-selection method that greatly improves the accuracy of phylogenetic estimates by incorporating a model of rate heterogeneity across sites not previously considered in this context and by allowing concurrent searches of model space and tree space.', 'counts_by_year': [[2022, 1574], [2021, 1643], [2020, 1142], [2019, 661], [2018, 258], [2017, 29], [2012, 1]]}, {'id': 'W2340897893', 'doi': 'https://doi.org/10.1109/cvpr.2016.350', 'title': 'The Cityscapes Dataset for Semantic Urban Scene Understanding', 'type': 'proceedings-article', 'publication_date': '2016-06-01', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2073169045', ['I891521709']], ['A2235469018', ['I149899117']], ['A2147165370', ['I891521709']], ['A2918551040', ['I891521709']], ['A2946213062', ['I891521709']], ['A2068802143', ['I149899117']], ['A2134082329', ['I891521709']], ['A2573922110', ['I31512782']], ['A1620943014', ['I149899117']]], 'cited_by_count': 5232, 'concepts': [['C41008148', '0.80225706'], ['C153083717', '0.7747723'], ['C154945302', '0.646611'], ['C79581498', '0.6389563'], ['C185798385', '0.62020206']], 'referenced_works': ['W19582845', 'W260801291', 'W1536680647', 'W1566135517', 'W1783315696', 'W1903029394', 'W1909234690', 'W1915480574', 'W1923115158', 'W1923184257', 'W1927251054', 'W1927486677', 'W1932937519', 'W1938976761', 'W1945608308', 'W1945811542', 'W1948751323', 'W1958328135', 'W1965086692', 'W1991367009', 'W1991384784', 'W1992178727', 'W1994488211', 'W1995444699', 'W1998299325', 'W2008969013', 'W2010357867', 'W2028284985', 'W2031454541', 'W2037227137', 'W2054389289', 'W2067107771', 'W2083597815', 'W2088049833', 'W2101938307', 'W2102605133', 'W2110764733', 'W2115579991', 'W2117539524', 'W2124592697', 'W2125215748', 'W2127251585', 'W2133228700', 'W2136724559', 'W2137097255', 'W2144409879', 'W2167687475', 'W2168356304', 'W2171943915', 'W2198291924', 'W2919115771', 'W2963108253', 'W2963563573', 'W2964162504'], 'abstract': 'Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations, 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.', 'counts_by_year': [[2022, 603], [2021, 1521], [2020, 1309], [2019, 1120], [2018, 471], [2017, 176], [2016, 28], [2015, 1]]}, {'id': 'W2560023338', 'doi': 'https://doi.org/10.1109/cvpr.2017.660', 'title': 'Pyramid Scene Parsing Network', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2516465340', ['I177725633']], ['A2105979260', ['I2945522305']], ['A2490753878', ['I177725633']], ['A2227253382', ['I177725633']], ['A2096891736', ['I177725633']]], 'cited_by_count': 5228, 'concepts': [['C75608658', '0.87323785'], ['C41008148', '0.84346557'], ['C186644900', '0.7809222'], ['C154945302', '0.713813'], ['C142575187', '0.59839606']], 'referenced_works': ['W1495267108', 'W1745334888', 'W1903029394', 'W1938976761', 'W1948751323', 'W2031489346', 'W2097117768', 'W2124592697', 'W2125215748', 'W2144794286', 'W2155893237', 'W2158842374', 'W2162915993', 'W2194775991', 'W2221898772', 'W2339172597', 'W2340897893', 'W2473131906', 'W2508741746', 'W2962891704', 'W2963108253', 'W2963563573'], 'abstract': 'Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.', 'counts_by_year': [[2022, 878], [2021, 1569], [2020, 1230], [2019, 1032], [2018, 463], [2017, 49], [2016, 1]]}, {'id': 'W2962858109', 'doi': 'https://doi.org/10.1109/iccv.2017.74', 'title': 'Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization', 'type': 'proceedings-article', 'publication_date': '2017-10-01', 'host_venue': 'V4306419272', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2531025681', ['I859038795']], ['A2106263932', ['I130701444']], ['A2786888888', ['I130701444']], ['A2032368582', ['I859038795']], ['A2223275083', ['I2252078561']], ['A2098683697', ['I859038795']]], 'cited_by_count': 5200, 'concepts': [['C157657479', '0.94829226'], ['C41008148', '0.82037914'], ['C97931131', '0.7655745'], ['C81363708', '0.7078301'], ['C154945302', '0.6693311']], 'referenced_works': ['W1586079445', 'W1895577753', 'W1900913856', 'W1903029394', 'W1905882502', 'W1931639407', 'W1933349210', 'W1941636933', 'W1945608308', 'W1982428585', 'W1994488211', 'W2102605133', 'W2142192571', 'W2155893237', 'W2161381512', 'W2163922914', 'W2190008860', 'W2194775991', 'W2257979135', 'W2282821441', 'W2295107390', 'W2962848674', 'W2963260436', 'W2963758027', 'W3037725825'], 'abstract': 'We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach – Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for ‘dog’ or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad- CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] and video at youtu.be/COjUB9Izk6E.', 'counts_by_year': [[2022, 1130], [2021, 1868], [2020, 1295], [2019, 696], [2018, 183], [2017, 18], [2012, 1]]}, {'id': 'W2963470893', 'doi': 'https://doi.org/10.1109/cvpr.2017.19', 'title': 'Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A1984512120', ['I143396566']], ['A2224929196', ['I143396566']], ['A1431255435', ['I113979032']], ['A2044919530', ['I47508984']], ['A2561765617', []], ['A3213904679', ['I113979032']], ['A2681437533', ['I113979032']], ['A2079343795', ['I113979032']], ['A2683573012', ['I113979032']], ['A2152099389', ['I113979032']], ['A2125315204', ['I113979032']]], 'cited_by_count': 5180, 'concepts': [['C37736160', '0.8635273'], ['C41008148', '0.6943697'], ['C2988773926', '0.6810634'], ['C115961682', '0.572576'], ['C154945302', '0.54079956']], 'referenced_works': ['W1677182931', 'W1791560514', 'W1885185971', 'W1910615622', 'W1919542679', 'W1930824406', 'W1949096787', 'W1950594372', 'W1973788353', 'W1977581467', 'W1983781364', 'W1991867157', 'W2035677848', 'W2047920195', 'W2049237558', 'W2091244316', 'W2097074225', 'W2097117768', 'W2104599718', 'W2121927366', 'W2133665775', 'W2149669120', 'W2149760002', 'W2150081556', 'W2165939075', 'W2170965888', 'W2172128189', 'W2190008860', 'W2194775991', 'W2200594420', 'W2202656999', 'W2214802144', 'W2275363859', 'W2302255633', 'W2475287302', 'W2476548250', 'W2534320940', 'W2543431435', 'W3104720471'], 'abstract': 'Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.', 'counts_by_year': [[2022, 606], [2021, 1274], [2020, 1346], [2019, 1137], [2018, 637], [2017, 163], [2016, 10], [2012, 1]]}, {'id': 'W2752782242', 'doi': 'https://doi.org/10.1109/cvpr.2018.00745', 'title': 'Squeeze-and-Excitation Networks', 'type': 'proceedings-article', 'publication_date': '2018-06-01', 'host_venue': 'V2597175965', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2752023548', ['I4210128818']], ['A2236987923', ['I40120149']], ['A2580435753', ['I40120149']], ['A2653808965', ['I4210112150']], ['A2238935163', ['I4210128818']]], 'cited_by_count': 5162, 'concepts': [['C41008148', '0.7756508'], ['C2777210771', '0.6891461'], ['C45347329', '0.6592771'], ['C2780801425', '0.5885019'], ['C81363708', '0.5843374']], 'referenced_works': ['W1568165162', 'W1677182931', 'W1903029394', 'W1966385142', 'W2064675550', 'W2097117768', 'W2113325037', 'W2115441154', 'W2117539524', 'W2128272608', 'W2130325614', 'W2144764737', 'W2183341477', 'W2194775991', 'W2221625691', 'W2288122362', 'W2400429454', 'W2531409750', 'W2549139847', 'W2550553598', 'W2952746495', 'W2962737770', 'W2962971773', 'W2963446712', 'W2963495494'], 'abstract': 'The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the  (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at this https URL.', 'counts_by_year': [[2022, 1685], [2021, 1576], [2020, 1089], [2019, 578], [2018, 179], [2017, 30]]}, {'id': 'W2766447205', 'doi': 'https://doi.org/10.1038/nature24270', 'title': 'Mastering the game of Go without human knowledge', 'type': 'journal-article', 'publication_date': '2017-10-19', 'host_venue': 'V137773608', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2593774290', ['I4210090411']], ['A2512603362', ['I4210090411']], ['A2033942250', ['I4210090411']], ['A291373107', ['I4210090411']], ['A2497967435', ['I4210090411']], ['A2015134264', ['I4210090411']], ['A2766000132', ['I4210090411']], ['A2766248405', ['I4210090411']], ['A2480833549', ['I4210090411']], ['A2765473962', ['I4210090411']], ['A2304614720', ['I4210090411']], ['A2952465156', ['I4210090411']], ['A2766955913', ['I4210090411']], ['A2697048878', ['I4210090411']], ['A2600316320', ['I4210090411']], ['A2032008572', ['I4210090411']], ['A4302276', ['I4210090411']]], 'cited_by_count': 5117, 'concepts': [['C2780465443', '0.94364274'], ['C154945302', '0.72547793'], ['C50644808', '0.69515437'], ['C97541855', '0.6648368'], ['C41008148', '0.631387']], 'referenced_works': ['W1497290640', 'W1509593372', 'W1542941925', 'W1563895031', 'W1572063013', 'W1574724904', 'W1579184372', 'W1587022413', 'W1625390266', 'W1714211023', 'W1863869622', 'W1977655452', 'W1977989560', 'W1997840820', 'W2041367235', 'W2099001564', 'W2101926813', 'W2107649750', 'W2124477018', 'W2126316555', 'W2145339207', 'W2153039919', 'W2157177347', 'W2161608691', 'W2192203593', 'W2194775991', 'W2257979135', 'W2574978968', 'W2787894218', 'W2789888349', 'W2913764096', 'W2919115771', 'W4214717370'], 'abstract': "A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo.", 'counts_by_year': [[2022, 689], [2021, 1269], [2020, 1295], [2019, 1172], [2018, 640], [2017, 43]]}, {'id': 'W2323326409', 'doi': 'https://doi.org/10.1038/nbt.3519', 'title': 'Near-optimal probabilistic RNA-seq quantification', 'type': 'journal-article', 'publication_date': '2016-05-01', 'host_venue': 'V106963461', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2112635063', ['I4210109258']], ['A2119008670', ['I95457486']], ['A2338733046', ['I165368041']], ['A2010759395', ['I95457486']]], 'cited_by_count': 5072, 'concepts': [['C107397762', '0.74051464'], ['C2780008327', '0.72997737'], ['C41008148', '0.6469559'], ['C2780513914', '0.6254077'], ['C67705224', '0.590553']], 'referenced_works': ['W1990293955', 'W1999574084', 'W2001798854', 'W2010361633', 'W2025943989', 'W2041391522', 'W2064397275', 'W2065128082', 'W2110417468', 'W2121211805', 'W2128009724', 'W2134526812', 'W2138773756', 'W2140729960', 'W2141458291', 'W2154431984', 'W2155705877'], 'abstract': 'We present kallisto, an RNA-seq quantification program that is two orders of magnitude faster than previous approaches and achieves similar accuracy. Kallisto pseudoaligns reads to a reference, producing a list of transcripts that are compatible with each read while avoiding alignment of individual bases. We use kallisto to analyze 30 million unaligned paired-end RNA-seq reads in <10 min on a standard laptop computer. This removes a major computational bottleneck in RNA-seq analysis.', 'counts_by_year': [[2022, 907], [2021, 1271], [2020, 1088], [2019, 820], [2018, 547], [2017, 323], [2016, 106], [2015, 4]]}, {'id': 'W2899760200', 'doi': 'https://doi.org/10.1093/nar/gky1106', 'title': 'The PRIDE database and related tools and resources in 2019: improving support for quantification data', 'type': 'journal-article', 'publication_date': '2019-01-08', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A1712178497', ['I1303153112']], ['A2049020796', ['I1303153112']], ['A2899711564', ['I1303153112']], ['A2398146675', ['I1303153112']], ['A2899777312', ['I1303153112']], ['A2900161042', ['I1303153112']], ['A2899998360', ['I1303153112']], ['A2159810253', ['I76134821', 'I1303153112']], ['A2717130303', ['I904495901']], ['A2122903861', ['I904495901']], ['A2899847125', ['I1303153112']], ['A2036931561', ['I904495901']], ['A2000385303', ['I8087733']], ['A209574460', ['I8087733']], ['A2894424682', ['I4210150093']], ['A2900083484', ['I4210150093']], ['A2223602229', ['I4210150093']], ['A273759975', ['I4210112713']], ['A2160114070', ['I1303153112']], ['A2693539585', ['I1303153112']], ['A1977635502', ['I1303153112']], ['A2134222363', ['I1303153112']], ['A2137679054', ['I1303153112']]], 'cited_by_count': 4921, 'concepts': [['C86803240', '0.92915404'], ['C77088390', '0.48887414'], ['C2779728303', '0.4648024'], ['C70721500', '0.42436376'], ['C2522767166', '0.3491094']], 'referenced_works': ['W1852508338', 'W1964336313', 'W1964904043', 'W1972247888', 'W1981593008', 'W1992436488', 'W2001473974', 'W2031095666', 'W2057761395', 'W2104764521', 'W2129390057', 'W2135581618', 'W2150457655', 'W2167089719', 'W2329614361', 'W2329674354', 'W2465480251', 'W2495112941', 'W2499626313', 'W2525035644', 'W2537132940', 'W2544360569', 'W2557530941', 'W2557925011', 'W2571859373', 'W2604622795', 'W2613847904', 'W2618709112', 'W2742576808', 'W2748545323', 'W2750561282', 'W2751436602', 'W2765745983', 'W2769591387', 'W2793610094', 'W2807198497', 'W2952666313', 'W4250359879', 'W4256316091'], 'abstract': 'The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world’s largest data repository of mass spectrometry-based proteomics data, and is one of the founding members of the global ProteomeXchange (PX) consortium. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update manuscript was published in Nucleic Acids Research in 2016. In the last 3\xa0years, public data sharing through PRIDE (as part of PX) has definitely become the norm in the field. In parallel, data re-use of public proteomics data has increased enormously, with multiple applications. We first describe the new architecture of PRIDE Archive, the archival component of PRIDE. PRIDE Archive and the related data submission framework have been further developed to support the increase in submitted data volumes and additional data types. A new scalable and fault tolerant storage backend, Application Programming Interface and web interface have been implemented, as a part of an ongoing process. Additionally, we emphasize the improved support for quantitative proteomics data through the mzTab format. At last, we outline key statistics on the current data contents and volume of downloads, and how PRIDE data are starting to be disseminated to added-value resources including Ensembl, UniProt and Expression Atlas.', 'counts_by_year': [[2022, 1070], [2021, 1955], [2020, 1385], [2019, 498], [2018, 6], [2017, 1]]}, {'id': 'W2290883490', 'doi': 'https://doi.org/10.1016/j.advengsoft.2016.01.008', 'title': 'The Whale Optimization Algorithm', 'type': 'journal-article', 'publication_date': '2016-05-01', 'host_venue': 'V16540516', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A3006208000', ['I2188195594']], ['A2166930145', ['I11701301']]], 'cited_by_count': 4842, 'concepts': [['C2777704720', '0.86450684'], ['C2987595161', '0.52650136'], ['C41008148', '0.51512235'], ['C11413529', '0.49212593'], ['C126255220', '0.37672234']], 'referenced_works': ['W1595159159', 'W1967402817', 'W1968370297', 'W1972462693', 'W1974915966', 'W1975510300', 'W1975661701', 'W1977065655', 'W1980246362', 'W1982452260', 'W1983362686', 'W1984130668', 'W1984762043', 'W1985334587', 'W1985460844', 'W1990966828', 'W1992656046', 'W1993885071', 'W1997600725', 'W1999284878', 'W2001422417', 'W2003751475', 'W2006694777', 'W2007351764', 'W2008166887', 'W2018495917', 'W2019611577', 'W2024060531', 'W2025500642', 'W2027996967', 'W2034988449', 'W2037960630', 'W2038984515', 'W2039577332', 'W2039595530', 'W2041629655', 'W2050927287', 'W2056811412', 'W2057633737', 'W2061001542', 'W2061438946', 'W2065401134', 'W2072955302', 'W2081730441', 'W2090884011', 'W2091638274', 'W2093195672', 'W2093646596', 'W2094611346', 'W2100499569', 'W2104670598', 'W2108179244', 'W2108752612', 'W2110674283', 'W2111393363', 'W2119802261', 'W2125281549', 'W2135879356', 'W2138537392', 'W2145479420', 'W2147271386', 'W2149815769', 'W2154061444', 'W2154756201', 'W2154943049', 'W2167580870', 'W2168081761', 'W2169934304', 'W2316286409', 'W4246598646', 'W4250503569', 'W4292083457'], 'abstract': 'The Whale Optimization Algorithm inspired by humpback whales is proposed.The WOA algorithm is benchmarked on 29 well-known test functions.The results on the unimodal functions show the superior exploitation of WOA.The exploration ability of WOA is confirmed by the results on multimodal functions.The results on structural design problems confirm the performance of WOA in practice. This paper proposes a novel nature-inspired meta-heuristic optimization algorithm, called Whale Optimization Algorithm (WOA), which mimics the social behavior of humpback whales. The algorithm is inspired by the bubble-net hunting strategy. WOA is tested with 29 mathematical optimization problems and 6 structural design problems. Optimization results prove that the WOA algorithm is very competitive compared to the state-of-art meta-heuristic algorithms as well as conventional methods. The source codes of the WOA algorithm are publicly available at http://www.alimirjalili.com/WOA.html', 'counts_by_year': [[2022, 1449], [2021, 1368], [2020, 930], [2019, 592], [2018, 311], [2017, 137], [2016, 27]]}, {'id': 'W2607129810', 'doi': 'https://doi.org/10.1093/nar/gkx247', 'title': 'GEPIA: a web server for cancer and normal gene expression profiling and interactive analyses', 'type': 'journal-article', 'publication_date': '2017-07-03', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2723801666', ['I20231570']], ['A2630712196', ['I4210160507']], ['A2673676330', ['I20231570']], ['A2130220720', ['I20231570']], ['A3126157780', ['I20231570']], ['A2183341720', ['I20231570']]], 'cited_by_count': 4805, 'concepts': [['C187191949', '0.52778006'], ['C86803240', '0.46101892'], ['C161078062', '0.44919822'], ['C18431079', '0.42362335'], ['C41008148', '0.4093949']], 'referenced_works': ['W1533942137', 'W1571917377', 'W1981509058', 'W1987572677', 'W2012034410', 'W2018838463', 'W2101780023', 'W2102889733', 'W2121158863', 'W2130951034', 'W2146749911', 'W2147109452', 'W2148130433', 'W2158485828', 'W2161446794', 'W2411861063', 'W2552653135', 'W2570481179'], 'abstract': 'Tremendous amount of RNA sequencing data have been produced by large consortium projects such as TCGA and GTEx, creating new opportunities for data mining and deeper understanding of gene functions. While certain existing web servers are valuable and widely used, many expression analysis functions needed by experimental biologists are still not adequately addressed by these tools. We introduce GEPIA (Gene Expression Profiling Interactive Analysis), a web-based tool to deliver fast and customizable functionalities based on TCGA and GTEx data. GEPIA provides key interactive and customizable functions including differential expression analysis, profiling plotting, correlation analysis, patient survival analysis, similar gene detection and dimensionality reduction analysis. The comprehensive expression analyses with simple clicking through GEPIA greatly facilitate data mining in wide research areas, scientific discussion and the therapeutic discovery process. GEPIA fills in the gap between cancer genomics big data and the delivery of integrated information to end users, thus helping unleash the value of the current data resources. GEPIA is available at http://gepia.cancer-pku.cn/.', 'counts_by_year': [[2022, 1252], [2021, 1533], [2020, 1184], [2019, 611], [2018, 206], [2017, 16]]}, {'id': 'W2537623931', 'doi': 'https://doi.org/10.1093/nar/gkw937', 'title': 'The STRING database in 2017: quality-controlled protein–protein association networks, made broadly accessible', 'type': 'journal-article', 'publication_date': '2017-01-04', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A1964902353', ['I12708293']], ['A2612353087', ['I180670191']], ['A2321541527', ['I2801134892']], ['A2789220638', ['I4210153543']], ['A1441202580', ['I12708293']], ['A1993364599', ['I12708293']], ['A2638971434', ['I2801134892']], ['A1992942017', ['I2801134892']], ['A2156011990', ['I12708293']], ['A2068107358', ['I4210138560']], ['A2122923977', ['I2801134892']], ['A71748352', ['I12708293']]], 'cited_by_count': 4718, 'concepts': [['C157486923', '0.7604834'], ['C86803240', '0.6134514'], ['C11804247', '0.4671302'], ['C14036430', '0.4646698'], ['C77088390', '0.45961517']], 'referenced_works': ['W1700570096', 'W1969327849', 'W1974156712', 'W1991832623', 'W1995683190', 'W2000860171', 'W2008213051', 'W2015059765', 'W2043764521', 'W2052968292', 'W2053039860', 'W2057545775', 'W2062263714', 'W2064585235', 'W2070040136', 'W2079818352', 'W2081931663', 'W2084709428', 'W2093186233', 'W2095387858', 'W2095403534', 'W2096173332', 'W2102221598', 'W2105924489', 'W2107644337', 'W2111514154', 'W2113613985', 'W2115746733', 'W2116443392', 'W2125118217', 'W2128517432', 'W2137531873', 'W2140883620', 'W2145506897', 'W2152274187', 'W2152871646', 'W2154915765', 'W2158197251', 'W2159675211', 'W2159996681', 'W2162151166', 'W2164813401', 'W2169589032', 'W2171030481', 'W2171631429', 'W2172574008', 'W2223119478', 'W2223752691', 'W2263739890', 'W2335512083', 'W2344319046', 'W2511125937', 'W2739999456', 'W2915975108', 'W4230962320', 'W4232638017', 'W4239055561'], 'abstract': 'A system-wide understanding of cellular function requires knowledge of all functional interactions between the expressed proteins. The STRING database aims to collect and integrate this information, by consolidating known and predicted protein-protein association data for a large number of organisms. The associations in STRING include direct (physical) interactions, as well as indirect (functional) interactions, as long as both are specific and biologically meaningful. Apart from collecting and reassessing available experimental data on protein-protein interactions, and importing known pathways and protein complexes from curated databases, interaction predictions are derived from the following sources: (i) systematic co-expression analysis, (ii) detection of shared selective signals across genomes, (iii) automated text-mining of the scientific literature and (iv) computational transfer of interaction knowledge between organisms based on gene orthology. In the latest version 10.5 of STRING, the biggest changes are concerned with data dissemination: the web frontend has been completely redesigned to reduce dependency on outdated browser technologies, and the database can now also be queried from inside the popular Cytoscape software framework. Further improvements include automated background analysis of user inputs for functional enrichments, and streamlined download options. The STRING resource is available online, at http://string-db.org/.', 'counts_by_year': [[2022, 648], [2021, 976], [2020, 1093], [2019, 1095], [2018, 771], [2017, 127], [2016, 4]]}, {'id': 'W2549139847', 'doi': 'https://doi.org/10.1109/cvpr.2017.634', 'title': 'Aggregated Residual Transformations for Deep Neural Networks', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2551560347', ['I36258959']], ['A2473549963', ['I2252078561']], ['A1944499404', ['I2252078561']], ['A2134014566', ['I36258959']], ['A2164292938', ['I2252078561']]], 'cited_by_count': 4703, 'concepts': [['C87117476', '0.8777781'], ['C41008148', '0.6789567'], ['C177264268', '0.67235965'], ['C33676613', '0.59874046'], ['C2780586882', '0.58770394']], 'referenced_works': ['W1536680647', 'W1677182931', 'W1903029394', 'W1996901117', 'W2097117768', 'W2102605133', 'W2117539524', 'W2147800946', 'W2151103935', 'W2161969291', 'W2183341477', 'W2194775991', 'W2288122362', 'W2964137095'], 'abstract': 'We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.', 'counts_by_year': [[2022, 744], [2021, 1515], [2020, 1210], [2019, 836], [2018, 344], [2017, 44], [2016, 2]]}, {'id': 'W2951912016', 'doi': 'https://doi.org/10.1093/bioinformatics/bty560', 'title': 'fastp: an ultra-fast all-in-one FASTQ preprocessor', 'type': 'journal-article', 'publication_date': '2018-09-01', 'host_venue': 'V52395412', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2890539751', ['I4210145761']], ['A2605569151', ['I4210086166']], ['A2793287426', ['I4210086166']], ['A2105125679', ['I4210145761']]], 'cited_by_count': 4687, 'concepts': [['C41008148', '0.864285'], ['C34736171', '0.80347705'], ['C519991488', '0.7506256'], ['C56951928', '0.72227436'], ['C177284502', '0.62716794']], 'referenced_works': ['W1680591717', 'W2011645981', 'W2036897871', 'W2103441770', 'W2108234281', 'W2131271579', 'W2152956782', 'W2170551349', 'W2329906190', 'W2578418759', 'W2598066557', 'W2774072836', 'W2952109315'], 'abstract': 'Quality control and preprocessing of FASTQ files are essential to providing clean data for downstream analysis. Traditionally, a different tool is used for each operation, such as quality control, adapter trimming and quality filtering. These tools are often insufficiently fast as most are developed using high-level programming languages (e.g. Python and Java) and provide limited multi-threading support. Reading and loading data multiple times also renders preprocessing slow and I/O inefficient.We developed fastp as an ultra-fast FASTQ preprocessor with useful quality control and data-filtering features. It can perform quality control, adapter trimming, quality filtering, per-read quality pruning and many other operations with a single scan of the FASTQ data. This tool is developed in C++ and has multi-threading support. Based on our evaluation, fastp is 2-5 times faster than other FASTQ preprocessing tools such as Trimmomatic or Cutadapt despite performing far more operations than similar tools.The open-source code and corresponding instructions are available at https://github.com/OpenGene/fastp.', 'counts_by_year': [[2022, 2117], [2021, 1614], [2020, 732], [2019, 188], [2018, 13], [2017, 1], [2016, 1]]}, {'id': 'W1885185971', 'doi': 'https://doi.org/10.1109/tpami.2015.2439281', 'title': 'Image Super-Resolution Using Deep Convolutional Networks', 'type': 'journal-article', 'publication_date': '2016-02-01', 'host_venue': 'V199944782', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2609354386', ['I177725633']], ['A2123758277', ['I177725633']], ['A2164292938', ['I4210113369']], ['A2166284823', ['I177725633']]], 'cited_by_count': 4643, 'concepts': [['C41008148', '0.84863925'], ['C81363708', '0.81599474'], ['C154945302', '0.7938075'], ['C108583219', '0.71025044'], ['C153180895', '0.56566596']], 'referenced_works': ['W1580389772', 'W1949096787', 'W1950594372', 'W1973567017', 'W1976416062', 'W2003749430', 'W2010070459', 'W2037642501', 'W2087380704', 'W2088254198', 'W2097074225', 'W2112796928', 'W2118963448', 'W2121058967', 'W2121927366', 'W2133665775', 'W2142884912', 'W2144468361', 'W2147800946', 'W2149760002', 'W2150081556', 'W2154815154', 'W2155893237', 'W2156547346', 'W2160547390', 'W2164551808', 'W2534320940'], 'abstract': 'We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.', 'counts_by_year': [[2022, 603], [2021, 1079], [2020, 1033], [2019, 903], [2018, 595], [2017, 320], [2016, 97], [2015, 2]]}, {'id': 'W2559588208', 'doi': 'https://doi.org/10.1093/nar/gkw1092', 'title': 'KEGG: new perspectives on genomes, pathways, diseases and drugs', 'type': 'journal-article', 'publication_date': '2017-01-04', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2151300008', ['I22299242']], ['A143568440', ['I22299242']], ['A2168774161', ['I22299242']], ['A2310244469', ['I2252096349']], ['A2328909099', ['I22299242']]], 'cited_by_count': 4642, 'concepts': [['C152724338', '0.9891751'], ['C141231307', '0.6938468'], ['C86803240', '0.6684059'], ['C148863701', '0.60243374'], ['C70721500', '0.56219596']], 'referenced_works': ['W1969404643', 'W1977025277', 'W1983922226', 'W2032936332', 'W2110256992', 'W2142678031', 'W2165674132', 'W2171437346', 'W2173732482', 'W2178043251', 'W2739999456', 'W4255659575'], 'abstract': 'KEGG (http://www.kegg.jp/ or http://www.genome.jp/kegg/) is an encyclopedia of genes and genomes. Assigning functional meanings to genes and genomes both at the molecular and higher levels is the primary objective of the KEGG database project. Molecular-level functions are stored in the KO (KEGG Orthology) database, where each KO is defined as a functional ortholog of genes and proteins. Higher-level functions are represented by networks of molecular interactions, reactions and relations in the forms of KEGG pathway maps, BRITE hierarchies and KEGG modules. In the past the KO database was developed for the purpose of defining nodes of molecular networks, but now the content has been expanded and the quality improved irrespective of whether or not the KOs appear in the three molecular network databases. The newly introduced addendum category of the GENES database is a collection of individual proteins whose functions are experimentally characterized and from which an increasing number of KOs are defined. Furthermore, the DISEASE and DRUG databases have been improved by systematic analysis of drug labels for better integration of diseases and drugs with the KEGG molecular networks. KEGG is moving towards becoming a comprehensive knowledge base for both functional interpretation and practical application of genomic information.', 'counts_by_year': [[2022, 735], [2021, 1021], [2020, 896], [2019, 1002], [2018, 711], [2017, 270], [2016, 2], [2014, 1]]}, {'id': 'W3136918052', 'doi': 'https://doi.org/10.1093/nar/gky1049', 'title': 'UniProt: a worldwide hub of protein knowledge', 'type': 'journal-article', 'publication_date': '2019-01-01', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2120643991', []]], 'cited_by_count': 4567, 'concepts': [['C202264299', '0.9849738'], ['C86803240', '0.718063'], ['C104397665', '0.70625377'], ['C206345919', '0.5603779'], ['C174183944', '0.49282673']], 'referenced_works': ['W1935434993', 'W1995683190', 'W2021210077', 'W2022484447', 'W2022770379', 'W2053551770', 'W2064795263', 'W2088382887', 'W2102461176', 'W2115746733', 'W2120141270', 'W2127589936', 'W2288906059', 'W2316262244', 'W2324589236', 'W2403990263', 'W2472887612', 'W2520925527', 'W2525128349', 'W2528456062', 'W2559577985', 'W2604823638', 'W2604973964', 'W2752636837', 'W2766822634', 'W2787247269', 'W2791349417', 'W2952894920', 'W2952944698', 'W4210702584', 'W4250359879', 'W4252710376'], 'abstract': 'The UniProt Knowledgebase is a collection of sequences and annotations for over 120 million proteins across all branches of life. Detailed annotations extracted from the literature by expert curators have been collected for over half a million of these proteins. These annotations are supplemented by annotations provided by rule based automated systems, and those imported from other resources. In this article we describe significant updates that we have made over the last 2 years to the resource. We have greatly expanded the number of Reference Proteomes that we provide and in particular we have focussed on improving the number of viral Reference Proteomes. The UniProt website has been augmented with new data visualizations for the subcellular localization of proteins as well as their structure and interactions. UniProt resources are available under a CC-BY (4.0) license via the web at https://www.uniprot.org/.', 'counts_by_year': [[2022, 804], [2021, 1734], [2020, 1569], [2019, 450], [2018, 4], [2017, 1]]}, {'id': 'W2592811885', 'doi': 'https://doi.org/10.1038/nmeth.4197', 'title': 'Salmon provides fast and bias-aware quantification of transcript expression', 'type': 'journal-article', 'publication_date': '2017-04-01', 'host_venue': 'V127827428', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2123906542', ['I59553526']], ['A1973757372', ['I4210130836']], ['A2514924063', ['I4210117453']], ['A2047086083', ['I136199984']], ['A2236568161', ['I74973139']]], 'cited_by_count': 4539, 'concepts': [['C2776214188', '0.6372054'], ['C77077793', '0.5694426'], ['C90559484', '0.5631295'], ['C41008148', '0.5541557'], ['C162317418', '0.5458981']], 'referenced_works': ['W1847439710', 'W1897931464', 'W1967220865', 'W1990293955', 'W2022097310', 'W2025943989', 'W2062872865', 'W2064397275', 'W2065128082', 'W2093253451', 'W2100779682', 'W2107564978', 'W2112876600', 'W2129916363', 'W2138773756', 'W2158485828', 'W2165446840', 'W2170551349', 'W2170624222', 'W2264449466', 'W2323326409', 'W2340210804', 'W2417621096', 'W2952944477', 'W3105872783'], 'abstract': 'We introduce Salmon, a lightweight method for quantifying transcript abundance from RNA-seq reads. Salmon combines a new dual-phase parallel inference algorithm and feature-rich bias models with an ultra-fast read mapping procedure. It is the first transcriptome-wide quantifier to correct for fragment GC-content bias, which, as we demonstrate here, substantially improves the accuracy of abundance estimates and the sensitivity of subsequent differential expression analysis.', 'counts_by_year': [[2022, 1093], [2021, 1354], [2020, 996], [2019, 654], [2018, 323], [2017, 111], [2016, 1]]}, {'id': 'W2513506562', 'doi': 'https://doi.org/10.7717/peerj.2584', 'title': 'VSEARCH: a versatile open source tool for metagenomics', 'type': 'journal-article', 'publication_date': '2016-10-18', 'host_venue': 'V1983995261', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A49387952', ['I184942183', 'I1281400175']], ['A2950693761', ['I71030271', 'I102335020']], ['A2157046855', ['I7882870']], ['A1097953758', ['I7882870', 'I39555362']], ['A2114945017', ['I153267046']]], 'cited_by_count': 4444, 'concepts': [['C41008148', '0.7967838'], ['C184898388', '0.6118194'], ['C15151743', '0.563712'], ['C43126263', '0.5534607'], ['C167927819', '0.5109405']], 'referenced_works': ['W1846216853', 'W1988925586', 'W1995135945', 'W1999167944', 'W2015915218', 'W2034285706', 'W2057970678', 'W2072970694', 'W2074231493', 'W2088833470', 'W2094890728', 'W2096885696', 'W2103441770', 'W2105368763', 'W2106399600', 'W2108718991', 'W2117608012', 'W2121911035', 'W2124351063', 'W2128711701', 'W2128769815', 'W2132548846', 'W2136879569', 'W2158714788', 'W2160209604', 'W2162667876', 'W2165654401', 'W2166706771', 'W2343050242', 'W2515231729', 'W2916086000', 'W4235169531', 'W4244030505', 'W4247991034', 'W4255011371'], 'abstract': 'Background VSEARCH is an open source and free of charge multithreaded 64-bit tool for processing and preparing metagenomics, genomics and population genomics nucleotide sequence data. It is designed as an alternative to the widely used USEARCH tool (Edgar, 2010) for which the source code is not publicly available, algorithm details are only rudimentarily described, and only a memory-confined 32-bit version is freely available for academic use. Methods When searching nucleotide sequences, VSEARCH uses a fast heuristic based on words shared by the query and target sequences in order to quickly identify similar sequences, a similar strategy is probably used in USEARCH. VSEARCH then performs optimal global sequence alignment of the query against potential target sequences, using full dynamic programming instead of the seed-and-extend heuristic used by USEARCH. Pairwise alignments are computed in parallel using vectorisation and multiple threads. Results VSEARCH includes most commands for analysing nucleotide sequences available in USEARCH version 7 and several of those available in USEARCH version 8, including searching (exact or based on global alignment), clustering by similarity (using length pre-sorting, abundance pre-sorting or a user-defined order), chimera detection (reference-based or de novo ), dereplication (full length or prefix), pairwise alignment, reverse complementation, sorting, and subsampling. VSEARCH also includes commands for FASTQ file processing, i.e., format detection, filtering, read quality statistics, and merging of paired reads. Furthermore, VSEARCH extends functionality with several new commands and improvements, including shuffling, rereplication, masking of low-complexity sequences with the well-known DUST algorithm, a choice among different similarity definitions, and FASTQ file format conversion. VSEARCH is here shown to be more accurate than USEARCH when performing searching, clustering, chimera detection and subsampling, while on a par with USEARCH for paired-ends read merging. VSEARCH is slower than USEARCH when performing clustering and chimera detection, but significantly faster when performing paired-end reads merging and dereplication. VSEARCH is available at https://github.com/torognes/vsearch under either the BSD 2-clause license or the GNU General Public License version 3.0. Discussion VSEARCH has been shown to be a fast, accurate and full-fledged alternative to USEARCH. A free and open-source versatile tool for sequence analysis is now available to the metagenomics community.', 'counts_by_year': [[2022, 1000], [2021, 1193], [2020, 1025], [2019, 672], [2018, 389], [2017, 148], [2016, 8], [2015, 1]]}, {'id': 'W2593511418', 'doi': 'https://doi.org/10.1038/nmeth.4193', 'title': 'MotionCor2: anisotropic correction of beam-induced motion for improved cryo-electron microscopy', 'type': 'journal-article', 'publication_date': '2017-04-01', 'host_venue': 'V127827428', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2114473139', ['I180670191']], ['A2404548112', ['I180670191']], ['A686834412', ['I180670191']], ['A2568084669', ['I180670191']], ['A2232298264', ['I1344073410']], ['A105410181', ['I1344073410']]], 'cited_by_count': 4437, 'concepts': [['C20702342', '0.7131537'], ['C93877712', '0.6822118'], ['C147080431', '0.6151696'], ['C85725439', '0.51124525'], ['C192562407', '0.5015149']], 'referenced_works': ['W1978288593', 'W2039175603', 'W2051925784', 'W2148941629', 'W2149360962', 'W2160559764', 'W2164659462', 'W2963188324'], 'abstract': 'MotionCor2 software corrects for beam-induced sample motion, improving the resolution of cryo-EM reconstructions.', 'counts_by_year': [[2022, 938], [2021, 1160], [2020, 1049], [2019, 697], [2018, 459], [2017, 130], [2016, 2]]}, {'id': 'W2900756811', 'doi': 'https://doi.org/10.1080/20013078.2018.1535750', 'title': 'Minimal information for studies of extracellular vesicles 2018 (MISEV2018): a position statement of the International Society for Extracellular Vesicles and update of the MISEV2014 guidelines', 'type': 'journal-article', 'publication_date': '2018-11-23', 'host_venue': 'V2764464234', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2154001821', ['I2746051580']], ['A2094399063', ['I145311948']], ['A2043704745', ['I136199984']], ['A2436089810', ['I16097986']], ['A2100930509', ['I84218800']], ['A33727202', ['I49451733']], ['A2901035154', ['I135140700']], ['A2904424552', ['I7171862']], ['A2171490403', ['I203339264']], ['A2268556282', ['I196829312']], ['A2901725965', ['I163902620']], ['A2901391197', []], ['A2901625575', ['I180923762']], ['A1553789921', ['I11750234']], ['A818618364', ['I136199984']], ['A1176397360', ['I197854408']], ['A2167670540', ['I83809506']], ['A2151764713', ['I196829312']], ['A2231385923', ['I114017466']], ['A2300960967', ['I5388228']], ['A2489810026', []], ['A2559776050', ['I43406934']], ['A2740299544', []], ['A121785252', ['I79940851']], ['A2397119635', ['I79619799']], ['A2123988880', ['I154130895']], ['A1415210023', ['I126596746']], ['A2461504338', ['I43406934']], ['A2915061085', []], ['A2262587082', ['I197604219']], ['A2113144296', ['I123044942']], ['A2911536219', []], ['A725913459', ['I154526488']], ['A1327244325', ['I136199984']], ['A2071998390', ['I98704320']], ['A2156877766', ['I97188460']], ['A300067860', ['I52357470']], ['A2071009854', ['I15057530']], ['A1900688000', ['I121797337']], ['A2291376492', ['I1334819555']], ['A2510534716', ['I187531555']], ['A2145610329', ['I59130452']], ['A2160877471', ['I98677209']], ['A2918393143', ['I153718931']], ['A2482487171', ['I79940851']], ['A2803503629', ['I62916508']], ['A2619150304', ['I55143463']], ['A2004943898', ['I7597260']], ['A2900907479', ['I27837315']], ['A1906947262', ['I55143463']], ['A2894208647', ['I124261462']], ['A2435464650', ['I196829312']], ['A2253934438', ['I154130895']], ['A2901109381', ['I16733864']], ['A2116171525', ['I145311948']], ['A2603537575', ['I25846049']], ['A2153095330', ['I315704651']], ['A2122912677', ['I196829312']], ['A2886350228', ['I1282927834']], ['A2570308311', ['I79510175']], ['A2023840503', ['I181391015']], ['A2911208098', ['I79510175']], ['A2188114632', ['I52357470']], ['A2250505815', ['I200719446']], ['A79576376', ['I182534213']], ['A2044391580', ['I40120149']], ['A2070107086', ['I887064364']], ['A2057749980', ['I142263535']], ['A1977853798', ['I881427289']], ['A3021476484', ['I17974374']], ['A1577102918', ['I107639228']], ['A3191320236', ['I136199984']], ['A2620230052', []], ['A2912816631', []], ['A105118834', ['I32597200']], ['A2130996809', []], ['A2474857123', ['I95674353']], ['A2668750855', ['I68522396']], ['A2109397008', ['I169199633']], ['A2319339645', ['I32597200']], ['A3181206572', ['I1282927834']], ['A1973486779', ['I35440088']], ['A307870814', ['I26415053']], ['A2121640800', ['I24354313']], ['A2112798047', ['I122346577']], ['A2506006738', ['I98381234']], ['A2614860660', ['I193662353']], ['A2472309363', []], ['A2277216900', ['I5023651']], ['A2607580104', ['I86467917']], ['A2120660079', ['I881427289']], ['A2590999576', ['I28166907']], ['A2912450050', []], ['A2044852885', ['I51556381']], ['A1997513518', ['I110594554']], ['A2109109031', ['I17974374']], ['A2335955310', ['I185261750']], ['A787982954', ['I921990950']], ['A2586240693', ['I101202996']], ['A2900912740', []], ['A2409936079', ['I17937529']], ['A2115255367', ['I91712215']], ['A1974796517', ['I28166907']], ['A2330945279', ['I123044942']], ['A2667148135', ['I45129253']], ['A2905663196', []], ['A2524220442', ['I68947357']], ['A2078895245', ['I123900574']], ['A2031196290', ['I62318514']], ['A2230950799', ['I43406934']], ['A1227966329', []], ['A1907210143', ['I26415053']], ['A1353956899', ['I40120149']], ['A2084899639', ['I28166907']], ['A2017736393', ['I18014758']], ['A2096033823', ['I196829312']], ['A2226490857', ['I74656192']], ['A1483385220', []], ['A2142575042', ['I1925986']], ['A2767592937', ['I185261750']], ['A1902055844', ['I891191580']], ['A1422616303', ['I166722992']], ['A2106893202', ['I79619799']], ['A95463702', ['I101202996']], ['A2923822565', ['I32597200']], ['A2103319164', ['I196829312']], ['A2139536562', ['I36258959']], ['A2169978360', ['I16038530']], ['A2073320175', ['I47508984']], ['A1786176770', ['I159176309']], ['A1446979897', ['I878454856']], ['A2264792422', ['I59130452']], ['A2352096062', ['I58200834']], ['A2139549371', []], ['A2096711698', ['I91136226']], ['A2433089434', ['I1282927834']], ['A306560493', ['I111088046']], ['A2052900678', ['I141584323']], ['A2089248912', ['I67581229']], ['A2901225727', ['I17937529']], ['A2885496841', ['I142263535']], ['A2424305997', ['I124055696']], ['A2037209745', ['I66946132']], ['A2148646499', ['I1330342723']], ['A2060821130', []], ['A2510586807', ['I196829312']], ['A2155963664', ['I28407311']], ['A23398490', ['I1299303238']], ['A2134710105', ['I1174212']], ['A2050881898', ['I1301076528']], ['A2136293146', []], ['A2172828790', ['I1343551460']], ['A2802857238', ['I145311948']], ['A2127117575', ['I1299303238']], ['A2591696617', ['I54696188']], ['A2120776137', ['I27837315']], ['A2901882853', ['I101202996']], ['A2901214982', ['I29607241']], ['A2179250044', ['I166722992']], ['A677745632', ['I184942183']], ['A2703466643', ['I35928602']], ['A701947391', ['I185261750']], ['A200104633', ['I170897317']], ['A1871788739', ['I12097938']], ['A2800035581', ['I91712215']], ['A2008623967', ['I4068193']], ['A2115719796', ['I101202996']], ['A2033461250', ['I197323543']], ['A2324200281', ['I159176309']], ['A3049750575', ['I2802600346']], ['A2069952044', ['I54594937']], ['A2170216974', ['I56590836']], ['A2522603607', ['I878454856']], ['A2243532185', ['I2801434414']], ['A2511694245', ['I63135867']], ['A2283531837', ['I149251103']], ['A1852364043', ['I51556381']], ['A2109988388', ['I881427289']], ['A2102895623', ['I36258959']], ['A3056984877', ['I2746051580']], ['A318270668', ['I105036370']], ['A2305516301', ['I49451733']], ['A2288342274', ['I21028721']], ['A2901728607', ['I115228651']], ['A2903870934', ['I52418104']], ['A2298503221', ['I153976015']], ['A2621559088', ['I36258959']], ['A2136760772', ['I141945490']], ['A2505533724', ['I59130452']], ['A3037774022', ['I241749']], ['A2486901747', ['I101202996']], ['A2161117269', ['I56590836']], ['A2440374422', ['I115228651']], ['A2179511157', ['I141777705']], ['A252507733', ['I74656192']], ['A2150130648', ['I184942183']], ['A2115794449', ['I95674353']], ['A2644164129', ['I193662353']], ['A2117023497', ['I101202996']], ['A1973941439', ['I881427289']], ['A2888498369', ['I26092322']], ['A2182008869', ['I205274468']], ['A2908276638', ['I154526488']], ['A2726652546', ['I136199984']], ['A1981905410', ['I99542240']], ['A811230576', ['I4432739']], ['A2141436896', ['I193662353']], ['A2322349317', ['I1330342723']], ['A2462936018', ['I16097986']], ['A2754486002', ['I189158943']], ['A2901093704', ['I1282927834']], ['A2045361157', []], ['A1975679947', ['I2746051580']], ['A2955464157', ['I49451733']], ['A2121763758', []], ['A2903823923', ['I2746051580']], ['A2152826247', ['I196829312']], ['A2001007184', ['I881427289']], ['A2240062852', ['I1174212']], ['A2312043420', ['I185261750']], ['A2001110058', ['I103163165']], ['A2112911793', ['I177877127']], ['A1976855447', ['I68522396']], ['A305203847', []], ['A2303319082', ['I217937681']], ['A2605455608', ['I891191580']], ['A283959153', ['I1299303238']], ['A1929294634', ['I913958620']], ['A1975816777', ['I95674353']], ['A2015539895', ['I138689650']], ['A1830301743', ['I51556381']], ['A2902709868', ['I62916508']], ['A2037217835', ['I145311948']], ['A2016119487', ['I26092322']], ['A1550721559', ['I27804330']], ['A2434025488', ['I881427289']], ['A2028213427', ['I17937529']], ['A2308543335', ['I204337017']], ['A2118819925', ['I1294671590']], ['A1994112208', ['I108290504']], ['A2096444713', ['I887064364']], ['A260576086', ['I122140584']], ['A2128930939', []], ['A2416490382', ['I193662353']], ['A1970532654', ['I1299303238']], ['A2291789302', ['I205274468']], ['A2684321864', ['I157674565']], ['A2077873555', ['I166337079']], ['A2048582882', ['I54696188']], ['A2134701842', ['I5023651']], ['A2288782178', ['I63634437']], ['A2097898479', ['I170201317']], ['A178816727', ['I105036370']], ['A2910013425', ['I101527212']], ['A2525680774', ['I24354313']], ['A2918370852', ['I123900574']], ['A2915678026', ['I887064364']], ['A2149861752', ['I2801357902']], ['A1227753465', []], ['A1966076528', ['I62916508']], ['A2291052436', ['I123431417']], ['A2295549191', ['I145872427']], ['A2619825130', ['I124261462']], ['A92813401', ['I170897317']], ['A89446530', ['I161103922']], ['A2767161970', ['I162148367']], ['A2047978331', ['I196829312']], ['A2575241938', ['I145311948']], ['A2148949073', ['I197604219']], ['A2246488226', ['I180670191']], ['A2954194105', ['I27804330']], ['A2098220925', ['I79940851']], ['A1937086207', ['I180670191']], ['A2106803088', ['I900890020']], ['A667933002', ['I5023651']], ['A2117599290', ['I52325']], ['A1997834357', ['I80043']], ['A2735009329', ['I27837315']], ['A2536031715', ['I53964585']], ['A2048173233', ['I159176309']], ['A2109422289', ['I130238516']], ['A2467971542', ['I1299303238']], ['A2900489945', ['I76903346']], ['A2112326310', []], ['A2140875013', ['I203339264']], ['A2157792115', ['I34352273']], ['A1262308624', ['I861853513']], ['A2565723515', ['I12097938']], ['A2913880529', []], ['A2547765908', ['I98704320']], ['A2752520251', ['I69737025']], ['A2474195524', []], ['A25591177', ['I165690674']], ['A2549199804', ['I31512782']], ['A2187174223', ['I193662353']], ['A2227924765', ['I185261750']], ['A2900645782', ['I145311948']], ['A2901829854', ['I36234482']], ['A2565238271', []], ['A2198608145', ['I161318765']], ['A2900806577', ['I105036370']], ['A2170311281', ['I202419968']], ['A2047211151', ['I881427289']], ['A2153349805', ['I91045830']], ['A1998543896', ['I42570316']], ['A2007610716', ['I133731052']], ['A2344722228', ['I182534213']], ['A2958200129', ['I80043']], ['A2900537082', ['I189590672']], ['A2115855099', ['I52325']], ['A258128127', ['I101202996']], ['A1944695551', ['I165143802']], ['A2570727802', ['I86467917']], ['A1975761679', ['I204465549']], ['A1958055862', ['I193662353']], ['A2170990878', ['I136199984']], ['A2136040066', ['I181369854']], ['A2247156752', ['I154130895']], ['A1865571757', ['I113306721']], ['A2197638311', ['I27837315']], ['A2996764954', ['I28407311']], ['A2118995661', ['I161370692']], ['A2255547828', ['I196829312']], ['A1982878258', ['I2746051580']], ['A2076912293', ['I165932596']], ['A2310374898', ['I21491767']], ['A2617961485', ['I88273585']], ['A2043097276', ['I157536573']], ['A1531514297', ['I136199984']], ['A1982207750', ['I27483092']], ['A2304109625', ['I193662353']], ['A228935561', ['I193662353']], ['A2030651303', ['I193662353']], ['A2017148782', ['I32597200']], ['A1991220391', ['I193662353']], ['A1377833092', ['I1284308235']], ['A241519543', ['I154526488']], ['A2293213080', ['I913958620']], ['A2908302064', ['I1330342723']], ['A2166631290', ['I182534213']], ['A2760935119', ['I143302722']], ['A2467776500', ['I130442723']], ['A2237517600', ['I165779595']], ['A1968851431', ['I90183372']], ['A1967835655', ['I154526488']], ['A2595831273', ['I184942183']], ['A2901866140', ['I153718931']], ['A2763282473', ['I101202996']], ['A2159510724', ['I101202996']], ['A1984271659', ['I881427289']], ['A2151621584', ['I58956616']], ['A106438434', ['I193662353']], ['A2103249281', ['I200719446']], ['A2121020023', ['I79510175']], ['A2280807891', ['I132976966']], ['A2040192472', ['I25974101']], ['A2123946502', ['I111236770']], ['A2786634500', ['I1299303238']], ['A2884100400', ['I887968799']], ['A2118931786', ['I28166907']], ['A1768169293', ['I101202996']], ['A2901540408', ['I74656192']], ['A2141696505', ['I19820366']], ['A2901157915', ['I21370196']], ['A1698259060', ['I88273585']], ['A3143911545', ['I18014758']], ['A2303384214', ['I191208505']], ['A2809021555', ['I16097986']], ['A2106338581', ['I99065089']], ['A2344011636', ['I83019370']], ['A1400391669', ['I138689650']], ['A2647470141', ['I161318765']], ['A2964496652', ['I173212132']], ['A2901213725', ['I92039509']], ['A2612872356', ['I145311948']], ['A2718668895', ['I58200834']], ['A2016714366', ['I27837315']], ['A2807798236', ['I28166907']], ['A2116145372', ['I21491767']], ['A1992778721', ['I84218800']], ['A2942111734', []], ['A101043871', ['I126596746']]], 'cited_by_count': 4397, 'concepts': [['C20518536', '0.9127412'], ['C2992929900', '0.836743'], ['C14036430', '0.64635885'], ['C2908689518', '0.6394769'], ['C131934819', '0.59555525']], 'referenced_works': ['W21200892', 'W1506167085', 'W1511544167', 'W1564861304', 'W1571387405', 'W1574663134', 'W1596785172', 'W1607030744', 'W1682077723', 'W1769060942', 'W1787140000', 'W1793480972', 'W1807400738', 'W1811603389', 'W1835592430', 'W1839188703', 'W1864285629', 'W1872071454', 'W1877686344', 'W1888751811', 'W1914452730', 'W1917937260', 'W1959468401', 'W1964340735', 'W1965152917', 'W1965774359', 'W1966089788', 'W1969891160', 'W1971853030', 'W1974540586', 'W1975320047', 'W1975585079', 'W1975718300', 'W1977678686', 'W1978250223', 'W1978816264', 'W1979751563', 'W1985207716', 'W1985844219', 'W1988192362', 'W1989090719', 'W1992226521', 'W1992825075', 'W1996289020', 'W1997122898', 'W1998307755', 'W2001849319', 'W2003343932', 'W2004152924', 'W2004252622', 'W2007499410', 'W2008627178', 'W2009111405', 'W2009649000', 'W2010821717', 'W2012528990', 'W2014869316', 'W2016008005', 'W2018106032', 'W2020863352', 'W2021065026', 'W2021861291', 'W2024206172', 'W2032696674', 'W2033112925', 'W2033598310', 'W2036999916', 'W2037876980', 'W2039313867', 'W2039731107', 'W2039764357', 'W2040392478', 'W2040465412', 'W2041275674', 'W2044638539', 'W2048939659', 'W2051720243', 'W2053508438', 'W2054077259', 'W2055424631', 'W2055791162', 'W2056418246', 'W2057713753', 'W2060248677', 'W2062155172', 'W2063331767', 'W2063919942', 'W2072167135', 'W2072476260', 'W2073626888', 'W2074239306', 'W2076637458', 'W2078424563', 'W2078894965', 'W2079069744', 'W2080876621', 'W2084770914', 'W2086500857', 'W2087048469', 'W2088276922', 'W2089943045', 'W2093457236', 'W2093608913', 'W2093838537', 'W2097343453', 'W2098259366', 'W2098494194', 'W2099491642', 'W2100886586', 'W2106583271', 'W2109179710', 'W2109337529', 'W2111019629', 'W2111560326', 'W2113671336', 'W2114032563', 'W2114688837', 'W2116268000', 'W2117890823', 'W2119764013', 'W2121875081', 'W2123030744', 'W2124242076', 'W2124663458', 'W2125087964', 'W2125198788', 'W2125451833', 'W2125691719', 'W2126210405', 'W2128111546', 'W2128384671', 'W2128638548', 'W2128891846', 'W2134921841', 'W2139143133', 'W2139250130', 'W2139406636', 'W2140362890', 'W2140684126', 'W2147356401', 'W2147854273', 'W2150079601', 'W2150475026', 'W2150623910', 'W2150707647', 'W2151940597', 'W2152990750', 'W2153733800', 'W2154479094', 'W2156694465', 'W2158347593', 'W2159401023', 'W2164783491', 'W2165342832', 'W2166595775', 'W2167038248', 'W2167591654', 'W2167604259', 'W2167844952', 'W2169323390', 'W2169491358', 'W2169894204', 'W2169977305', 'W2170194949', 'W2176301470', 'W2180207848', 'W2195965780', 'W2200617834', 'W2204776416', 'W2207291742', 'W2214616458', 'W2218656852', 'W2226953621', 'W2242548945', 'W2255264739', 'W2259481044', 'W2274860869', 'W2275491855', 'W2281182212', 'W2288512038', 'W2289938426', 'W2309727226', 'W2313785159', 'W2324322428', 'W2338060077', 'W2340514456', 'W2342982379', 'W2345987556', 'W2346513181', 'W2405783129', 'W2409619909', 'W2412782622', 'W2414083225', 'W2460208024', 'W2469270620', 'W2486624425', 'W2504479221', 'W2505772441', 'W2511160790', 'W2511380210', 'W2513129213', 'W2514688978', 'W2517735605', 'W2520225690', 'W2520566087', 'W2520985535', 'W2521338131', 'W2521450141', 'W2522802829', 'W2522821491', 'W2527247503', 'W2540034041', 'W2547530520', 'W2548267094', 'W2549584294', 'W2552630650', 'W2553886049', 'W2554427090', 'W2559827920', 'W2560492303', 'W2561224646', 'W2568400808', 'W2569150814', 'W2575039370', 'W2580861781', 'W2581984518', 'W2585739546', 'W2586982562', 'W2587366712', 'W2588263337', 'W2592131051', 'W2592524105', 'W2593629244', 'W2593896392', 'W2594599876', 'W2597520820', 'W2597876129', 'W2600094664', 'W2601301923', 'W2605627625', 'W2607253888', 'W2607716704', 'W2610623677', 'W2612774616', 'W2613857401', 'W2619136320', 'W2621601071', 'W2624735470', 'W2625200959', 'W2684501113', 'W2725390633', 'W2725858897', 'W2725999880', 'W2730553355', 'W2734045880', 'W2735330142', 'W2739380147', 'W2741588972', 'W2743262744', 'W2744919507', 'W2745104750', 'W2745201210', 'W2746180191', 'W2746273252', 'W2746580132', 'W2747721696', 'W2748508681', 'W2748556198', 'W2749156271', 'W2749873219', 'W2751170717', 'W2752823190', 'W2754283165', 'W2754569507', 'W2756112558', 'W2758102094', 'W2758269250', 'W2761458899', 'W2762774283', 'W2763817699', 'W2766340448', 'W2766975063', 'W2767654638', 'W2767903251', 'W2769092861', 'W2769100785', 'W2769610250', 'W2774100359', 'W2774564892', 'W2779044993', 'W2779444716', 'W2781579322', 'W2782865844', 'W2783823932', 'W2785371699', 'W2786202610', 'W2787148816', 'W2788878978', 'W2789596991', 'W2789802139', 'W2789893810', 'W2789992655', 'W2790088751', 'W2790165362', 'W2790701366', 'W2790927266', 'W2791575850', 'W2793688962', 'W2793809429', 'W2794653051', 'W2795741191', 'W2796790338', 'W2801026471', 'W2804149102', 'W2804499694', 'W2807926135', 'W2950481872', 'W4210471029'], 'abstract': 'The last decade has seen a sharp increase in the number of scientific publications describing physiological and pathological functions of extracellular vesicles (EVs), a collective term covering various subtypes of cell-released, membranous structures, called exosomes, microvesicles, microparticles, ectosomes, oncosomes, apoptotic bodies, and many other names. However, specific issues arise when working with these entities, whose size and amount often make them difficult to obtain as relatively pure preparations, and to characterize properly. The International Society for Extracellular Vesicles (ISEV) proposed Minimal Information for Studies of Extracellular Vesicles ("MISEV") guidelines for the field in 2014. We now update these "MISEV2014" guidelines based on evolution of the collective knowledge in the last four years. An important point to consider is that ascribing a specific function to EVs in general, or to subtypes of EVs, requires reporting of specific information beyond mere description of function in a crude, potentially contaminated, and heterogeneous preparation. For example, claims that exosomes are endowed with exquisite and specific activities remain difficult to support experimentally, given our still limited knowledge of their specific molecular machineries of biogenesis and release, as compared with other biophysically similar EVs. The MISEV2018 guidelines include tables and outlines of suggested protocols and steps to follow to document specific EV-associated functional activities. Finally, a checklist is provided with summaries of key points.', 'counts_by_year': [[2022, 1366], [2021, 1522], [2020, 1069], [2019, 421], [2018, 7]]}, {'id': 'W2331128040', 'doi': 'https://doi.org/10.1007/978-3-319-46475-6_43', 'title': 'Perceptual Losses for Real-Time Style Transfer and Super-Resolution', 'type': 'book-chapter', 'publication_date': '2016-10-08', 'host_venue': 'V106296714', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2345205178', ['I97018004']], ['A2060491216', ['I97018004']], ['A1984838606', ['I97018004']]], 'cited_by_count': 4374, 'concepts': [['C41008148', '0.8098537'], ['C2776445246', '0.65994877'], ['C26760741', '0.5980827'], ['C2776175482', '0.5300096'], ['C138268822', '0.4816893']], 'referenced_works': ['W7682646', 'W935139217', 'W1579828764', 'W1745334888', 'W1791560514', 'W1861492603', 'W1885185971', 'W1899309388', 'W1903029394', 'W1905829557', 'W1915485278', 'W1930824406', 'W1932198206', 'W1950594372', 'W1976416062', 'W1981199328', 'W1982428585', 'W2022508996', 'W2046119925', 'W2047920195', 'W2097074225', 'W2112455692', 'W2114770744', 'W2117539524', 'W2121058967', 'W2124592697', 'W2125416623', 'W2126471044', 'W2133665775', 'W2141983208', 'W2148312679', 'W2149669120', 'W2160635556', 'W2164551808', 'W2170965888', 'W2194775991', 'W2294363656', 'W2295537950', 'W2326925005', 'W2475287302', 'W2489661638', 'W2534320940', 'W2963920537', 'W2963989815'], 'abstract': 'We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \\emph{per-pixel} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \\emph{perceptual} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.', 'counts_by_year': [[2022, 412], [2021, 1044], [2020, 1196], [2019, 953], [2018, 533], [2017, 197], [2016, 35], [2012, 1]]}, {'id': 'W2302255633', 'doi': 'https://doi.org/10.1007/978-3-319-46493-0_38', 'title': 'Identity Mappings in Deep Residual Networks', 'type': 'book-chapter', 'publication_date': '2016-10-08', 'host_venue': 'V4306463941', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2164292938', ['I4210164937']], ['A2499063207', ['I4210164937']], ['A2119543935', ['I4210164937']], ['A2200192130', ['I4210164937']]], 'cited_by_count': 4314, 'concepts': [['C155512373', '0.7366471'], ['C2778355321', '0.62481725'], ['C41008148', '0.41822764'], ['C33923547', '0.3218533'], ['C142362112', '0.2388781']], 'referenced_works': ['W1677182931', 'W1861492603', 'W2064675550', 'W2097117768', 'W2117539524', 'W2147800946', 'W2183341477', 'W2194775991'], 'abstract': 'Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers', 'counts_by_year': [[2022, 420], [2021, 1068], [2020, 1049], [2019, 851], [2018, 581], [2017, 281], [2016, 60], [2015, 3]]}, {'id': 'W2725897987', 'doi': 'https://doi.org/10.1016/j.rse.2017.06.031', 'title': 'Google Earth Engine: Planetary-scale geospatial analysis for everyone', 'type': 'journal-article', 'publication_date': '2017-07-06', 'host_venue': 'V141808269', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A1986177361', ['I4210100430']], ['A2705089234', ['I1291425158']], ['A2602651268', ['I1291425158']], ['A2728516114', ['I1291425158']], ['A1541877505', ['I1291425158']], ['A2288904679', ['I1291425158']]], 'cited_by_count': 4296, 'concepts': [['C9770341', '0.87760675'], ['C62649853', '0.75578845'], ['C2778755073', '0.63569367'], ['C26148502', '0.52635753'], ['C41008148', '0.5093916']], 'referenced_works': ['W612661449', 'W1873458742', 'W1964672965', 'W1981213426', 'W1981420413', 'W1987161776', 'W2003668322', 'W2047549267', 'W2086620533', 'W2139198479', 'W2151456308', 'W2290326488', 'W2332115271', 'W2344384134', 'W2493156582', 'W2560167313', 'W3137759927', 'W4241298546'], 'abstract': "Abstract   Google Earth Engine is a cloud-based platform for planetary-scale geospatial analysis that brings Google's massive computational capabilities to bear on a variety of high-impact societal issues including deforestation, drought, disaster, disease, food security, water management, climate monitoring and environmental protection. It is unique in the field as an integrated platform designed to empower not only traditional remote sensing scientists, but also a much wider audience that lacks the technical capacity needed to utilize traditional supercomputers or large-scale commodity cloud computing resources.", 'counts_by_year': [[2022, 1210], [2021, 1325], [2020, 961], [2019, 551], [2018, 219], [2017, 23], [2016, 1]]}, {'id': 'W2789843538', 'doi': 'https://doi.org/10.1093/bioinformatics/bty191', 'title': 'Minimap2: pairwise alignment for nucleotide sequences', 'type': 'journal-article', 'publication_date': '2018-09-15', 'host_venue': 'V52395412', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2596317628', ['I107606265']]], 'cited_by_count': 4273, 'concepts': [['C59582021', '0.7854433'], ['C41008148', '0.62555224'], ['C119054055', '0.52386564'], ['C180384323', '0.485821'], ['C70721500', '0.45033908']], 'referenced_works': ['W1579534339', 'W1980038595', 'W1983265703', 'W1998268440', 'W2017265532', 'W2021341670', 'W2068448872', 'W2068519185', 'W2103441770', 'W2106678197', 'W2108234281', 'W2109521234', 'W2137759177', 'W2141978199', 'W2144560237', 'W2146379626', 'W2158714788', 'W2162889091', 'W2168133698', 'W2169456326', 'W2170551349', 'W2191136822', 'W2194172909', 'W2260037449', 'W2337261418', 'W2521124250', 'W2604364312', 'W2607669908', 'W2779975541', 'W2784788330', 'W2788876413', 'W2950121474', 'W2953020124', 'W2953174564', 'W4247053599', 'W4253539230'], 'abstract': 'Motivation: Recent advances in sequencing technologies promise ultra-long reads of $\\sim$100 kilo bases (kb) in average, full-length mRNA or cDNA reads in high throughput and genomic contigs over 100 mega bases (Mb) in length. Existing alignment programs are unable or inefficient to process such data at scale, which presses for the development of new alignment algorithms. Results: Minimap2 is a general-purpose alignment program to map DNA or long mRNA sequences against a large reference database. It works with accurate short reads of $\\ge$100bp in length, $\\ge$1kb genomic reads at error rate $\\sim$15%, full-length noisy Direct RNA or cDNA reads, and assembly contigs or closely related full chromosomes of hundreds of megabases in length. Minimap2 does split-read alignment, employs concave gap cost for long insertions and deletions (INDELs) and introduces new heuristics to reduce spurious alignments. It is 3-4 times faster than mainstream short-read mappers at comparable accuracy and $\\ge$30 times faster at higher accuracy for both genomic and mRNA reads, surpassing most aligners specialized in one type of alignment. Availability and implementation: https://github.com/lh3/minimap2 Contact: hengli@broadinstitute.org', 'counts_by_year': [[2022, 1267], [2021, 1495], [2020, 981], [2019, 434], [2018, 89], [2017, 2]]}, {'id': 'W2963091558', 'doi': 'https://doi.org/10.1109/cvpr.2018.00813', 'title': 'Non-local Neural Networks', 'type': 'proceedings-article', 'publication_date': '2018-06-18', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2589938230', ['I2252078561']], ['A2473549963', ['I2252078561']], ['A2099263982', ['I74973139']], ['A2164292938', ['I2252078561']]], 'cited_by_count': 4258, 'concepts': [['C41008148', '0.78610873'], ['C2777210771', '0.72703433'], ['C154945302', '0.61670005'], ['C2776760102', '0.5896808'], ['C79581498', '0.56929994']], 'referenced_works': ['W197865394', 'W639708223', 'W1498436455', 'W1522734439', 'W1677182931', 'W1923404803', 'W1944615693', 'W1947481528', 'W2037642501', 'W2056370875', 'W2064675550', 'W2097073572', 'W2099244020', 'W2105101328', 'W2116341502', 'W2117539524', 'W2124592697', 'W2147800946', 'W2194775991', 'W2519224033', 'W2534320940', 'W2549139847', 'W2565639579', 'W2583815496', 'W2776638780', 'W2963150697', 'W2963321359', 'W2963524571', 'W2964046397', 'W4240805545', 'W4249710618'], 'abstract': 'Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code is available at https://github.com/facebookresearch/video-nonlocal-net .', 'counts_by_year': [[2022, 880], [2021, 1603], [2020, 1100], [2019, 573], [2018, 95], [2017, 1]]}, {'id': 'W2579533542', 'doi': 'https://doi.org/10.1099/ijsem.0.001755', 'title': 'Introducing EzBioCloud: a taxonomically united database of 16S rRNA gene sequences and whole-genome assemblies', 'type': 'journal-article', 'publication_date': '2017-05-30', 'host_venue': 'V102181007', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2576637783', ['I139264467']], ['A2124107405', ['I139264467']], ['A3206194872', ['I139264467']], ['A2579629041', ['I139264467']], ['A2572692495', ['I139264467']], ['A3080618076', ['I139264467']], ['A2114569273', ['I139264467']]], 'cited_by_count': 4159, 'concepts': [['C86803240', '0.8094349'], ['C141231307', '0.7666136'], ['C124104306', '0.5525982'], ['C104317684', '0.46253327'], ['C192953774', '0.4335699']], 'referenced_works': ['W2034285706', 'W2072496155', 'W2073925485', 'W2079222081', 'W2081676964', 'W2090747615', 'W2108215151', 'W2110300022', 'W2119888547', 'W2121055398', 'W2124351063', 'W2126321303', 'W2134878206', 'W2137516483', 'W2141052558', 'W2146528501', 'W2151350595', 'W2154071138', 'W2160107164', 'W2164852757', 'W2175094144', 'W2206071891'], 'abstract': 'The recent advent of DNA sequencing technologies facilitates the use of genome sequencing data that provide means for more informative and precise classification and identification of members of the Bacteria and Archaea. Because the current species definition is based on the comparison of genome sequences between type and other strains in a given species, building a genome database with correct taxonomic information is of paramount need to enhance our efforts in exploring prokaryotic diversity and discovering novel species as well as for routine identifications. Here we introduce an integrated database, called EzBioCloud, that holds the taxonomic hierarchy of the Bacteria and Archaea, which is represented by quality-controlled 16S rRNA gene and genome sequences. Whole-genome assemblies in the NCBI Assembly Database were screened for low quality and subjected to a composite identification bioinformatics pipeline that employs gene-based searches followed by the calculation of average nucleotide identity. As a result, the database is made of 61 700 species/phylotypes, including 13 132 with validly published names, and 62 362 whole-genome assemblies that were identified taxonomically at the genus, species and subspecies levels. Genomic properties, such as genome size and DNA G+C content, and the occurrence in human microbiome data were calculated for each genus or higher taxa. This united database of taxonomy, 16S rRNA gene and genome sequences, with accompanying bioinformatics tools, should accelerate genome-based classification and identification of members of the Bacteria and Archaea. The database and related search tools are available at www.ezbiocloud.net/.', 'counts_by_year': [[2022, 738], [2021, 911], [2020, 980], [2019, 703], [2018, 539], [2017, 281]]}, {'id': 'W2752326209', 'doi': 'https://doi.org/10.1080/15548627.2015.1100356', 'title': 'Guidelines for the use and interpretation of assays for monitoring autophagy (3rd edition)', 'type': 'journal-article', 'publication_date': '2016-01-21', 'host_venue': 'V102544370', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2062952991', ['I27837315']], ['A97174447', ['I1299303238']], ['A2125579409', ['I158929114']], ['A3023920689', ['I130238516']], ['A21303531', ['I197251160']], ['A2751079618', []], ['A2133544915', ['I188700360']], ['A2764585177', ['I126307644']], ['A2138419543', ['I7882870']], ['A2072311830', ['I185261750']], ['A732334930', ['I33213144']], ['A2155373771', ['I161318765']], ['A2100574354', ['I124227911']], ['A2503123072', ['I51713134']], ['A735804445', ['I180670191']], ['A2404903294', ['I900890020']], ['A1965624038', ['I99464096']], ['A2139837820', ['I55302922']], ['A43170550', ['I98704320']], ['A242635728', ['I107606265']], ['A3125290140', ['I204730241']], ['A3122730898', ['I129774422']], ['A2781974264', []], ['A1905676221', ['I100930933']], ['A397053463', ['I165339363']], ['A2123555955', ['I184565670']], ['A2066777390', ['I19630809']], ['A2199310291', ['I157536573']], ['A2798709620', []], ['A2042942587', ['I62916508']], ['A2019968094', ['I123431417']], ['A124242379', ['I110594554']], ['A2129500941', ['I1316902750']], ['A2753220469', []], ['A2295758653', ['I149251103']], ['A2138141960', []], ['A2081967350', ['I1299303238']], ['A1234201154', ['I102322052']], ['A2561721157', ['I24354313']], ['A2072807102', ['I45129253']], ['A2183104047', ['I277688954']], ['A3065041853', ['I197604219']], ['A2099077009', ['I98285908']], ['A6361767', ['I112859197']], ['A2142812909', ['I71999127']], ['A1988621007', ['I90344618']], ['A2319894532', ['I52357470']], ['A48555731', ['I185443292']], ['A2102636494', ['I63966007']], ['A2110741747', ['I180670191']], ['A339249285', ['I150468666']], ['A2650875763', ['I204337017']], ['A1574434692', ['I204465549']], ['A2299117264', ['I153648349']], ['A305159327', []], ['A2175416313', ['I255234318']], ['A2186564742', ['I1301076528']], ['A32095362', ['I11701301']], ['A2751208959', ['I161046081']], ['A2592178723', ['I71395657']], ['A2204015732', ['I16097986']], ['A2073458918', ['I45204951']], ['A40251337', ['I116067653']], ['A2131365920', ['I150468666']], ['A432088564', ['I53964585']], ['A2275918700', ['I134820265']], ['A2102469745', ['I115498069']], ['A2617688880', ['I118564535']], ['A2628948505', ['I19772626']], ['A2151838856', ['I201537933']], ['A2618425363', ['I27837315']], ['A2106789244', ['I5728261']], ['A2043824789', []], ['A2779387631', ['I174216632']], ['A2030496984', ['I22299242']], ['A2126219707', ['I1174212']], ['A2022193250', ['I63341726']], ['A1993072468', ['I43777268']], ['A2058568609', ['I25041050']], ['A2058205872', ['I99043593']], ['A141613322', ['I97018004']], ['A1845755334', []], ['A576317733', ['I114090438']], ['A2335818633', ['I126744593']], ['A27096895', ['I55143463']], ['A2464430629', ['I189158943']], ['A2053374639', ['I184565670']], ['A2250547088', ['I16391192']], ['A2155462942', ['I151328261']], ['A2018568619', ['I42766147']], ['A3113967119', ['I2801463291']], ['A2167986344', ['I19772626']], ['A2137968664', ['I129604602']], ['A2435415975', ['I31419693']], ['A2001661316', ['I4575257']], ['A2573652718', ['I193775966']], ['A1987294923', ['I166722992']], ['A2113556841', ['I57664883']], ['A1301513852', []], ['A701337834', ['I59411706']], ['A2468898917', ['I27804330']], ['A3212197073', ['I10660446']], ['A2991103490', []], ['A2947703008', ['I68947357']], ['A2141276012', ['I59270414']], ['A1845317305', ['I190397597']], ['A1815588730', ['I71267560']], ['A2000682892', ['I197854408']], ['A2113851855', ['I55664587']], ['A2421348078', ['I101202996']], ['A2779041688', ['I177388780']], ['A2188068818', ['I159129438']], ['A2465849677', ['I16097986']], ['A2116984680', ['I170486558']], ['A1987562324', ['I74775410']], ['A2094894629', ['I78577930']], ['A2173385947', ['I173911158']], ['A2582948780', []], ['A2110006246', ['I919571938']], ['A2124273703', ['I165139151']], ['A2145678653', ['I76903346']], ['A1997688117', ['I95674353']], ['A2233300206', ['I122534668']], ['A1993788113', ['I204465549']], ['A2663041486', ['I127845322']], ['A1355339917', ['I51713134']], ['A2303025797', ['I241749']], ['A2175516053', ['I135117807']], ['A1970251140', ['I150468666']], ['A2627770449', ['I181369854']], ['A2018060247', ['I51713134']], ['A2055574222', ['I98251732']], ['A2071871333', ['I36522303']], ['A2100625594', ['I392282']], ['A2095967768', ['I197323543']], ['A1974549842', ['I114090438']], ['A2089034898', ['I34809795']], ['A2277499036', ['I33213144']], ['A2101331204', ['I19772626']], ['A2306194708', []], ['A2184580363', ['I861853513']], ['A2311361155', []], ['A1785967850', []], ['A1984494885', ['I166337079']], ['A2054838143', ['I180923762']], ['A2118651687', ['I887064364']], ['A3100784803', ['I36443711']], ['A2199120585', ['I27837315']], ['A2304242225', ['I70931966']], ['A94430418', ['I116067653']], ['A2806852189', ['I90344618']], ['A2305327074', ['I170897317']], ['A2250888708', ['I1294671590']], ['A2302045746', ['I124601658']], ['A2568377221', ['I111979921']], ['A2105256836', ['I45294948']], ['A2442548728', ['I16292982']], ['A2017962703', ['I62916508']], ['A2198028435', ['I204730241']], ['A218660077', []], ['A2282892667', ['I106118109']], ['A2569807392', ['I88273585']], ['A2098398852', ['I183934855']], ['A2719323645', ['I97018004']], ['A694240815', ['I39804081']], ['A2023306649', ['I45129253']], ['A2574318548', ['I1299303238']], ['A2289238085', ['I1302444339']], ['A2503388809', ['I24354313']], ['A2254278429', ['I184942183']], ['A2001894184', ['I184999862']], ['A113056040', ['I54009628']], ['A2142463962', ['I193662353']], ['A672175117', ['I79576946']], ['A2432280163', ['I150468666']], ['A2098474006', []], ['A2119482465', ['I102134673']], ['A172243817', ['I138689650']], ['A2021850868', ['I57206974']], ['A2144331228', ['I145311948']], ['A2056287453', ['I200284239']], ['A2352507861', ['I204730241']], ['A2102853187', []], ['A2052839607', ['I861853513']], ['A170052077', ['I15057530']], ['A2149408982', ['I135117807']], ['A2119814604', ['I592451']], ['A233966980', ['I1174212']], ['A2304587971', ['I134820265']], ['A1788623709', ['I90843659']], ['A19052523', ['I298625061']], ['A2151984788', ['I17937529']], ['A2102904685', ['I47508984']], ['A1916189758', ['I129043915']], ['A29706962', ['I74656192']], ['A1267999314', []], ['A2120637674', ['I63772739']], ['A287078351', ['I135310074']], ['A1974398187', []], ['A2148389392', ['I160606119']], ['A2138916410', []], ['A2283354693', ['I138689650']], ['A2346362044', ['I17974374']], ['A2627282981', ['I28166907']], ['A2298302593', ['I5388228']], ['A2593789273', ['I183934855']], ['A2122495236', ['I153648349']], ['A156432546', ['I592451']], ['A1755755519', ['I198244214']], ['A2069055783', ['I17974374']], ['A2949743557', ['I185261750']], ['A2283096932', ['I71267560']], ['A2343265158', ['I56590836']], ['A2145610329', ['I59130452']], ['A2027177565', ['I40120149']], ['A1156462912', ['I134235054']], ['A120861803', []], ['A1923956633', ['I114027177']], ['A1819659627', ['I99464096']], ['A1705903775', ['I4068193']], ['A298028111', ['I70931966']], ['A2113881714', ['I78577930']], ['A2110336297', ['I27837315']], ['A2250738331', ['I118564535']], ['A743425586', ['I193223587']], ['A1984280736', ['I57206974']], ['A1152800024', []], ['A2894007689', ['I19772626']], ['A3164704702', ['I27781120']], ['A2777979304', ['I102322142']], ['A2162448071', ['I16097986']], ['A684310406', ['I15057530']], ['A2055567159', ['I45129253']], ['A2107254091', ['I36258959']], ['A2804008652', ['I205274468']], ['A2069796616', ['I116067653']], ['A1870196401', []], ['A289635029', ['I185261750']], ['A2789201127', ['I19630809']], ['A2122276302', ['I139660479']], ['A2204057911', ['I155093810']], ['A1059313495', ['I197809005']], ['A2608208751', ['I861853513']], ['A2102491531', ['I76903346']], ['A2013649777', ['I1316902750']], ['A2710310220', ['I188760350']], ['A2106920987', ['I58956616']], ['A2146505024', ['I190397597']], ['A2112563861', ['I90267481']], ['A2008547561', ['I15766117']], ['A2617761156', ['I122140584']], ['A2064575126', ['I122534668']], ['A2020062316', ['I122346577']], ['A3054991313', ['I21491767']], ['A2168560610', ['I84884186']], ['A2137214377', ['I123044942']], ['A2252044573', ['I134820265']], ['A2013597901', []], ['A3037662849', ['I1850255']], ['A1003584902', ['I8961855']], ['A2019572061', ['I108290504']], ['A2034179944', ['I2801463291']], ['A1982332769', ['I116067653']], ['A335636369', ['I98704320']], ['A2618734378', ['I79189158']], ['A1971882452', ['I154387261']], ['A248330833', []], ['A2112250469', ['I138938424']], ['A1985713778', ['I9360294']], ['A2066210558', ['I202697423']], ['A2113169165', ['I80611190']], ['A1994387052', ['I28166907']], ['A2954009708', ['I152743029']], ['A2638197032', ['I106542073']], ['A2035329464', ['I142617266']], ['A2147668158', ['I181647926']], ['A2597374299', ['I177725633']], ['A2307624335', ['I1292894508']], ['A2129542612', ['I91080651']], ['A2130362025', ['I91807558']], ['A2122723856', ['I889458895']], ['A2750892162', ['I107672454']], ['A2041301040', ['I32389192']], ['A2096729176', ['I155781252']], ['A2176951932', []], ['A2779748107', []], ['A2099989703', ['I45129253']], ['A169532657', ['I31746571']], ['A2631005242', []], ['A2608476590', ['I24943067']], ['A2561325315', ['I84653119']], ['A2288801721', ['I183067930']], ['A2548455987', ['I183067930']], ['A2751862071', ['I204250578']], ['A2917340449', ['I25041050']], ['A2660592632', ['I191996457']], ['A2750887402', ['I37461747']], ['A2622129133', ['I138689650']], ['A2617151042', ['I146416000']], ['A3212449360', ['I19820366']], ['A2753331724', ['I173093425']], ['A2424070985', ['I1320320070']], ['A2126140444', ['I84653119']], ['A2585669862', ['I76130692']], ['A2750786225', ['I75430998']], ['A2751690187', ['I154099455']], ['A2428743140', ['I187400657']], ['A3160533015', []], ['A2752400030', ['I107470533']], ['A2008602815', ['I99065089']], ['A3212819927', ['I138006243']], ['A2250757169', ['I20231570']], ['A2751924205', ['I38877650']], ['A3214254088', ['I2802753785']], ['A2789132723', ['I157773358']], ['A2616019140', ['I151075929']], ['A2592111858', ['I24943067']], ['A2096211601', ['I76130692']], ['A2620044053', ['I142740786']], ['A2113532172', ['I177725633']], ['A2939043364', ['I126744593']], ['A2599321805', []], ['A1976984944', ['I79576946']], ['A2617148822', ['I142740786']], ['A2300725457', ['I91807558']], ['A142808697', ['I56067802']], ['A2852215332', ['I173093425']], ['A3175229914', ['I197347611']], ['A1822550092', []], ['A2151242234', ['I130769515']], ['A1993516271', ['I900890020']], ['A2429680419', ['I197604219']], ['A2164603091', []], ['A2558042029', ['I150468666']], ['A2127330039', ['I69737025']], ['A2048627886', ['I165932596']], ['A2134718508', ['I35928602']], ['A2112067691', ['I24062138']], ['A2495023647', []], ['A2751741941', ['I87111246']], ['A2725402021', ['I2802905097']], ['A2149710874', ['I205783295']], ['A2183044892', ['I197347611']], ['A2377666811', ['I146824383']], ['A2490469437', ['I40347166']], ['A3212288839', ['I205783295']], ['A2510949822', ['I193775966']], ['A2125848788', []], ['A2557527220', ['I2801843802']], ['A2034828475', ['I63135867']], ['A2049808276', ['I56085075']], ['A2579216459', ['I162838928']], ['A2033956741', ['I149899117']], ['A2126331992', ['I170201317']], ['A2053543797', ['I165441096']], ['A2163782208', ['I197347611']], ['A2603703733', ['I24062138']], ['A2120011141', ['I4921948']], ['A3213286010', ['I2801316944']], ['A2193795260', ['I57664883']], ['A900027324', []], ['A1927358253', ['I71267560']], ['A2000943190', ['I16778253']], ['A698860583', ['I116067653']], ['A2057299282', ['I861853513']], ['A2792344208', ['I99464096']], ['A2147480958', ['I146655781']], ['A2066803314', ['I71999127']], ['A2286711439', ['I97565354']], ['A2808359499', ['I184565670']], ['A2202430032', []], ['A2538907523', ['I169521973']], ['A241388101', ['I132053463']], ['A2275360079', ['I27897274']], ['A1997758898', ['I5561750']], ['A165075948', ['I204730241']], ['A1894037254', ['I170897317']], ['A2471472485', ['I36258959']], ['A2304288321', ['I197604219']], ['A1981786762', ['I19772626']], ['A2311959065', ['I134820265']], ['A96816623', ['I1294671590']], ['A2309093542', ['I25217355']], ['A1955615429', ['I27897274']], ['A2157091986', ['I47251452']], ['A1965578369', ['I181647926']], ['A2289182902', ['I137317281']], ['A2170226975', ['I45129253']], ['A2298831061', ['I145311948']], ['A2067248725', ['I36443711']], ['A32865991', ['I116067653']], ['A3124504228', ['I69737025']], ['A2789316717', []], ['A2149341375', ['I79238269']], ['A2549735245', ['I53110688']], ['A1217155227', ['I204730241']], ['A199986211', ['I122346577']], ['A824680934', ['I55143463']], ['A3105170005', ['I161318765']], ['A2250300627', ['I136199984']], ['A2085485566', ['I165339363']], ['A1998293123', ['I97565354']], ['A1903996873', ['I149744451']], ['A2274028960', ['I102322142']], ['A1988190810', ['I153297377']], ['A2139444679', ['I180670191']], ['A1172121218', ['I195460627']], ['A2106342347', ['I170201317']], ['A1971769069', ['I43439940']], ['A2071692941', ['I143302722']], ['A2273056064', ['I55143463']], ['A2614261195', ['I79238269']], ['A2203873381', ['I69737025']], ['A1936735960', []], ['A2252009367', ['I76903346']], ['A2100022661', ['I19772626']], ['A2042819249', ['I63634437']], ['A2750716430', ['I155781252']], ['A2033785274', ['I166337079']], ['A1501355792', ['I19772626']], ['A2223389584', ['I63135867']], ['A1734221669', ['I134113660']], ['A2075801399', ['I76198965']], ['A2779487603', ['I83519826']], ['A2776410736', ['I156144747']], ['A2140381468', ['I184840846']], ['A2090405770', ['I86519309']], ['A2120836314', ['I138689650']], ['A731807123', ['I198244214']], ['A1961309355', []], ['A2150032270', ['I32021983']], ['A991213191', ['I198244214']], ['A2054009810', ['I204730241']], ['A2194625106', ['I32389192']], ['A2042593249', ['I1316902750']], ['A1968639568', ['I1285204247']], ['A2158932840', ['I114832834']], ['A2460033777', ['I205640436']], ['A2307458202', ['I16038530']], ['A2030509519', ['I157614274']], ['A1977754345', ['I121748325']], ['A2159072619', ['I166722992']], ['A2261126034', ['I145311948']], ['A2009888011', ['I145311948']], ['A2133075821', ['I79238269']], ['A2130708453', ['I47508984']], ['A2126251441', ['I91045830']], ['A2125044841', ['I52325']], ['A2097969508', ['I134820265']], ['A2120968361', ['I71267560']], ['A2299335153', []], ['A2169073259', ['I149213910']], ['A2018778488', ['I28166907']], ['A2122951034', ['I190397597']], ['A2158937048', ['I122140584']], ['A2075172644', ['I108290504']], ['A1979177504', []], ['A1188407855', ['I180670191']], ['A2019997016', ['I35440088']], ['A1979304865', ['I99464096']], ['A1967268215', ['I188760350']], ['A2762653225', ['I15057530']], ['A2133473537', ['I102064193']], ['A2585928407', ['I102322142']], ['A191410735', ['I90843659']], ['A2007283861', ['I165779595']], ['A1886444242', ['I95674353']], ['A2303473142', ['I27837315']], ['A2103786776', ['I162266279']], ['A1386858091', ['I161046081']], ['A2781717195', ['I24715933']], ['A2139702396', ['I184840846']], ['A2143956694', ['I114027177']], ['A234037099', ['I169521973']], ['A2520130624', ['I35440088']], ['A2727717772', ['I277688954']], ['A2553105995', ['I91045830']], ['A1943826203', ['I56590836']], ['A2033079444', ['I116067653']], ['A2059329663', ['I116067653']], ['A2431181791', ['I861853513']], ['A1969138530', ['I136199984']], ['A2700178875', []], ['A2146372794', ['I53110688']], ['A1973143600', ['I861853513']], ['A2751088229', ['I204465549']], ['A2120129284', ['I69737025']], ['A315064625', ['I189268942']], ['A2059569956', ['I2799424032']], ['A2220364637', ['I63634437']], ['A2089486897', ['I2613432']], ['A2919351947', ['I143302722']], ['A1858503396', ['I139264467']], ['A2088778053', ['I98677209']], ['A63016252', ['I114090438']], ['A2752100487', ['I84218800']], ['A2639296829', []], ['A2139898110', ['I146416000']], ['A3211724411', ['I79620101']], ['A2108145954', ['I142910587']], ['A1819904704', ['I181369854']], ['A2565399824', ['I204465549']], ['A607842527', ['I15057530']], ['A235560236', ['I149851306']], ['A1994636281', ['I185492890']], ['A445731383', ['I114090438']], ['A925862492', ['I169521973']], ['A2034612067', ['I2801843802']], ['A2303585742', ['I119439378']], ['A2902272759', ['I592451']], ['A3033411755', ['I162577319']], ['A2228214167', ['I25041050']], ['A2131602389', ['I59130452']], ['A2753087162', ['I1322918889']], ['A1952460885', ['I26538001']], ['A2291787506', ['I39387349']], ['A2018814879', ['I204465549']], ['A2292567407', ['I149851306']], ['A1987132760', ['I2799691083']], ['A2184089308', ['I16391192']], ['A2751550427', ['I183519381']], ['A2106070669', []], ['A2752664866', []], ['A2575899641', ['I26092322']], ['A2306829193', ['I166337079']], ['A3213750508', ['I1285301757']], ['A2608827944', ['I52357470']], ['A2109664013', ['I1317621060']], ['A66637187', ['I45129253']], ['A2948851434', ['I97565354']], ['A219810223', ['I7171862']], ['A409299757', []], ['A326065705', ['I161046081']], ['A2100219142', ['I27837315']], ['A2115253457', ['I79510175']], ['A2298740752', ['I33213144']], ['A2464887212', ['I204730241']], ['A2952864920', ['I68947357']], ['A2294081976', ['I15057530']], ['A2980530246', ['I5023651']], ['A2498068458', ['I15057530']], ['A1605029031', ['I170201317']], ['A2020905578', ['I6902469']], ['A2433992774', ['I223822909']], ['A2569603159', ['I157536573']], ['A2000492772', ['I76134821']], ['A1922701239', ['I921990950']], ['A2142078583', ['I204250578']], ['A2005875279', ['I180923762']], ['A2103629023', ['I15766117']], ['A1240465923', ['I53964585']], ['A2064956975', ['I181547552']], ['A70880627', ['I1289437631']], ['A2176211917', []], ['A2094799416', ['I53964585']], ['A3165530356', ['I16391192']], ['A2805455174', ['I52099693']], ['A2277181621', ['I25217355']], ['A2527389921', ['I185261750']], ['A76540284', ['I184942183']], ['A2127270170', ['I26092322']], ['A2473116326', ['I174306211']], ['A1398237133', ['I184942183']], ['A2119851776', ['I904495901']], ['A1210303304', ['I141777705']], ['A1857391014', ['I129801699']], ['A2243793790', ['I44461941']], ['A2127507399', ['I153976015']], ['A2168792435', ['I134820265']], ['A2166899317', ['I133731052']], ['A2502234185', []], ['A218735301', ['I134820265']], ['A2573597022', ['I7882870']], ['A2284609762', ['I180923762']], ['A1405686227', ['I134820265']], ['A2056912419', ['I861853513']], ['A248206365', []], ['A2251215403', ['I177969490']], ['A1783718244', ['I861853513']], ['A2251448882', ['I196829312']], ['A2140574949', []], ['A2779131888', ['I155781252']], ['A2471634354', ['I170201317']], ['A2419691866', ['I126744593']], ['A2618704973', ['I183935753']], ['A2073481180', ['I79940851']], ['A2131587182', []], ['A3024374966', ['I1294671590']], ['A2293616514', []], ['A58215176', ['I188760350']], ['A2141249670', ['I116067653']], ['A2751295230', ['I141649914']], ['A2097410284', ['I1299303238']], ['A2102637071', ['I151075929']], ['A2136030149', ['I889458895']], ['A2264729347', ['I27837315']], ['A2585833027', ['I204465549']], ['A2295581729', ['I165339363']], ['A2305946338', ['I88155538']], ['A2033536086', ['I1174212']], ['A2109312725', ['I8833935']], ['A2482639820', ['I1330342723']], ['A1748372707', ['I95674353']], ['A2091118196', []], ['A1989119223', ['I181391015']], ['A107813633', ['I132735039']], ['A1890246033', ['I26538001']], ['A2595450574', ['I1330342723']], ['A2006155110', ['I130442723']], ['A46431645', ['I116067653']], ['A334533722', ['I142910587']], ['A1624713514', ['I126307644']], ['A2100676444', ['I180670191']], ['A1628565618', ['I1299303238']], ['A1969366994', ['I71267560']], ['A2311321665', ['I184840846']], ['A1302321307', ['I83399316']], ['A2274495038', ['I9360294']], ['A2090481706', ['I1302113299']], ['A1973187680', ['I71267560']], ['A2000874194', ['I241749']], ['A2066634518', []], ['A2052785393', ['I123338534']], ['A2041105566', ['I5023651']], ['A192968890', ['I108290504']], ['A2830831249', ['I223822909']], ['A1356442698', []], ['A2134187042', ['I114395901']], ['A2274079587', ['I43406934']], ['A2752999466', []], ['A2146565413', ['I124055696']], ['A2557301570', ['I1299303238']], ['A3021159216', ['I32021983']], ['A1963823881', ['I203339264']], ['A2060894762', ['I241749']], ['A2207498872', ['I98704320']], ['A2288791216', ['I44461941']], ['A2171701296', ['I126231945']], ['A2467482974', []], ['A2123249864', ['I919571938']], ['A2428267603', ['I34077901']], ['A2091748794', []], ['A2017760731', ['I147809708']], ['A2120063332', ['I201537933']], ['A2200687103', ['I114090438']], ['A2122487794', ['I2800188116']], ['A2779052571', ['I864159182']], ['A757228259', ['I34809795']], ['A2392455884', ['I95674353']], ['A1996658459', ['I170230895']], ['A2292263186', ['I201726411']], ['A2059941855', ['I53964585']], ['A2476509839', []], ['A2489868075', []], ['A2014490180', ['I159176309']], ['A220222245', []], ['A1642041372', ['I190397597']], ['A1909302936', ['I39804081']], ['A2776327229', ['I98677209']], ['A2817296893', ['I98704320']], ['A2009663580', ['I204250578']], ['A2444022414', ['I5681781']], ['A611193023', ['I177639307']], ['A2719634265', ['I114457229']], ['A2299453807', ['I166722992']], ['A2664428239', ['I9916479']], ['A2752662508', ['I183067930']], ['A2752962296', ['I69737025']], ['A2010713884', ['I151201029']], ['A3213873075', ['I19772626']], ['A3190840751', ['I71999127']], ['A2138651558', ['I99464096']], ['A2464803758', ['I63739035']], ['A3080515438', ['I63634437']], ['A2068813176', ['I149899117']], ['A2000100916', ['I27837315']], ['A2068882297', ['I21491767']], ['A2085571681', ['I154526488']], ['A1683601584', ['I19772626']], ['A2429451350', ['I95457486']], ['A2751944191', ['I194450716']], ['A2106290078', ['I183067930']], ['A2061493007', ['I91807558']], ['A1996057564', ['I2800188116']], ['A2113876376', ['I123338534']], ['A2599094487', ['I136199984']], ['A1895764903', ['I1294671590']], ['A2093256426', ['I184942183']], ['A334218058', ['I180670191']], ['A628281784', ['I184840846']], ['A2169015619', ['I46247651']], ['A2157863875', ['I21491767']], ['A2066213164', ['I63739035']], ['A242262578', ['I27897274']], ['A1957954988', ['I122534668']], ['A1988829433', ['I861853513']], ['A2003890719', ['I147962203']], ['A2051944495', ['I153718931']], ['A2145588589', ['I122346577']], ['A2113486675', ['I46247651']], ['A1877568193', ['I97565354']], ['A2164992623', ['I84392919']], ['A2083311283', ['I153648349']], ['A2169020855', ['I108290504']], ['A2114531417', ['I185261750']], ['A2113267480', ['I114090438']], ['A2269977528', []], ['A282398160', ['I1316902750']], ['A2085983594', ['I69737025']], ['A2307521781', ['I141945490']], ['A2194929702', []], ['A2120898081', ['I165733156']], ['A2779611865', ['I1320320070']], ['A2114497933', ['I17974374']], ['A645313422', []], ['A2952378075', ['I181190671']], ['A2319310052', ['I84218800']], ['A2152035076', []], ['A2145379096', ['I170201317']], ['A1178589', ['I919571938']], ['A2481041709', []], ['A2107486397', ['I17974374']], ['A2046710218', ['I41641357']], ['A3213399862', ['I205237279']], ['A2618425147', []], ['A2104922404', ['I16465266']], ['A982733498', ['I79576946']], ['A2970602764', ['I134820265']], ['A2204912587', []], ['A1635648473', ['I154425047']], ['A2074750937', ['I223532165']], ['A1980504200', ['I148314036']], ['A2092948759', ['I185261750']], ['A2136659740', ['I188760350']], ['A2017736393', ['I18014758']], ['A180479745', ['I136199984']], ['A218742139', ['I22299242']], ['A2049115118', ['I134820265']], ['A1978553191', ['I1282927834']], ['A336238745', ['I16391192']], ['A115816721', ['I134235054']], ['A2113854999', ['I21491767']], ['A2172142766', ['I149899117']], ['A2799149724', ['I84392919']], ['A2126563126', ['I11701301']], ['A2585408547', ['I184840846']], ['A1999162252', ['I26415053']], ['A2246107347', []], ['A2016396076', ['I36234482']], ['A2153167331', ['I51768193']], ['A2691166389', ['I30771326']], ['A2161068210', ['I68947357']], ['A2263361439', ['I126307644']], ['A2752300910', ['I2799424032']], ['A2227851217', ['I76134821']], ['A1604757112', ['I114090438']], ['A2227985847', []], ['A2019174117', ['I63135867']], ['A2136733111', ['I36258959']], ['A2262041758', ['I177969490']], ['A2488142112', ['I121748325']], ['A1502947018', ['I1316902750']], ['A2018588354', ['I99464096']], ['A2147362408', ['I116953780']], ['A3213742315', ['I1320320070']], ['A2902793065', ['I161318765']], ['A2777704282', ['I115592961']], ['A2940128353', ['I92039509']], ['A2008823434', ['I8087733']], ['A2275745605', ['I36258959']], ['A2096898231', ['I129604602']], ['A2096677948', ['I2801081054']], ['A2751192141', []], ['A2579748658', ['I135140700']], ['A2028790154', ['I6902469']], ['A2302598285', ['I1314466530']], ['A2158451593', ['I94402716']], ['A2441277597', ['I1285204247']], ['A2112477647', ['I46247651']], ['A2892653868', ['I17937529']], ['A3206139196', []], ['A2502803536', ['I204730241']], ['A2022791324', ['I114090438']], ['A2119845981', ['I98285908']], ['A2308165470', []], ['A2120482186', []], ['A2126179661', ['I40120149']], ['A2125498669', ['I76130692']], ['A2694147710', ['I76130692']], ['A2097210223', ['I145311948']], ['A2301271452', ['I1299303238']], ['A2799267568', ['I2799424032']], ['A3213191344', ['I188700360']], ['A2076719302', ['I4068193']], ['A2168016840', ['I136199984']], ['A1975579098', ['I28022161']], ['A3061773886', ['I40120149']], ['A3208112644', ['I56590836']], ['A2307293023', ['I114457229']], ['A2403581015', ['I161046081']], ['A1970918326', ['I107139324']], ['A2138393985', ['I193531525']], ['A2760848513', ['I193531525']], ['A2154641579', ['I111979921']], ['A2110054859', ['I165932596']], ['A2751992980', ['I151075929']], ['A2189498044', ['I24185976']], ['A2120091043', ['I159948400']], ['A2709232897', ['I159948400']], ['A2134299596', ['I170897317']], ['A2512376848', ['I40347166']], ['A2101318612', ['I196021976']], ['A2062490919', ['I70931966']], ['A2121579545', ['I1299303238']], ['A3212264896', ['I7882870']], ['A2679144697', ['I170658231']], ['A2003357750', ['I136199984']], ['A1990181320', ['I72951846']], ['A2028092561', ['I52357470']], ['A2145756601', ['I17974374']], ['A2105119795', ['I16097986']], ['A1488940306', ['I121748325']], ['A721498194', ['I69737025']], ['A2711302758', ['I162577319']], ['A2293492197', ['I4588055']], ['A1980059047', ['I134820265']], ['A2162598946', ['I142740786']], ['A2161812406', ['I162577319']], ['A2156446963', ['I25041050']], ['A2090435851', ['I189996419']], ['A2006738463', ['I298625061']], ['A2153223177', []], ['A2326295740', ['I62916508']], ['A1312528354', ['I135140700']], ['A1868854485', ['I19772626']], ['A2155310753', ['I104338594']], ['A2147645749', ['I192455969']], ['A2048086104', ['I865915315']], ['A2137586427', ['I180923762']], ['A2776222256', ['I152743029']], ['A3108188188', ['I173093425']], ['A3213042078', ['I16733864']], ['A2914314730', ['I184681353']], ['A2776444514', ['I39422238']], ['A2412818606', ['I2801670537']], ['A2919861114', ['I52158045']], ['A2130385605', ['I867280407']], ['A2147011160', ['I25846049']], ['A2097497398', ['I200296433']], ['A2581508922', ['I200296433']], ['A2893958724', ['I27837315']], ['A2128206857', ['I24185976']], ['A3214022921', ['I80327900']], ['A2753223325', ['I16733864']], ['A3211394524', ['I173093425']], ['A2608997764', ['I81020160']], ['A2099786460', ['I183067930']], ['A2155652130', ['I16733864']], ['A2914848161', ['I183067930']], ['A2722235812', ['I98704320']], ['A3159475531', ['I77920804']], ['A2126627897', ['I161046081']], ['A252668473', ['I32021983']], ['A2193993277', ['I139264467']], ['A2175488856', ['I94722563']], ['A1989768443', ['I196345858']], ['A2671742104', ['I95457486']], ['A2750636948', []], ['A3162291727', ['I5023651']], ['A2514843495', ['I1299303238']], ['A2149748435', ['I40542001']], ['A2122460803', ['I40347166']], ['A3184628681', ['I1296604144']], ['A2779030437', []], ['A2346459153', ['I34077901']], ['A24099854', ['I122346577']], ['A2689042768', ['I142822886']], ['A2310294769', ['I142822886']], ['A1976882032', ['I131729948']], ['A2460166534', ['I21491767']], ['A1542896051', ['I149251103']], ['A1908967510', ['I200769079']], ['A238362355', ['I136199984']], ['A2159354666', ['I166401450']], ['A1993984911', ['I98285908']], ['A2245640339', ['I4068193']], ['A62206334', ['I79576946']], ['A2225347596', ['I170201317']], ['A2624231123', ['I66760702']], ['A2592666362', ['I201537933']], ['A3212564685', ['I203951103']], ['A1375653276', ['I131249849']], ['A2069760977', ['I123338534']], ['A2462178306', ['I60134161']], ['A2033306433', ['I62916508']], ['A70360074', ['I124055696']], ['A2296790836', ['I165932596']], ['A2064210576', ['I159385669']], ['A2482388275', ['I184840846']], ['A2128705588', ['I151328261']], ['A2982732194', ['I63634437']], ['A2126990351', ['I8588240']], ['A2991238974', []], ['A484677927', []], ['A2108932855', ['I59553526']], ['A2617652890', ['I74656192']], ['A2262439535', ['I126744593']], ['A2150395558', ['I63739035']], ['A2111781335', ['I45129253']], ['A2924446626', ['I919571938']], ['A2918605979', ['I184942183']], ['A2302115805', ['I85332549']], ['A2124273457', ['I52010207']], ['A2157609097', ['I921990950']], ['A1958496662', []], ['A1968066906', ['I17974374']], ['A2089169227', ['I129604602']], ['A2137676666', ['I135117807']], ['A1945605825', ['I7877124']], ['A2178615995', ['I139264467']], ['A2044883163', ['I204337017']], ['A2436050256', ['I163753206']], ['A2892528332', ['I63772739']], ['A2164589762', ['I24943067']], ['A2423854389', ['I919571938']], ['A2292172473', ['I156144747']], ['A2170049755', ['I177725633']], ['A2964530428', ['I83519826']], ['A2140669127', ['I141945490']], ['A2902965881', ['I99542240']], ['A2054979584', ['I57206974']], ['A2990591823', ['I204823248']], ['A2302432925', ['I184999862']], ['A3213281626', []], ['A2150988366', ['I76130692']], ['A2618814326', ['I78757542']], ['A2307828327', ['I27837315']], ['A2135728491', ['I102322142']], ['A306780888', ['I2613432']], ['A2152812680', ['I196345858']], ['A2162717239', []], ['A2226676495', ['I170201317']], ['A2257975409', ['I5388228']], ['A2112878513', ['I141945490']], ['A2892938091', ['I919571938']], ['A2306793942', []], ['A1781707349', ['I145872427']], ['A2192507810', ['I79189158']], ['A2167124159', ['I33213144']], ['A2135030763', ['I28166907']], ['A2030992978', ['I69552723']], ['A3043264167', ['I1327237609']], ['A2775918534', ['I59553526']], ['A2902424099', ['I16733864']], ['A2521229291', ['I887064364']], ['A2102275639', ['I106118109']], ['A2966715942', ['I139264467']], ['A2117346149', ['I1174212']], ['A2516749825', ['I139264467']], ['A2067993094', ['I183935753']], ['A2951643955', ['I921990950']], ['A762671451', ['I34352273']], ['A1896798136', ['I31746571']], ['A2183611681', ['I175532246']], ['A2222546210', ['I56085075']], ['A1394834835', []], ['A123287960', []], ['A616474615', ['I102134673']], ['A2022066940', ['I27837315']], ['A1808995013', ['I180670191']], ['A3211940447', []], ['A2343320758', []], ['A2222872012', ['I126744593']], ['A3214346672', ['I199525922']], ['A2086458900', ['I147962203']], ['A2033294462', ['I28166907']], ['A159335316', ['I169381384']], ['A2569053166', ['I185443292']], ['A3213785372', ['I136199984']], ['A2465374383', ['I170201317']], ['A2750665609', ['I146824383']], ['A2106508469', ['I71395657']], ['A2225155204', []], ['A2071060834', ['I201537933']], ['A668189161', ['I173911158']], ['A2190230434', ['I63772739']], ['A227106492', ['I2802600346']], ['A2019710964', []], ['A1256411775', ['I1285764155']], ['A2694550555', ['I130238516']], ['A2232187126', ['I170979836']], ['A2013729717', ['I241749']], ['A2098755745', ['I19772626']], ['A1964994666', ['I114027177']], ['A2551074079', ['I170201317']], ['A2256792163', ['I173093425']], ['A2472002070', ['I113940042']], ['A2549222425', ['I1299303238']], ['A2115343302', ['I165690674']], ['A2130135582', ['I157725225']], ['A1994062704', ['I200719446']], ['A1360065292', []], ['A2628767481', ['I197323543']], ['A2941991093', []], ['A2003829813', ['I185443292']], ['A1217075089', ['I45129253']], ['A2224262441', ['I17974374']], ['A2609841561', ['I592451']], ['A2149337490', ['I102335020']], ['A2513635267', []], ['A3020857043', ['I1320320070']], ['A3213347618', ['I148314036']], ['A2073659193', ['I36258959']], ['A2158856254', ['I205349734']], ['A2751510882', ['I78577930']], ['A2750952448', []], ['A2491464120', ['I189442560']], ['A3182391821', ['I130238516']], ['A2145954688', ['I193775966']], ['A2619607362', ['I51226738']], ['A2746239598', ['I77079311']], ['A2615466396', ['I33213144']], ['A3211791786', ['I139264467']], ['A2603887327', ['I40542001']], ['A2752934330', ['I139264467']], ['A2110066973', ['I200719446']], ['A2753140570', ['I145608581']], ['A2892575200', []], ['A2426925343', ['I185261750']], ['A2129697001', ['I40542001']], ['A2130986328', []], ['A2162485085', ['I146824383']], ['A2750427552', ['I17301866']], ['A2464500172', ['I53964585']], ['A851018909', ['I136199984']], ['A2121957560', ['I169521973']], ['A2439269262', ['I91136226']], ['A1774299471', ['I97018004']], ['A1247280676', ['I164928964']], ['A2309344059', ['I46247651']], ['A2015748223', ['I123431417']], ['A2291692136', ['I201537933']], ['A1819414021', ['I74801974']], ['A2098807733', ['I163917720']], ['A1992729719', ['I43777268']], ['A2121619601', ['I904495901']], ['A2224583542', ['I138006243']], ['A1981609400', ['I114090438']], ['A236084408', ['I181369854']], ['A132108364', ['I70931966']], ['A2123701451', []], ['A2243290243', ['I114090438']], ['A2012123398', ['I91279580']], ['A3211722124', ['I135364568']], ['A2154864044', ['I74656192']], ['A2753715258', []], ['A2243370717', ['I22465464']], ['A2138674825', ['I146824383']], ['A2689935892', ['I35440088']], ['A2516274388', ['I15766117']], ['A2139981849', ['I34077901']], ['A2154508352', ['I71395657']], ['A81319667', ['I34077901']], ['A2751848658', ['I5740404']], ['A2616243850', ['I15032026']], ['A2072642066', ['I200777214']], ['A2111178729', ['I1314135232']], ['A51613867', []], ['A2020538124', ['I133731052']], ['A36345969', ['I84884186']], ['A3080682343', ['I919571938']], ['A3048400258', ['I162266279']], ['A70431856', ['I147962203']], ['A38681195', ['I79576946']], ['A2128003500', ['I106118109']], ['A2309647198', ['I106118109']], ['A2056286786', ['I35440088']], ['A1821610080', ['I16568816']], ['A2130299370', ['I129774422']], ['A1999238401', ['I111979921']], ['A3213823506', ['I162577319']], ['A2013557670', ['I4068193']], ['A192613858', ['I35440088']], ['A2487164362', ['I203339264']], ['A1872612610', ['I74656192']], ['A2599897309', ['I90871651']], ['A2022506579', ['I45129253']], ['A1360065292', []], ['A1986960588', ['I138006243']], ['A2165568782', ['I186903577']], ['A15702574', ['I1302113299']], ['A1996881118', ['I161296585']], ['A3082413885', ['I32021983']], ['A2145013741', ['I45438204']], ['A2115204849', ['I27837315']], ['A3115080906', ['I142740786']], ['A2276960644', ['I174612323']], ['A2224509441', ['I91080651']], ['A2892472189', ['I193531525']], ['A2187948114', ['I170239107']], ['A2140349750', []], ['A2578736639', ['I84218800']], ['A2439751444', ['I141882310']], ['A2169501340', ['I78577930']], ['A2189069753', ['I52357470']], ['A1982407881', ['I102134673']], ['A2041726998', ['I56590836']], ['A2097815158', ['I52010207']], ['A2162184194', ['I139264467']], ['A1596503628', ['I142617266']], ['A1597565638', ['I36258959']], ['A2232751947', ['I7171862']], ['A1164815062', ['I592451']], ['A2299327539', ['I135310074']], ['A2440559524', ['I919571938']], ['A1921666821', []], ['A2078100506', ['I7171862']], ['A1829579451', ['I138006243']], ['A2128367244', ['I129604602']], ['A2132824002', ['I36234482']], ['A2012100389', []], ['A134135217', []], ['A2007554175', ['I27804330']], ['A3206000835', ['I68947357']], ['A2937094180', []], ['A2139461416', ['I51556381']], ['A1241258664', ['I69737025']], ['A1976449036', ['I174306211']], ['A2045477652', ['I136199984']], ['A2764937334', ['I111950717']], ['A2764405036', ['I14243506']], ['A2764577755', ['I185261750']], ['A1995665993', ['I142263535']], ['A2132837327', ['I184999862']], ['A2970634057', ['I154526488']], ['A1841977250', ['I103635307']], ['A3150590530', ['I204730241']], ['A1838555939', ['I25399158']], ['A2214856854', ['I157536573']], ['A2121625664', ['I139264467']], ['A3211657094', ['I184693016']], ['A2162249754', ['I196829312']], ['A2163637691', ['I157485424']], ['A2159600766', ['I24062138']], ['A2122525110', ['I16733864']], ['A2294477308', ['I197347611']], ['A3212998215', ['I36258959']], ['A2420628768', ['I57206974']], ['A2146836489', ['I27837315']], ['A2968523170', ['I146429904']], ['A2251802237', ['I193775966']], ['A2168299160', ['I32971472']], ['A2103897366', ['I136199984']], ['A2486459688', ['I139264467']], ['A2750740843', ['I165441096']], ['A2353595567', ['I189590672']], ['A2105867777', ['I87111246']], ['A2117940355', ['I35440088']], ['A2892934792', ['I197347611']], ['A2406497802', ['I170897317']], ['A2110980550', []], ['A2288397916', ['I170201317']], ['A2497646683', ['I1299303238']], ['A554426359', ['I33213144']], ['A2651234109', ['I141945490']], ['A359598977', ['I277688954']], ['A2778478835', ['I99065089']], ['A2177535625', ['I24943067']], ['A2605505313', ['I107672454']], ['A1940962960', []], ['A1950382976', ['I132735039']], ['A2107535010', ['I1294671590']], ['A110384142', ['I153297377']], ['A2283696713', ['I223822909']], ['A2078631022', []], ['A2776831179', ['I92039509']], ['A2104728905', ['I130442723']], ['A2923330299', ['I108290504']], ['A2124652669', ['I1330342723']], ['A2033986293', ['I116067653']], ['A2102690479', ['I79576946']], ['A2169992334', ['I7882870']], ['A2153569924', ['I162577319']], ['A2138292393', ['I45129253']], ['A3179773628', ['I134560555']], ['A2892482235', ['I142740786']], ['A2602933171', ['I204465549']], ['A2752536330', ['I165932596']], ['A2619577251', ['I200719446']], ['A2701422740', ['I200296433']], ['A2618730570', ['I150468666']], ['A3187450718', ['I141568987']], ['A3084817636', ['I19820366']], ['A2883019131', ['I5023651']], ['A3205402657', []], ['A2618582769', ['I19820366']], ['A2303072913', ['I66867065']], ['A2767350564', ['I76214153']], ['A2752193848', ['I151075929']], ['A2134625078', ['I1174212']], ['A3189165778', ['I135364568']], ['A2168849604', ['I58200834']], ['A2688360339', ['I76903346']], ['A2434365487', []], ['A2751825327', ['I125043107']], ['A2101217887', ['I27837315']], ['A2791568493', ['I24062138']], ['A2164707073', ['I165932596']], ['A2170194773', ['I196345858']], ['A2123861521', ['I182534213']], ['A2118536291', ['I142974352']], ['A2578384365', ['I47519274']], ['A3213188914', ['I3923682']], ['A2889344519', ['I78577930']], ['A2170426116', ['I76130692']], ['A2614760855', ['I183934855']], ['A2112054143', ['I173093425']], ['A2131480036', ['I52357470']], ['A2101388595', ['I191208505']], ['A2227742481', ['I16733864']], ['A2098834272', ['I91807558']], ['A2610047428', ['I1305258596']], ['A2152880264', ['I122140584']], ['A2273627582', ['I133731052']], ['A2022287044', ['I165779595']], ['A296284316', ['I74656192']], ['A2153885781', ['I32021983']], ['A1128521249', ['I162714631']], ['A2955242832', ['I126744593']], ['A1976383558', ['I126307644']], ['A238801926', ['I28407311']], ['A165226029', ['I170897317']], ['A2589542415', ['I24185976']], ['A2671227342', ['I177933477']], ['A2620402277', ['I3923682']], ['A2304192564', ['I27837315']], ['A2123380830', ['I162838928']], ['A2336352570', []], ['A3086902963', ['I36258959']], ['A2750810719', ['I173093425']], ['A3159610719', ['I24185976']], ['A2096267119', ['I91045830']], ['A2149329083', ['I111950717']], ['A2109461231', ['I191996457']], ['A2753545238', ['I197869895']], ['A2925131902', ['I92039509']], ['A2148396606', ['I58200834']], ['A2790898102', ['I76130692']], ['A3138559220', ['I919571938']], ['A2342447464', ['I154099455']], ['A2704785703', ['I76130692']], ['A2679367813', ['I881766915']], ['A2303017009', ['I27837315']], ['A2752184305', ['I204823248']], ['A3158501833', ['I162577319']], ['A2111592951', ['I99065089']], ['A2311173724', ['I47720641']], ['A2600460400', ['I5740404']], ['A245233212', ['I19700959']], ['A2219140059', []], ['A13932995', ['I188760350']], ['A2306047567', ['I204465549']], ['A1965088654', ['I188760350']], ['A1179666901', ['I25041050']], ['A2751690492', ['I165932596']], ['A2024843826', ['I150468666']], ['A2310780896', ['I26092322']], ['A119169988', ['I165339363']], ['A2329795154', ['I71999127']], ['A2145421720', ['I121748325']], ['A725635384', ['I919571938']], ['A2801896375', ['I106118109']], ['A3213903426', ['I864159182']], ['A2047759535', ['I170201317']], ['A1762709', ['I84884186']], ['A3214403996', ['I170201317']], ['A2144299288', ['I149251103']], ['A2133475979', ['I204512498']], ['A3211750422', ['I27804330']], ['A2752986993', ['I87445476']], ['A2411583955', ['I180857899']], ['A3211912227', ['I168719708']], ['A2112476138', ['I201841394']], ['A151974354', ['I84218800']], ['A2275064168', ['I16835326']], ['A2179011611', ['I99682543']], ['A2252167252', ['I1322918889']], ['A1703265927', ['I27837315']], ['A2234739968', ['I212119943']], ['A2131581496', ['I124055696']], ['A2213195343', ['I141945490']], ['A2709963644', ['I143302722']], ['A2110597439', ['I897542642']], ['A27243130', ['I900890020']], ['A2172153628', ['I126231945']], ['A2748043197', ['I52357470']], ['A3213196971', ['I183067930']], ['A2306059988', ['I156144747']], ['A2139378632', ['I5740404']], ['A2544297585', ['I17974374']], ['A2080659213', ['I17974374']], ['A3178444176', ['I19772626']], ['A2465402738', ['I173911158']], ['A2527365352', ['I1317189400']], ['A2293805872', ['I40347166']], ['A344677311', ['I32971472']], ['A2296485090', ['I79620101']], ['A2097409745', ['I15766117']], ['A190605784', ['I84392919']], ['A246210690', ['I19772626']], ['A2147902604', ['I58956616']], ['A2171042865', ['I74801974']], ['A140443645', ['I33213144']], ['A341438660', ['I102064193']], ['A2422431356', ['I99464096']], ['A2125127581', ['I63634437']], ['A2942664097', ['I102322142']], ['A2077942018', ['I145894827']], ['A2291975411', []], ['A2010089562', []], ['A2279142043', ['I1285301757']], ['A2073581922', ['I202276237']], ['A250898896', ['I27897274']], ['A2004260037', ['I165690674']], ['A13319392', ['I2801843802']], ['A3009812243', ['I126520041']], ['A1880732984', ['I136199984']], ['A2507702655', ['I2800451603']], ['A1567612894', ['I169521973']], ['A2007172507', []], ['A3165078136', ['I100532134']], ['A2170628017', ['I45129253']], ['A2160977220', ['I136199984']], ['A2596362111', ['I150468666']], ['A2752779002', ['I157773358']], ['A2932004', ['I1302444339']], ['A2157981412', ['I189158943']], ['A157154497', ['I110736937']], ['A2608972451', ['I197809005']], ['A2083012349', ['I180670191']], ['A49026025', ['I101202996']], ['A2150872280', ['I193662353']], ['A2430556241', ['I1174212']], ['A2751870832', []], ['A2461067341', []], ['A2141483167', ['I165339363']], ['A2220817787', ['I4068193']], ['A2121495985', ['I146416000']], ['A2149045475', ['I9360294']], ['A2113986269', ['I129774422']], ['A2111119233', ['I1317189400']], ['A2113744612', ['I205274468']], ['A2156202600', ['I99464096']], ['A2310596126', ['I134820265']], ['A244530116', []], ['A2507887232', []], ['A131183934', ['I149213910']], ['A2806988037', ['I1299303238']], ['A3022753522', ['I19772626']], ['A1976232212', ['I149251103']], ['A2752098531', []], ['A259198985', []], ['A2708078709', ['I17974374']], ['A1988207911', ['I203765153']], ['A2230581659', ['I40347166']], ['A2106718804', ['I131729948']], ['A2157759711', ['I27897274']], ['A2591419044', ['I97018004']], ['A2210185534', ['I40677808']], ['A2141912639', ['I89630735']], ['A2989752567', ['I65837984']], ['A2506652829', ['I159385669']], ['A2312024874', ['I15807432']], ['A1891667298', ['I1299303238']], ['A2003836090', ['I69737025']], ['A1987478193', ['I154387261']], ['A2304678047', ['I130238516']], ['A2104535929', ['I97565354']], ['A1921121999', ['I5023651']], ['A1811775135', ['I204205638']], ['A2752605122', ['I125043107']], ['A2137516516', ['I111088046']], ['A2113113424', ['I129902397']], ['A1850674520', ['I28166907']], ['A2508180301', ['I135310074']], ['A2133363501', ['I27577105']], ['A2019737734', ['I170897317']], ['A2069359380', ['I7882870']], ['A304090729', ['I80043']], ['A2778802860', ['I887064364']], ['A2171692666', []], ['A2081434776', ['I227486990']], ['A2915856741', ['I204730241']], ['A2285887256', ['I79620101']], ['A2112253736', ['I126520041']], ['A2288432791', ['I166337079']], ['A2103904840', ['I887064364']], ['A2119964541', ['I174216632']], ['A1789312898', ['I116067653']], ['A1510307710', ['I116067653']], ['A2169017652', ['I3130050927']], ['A2159810634', []], ['A2253180405', ['I185261750']], ['A2112137136', []], ['A2127250876', ['I203088144']], ['A2907211391', ['I87445476']], ['A2099420828', []], ['A2902503494', ['I191996457']], ['A2025253396', ['I116067653']], ['A353205705', ['I149251103']], ['A2342027299', ['I52325']], ['A2179695644', ['I34809795']], ['A2101649411', ['I68368234']], ['A2304749222', ['I2800188116']], ['A2165222443', []], ['A2331474132', ['I129604602']], ['A2782421992', ['I190249584']], ['A2145517252', ['I27897274']], ['A2137478961', ['I170897317']], ['A2751203643', ['I151075929']], ['A2127980544', ['I177933477']], ['A2016725799', ['I2800188116']], ['A2464867937', []], ['A2291204823', []], ['A2101164981', ['I98704320']], ['A2034174410', ['I129604602']], ['A2001738667', ['I56590836']], ['A1855057189', []], ['A2146956581', ['I154387261']], ['A1090273509', ['I241749']], ['A2108643002', ['I919571938']], ['A2147158538', ['I126231945']], ['A2284622197', ['I181369854']], ['A191727320', ['I185261750']], ['A2152660246', ['I154338468']], ['A2130785121', ['I1313323035']], ['A2092663359', ['I298625061']], ['A176042587', ['I165779595']], ['A2089413804', ['I189158943']], ['A2085026211', []], ['A2141606575', ['I79576946']], ['A3105362818', ['I36258959']], ['A2152313626', ['I158929114']], ['A1985327407', ['I74801974']], ['A1958020847', ['I188760350']], ['A3105148803', []], ['A2120036141', ['I102134673']], ['A2263230484', ['I48057805']], ['A2115930151', []], ['A2569826001', ['I5124864']], ['A2942730159', ['I204337017']], ['A2688173384', ['I113428412']], ['A2260118538', ['I184999862']], ['A3214675137', []], ['A568277566', ['I126307644']], ['A1988623638', ['I71267560']], ['A2229862973', ['I154570441']], ['A3205900843', ['I1317189400']], ['A2132063793', ['I23923803']], ['A2100364792', ['I31944674']], ['A2129030445', ['I76903346']], ['A2430043349', ['I204730241']], ['A2159556512', ['I861853513']], ['A2323091549', ['I119003972']], ['A2609098379', ['I51713134']], ['A2220323812', ['I39804081']], ['A2515045469', ['I72253084']], ['A2162154216', ['I170239107']], ['A2169280410', ['I47838141']], ['A2916781309', ['I162148367']], ['A143629990', ['I2799424032']], ['A2000170166', ['I169521973']], ['A1302453134', ['I90344618']], ['A1523101531', ['I149899117']], ['A131924544', ['I157674565']], ['A573175732', ['I52099693']], ['A2510984620', ['I184565670']], ['A1925615927', ['I200777214']], ['A2156906139', ['I135310074']], ['A2153045259', ['I51713134']], ['A2609177278', ['I68947357']], ['A1998026466', []], ['A2578885609', []], ['A1974549842', ['I202697423']], ['A2125658460', ['I102322052']], ['A2317416293', ['I205274468']], ['A2108499812', ['I183934855']], ['A312358705', ['I204465549']], ['A2050226338', ['I141945490']], ['A2442950858', ['I112859197']], ['A2016402814', ['I28166907']], ['A2165359933', ['I165522056']], ['A2923839752', []], ['A2176131684', ['I168356945']], ['A2136294689', ['I145872427']], ['A3212811569', ['I106118109']], ['A2304473344', ['I592451']], ['A2098983090', ['I12834331']], ['A2109561020', ['I129634264']], ['A2069382544', ['I114531698']], ['A2150383515', ['I2613432']], ['A2116847508', []], ['A2280545772', ['I162266279']], ['A2344958027', []], ['A2915232689', ['I136199984']], ['A2179696547', ['I241749']], ['A2509118969', ['I204730241']], ['A2124312915', ['I184840846']], ['A2150284267', ['I17974374']], ['A581233313', ['I45438204']], ['A2485238702', ['I36258959']], ['A1981358432', ['I39422238']], ['A2146540692', ['I149251103']], ['A2079273259', ['I201324441']], ['A2291808999', ['I145872427']], ['A1496881585', ['I145872427']], ['A2087001055', ['I60858718']], ['A2042689483', ['I1343675862']], ['A1857494896', ['I39555362']], ['A2752477182', ['I198244214']], ['A3212135970', ['I8087733']], ['A2595565838', ['I68947357']], ['A2339085104', ['I184942183']], ['A2118899956', ['I28166907']], ['A2134669963', ['I199525922']], ['A1969615809', []], ['A1819781403', []], ['A2789742885', ['I3923682']], ['A2161986744', ['I57206974']], ['A2145403019', ['I126744593']], ['A2101044136', ['I98285908']], ['A613031280', ['I180923762']], ['A2667485434', ['I69552723']], ['A2300640557', ['I28166907']], ['A2754287920', ['I149213910']], ['A1366312813', ['I40734275']], ['A1540118155', ['I136199984']], ['A2270877170', ['I33213144']], ['A2591293673', ['I76134821']], ['A2289136840', ['I34077901']], ['A2789534', ['I8087733']], ['A2029422335', ['I79510175']], ['A2108080164', ['I27577105']], ['A568361088', ['I79576946']], ['A2011515881', ['I17937529']], ['A1497142908', ['I134820265']], ['A2136580040', ['I1299303238']], ['A308466591', ['I153297377']], ['A2002986278', ['I163917720']], ['A2926244110', ['I193775966']], ['A89931551', ['I96036126']], ['A2097891424', ['I89098413']], ['A2160717809', ['I45294948']], ['A2327835557', ['I98285908']], ['A2696766224', ['I16568816']], ['A2568270822', ['I134820265']], ['A567043044', ['I102134673']], ['A2311843655', ['I124055696']], ['A2118604696', ['I27837315']], ['A2776236022', ['I36258959']], ['A2727572233', ['I204730241']], ['A2294440266', ['I227486990']], ['A2006692374', ['I51556381']], ['A2048429378', ['I16097986']], ['A2140409773', ['I16097986']], ['A51120859', ['I27897274']], ['A2055504964', ['I56590836']], ['A2105954712', ['I201448701']], ['A2321259327', ['I125602781']], ['A1705070746', ['I114090438']], ['A2171312956', ['I98285908']], ['A1761085357', ['I183935753']], ['A2616246910', ['I1174212']], ['A2169182293', ['I74656192']], ['A2105390063', ['I159948400']], ['A2647276073', ['I170201317']], ['A1795164152', ['I1334819555']], ['A1959572629', ['I169521973']], ['A2617445435', ['I111979921']], ['A3193786759', ['I919571938']], ['A2500298015', ['I70931966']], ['A665282295', []], ['A2139797130', ['I32881790']], ['A2596356635', []], ['A179892997', ['I71267560']], ['A1880391771', ['I99542240']], ['A2309064161', ['I170201317']], ['A250948042', ['I188760350']], ['A2081957017', ['I84218800']], ['A491390281', ['I21250087']], ['A2790595355', ['I113428412']], ['A2043041959', ['I32021983']], ['A2132160974', ['I154526488']], ['A2121021919', ['I119439378']], ['A1978412612', ['I204337017']], ['A2307651376', ['I116067653']], ['A2002663481', ['I25217355']], ['A2583328809', ['I19772626']], ['A2884867384', ['I76130692']], ['A2752684509', ['I58200834']], ['A2615312383', ['I28166907']], ['A2187228880', ['I91045830']], ['A1265710658', ['I134820265']], ['A1998081531', []], ['A2293256855', ['I24354313']], ['A2429102547', ['I200777214']], ['A2128255761', ['I45084792']], ['A1969568480', ['I79620101']], ['A2149681208', ['I255234318']], ['A2152015205', ['I200719446']], ['A1999562055', ['I71267560']], ['A2568907187', ['I204308271']], ['A2424673218', ['I193775966']], ['A2136538872', ['I197347611']], ['A3150100526', ['I51713134']], ['A777279592', ['I196349391']], ['A2025758292', ['I99464096']], ['A1898643240', ['I27837315']], ['A2513961881', ['I15057530']], ['A2609979139', ['I3018075036']], ['A1824587455', ['I129902397']], ['A2963057689', ['I74656192']], ['A2312086799', []], ['A2721407907', ['I189957204']], ['A2268780655', ['I5023651']], ['A3088557330', ['I22465464']], ['A2461067341', []], ['A116241134', ['I162148367']], ['A2151829138', ['I80606768']], ['A2119251795', ['I134820265']], ['A2195810570', ['I204730241']], ['A2424615314', ['I40963666']], ['A2038538743', ['I55143463']], ['A2151442323', ['I138211613']], ['A338555662', ['I125043107']], ['A2161907732', ['I165779595']], ['A2626132259', ['I88273585']], ['A2599350643', ['I76903346']], ['A2753724287', []], ['A2311637924', ['I79238269']], ['A2019828037', ['I8833935']], ['A55119260', ['I134820265']], ['A2162831485', []], ['A1980677892', ['I20388574']], ['A2606371788', ['I170201317']], ['A2223604068', ['I45204951']], ['A2131666552', ['I165932596']], ['A2126774721', ['I175532246']], ['A2618753083', ['I19772626']], ['A2153844047', ['I865915315']], ['A2345848967', ['I124055696']], ['A271630793', ['I921990950']], ['A614307285', ['I5023651']], ['A21050232', ['I184942183']], ['A2115914357', ['I1299303238']], ['A2116023480', ['I116067653']], ['A1993718659', ['I27897274']], ['A2344826705', []], ['A734988865', []], ['A1962740324', []], ['A1937633015', ['I184999862']], ['A2136304127', ['I134820265']], ['A2752948877', []], ['A2530370213', ['I16391192']], ['A2164534774', ['I122346577']], ['A2126708356', ['I201324441']], ['A2923806565', ['I19772626']], ['A1773671848', ['I141945490']], ['A2608072300', ['I111979921']], ['A2054314735', ['I904495901']], ['A2617691925', ['I97018004']], ['A108857141', ['I74656192']], ['A1483053713', []], ['A2295254602', ['I74788687']], ['A1853714356', ['I189158943']], ['A2156648439', ['I5023651']], ['A2010686301', ['I27837315']], ['A1870956708', ['I74656192']], ['A2227852279', ['I864159182']], ['A2618266482', ['I121934306']], ['A2131637617', ['I40120149']], ['A2107682808', ['I33213144']], ['A2293806389', ['I84218800']], ['A2615592242', ['I134820265']], ['A1830090278', ['I45129253']], ['A2150830482', ['I197323543']], ['A2137851853', ['I56590836']], ['A175375135', ['I15057530']], ['A2297755505', ['I157614274']], ['A1829862504', ['I1299303238']], ['A24913793', ['I8087733']], ['A68695590', ['I62916508']], ['A2039733517', ['I34771391']], ['A545702560', ['I185443292']], ['A2156229566', ['I1299303238']], ['A2017649007', ['I51713134']], ['A203561907', ['I135310074']], ['A2167178050', []], ['A2165367570', ['I97565354']], ['A1918372463', ['I51713134']], ['A2708112663', ['I58956616']], ['A2753713354', ['I76130692']], ['A2104655788', ['I3923682']], ['A2337948740', []], ['A2776008590', ['I162266279']], ['A71693736', ['I151746483']], ['A2296335645', ['I165733156']], ['A2067970811', ['I1299303238']], ['A2063959728', ['I170201317']], ['A2002887331', []], ['A2251078558', ['I107672454']], ['A2108613514', ['I184840846']], ['A2107691084', ['I51713134']], ['A2286522423', ['I32389192']], ['A2505654191', ['I8692664']], ['A2126863223', ['I114090438']], ['A2032928414', ['I129801699']], ['A2015323836', ['I241749']], ['A2119543815', ['I45438204']], ['A2097662861', ['I1320320070']], ['A2097316953', ['I55302922']], ['A2923502269', ['I223822909']], ['A2295491084', ['I145311948']], ['A2259363104', ['I204730241']], ['A2161627065', ['I155781252']], ['A2616971910', ['I204465549']], ['A2618156792', ['I151746483']], ['A1966201655', ['I193662353']], ['A2008441592', ['I142762351']], ['A2163903781', ['I44260953']], ['A2099017714', ['I149899117']], ['A2030113607', ['I91045830']], ['A1985477870', ['I887064364']], ['A3179013793', ['I12834331']], ['A2112230714', ['I130769515']], ['A2135229817', ['I241749']], ['A2295235684', ['I51713134']], ['A2620216424', ['I184999862']], ['A2924795008', ['I135310074']], ['A2585315332', ['I47838141']], ['A2255213829', ['I5681781']], ['A2141870759', ['I129604602']], ['A2015348626', ['I39804081']], ['A2026904872', ['I138006243']], ['A2507574558', ['I90183372']], ['A2252002679', ['I52357470']], ['A2463272352', ['I191632667']], ['A1996985638', ['I32021983']], ['A1368035416', ['I122534668']], ['A2017940063', ['I147962203']], ['A2608839781', ['I124601658']], ['A2153816640', ['I110200422']], ['A2140192320', ['I592451']], ['A3124150036', ['I63135867']], ['A2100193814', ['I141945490']], ['A2752180067', ['I94509681']], ['A2024012048', ['I900890020']], ['A2194660619', ['I177639307']], ['A2466762143', ['I141596103']], ['A2583550443', []], ['A2119878464', ['I134820265']], ['A2063205907', ['I134820265']], ['A2224200122', ['I189590672']], ['A2122532761', ['I114090438']], ['A209469506', ['I120156002']], ['A2164008391', ['I153297377']], ['A2010195370', ['I115752224']], ['A2142251104', ['I27483092']], ['A2154621144', ['I15881080']], ['A2605161176', ['I53110688']], ['A2144078314', ['I71999127']], ['A2796631224', ['I348769827']], ['A2132062588', ['I129902397']], ['A1860901046', ['I32021983']], ['A531715290', ['I864159182']], ['A2142123865', ['I32389192']], ['A2083900901', []], ['A1381959524', ['I39804081']], ['A2157792115', ['I34352273']], ['A2464166708', ['I136199984']], ['A2114148199', ['I79238269']], ['A260283546', ['I241749']], ['A2299426471', ['I143302722']], ['A1546364826', ['I124227911']], ['A2134687508', ['I21250087']], ['A2777405929', ['I112756935']], ['A2066282525', ['I1850255']], ['A2146064664', ['I53110688']], ['A1975750878', ['I56590836']], ['A78390493', ['I189158943']], ['A2599651151', ['I22759111']], ['A2126579239', ['I197604219']], ['A3038676067', ['I84392919']], ['A2131204719', ['I45204951']], ['A1991086061', ['I184942183']], ['A1208611782', ['I1313323035']], ['A2603128508', ['I7882870']], ['A289791149', ['I205783295']], ['A2246745663', ['I63966007']], ['A2436717684', ['I5023651']], ['A2485254871', []], ['A2132372358', ['I1299303238']], ['A129524154', ['I102322142']], ['A222975609', ['I32021983']], ['A274731266', ['I16391192']], ['A2060255852', ['I129604602']], ['A1931704802', ['I45438204']], ['A2936597170', ['I80188885']], ['A2143737754', ['I922474255']], ['A2166409687', ['I165735259']], ['A112302362', ['I114531698']], ['A1944283370', ['I139264467']], ['A243792093', ['I2801357902']], ['A3073924587', ['I45129253']], ['A2430822470', ['I130238516']], ['A2974731772', ['I1301076528']], ['A181638723', ['I27897274']], ['A208194016', ['I188760350']], ['A2106016213', []], ['A1969002052', ['I95013407']], ['A2005445360', ['I79189158']], ['A2526901993', []], ['A2096077750', ['I59722631']], ['A1263185259', ['I63966007']], ['A2073091047', ['I19772626']], ['A2022903173', ['I112859197']], ['A2151365744', ['I203088144']], ['A2300620351', ['I152429107']], ['A2258773588', ['I181547552']], ['A2518352874', ['I185261750']], ['A2416679303', ['I125043107']], ['A2281998230', ['I79619799']], ['A2751929498', []], ['A97236879', ['I180670191']], ['A2742023288', ['I159385669']], ['A2939240821', ['I10091056']], ['A2134380198', ['I106118109']], ['A2101099498', ['I165735259']], ['A2127758285', ['I165735259']], ['A1966839084', ['I36258959']], ['A1543134070', ['I145608581']], ['A1353274267', ['I241749']], ['A2127081219', ['I114090438']], ['A2780199462', []], ['A1970937486', ['I202697423']], ['A20352627', ['I168635309']], ['A2212820600', ['I95457486']], ['A2431292898', ['I865915315']], ['A2104312022', ['I116067653']], ['A2024253920', ['I5023651']], ['A2600327442', ['I1299303238']], ['A2145146070', ['I74656192']], ['A2621621490', ['I95793202']], ['A3185773931', []], ['A2117532963', ['I196349391']], ['A3212558598', ['I19772626']], ['A639184022', ['I78577930']], ['A2635222727', ['I202697423']], ['A316540963', ['I1174212']], ['A269799851', ['I97565354']], ['A2989609650', ['I32021983']], ['A2019990030', ['I223822909']], ['A2125847360', ['I1330342723']], ['A2568263559', []], ['A2140483666', ['I136199984']], ['A1994348810', ['I102322142']], ['A1956746426', ['I102322142']], ['A2303397834', ['I197604219']], ['A2066305482', ['I185261750']], ['A1968973730', ['I133731052']], ['A89239059', ['I182534213']], ['A2312094102', ['I202276237']], ['A153716914', ['I74775410']], ['A1233868224', ['I39422238']], ['A1226450405', ['I184942183']], ['A1971378117', ['I60053951']], ['A1977499671', ['I69737025']], ['A1902309041', []], ['A1986842802', ['I1282927834']], ['A2274994864', ['I72816309']], ['A304465661', ['I204465549']], ['A2007652857', ['I145311948']], ['A2002131078', ['I142740786']], ['A2291017571', ['I202697423']], ['A1353490395', ['I134820265']], ['A2008965809', ['I145311948']], ['A2001513007', ['I107139324']], ['A1979775746', []], ['A2245463508', ['I32389192']], ['A1076654334', ['I184565670']], ['A2037979195', ['I149251103']], ['A2497642997', ['I66946132']], ['A2155920882', ['I150468666']], ['A2753071030', ['I84653119']], ['A3001541148', []], ['A2167088773', ['I165932596']], ['A2779646864', ['I188760350']], ['A2751346159', ['I183067930']], ['A2126270369', ['I3923682']], ['A2777511570', ['I96852419']], ['A2955138739', ['I1299303238']], ['A2165097901', ['I125749732']], ['A2226437572', ['I141945490']], ['A2751948669', ['I1320320070']], ['A2115068654', ['I126520041']], ['A1974971413', ['I130769515']], ['A2094326803', ['I173915773']], ['A2250444872', ['I1323638106']], ['A2609480776', ['I132400726']], ['A2552071644', ['I162838928']], ['A3212597016', ['I47519274']], ['A2116125908', ['I115498069']], ['A2126918581', ['I125602781']], ['A2753700445', ['I77107153']], ['A2142726140', ['I170897317']], ['A3214528059', ['I65837984']], ['A2165668089', ['I201537933']], ['A2339282160', ['I22299242']], ['A1473131740', ['I135598925']], ['A148681648', ['I34352273']], ['A227096436', ['I111088046']], ['A2136735004', ['I5023651']], ['A2693536201', []], ['A2197820532', ['I39422238']], ['A2529366122', ['I98559091']], ['A2133857935', []], ['A2008431922', ['I36258959']], ['A1789537721', ['I57206974']], ['A1810372437', ['I66760702']], ['A1243789238', []], ['A2307793828', ['I17974374']], ['A155836766', []], ['A2153572625', ['I170201317']], ['A3016306393', ['I1320320070']], ['A2061331518', ['I196349391']], ['A3211373018', ['I40120149']], ['A2070244911', ['I118564535']], ['A2129015930', ['I5561750']], ['A2435991314', ['I110736937']], ['A1979244625', ['I184942183']], ['A2120846984', ['I19772626']], ['A2129914824', ['I170201317']], ['A2112606733', ['I45294948']], ['A2158185730', ['I145311948']], ['A2141651828', ['I57328836']], ['A2114557256', ['I1330342723']], ['A1969619859', ['I99542240']], ['A2108429819', ['I66760702']], ['A1965201369', ['I26092322']], ['A2031739080', ['I39804081']], ['A2584022586', ['I14243506']], ['A2059172442', ['I147962203']], ['A49109219', ['I169333911']], ['A2021364189', ['I153718931']], ['A803626524', ['I21449261']], ['A2032512628', ['I12912129']], ['A1988342515', ['I88273585']], ['A804628237', ['I2613432']], ['A2796786654', ['I25399158']], ['A2122419199', ['I99464096']], ['A2425413206', ['I27837315']], ['A2019135271', ['I1330204332']], ['A2131388106', ['I59270414']], ['A2300882143', ['I138925566']], ['A2003502610', ['I99729588']], ['A2297761146', []], ['A2156030648', ['I154099455']], ['A2463474097', ['I197347611']], ['A2617073854', ['I141568987']], ['A3212883995', ['I5023651']], ['A3037786015', ['I196829312']], ['A2159993259', ['I1343551460']], ['A1968190172', ['I165932596']], ['A69519358', ['I36258959']], ['A2464700080', ['I861853513']], ['A195607222', ['I28407311']], ['A2619842053', ['I47251452']], ['A3180199207', ['I108108428']], ['A2169748006', ['I99682543']], ['A109559607', ['I121797337']], ['A2411217927', ['I145872427']], ['A1928426505', ['I1850255']], ['A2038993062', ['I5388228']], ['A2143947882', ['I169541294']], ['A2061490125', ['I36258959']], ['A2129149939', ['I7877124']], ['A2147330786', ['I1330342723']], ['A2004433969', ['I143302722']], ['A1851683297', ['I79940851']], ['A270902571', ['I7171862']], ['A2126951856', ['I170201317']], ['A146363972', ['I1285204247']], ['A3026619529', ['I162577319']], ['A47315345', ['I54009628']], ['A2751165494', ['I36258959']], ['A2016597410', ['I206127235']], ['A1206956775', ['I204250578']], ['A1816730750', ['I919571938']], ['A2123248527', ['I184942183']], ['A2750922550', []], ['A2121184109', ['I114662689']], ['A2590346782', ['I251738']], ['A1823611366', ['I78577930']], ['A2343573258', []], ['A168310380', ['I149899117']], ['A2840132692', ['I44260953']], ['A2006104537', ['I133731052']], ['A158644897', ['I200777214']], ['A1565936035', ['I887968799']], ['A2019704340', ['I52357470']], ['A2226321027', ['I161593684']], ['A2271206384', ['I123946342']], ['A2265343693', []], ['A2776425861', ['I889458895']], ['A2143926894', ['I115592961']], ['A291825067', ['I58956616']], ['A2280611603', ['I123431417']], ['A2140359118', ['I1971141']], ['A2617232190', ['I146824383']], ['A2616663424', ['I76130692']], ['A101690631', ['I76134821']], ['A1993434325', ['I78577930']], ['A2752106851', ['I116953780']], ['A2913573289', ['I55302922']], ['A2602633962', ['I39422238']], ['A2104863192', ['I150468666']], ['A2465892409', ['I881766915']], ['A2149606668', ['I27837315']], ['A2608852522', []], ['A2157412934', ['I99043593']], ['A2248414149', ['I177725633']], ['A2111346884', ['I203951103']], ['A2110073039', ['I74801974']], ['A2104999345', ['I201537933']], ['A3038011219', []], ['A2570235797', ['I184565670']], ['A2035858674', ['I27837315']], ['A2159009512', ['I45129253']], ['A2132346839', ['I187531555']], ['A2134417995', ['I66760702']], ['A2126864243', ['I52099693']], ['A2168461250', ['I71730758']], ['A1963529609', ['I106118109']], ['A2005836928', ['I188760350']], ['A1921613322', ['I166337079']], ['A313818866', ['I919571938']], ['A1906938516', ['I861853513']], ['A1924276330', ['I899635006']], ['A2132407692', ['I7882870']], ['A2294200069', ['I106118109']], ['A2111913918', ['I130769515']], ['A2077742618', ['I106118109']], ['A2157867984', ['I142822886']], ['A2780368425', ['I189996419']], ['A2023053835', ['I23923803']], ['A2052905804', ['I55143463']], ['A1919754729', ['I204730241']], ['A2750904533', ['I157773358']], ['A2965269762', ['I108688024']], ['A2916111151', ['I139322472']], ['A2095874177', ['I83809506']], ['A2117987258', ['I115228651']], ['A2105396419', ['I36426075']], ['A2103109355', ['I40677808']], ['A2120219952', ['I170201317']], ['A2192605831', []], ['A2115837523', ['I78577930']], ['A2032693781', ['I34077901']], ['A2138292136', ['I146516829']], ['A1279066622', ['I136199984']], ['A2124823554', ['I80606768']], ['A2650753848', ['I19772626']], ['A2105248112', ['I27804330']], ['A1883125522', ['I98704320']], ['A1292097617', ['I8901234']], ['A2099570078', ['I121934306']], ['A2154553068', ['I79619799']], ['A2171905270', ['I170897317']], ['A2118668419', []], ['A2160873869', ['I194839184']], ['A180485607', ['I1313323035']], ['A2051814685', ['I79510175']], ['A33587572', ['I204730241']], ['A2429862749', ['I142740786']], ['A2589325248', ['I158708052']], ['A2918746204', ['I919571938']], ['A3211395391', ['I204308271']], ['A310060700', ['I65806277']], ['A2078905243', ['I115752224']], ['A1909499638', ['I2802366037']], ['A2126209746', ['I69552723']], ['A2337950892', ['I1354457']], ['A1976485096', ['I130442723']], ['A2006365224', ['I59130452']], ['A2019068566', ['I51713134']], ['A2433636162', ['I95457486']], ['A2002588768', ['I34809795']], ['A2081697108', ['I74656192']], ['A2116833211', ['I90344618']], ['A2308272796', []], ['A2152394751', ['I36258959']], ['A2104810813', ['I114027177']], ['A2217764608', ['I60158472']], ['A2752003204', ['I141945490']], ['A1550233908', ['I184840846']], ['A1828377105', ['I2801316944']], ['A831611765', ['I5023651']], ['A2010504915', ['I184942183']], ['A2152448233', ['I2800188116']], ['A2212373866', []], ['A2617930958', ['I861853513']], ['A1978781692', ['I28407311']], ['A2177327635', ['I27837315']], ['A2053575444', ['I4068193']], ['A2780586292', ['I122140584']], ['A2007359196', ['I134820265']], ['A2150229191', ['I91045830']], ['A2345032126', []], ['A2103345470', ['I69737025']], ['A2207145338', ['I200777214']], ['A1225474598', ['I97565354']], ['A2098329111', ['I91807558']], ['A2140458513', ['I118564535']], ['A2844363867', ['I173093425']], ['A2297720894', ['I43777268']], ['A2144716352', ['I170201317']], ['A2297409658', ['I919571938']], ['A3213625245', ['I183067930']], ['A2752748428', ['I25846049']], ['A2987135446', ['I5561750']], ['A53849724', ['I43439940']], ['A2191379577', []], ['A2147300610', []], ['A2140696355', ['I141945490']], ['A1453368091', ['I184942183']], ['A2615167005', ['I142740786']], ['A2305747350', ['I205349734']], ['A2330766798', ['I34077901']], ['A2022733508', ['I95793202']], ['A2113699752', ['I34077901']], ['A2019746986', ['I135768898']], ['A2044476897', ['I6446798']], ['A2154245104', []], ['A1956344050', ['I170658231']], ['A2027439199', ['I88273585']], ['A2353693571', ['I168356945']], ['A111492307', ['I2613432']], ['A2102124879', ['I82002896']], ['A2157854163', []], ['A2109651866', ['I24354313']], ['A309766548', ['I202391551']], ['A295204489', ['I28166907']], ['A2751754130', []], ['A2171238558', ['I131729948']], ['A1277714751', ['I97188460']], ['A2100520251', ['I134820265']], ['A2163151538', ['I99464096']], ['A2150653324', ['I99464096']], ['A3171408611', ['I887064364']], ['A375951010', ['I5124864']], ['A2114174222', ['I169381384']], ['A686507523', ['I913958620']], ['A283800304', ['I84218800']], ['A2015882587', ['I865915315']], ['A2343664021', ['I86501945']], ['A178345782', ['I200719446']], ['A1484042476', ['I183934855']], ['A2609699', ['I32597200']], ['A791087812', ['I99464096']], ['A328441968', ['I99464096']], ['A266909695', ['I134820265']], ['A2166631290', ['I182534213']], ['A2303337683', ['I74656192']], ['A313513517', ['I136199984']], ['A313756351', ['I76903346']], ['A2103890567', ['I121748325']], ['A2044383332', ['I200777214']], ['A2048901061', ['I166722992']], ['A6881710', ['I106118109']], ['A3175579817', ['I169381384']], ['A2234827581', ['I204337017']], ['A2108309926', ['I919571938']], ['A2159152791', ['I44260953']], ['A2118121057', ['I123044942']], ['A2002889887', ['I52325']], ['A2434721912', ['I65806277']], ['A1957883122', ['I29770179']], ['A2055184486', ['I1343551460']], ['A2424933637', ['I1294671590']], ['A2754403123', ['I203088144']], ['A2303518653', ['I204465549']], ['A2148154317', ['I919571938']], ['A2147061677', ['I1629065']], ['A2111823582', ['I123044942']], ['A2155664619', ['I134820265']], ['A2597640569', ['I15057530']], ['A2342737236', ['I19772626']], ['A2284797097', []], ['A2056030786', ['I138689650']], ['A2095313447', []], ['A2261171076', ['I189158943']], ['A2146896544', ['I79576946']], ['A2432673693', ['I1313323035']], ['A126580252', ['I7877124']], ['A1953376095', ['I8204097']], ['A1978657226', ['I79620101']], ['A2619258540', []], ['A2776790261', ['I2799424032']], ['A2162128314', ['I136199984']], ['A1285721328', ['I27577105']], ['A2161157219', ['I91045830']], ['A2839970414', ['I165143802']], ['A2102511350', ['I135140700']], ['A2751898618', ['I143578492']], ['A3125528527', ['I157773358']], ['A2303994238', ['I154099455']], ['A2651411558', ['I204465549']], ['A2167509343', ['I173093425']], ['A2751155692', []], ['A2679883042', ['I63135867']], ['A2133390717', ['I66867065']], ['A3211753586', ['I156144747']], ['A2255519943', ['I91045830']], ['A2753086158', []], ['A2164723776', ['I3923682']], ['A2753311718', ['I24943067']], ['A2159077351', ['I1302444339']], ['A2155870438', ['I130769515']], ['A2656599956', ['I189957204']], ['A2145351633', ['I25846049']], ['A2240993880', []], ['A2938651325', ['I52158045']], ['A2303179901', ['I165932596']], ['A2750725501', ['I9916479']], ['A2600843735', ['I16733864']], ['A2609642158', ['I24943067']], ['A3213690927', ['I162577319']], ['A2698693482', ['I19820366']], ['A2162632167', ['I84653119']], ['A2581211627', ['I76130692']], ['A2750820016', []], ['A2677307436', ['I76130692']], ['A2526954208', ['I136199984']], ['A2128621641', ['I189957204']], ['A2793547569', ['I99065089']], ['A3214618265', ['I130769515']], ['A2699038445', ['I2802957242']], ['A2157769176', ['I91807558']], ['A3037721064', ['I2800451603']], ['A2490185788', ['I889458895']], ['A2751437236', ['I141945490']], ['A2514399632', ['I185261750']], ['A2750675639', ['I177933477']], ['A2259103958', ['I2801749607']], ['A2582673382', ['I79619799']], ['A2130958721', ['I223532165']], ['A2196416593', ['I166337079']], ['A104363460', ['I34077901']], ['A2616882364', ['I40677808']], ['A2022051210', ['I125602781']], ['A2892787391', ['I63135867']], ['A2608426538', ['I51713134']], ['A2752353593', ['I881766915']], ['A2098442291', ['I22465464']], ['A2132664903', ['I204465549']], ['A1966196919', ['I75951250']], ['A2176087347', ['I150729083']], ['A2234120362', ['I126520041']], ['A2441904486', ['I27837315']], ['A3211522243', ['I1343551460']], ['A2097329824', ['I54009628']], ['A2094069715', ['I97018004']], ['A2134576896', ['I165779595']], ['A2125158145', ['I102322142']], ['A1827375830', ['I123431417']], ['A2809722427', ['I90344618']], ['A182804047', ['I2801843802']], ['A2151061059', ['I112756935']], ['A2789176749', ['I887064364']], ['A661657007', ['I1118541']], ['A2751525263', ['I2799424032']], ['A2096433066', ['I98677209']], ['A2097944521', ['I44260953']], ['A2606645560', ['I169381384']], ['A3124105374', ['I1320320070']], ['A2104712584', ['I1299303238']], ['A1970754230', ['I904495901']], ['A2120079196', ['I205783295']], ['A2620416145', ['I33213144']], ['A2064709678', ['I149899117']], ['A2040734874', ['I165143802']], ['A2417061709', ['I172675005']], ['A2751068181', ['I145311948']], ['A2619624485', ['I10091056']], ['A2101722725', ['I111950717']], ['A2234114711', ['I2801241159']], ['A2467809829', ['I67415387']], ['A2167082729', ['I121820613']], ['A2767375169', ['I98704320']], ['A2152647425', ['I185443292']], ['A2247002168', ['I24943067']], ['A2902705538', ['I126744593']], ['A2110828893', ['I126520041']], ['A2312131519', ['I24571045']], ['A2099481737', ['I27781120']], ['A2066731401', ['I177725633']], ['A2753431554', ['I156144747']], ['A2938968714', ['I52158045']], ['A2153380939', ['I182534213']], ['A2095879290', ['I136199984']], ['A2753142310', []], ['A2168095834', ['I161318765']], ['A2751067713', ['I183067930']], ['A2925180261', ['I92039509']], ['A2581778956', ['I24185976']], ['A2751201278', ['I177933477']], ['A2099477974', ['I157773358']], ['A2776131358', ['I19820366']], ['A2719092037', ['I27837315']], ['A2101081692', ['I183067930']], ['A2717112005', ['I8692664']], ['A2027057101', ['I206127235']], ['A2102325233', ['I154338468']], ['A2752606168', ['I177725633']], ['A2603307224', ['I183067930']], ['A2684370843', ['I24943067']], ['A2148873373', ['I27837315']], ['A2470103401', ['I156144747']], ['A2645172703', ['I8692664']], ['A2434454559', ['I32574673']], ['A2752397259', ['I25041050']], ['A3189611677', ['I146416000']], ['A2899708064', ['I1330342723']], ['A2935925049', ['I136199984']], ['A2938020161', ['I194450716']], ['A2244227187', ['I32389192']], ['A2777742281', ['I27837315']], ['A2660475775', ['I47720641']], ['A2110959421', ['I205349734']], ['A2522774012', ['I78577930']], ['A2237074504', ['I60134161']], ['A2194054657', ['I34077901']], ['A2073985444', ['I102322142']], ['A2595283966', ['I154099455']], ['A2969473503', []], ['A2704297565', ['I204823248']], ['A2614705480', ['I51556381']], ['A2060971526', ['I2799299286']], ['A2631566280', ['I57206974']], ['A2117646035', ['I130769515']], ['A3191665970', ['I9916479']], ['A2892483973', ['I139660479']], ['A2919328253', ['I47519274']], ['A3211455198', ['I126744593']], ['A2427709379', ['I9916479']], ['A2346928963', ['I1293487830']], ['A2246404642', ['I84653119']], ['A2340634015', ['I159948400']], ['A2430169722', ['I163151501']], ['A3037391176', ['I183067930']], ['A2105104179', ['I97018004']], ['A2133270281', ['I154338468']], ['A2153017491', ['I84653119']], ['A2964771805', ['I1299303238']], ['A2751940155', ['I191996457']], ['A2752347216', []], ['A2152398546', ['I27837315']], ['A2003408062', ['I102502594']], ['A2937152171', ['I9916479']], ['A6119018', ['I150468666']], ['A1951356172', ['I185261750']], ['A1967483855', ['I153297377']], ['A2071602200', ['I98677209']], ['A2750981415', ['I154099455']], ['A2776665768', ['I151075929']], ['A2922854024', ['I592451']], ['A2263289907', ['I141945490']], ['A2750765767', ['I193775966']], ['A2222492810', ['I51226738']], ['A2856262782', ['I40542001']], ['A2936570707', ['I16656306']], ['A2468656760', ['I98285908']], ['A2135055977', ['I919571938']], ['A2936820413', []], ['A2165179713', ['I136199984']], ['A2137176840', ['I108688024']], ['A2144260202', ['I177725633']], ['A2121964278', ['I99065089']], ['A2600279008', ['I78577930']], ['A2752410089', ['I194450716']], ['A2898575476', ['I151075929']], ['A2135638963', ['I136199984']], ['A2120689659', ['I136199984']], ['A1972197648', ['I39422238']], ['A2472832436', ['I168719708']], ['A2220838708', ['I98704320']], ['A1768935960', ['I27837315']], ['A264295317', ['I1325899441']], ['A660231712', []], ['A2182499639', []], ['A2776014699', ['I174216632']], ['A1501353016', ['I241749']], ['A2687974460', ['I157773358']], ['A2607594914', ['I79731167']], ['A2109840822', ['I115228651']], ['A2779974929', ['I106165777']], ['A2122455835', ['I138006243']], ['A2775318612', ['I135198339']], ['A3212632999', []], ['A2751097806', ['I157725225']], ['A2237076740', ['I32574673']], ['A2567308267', ['I19820366']], ['A2892845234', ['I20231570']], ['A2963801832', ['I9916479']], ['A2296962577', ['I91045830']], ['A2976998024', ['I32389192']], ['A2615109963', ['I200296433']], ['A2576514370', []], ['A2654564532', ['I79576946']], ['A2597612040', ['I76130692']], ['A3212066601', ['I19820366']], ['A3205732574', ['I24943067']], ['A2142328456', ['I78757542']], ['A2688530010', ['I74485261']], ['A2632624618', ['I44461941']], ['A2894080343', ['I66946132']], ['A2738367764', ['I9916479']], ['A2123206340', ['I126520041']], ['A2607797499', ['I87445476']], ['A2752864736', ['I183067930']], ['A2435566601', ['I183067930']], ['A2697789819', ['I19820366']], ['A2615643908', ['I20231570']], ['A2225158707', ['I27837315']], ['A2795490405', ['I154099455']], ['A2289741416', ['I200296433']], ['A3214411215', ['I8692664']], ['A2137288553', ['I200296433']], ['A2635821836', ['I168635309']], ['A2704006679', ['I76130692']], ['A1940586810', ['I19880235']], ['A2766164555', ['I162577319']], ['A3109109503', ['I36152291']], ['A2751961996', ['I39422238']], ['A2798430120', ['I184840846']], ['A2752222648', ['I2613432']], ['A2103395540', ['I20231570']], ['A2893057991', ['I183067930']], ['A2658969592', ['I52357470']], ['A2618642680', ['I20231570']], ['A2907174749', ['I87445476']], ['A3211739559', ['I157773358']], ['A2498627003', ['I52158045']], ['A2115448036', ['I157773358']], ['A1916586353', ['I861853513']], ['A3213690738', ['I40120149']], ['A2017920052', ['I99542240']], ['A2269177169', ['I59553526']], ['A2099913023', ['I71999127']], ['A114901541', ['I150468666']]], 'cited_by_count': 4151, 'concepts': [['C86803240', '0.84493244'], ['C203522944', '0.77674335'], ['C527412718', '0.51854616'], ['C70721500', '0.48668128'], ['C54355233', '0.194352']], 'referenced_works': ['W1919353', 'W21038192', 'W55875826', 'W102256019', 'W115026104', 'W126188432', 'W146759131', 'W148258150', 'W163282691', 'W163763239', 'W181726219', 'W182607020', 'W188890781', 'W265817920', 'W394707004', 'W642194586', 'W853095808', 'W863219595', 'W986700140', 'W1458331036', 'W1484432552', 'W1488216507', 'W1489955133', 'W1491311046', 'W1493534235', 'W1495057561', 'W1496026049', 'W1496274565', 'W1498860801', 'W1501786946', 'W1502170122', 'W1507061322', 'W1507759123', 'W1508661542', 'W1509820341', 'W1510443235', 'W1512227914', 'W1513065952', 'W1514391840', 'W1517497707', 'W1518597475', 'W1522857888', 'W1523693806', 'W1524750248', 'W1525943215', 'W1526993239', 'W1530987085', 'W1531984412', 'W1534476908', 'W1534812231', 'W1534829748', 'W1535436687', 'W1538873343', 'W1539484291', 'W1543411779', 'W1545558213', 'W1546731875', 'W1549295637', 'W1550693054', 'W1553918158', 'W1555054913', 'W1555144266', 'W1556031108', 'W1558162216', 'W1559561854', 'W1559587226', 'W1561835222', 'W1562156325', 'W1565580628', 'W1565707194', 'W1566724399', 'W1568601491', 'W1569812687', 'W1570925259', 'W1571208010', 'W1578979147', 'W1579289801', 'W1580236405', 'W1580844082', 'W1580924836', 'W1582622248', 'W1583543179', 'W1588358739', 'W1593406565', 'W1593850395', 'W1594293927', 'W1594399374', 'W1599366903', 'W1600604896', 'W1601578978', 'W1602095849', 'W1605945659', 'W1608504671', 'W1613089073', 'W1613566907', 'W1624868929', 'W1631725776', 'W1642988509', 'W1643149493', 'W1646601758', 'W1664365918', 'W1670406330', 'W1678795445', 'W1680732383', 'W1684533821', 'W1709534931', 'W1760082342', 'W1760596310', 'W1764833860', 'W1769471103', 'W1772777894', 'W1779577685', 'W1784425676', 'W1788453541', 'W1793840948', 'W1797345114', 'W1803199265', 'W1807206891', 'W1834237621', 'W1834837759', 'W1849512654', 'W1851347272', 'W1855321011', 'W1855390275', 'W1857680029', 'W1858567345', 'W1861728615', 'W1867296623', 'W1872816333', 'W1873402352', 'W1875936957', 'W1884934928', 'W1890915339', 'W1900818641', 'W1904531393', 'W1904622810', 'W1908930362', 'W1918329266', 'W1918384196', 'W1919124805', 'W1930205043', 'W1932339177', 'W1935781822', 'W1942176190', 'W1943747098', 'W1948872103', 'W1954209695', 'W1955837289', 'W1956374182', 'W1961053300', 'W1963504630', 'W1963521522', 'W1963521777', 'W1963612967', 'W1963622453', 'W1963688739', 'W1963790204', 'W1964014604', 'W1964069410', 'W1964257510', 'W1964322868', 'W1964804710', 'W1964888681', 'W1964976537', 'W1965097583', 'W1965263236', 'W1965493795', 'W1965501489', 'W1965583970', 'W1965841021', 'W1965849508', 'W1965885099', 'W1966018459', 'W1966122416', 'W1966134982', 'W1966391015', 'W1966590334', 'W1966635422', 'W1966706887', 'W1966759451', 'W1966860807', 'W1966881634', 'W1967085696', 'W1967199837', 'W1967289736', 'W1967743942', 'W1967969910', 'W1968016394', 'W1968322176', 'W1968432606', 'W1968513003', 'W1968843321', 'W1968936285', 'W1969303884', 'W1969342645', 'W1969527606', 'W1969752219', 'W1969752287', 'W1969834123', 'W1969875733', 'W1969889291', 'W1969971911', 'W1970085556', 'W1970240298', 'W1970379960', 'W1970426726', 'W1970428129', 'W1970521215', 'W1970598278', 'W1970637701', 'W1970661950', 'W1970706402', 'W1970791163', 'W1970799201', 'W1970887825', 'W1971023090', 'W1971055167', 'W1971058819', 'W1971308852', 'W1971378307', 'W1971420552', 'W1971597300', 'W1971603587', 'W1971738234', 'W1971861623', 'W1972133970', 'W1972145831', 'W1972184193', 'W1972336737', 'W1972432707', 'W1972489536', 'W1972490217', 'W1972517547', 'W1972618720', 'W1972977158', 'W1972986619', 'W1972995615', 'W1973026523', 'W1973087480', 'W1973156695', 'W1973371164', 'W1973472942', 'W1973570510', 'W1973638222', 'W1973811917', 'W1973836920', 'W1973963032', 'W1974067508', 'W1974161033', 'W1974229080', 'W1974251566', 'W1974504670', 'W1975050610', 'W1975200363', 'W1975293376', 'W1975375868', 'W1975567435', 'W1975622992', 'W1975702709', 'W1975709628', 'W1975915706', 'W1975927237', 'W1975962457', 'W1975981696', 'W1976010532', 'W1976066524', 'W1976080126', 'W1976196055', 'W1976353903', 'W1976531450', 'W1976761774', 'W1976848492', 'W1976855961', 'W1976881265', 'W1977077130', 'W1977352903', 'W1977396132', 'W1977504928', 'W1977648435', 'W1977767269', 'W1977820470', 'W1977844532', 'W1977893444', 'W1978112795', 'W1978118583', 'W1978176829', 'W1978271293', 'W1978397479', 'W1978431358', 'W1978448350', 'W1978685530', 'W1978730497', 'W1978751572', 'W1978791549', 'W1978991502', 'W1979081257', 'W1979198879', 'W1979213765', 'W1979228758', 'W1979259737', 'W1979273897', 'W1979284113', 'W1979318608', 'W1979321726', 'W1979350693', 'W1979490323', 'W1979526109', 'W1979611479', 'W1979739328', 'W1980084094', 'W1980192302', 'W1980439394', 'W1980942517', 'W1980968914', 'W1981122961', 'W1981123941', 'W1981178116', 'W1981224101', 'W1981358316', 'W1981448470', 'W1981451175', 'W1981453024', 'W1981465219', 'W1981749506', 'W1981846819', 'W1981888655', 'W1981897621', 'W1981927394', 'W1982062616', 'W1982112254', 'W1982281067', 'W1982442386', 'W1982502197', 'W1982597495', 'W1982635949', 'W1982985819', 'W1983034682', 'W1983151586', 'W1983161785', 'W1983275934', 'W1983363932', 'W1983453748', 'W1983467638', 'W1983619827', 'W1983701258', 'W1983724570', 'W1983831622', 'W1984000730', 'W1984281474', 'W1984537066', 'W1984652255', 'W1984785884', 'W1985008568', 'W1985028337', 'W1985139784', 'W1985188363', 'W1985408391', 'W1985478291', 'W1985754630', 'W1985910662', 'W1985944042', 'W1985967765', 'W1986301677', 'W1986325384', 'W1986478574', 'W1986536654', 'W1986616897', 'W1986650581', 'W1986875278', 'W1986900494', 'W1986911872', 'W1987038381', 'W1987077730', 'W1987155530', 'W1987298361', 'W1987473208', 'W1987566410', 'W1987566768', 'W1987589362', 'W1987614211', 'W1987636668', 'W1987777400', 'W1988044873', 'W1988089345', 'W1988250463', 'W1988459632', 'W1988484495', 'W1988918021', 'W1989042684', 'W1989051519', 'W1989061093', 'W1989150988', 'W1989165480', 'W1989212506', 'W1989293830', 'W1989511016', 'W1989634299', 'W1989665460', 'W1989745622', 'W1989812239', 'W1989934289', 'W1990138203', 'W1990204346', 'W1990247684', 'W1990387678', 'W1990411057', 'W1990437594', 'W1990490178', 'W1990622149', 'W1990677550', 'W1990868872', 'W1990890678', 'W1990949451', 'W1990953681', 'W1990988003', 'W1990995698', 'W1991006612', 'W1991075835', 'W1991147034', 'W1991300803', 'W1991389529', 'W1991421091', 'W1991489644', 'W1991853144', 'W1992052850', 'W1992300913', 'W1992306174', 'W1992393206', 'W1992538476', 'W1992565343', 'W1992643412', 'W1992659717', 'W1992684857', 'W1992881494', 'W1992929247', 'W1992947862', 'W1992969424', 'W1993098682', 'W1993231978', 'W1993327137', 'W1993388062', 'W1993458861', 'W1993582616', 'W1993690543', 'W1993724469', 'W1993741004', 'W1993829398', 'W1993902882', 'W1993932802', 'W1993984878', 'W1993988048', 'W1993996853', 'W1994051650', 'W1994234578', 'W1994239968', 'W1994305081', 'W1994602517', 'W1994608257', 'W1994668150', 'W1994709452', 'W1994806459', 'W1994840640', 'W1995363050', 'W1995383659', 'W1995391763', 'W1995455069', 'W1995462290', 'W1995582211', 'W1995751266', 'W1995764265', 'W1995847267', 'W1995873922', 'W1995880103', 'W1995966034', 'W1996037209', 'W1996068251', 'W1996152690', 'W1996379182', 'W1996427463', 'W1996735611', 'W1997056914', 'W1997112179', 'W1997128330', 'W1997184663', 'W1997281629', 'W1997295126', 'W1997346887', 'W1997360166', 'W1997462050', 'W1997581921', 'W1997591918', 'W1997725738', 'W1997732325', 'W1997913020', 'W1997943249', 'W1998019811', 'W1998062990', 'W1998170956', 'W1998210753', 'W1998294204', 'W1998313923', 'W1998354357', 'W1998359020', 'W1998399313', 'W1998588514', 'W1998707180', 'W1998735201', 'W1998765276', 'W1998778035', 'W1999150533', 'W1999211990', 'W1999394846', 'W1999558691', 'W1999639022', 'W1999694989', 'W1999795482', 'W1999836892', 'W1999998145', 'W2000111131', 'W2000201704', 'W2000376927', 'W2000702106', 'W2000723562', 'W2000831282', 'W2001036106', 'W2001457226', 'W2001467419', 'W2001520273', 'W2001568034', 'W2001569984', 'W2001577911', 'W2001612492', 'W2001623365', 'W2001815700', 'W2002090266', 'W2002285262', 'W2002288000', 'W2002483881', 'W2002563818', 'W2002671660', 'W2002773051', 'W2002917015', 'W2003108613', 'W2003407164', 'W2003643542', 'W2003658277', 'W2003861577', 'W2003887174', 'W2003958562', 'W2003991742', 'W2004062537', 'W2004396698', 'W2004582874', 'W2004756981', 'W2004865303', 'W2005117785', 'W2005144914', 'W2005249937', 'W2005270729', 'W2005368600', 'W2005402647', 'W2005485591', 'W2005566561', 'W2005678965', 'W2005727437', 'W2005861566', 'W2005884107', 'W2005891025', 'W2005913272', 'W2005997790', 'W2006108264', 'W2006148298', 'W2006215820', 'W2006260182', 'W2006379278', 'W2006544096', 'W2006566278', 'W2006985224', 'W2007016963', 'W2007083494', 'W2007193116', 'W2007225723', 'W2007505836', 'W2007630314', 'W2007647318', 'W2007731525', 'W2007743440', 'W2007906101', 'W2007970201', 'W2007980848', 'W2008228302', 'W2008417639', 'W2008581684', 'W2008708966', 'W2008916300', 'W2009125409', 'W2009165358', 'W2009225398', 'W2009258758', 'W2009303397', 'W2009311291', 'W2009450136', 'W2009544316', 'W2009733123', 'W2009734791', 'W2009787733', 'W2009839229', 'W2009874228', 'W2009925347', 'W2009971480', 'W2009988618', 'W2010008722', 'W2010050523', 'W2010211837', 'W2010618050', 'W2010758581', 'W2011014580', 'W2011107247', 'W2011117196', 'W2011124366', 'W2011241228', 'W2011290663', 'W2011524264', 'W2012015257', 'W2012040176', 'W2012068653', 'W2012255493', 'W2012318406', 'W2012337697', 'W2012451546', 'W2012774065', 'W2012917947', 'W2012945939', 'W2013114609', 'W2013381376', 'W2013544053', 'W2013739118', 'W2013747211', 'W2013829677', 'W2013878893', 'W2013899838', 'W2013902569', 'W2013944211', 'W2013958792', 'W2014007576', 'W2014086521', 'W2014154241', 'W2014177248', 'W2014267867', 'W2014319455', 'W2014459126', 'W2014506057', 'W2014707404', 'W2014790153', 'W2014817710', 'W2014829450', 'W2014885180', 'W2014918593', 'W2014943313', 'W2014947357', 'W2015272827', 'W2015314427', 'W2015483723', 'W2015503148', 'W2015520940', 'W2015522350', 'W2015541033', 'W2015589839', 'W2015650996', 'W2015781356', 'W2015902169', 'W2016170088', 'W2016204905', 'W2016456601', 'W2016467209', 'W2016556242', 'W2016598095', 'W2016612145', 'W2016797132', 'W2017016244', 'W2017028262', 'W2017230136', 'W2017240600', 'W2017420757', 'W2017466590', 'W2017500053', 'W2017533906', 'W2017593971', 'W2017695960', 'W2017779665', 'W2017831590', 'W2017948700', 'W2018041853', 'W2018071293', 'W2018082393', 'W2018296815', 'W2018318551', 'W2018400498', 'W2018418660', 'W2018495260', 'W2018695916', 'W2018938558', 'W2018964965', 'W2019215631', 'W2019341627', 'W2019615636', 'W2019690058', 'W2019782533', 'W2020017535', 'W2020036405', 'W2020047881', 'W2020197246', 'W2020272622', 'W2020352312', 'W2020396176', 'W2020492581', 'W2020563014', 'W2020618744', 'W2020638143', 'W2020701385', 'W2020720216', 'W2020912000', 'W2021011570', 'W2021039359', 'W2021121989', 'W2021469711', 'W2021532580', 'W2022021790', 'W2022106349', 'W2022456133', 'W2022592000', 'W2022624659', 'W2022675211', 'W2022716167', 'W2022812113', 'W2022812411', 'W2022964430', 'W2023028041', 'W2023134133', 'W2023215532', 'W2023445018', 'W2023598683', 'W2023758154', 'W2023941328', 'W2023994136', 'W2024099469', 'W2024105361', 'W2024182435', 'W2024231177', 'W2024336151', 'W2024502954', 'W2024538345', 'W2024796534', 'W2024858284', 'W2024927783', 'W2025044590', 'W2025060864', 'W2025129963', 'W2025192214', 'W2025364264', 'W2025843509', 'W2026312959', 'W2026460725', 'W2026531845', 'W2026563152', 'W2026622074', 'W2026706442', 'W2026753135', 'W2026860604', 'W2026876894', 'W2026984235', 'W2027169351', 'W2027196741', 'W2027503160', 'W2027823142', 'W2027951311', 'W2028161249', 'W2028283089', 'W2028298579', 'W2028536701', 'W2028552491', 'W2028684526', 'W2029063751', 'W2029098242', 'W2029236887', 'W2029327556', 'W2029420066', 'W2029458900', 'W2029558360', 'W2029593316', 'W2029607802', 'W2029638626', 'W2029695281', 'W2029705149', 'W2029709159', 'W2029777285', 'W2029856528', 'W2029868511', 'W2029955148', 'W2030275032', 'W2030307261', 'W2030430356', 'W2030664247', 'W2030667534', 'W2030779508', 'W2030864103', 'W2031305815', 'W2031435441', 'W2031626170', 'W2031675835', 'W2031693373', 'W2031886823', 'W2031949522', 'W2032007095', 'W2032109805', 'W2032169043', 'W2032169416', 'W2032447771', 'W2032465571', 'W2032601500', 'W2032613220', 'W2033345201', 'W2033454646', 'W2033506933', 'W2033627048', 'W2033683607', 'W2033727340', 'W2033838686', 'W2033851561', 'W2034041091', 'W2034043992', 'W2034087770', 'W2034198209', 'W2034218807', 'W2034275112', 'W2034400585', 'W2034766627', 'W2034872680', 'W2034911317', 'W2035134284', 'W2035184193', 'W2035244376', 'W2035307362', 'W2035312550', 'W2035353547', 'W2035444573', 'W2035476842', 'W2035487622', 'W2035520103', 'W2035521247', 'W2035685438', 'W2035692638', 'W2035803015', 'W2035875582', 'W2035901470', 'W2035967549', 'W2035986180', 'W2036275511', 'W2036568535', 'W2036621981', 'W2036654148', 'W2036692010', 'W2036768594', 'W2036787961', 'W2036818036', 'W2036871097', 'W2036994468', 'W2037127469', 'W2037185359', 'W2037186850', 'W2037320436', 'W2037354325', 'W2037429401', 'W2037468053', 'W2037550202', 'W2037588135', 'W2037627674', 'W2037814856', 'W2037843307', 'W2037951366', 'W2037959286', 'W2037961420', 'W2037967087', 'W2038167955', 'W2038295628', 'W2038466882', 'W2038520499', 'W2038708830', 'W2038756774', 'W2038797229', 'W2038802997', 'W2038834667', 'W2039276829', 'W2039419943', 'W2039438770', 'W2039474189', 'W2039525732', 'W2039594437', 'W2039619137', 'W2039628288', 'W2039646238', 'W2039688071', 'W2039838459', 'W2040089067', 'W2040254484', 'W2040546382', 'W2040656353', 'W2040851066', 'W2041009706', 'W2041161710', 'W2041255615', 'W2041368867', 'W2041404026', 'W2041491503', 'W2041687573', 'W2042073478', 'W2042198408', 'W2042260827', 'W2042341230', 'W2042610900', 'W2042950864', 'W2042963074', 'W2043039094', 'W2043092535', 'W2043276141', 'W2043280980', 'W2043287555', 'W2043351037', 'W2043409309', 'W2043464646', 'W2043543810', 'W2043547083', 'W2043572653', 'W2043621157', 'W2043649801', 'W2043724674', 'W2043764225', 'W2043886213', 'W2043946707', 'W2044017173', 'W2044023688', 'W2044081873', 'W2044083819', 'W2044159918', 'W2044169060', 'W2044221417', 'W2044272637', 'W2044515182', 'W2044525934', 'W2044537731', 'W2044541859', 'W2045240728', 'W2045332860', 'W2045373134', 'W2045582299', 'W2045611938', 'W2045647507', 'W2045679430', 'W2045967139', 'W2045970460', 'W2045982363', 'W2045992894', 'W2046099512', 'W2046216514', 'W2046282973', 'W2046388378', 'W2046427450', 'W2046447620', 'W2046593879', 'W2046607232', 'W2046633980', 'W2046671797', 'W2046728284', 'W2046756835', 'W2046879630', 'W2046957125', 'W2047144539', 'W2047148496', 'W2047242847', 'W2047439821', 'W2047458976', 'W2047507923', 'W2047667443', 'W2047690887', 'W2047804855', 'W2048030072', 'W2048098925', 'W2048174920', 'W2048226330', 'W2048244283', 'W2048259679', 'W2048498664', 'W2048519887', 'W2048552052', 'W2048668585', 'W2048831047', 'W2049279407', 'W2049284882', 'W2049362524', 'W2049566430', 'W2049749131', 'W2049991946', 'W2050113854', 'W2050136273', 'W2050462870', 'W2050532980', 'W2050680004', 'W2050697866', 'W2050916209', 'W2051062186', 'W2051244976', 'W2051674092', 'W2051811673', 'W2051829446', 'W2051835565', 'W2051938843', 'W2052041444', 'W2052094075', 'W2052101545', 'W2052104837', 'W2052107105', 'W2052131829', 'W2052176070', 'W2052303867', 'W2052403003', 'W2052449777', 'W2052470191', 'W2052557421', 'W2052562848', 'W2052578642', 'W2052618262', 'W2052763184', 'W2052792097', 'W2052889619', 'W2052986121', 'W2053038488', 'W2053112016', 'W2053148264', 'W2053168923', 'W2053351357', 'W2053447053', 'W2053456190', 'W2053512476', 'W2053668936', 'W2053670653', 'W2053676985', 'W2053950470', 'W2053998409', 'W2054078002', 'W2054150643', 'W2054159643', 'W2054293224', 'W2054295214', 'W2054357322', 'W2054541266', 'W2054550570', 'W2054697304', 'W2054889218', 'W2054961000', 'W2055064168', 'W2055346100', 'W2055624698', 'W2055654995', 'W2055735666', 'W2055751151', 'W2055920490', 'W2055970105', 'W2056031439', 'W2056043482', 'W2056133106', 'W2056136909', 'W2056331158', 'W2056663185', 'W2056862039', 'W2056941102', 'W2056961674', 'W2057173538', 'W2057220764', 'W2057274775', 'W2057340956', 'W2057530007', 'W2057556822', 'W2057623698', 'W2057762456', 'W2057764483', 'W2058065156', 'W2058086280', 'W2058131268', 'W2058188455', 'W2058250204', 'W2058422567', 'W2058461412', 'W2058498754', 'W2058593070', 'W2058648267', 'W2058663392', 'W2058780066', 'W2058790501', 'W2058791849', 'W2058911232', 'W2058988966', 'W2059055543', 'W2059107485', 'W2059192365', 'W2059348339', 'W2059520792', 'W2059536255', 'W2059637124', 'W2059773828', 'W2060129186', 'W2060210383', 'W2060215125', 'W2060290354', 'W2060302237', 'W2060309271', 'W2060410266', 'W2060438440', 'W2060522415', 'W2060579362', 'W2060722219', 'W2060929195', 'W2061091970', 'W2061115935', 'W2061209982', 'W2061235852', 'W2061378589', 'W2061414976', 'W2061541746', 'W2061603378', 'W2061686528', 'W2061700186', 'W2061704745', 'W2061773040', 'W2061863029', 'W2062072434', 'W2062147078', 'W2062223094', 'W2062299371', 'W2062531561', 'W2062710441', 'W2062754223', 'W2062760235', 'W2062797164', 'W2063033676', 'W2063043143', 'W2063059182', 'W2063141610', 'W2063232329', 'W2063347034', 'W2063606145', 'W2063691589', 'W2063717992', 'W2063761428', 'W2063768648', 'W2063855864', 'W2063920469', 'W2063980929', 'W2063994372', 'W2064064392', 'W2064127635', 'W2064231174', 'W2064381459', 'W2064538938', 'W2064597059', 'W2064733220', 'W2064746187', 'W2064772676', 'W2064790354', 'W2064903115', 'W2064967819', 'W2064992248', 'W2065031087', 'W2065102767', 'W2065233558', 'W2065582101', 'W2065592028', 'W2065692451', 'W2065744913', 'W2065841250', 'W2065850299', 'W2066107334', 'W2066107559', 'W2066194653', 'W2066217023', 'W2066408344', 'W2066461146', 'W2066488891', 'W2066662062', 'W2066697766', 'W2066707374', 'W2066727453', 'W2066828509', 'W2067049091', 'W2067097574', 'W2067104954', 'W2067166149', 'W2067168686', 'W2067256454', 'W2067458397', 'W2067490363', 'W2067493968', 'W2067594098', 'W2067683713', 'W2067684692', 'W2067724561', 'W2067892402', 'W2067896239', 'W2067958598', 'W2067958753', 'W2068133527', 'W2068153705', 'W2068159429', 'W2068164166', 'W2068213179', 'W2068288454', 'W2068365153', 'W2068383276', 'W2068611573', 'W2068623400', 'W2068687154', 'W2068700852', 'W2068758235', 'W2068862752', 'W2068921232', 'W2069317466', 'W2069376224', 'W2069387298', 'W2069403916', 'W2069418463', 'W2069446662', 'W2069565087', 'W2069575193', 'W2069581626', 'W2069938223', 'W2070224555', 'W2070254246', 'W2070295474', 'W2070340709', 'W2070586208', 'W2070658923', 'W2070677730', 'W2070871404', 'W2070948814', 'W2070989278', 'W2070993664', 'W2071214201', 'W2071453130', 'W2071565980', 'W2071657693', 'W2071806220', 'W2071899271', 'W2071993554', 'W2072264450', 'W2072313584', 'W2072361357', 'W2072469560', 'W2072522514', 'W2072544389', 'W2072581177', 'W2072799497', 'W2072948635', 'W2073047120', 'W2073115974', 'W2073133786', 'W2073144689', 'W2073586734', 'W2073672620', 'W2073745520', 'W2073841074', 'W2073928519', 'W2073941831', 'W2073956836', 'W2073963703', 'W2074030379', 'W2074089690', 'W2074185046', 'W2074244804', 'W2074297329', 'W2074301486', 'W2074306185', 'W2074680075', 'W2074857405', 'W2074858391', 'W2074911710', 'W2075058886', 'W2075088486', 'W2075103814', 'W2075207920', 'W2075262517', 'W2075275025', 'W2075335654', 'W2075531472', 'W2075582811', 'W2075846958', 'W2075959152', 'W2076062022', 'W2076129087', 'W2076166231', 'W2076194625', 'W2076398088', 'W2076452392', 'W2076544864', 'W2076570319', 'W2076592306', 'W2076683315', 'W2076743021', 'W2076769172', 'W2076781430', 'W2076874385', 'W2076930147', 'W2077160150', 'W2077165816', 'W2077338089', 'W2077418979', 'W2077499130', 'W2077524133', 'W2077569666', 'W2077731318', 'W2077929218', 'W2078008047', 'W2078184757', 'W2078208477', 'W2078212551', 'W2078252863', 'W2078280789', 'W2078346053', 'W2078505892', 'W2078852360', 'W2078903854', 'W2079116237', 'W2079123204', 'W2079282879', 'W2079499097', 'W2079537922', 'W2079683011', 'W2079838847', 'W2079871208', 'W2079977655', 'W2080015061', 'W2080100430', 'W2080106668', 'W2080208339', 'W2080270879', 'W2080299092', 'W2080390339', 'W2080490788', 'W2080557697', 'W2080742522', 'W2080746576', 'W2080771381', 'W2081057817', 'W2081265176', 'W2081419608', 'W2081431643', 'W2081522223', 'W2081722857', 'W2081920342', 'W2081962941', 'W2081987148', 'W2082062156', 'W2082182585', 'W2082189258', 'W2082307109', 'W2082375192', 'W2082417169', 'W2082425146', 'W2082476985', 'W2082637970', 'W2082796078', 'W2082905129', 'W2083087851', 'W2083123438', 'W2083134984', 'W2083193063', 'W2083390185', 'W2083426418', 'W2083595371', 'W2083646314', 'W2083696908', 'W2083753651', 'W2083919459', 'W2083932867', 'W2084043096', 'W2084067121', 'W2084085700', 'W2084195349', 'W2084495150', 'W2084555855', 'W2084562857', 'W2084584784', 'W2084648807', 'W2084756361', 'W2084787346', 'W2084794049', 'W2084880710', 'W2085025875', 'W2085076719', 'W2085102335', 'W2085258486', 'W2085419794', 'W2085568550', 'W2085993297', 'W2086088355', 'W2086096948', 'W2086623584', 'W2086871263', 'W2087118264', 'W2087229457', 'W2087275804', 'W2087360216', 'W2087417955', 'W2087580204', 'W2087599863', 'W2087614405', 'W2087630325', 'W2087652476', 'W2087866146', 'W2087920955', 'W2087969938', 'W2088002295', 'W2088063295', 'W2088114584', 'W2088140877', 'W2088146743', 'W2088240601', 'W2088269939', 'W2088336227', 'W2088381537', 'W2088607080', 'W2088823312', 'W2088970299', 'W2089254050', 'W2089470782', 'W2089558173', 'W2089644592', 'W2089693172', 'W2089842090', 'W2090072191', 'W2090407808', 'W2090454191', 'W2090618803', 'W2090644188', 'W2090926709', 'W2091046505', 'W2091060631', 'W2091169789', 'W2091209638', 'W2091210052', 'W2091240085', 'W2091250038', 'W2091411865', 'W2091512988', 'W2091673401', 'W2091721155', 'W2091896084', 'W2091968512', 'W2092069252', 'W2092321143', 'W2092441027', 'W2092566326', 'W2092585317', 'W2092698402', 'W2092699248', 'W2092890613', 'W2093048712', 'W2093154482', 'W2093467977', 'W2093509030', 'W2093593077', 'W2093614553', 'W2093860294', 'W2093869152', 'W2093882277', 'W2093882353', 'W2093962878', 'W2094186182', 'W2094205174', 'W2094241924', 'W2094297139', 'W2094388167', 'W2094416412', 'W2094503313', 'W2094530130', 'W2094584720', 'W2094624168', 'W2094697438', 'W2094791588', 'W2094817491', 'W2094898345', 'W2094989185', 'W2094992876', 'W2095000965', 'W2095112780', 'W2095166308', 'W2095187475', 'W2095240020', 'W2095347614', 'W2095348472', 'W2095485251', 'W2095530678', 'W2095640779', 'W2095706874', 'W2095917499', 'W2095998776', 'W2096040928', 'W2096061894', 'W2096065711', 'W2096255336', 'W2096290681', 'W2096330480', 'W2096437959', 'W2096441602', 'W2096494569', 'W2096567395', 'W2096595900', 'W2096661568', 'W2096910745', 'W2096920300', 'W2097066287', 'W2097248196', 'W2097367553', 'W2097566177', 'W2097630079', 'W2097641375', 'W2097651960', 'W2097891553', 'W2097981960', 'W2098057816', 'W2098160259', 'W2098261829', 'W2098264151', 'W2098337864', 'W2098561880', 'W2098628437', 'W2098912234', 'W2098947405', 'W2098985594', 'W2099146007', 'W2099301077', 'W2099521715', 'W2099538852', 'W2099914491', 'W2100022630', 'W2100234350', 'W2100388369', 'W2100472603', 'W2100605768', 'W2101011430', 'W2101083009', 'W2101220662', 'W2101360897', 'W2101487297', 'W2101540252', 'W2101767754', 'W2101871339', 'W2101874536', 'W2101885674', 'W2101895560', 'W2102327538', 'W2102343982', 'W2102354517', 'W2102375119', 'W2102452525', 'W2102601355', 'W2102614059', 'W2102657962', 'W2102686488', 'W2102694693', 'W2102758832', 'W2102891867', 'W2103046269', 'W2103339779', 'W2103377481', 'W2103470698', 'W2103545941', 'W2103596198', 'W2103659405', 'W2103669547', 'W2103683284', 'W2103800516', 'W2103972359', 'W2104135702', 'W2104278293', 'W2104339159', 'W2104345850', 'W2104359885', 'W2104362337', 'W2104383425', 'W2104412320', 'W2104612823', 'W2104760708', 'W2105094451', 'W2105409495', 'W2105464235', 'W2105527814', 'W2105869871', 'W2106031420', 'W2106078939', 'W2106097763', 'W2106125288', 'W2106195012', 'W2106255061', 'W2106444069', 'W2106509663', 'W2106569931', 'W2106627170', 'W2106782035', 'W2107631426', 'W2107709008', 'W2107995151', 'W2108134801', 'W2108561925', 'W2108660154', 'W2108684624', 'W2109058559', 'W2109070574', 'W2109167860', 'W2109194958', 'W2109323673', 'W2109586670', 'W2109716633', 'W2109773179', 'W2109790966', 'W2110448073', 'W2110526200', 'W2110531643', 'W2110676385', 'W2110845720', 'W2110866976', 'W2110950722', 'W2111143308', 'W2111362374', 'W2111423166', 'W2111567408', 'W2111607026', 'W2111608545', 'W2111754392', 'W2111822899', 'W2111892050', 'W2111990033', 'W2112156826', 'W2112171625', 'W2112304096', 'W2112329697', 'W2112693824', 'W2112800403', 'W2112989471', 'W2113033693', 'W2113118283', 'W2113185199', 'W2113190139', 'W2113382515', 'W2113423279', 'W2113430269', 'W2113432343', 'W2113574245', 'W2113636792', 'W2113775510', 'W2113892331', 'W2113985551', 'W2113995129', 'W2114024548', 'W2114072930', 'W2114200930', 'W2114242405', 'W2114480175', 'W2114795284', 'W2114806766', 'W2114816710', 'W2114823344', 'W2114847797', 'W2114867673', 'W2115021528', 'W2115161796', 'W2115282304', 'W2115306772', 'W2115632183', 'W2115734610', 'W2115742671', 'W2116056961', 'W2116504706', 'W2116505693', 'W2116690246', 'W2116807721', 'W2116832944', 'W2116868916', 'W2116991826', 'W2117074858', 'W2117119983', 'W2117137379', 'W2117212522', 'W2117212773', 'W2117217282', 'W2117290428', 'W2117695104', 'W2117887677', 'W2118498973', 'W2118612600', 'W2118956678', 'W2119041481', 'W2119170183', 'W2119334874', 'W2119366144', 'W2119399459', 'W2119421712', 'W2119510088', 'W2119563361', 'W2119626361', 'W2119688105', 'W2119731784', 'W2119971031', 'W2120012725', 'W2120039555', 'W2120348861', 'W2120426523', 'W2120490039', 'W2120501245', 'W2120507868', 'W2120640283', 'W2120648308', 'W2120762768', 'W2120839885', 'W2120865355', 'W2120954469', 'W2120987375', 'W2121053966', 'W2121077787', 'W2121222036', 'W2121429373', 'W2121469024', 'W2121614812', 'W2122065834', 'W2122234266', 'W2122236130', 'W2122351631', 'W2122469592', 'W2122477201', 'W2122508074', 'W2122514972', 'W2122556106', 'W2122651871', 'W2123180659', 'W2123343610', 'W2123611234', 'W2123779312', 'W2124052876', 'W2124068063', 'W2124069411', 'W2124140485', 'W2124286005', 'W2124536575', 'W2124644922', 'W2124877832', 'W2125022522', 'W2125083914', 'W2125123150', 'W2125452667', 'W2125921982', 'W2125995561', 'W2126446637', 'W2126530026', 'W2126764631', 'W2126943184', 'W2126968146', 'W2127013861', 'W2127187639', 'W2127440157', 'W2127481048', 'W2127496189', 'W2127516219', 'W2127590223', 'W2127984419', 'W2128101513', 'W2128162080', 'W2128166172', 'W2128236448', 'W2128316270', 'W2128537698', 'W2128617170', 'W2128668040', 'W2128677681', 'W2128694891', 'W2128898381', 'W2129524561', 'W2129706359', 'W2129712803', 'W2129714618', 'W2129750781', 'W2129874885', 'W2130053803', 'W2130059531', 'W2130104604', 'W2130371399', 'W2130498948', 'W2130542877', 'W2130832525', 'W2130979925', 'W2131007152', 'W2131089732', 'W2131333624', 'W2131520472', 'W2131771296', 'W2131799247', 'W2131872983', 'W2131936131', 'W2131972618', 'W2132038594', 'W2132357771', 'W2132421343', 'W2132429277', 'W2132463881', 'W2132700768', 'W2132741387', 'W2133285107', 'W2133544346', 'W2133830708', 'W2133880825', 'W2133922530', 'W2134197723', 'W2134318583', 'W2134522616', 'W2134719928', 'W2134792908', 'W2134892601', 'W2134955784', 'W2135353818', 'W2135453787', 'W2135514753', 'W2135574023', 'W2135592618', 'W2135664206', 'W2135686999', 'W2135690913', 'W2135901758', 'W2135918225', 'W2135949063', 'W2135954438', 'W2136026438', 'W2136490136', 'W2136688111', 'W2136733611', 'W2136994247', 'W2137339435', 'W2137434813', 'W2137498550', 'W2137523482', 'W2137546031', 'W2137704349', 'W2137721638', 'W2137823095', 'W2138092997', 'W2138226825', 'W2138327656', 'W2138394833', 'W2138544728', 'W2138585051', 'W2138709778', 'W2138795497', 'W2138948114', 'W2139128868', 'W2139164398', 'W2139414608', 'W2139560017', 'W2139753192', 'W2139772062', 'W2139941678', 'W2140003513', 'W2140259967', 'W2140267261', 'W2140354916', 'W2140394111', 'W2140654974', 'W2140675859', 'W2140679396', 'W2140915719', 'W2141165347', 'W2141284723', 'W2141285069', 'W2141352664', 'W2141379701', 'W2141533959', 'W2141639422', 'W2141654757', 'W2141976464', 'W2142044474', 'W2142222850', 'W2142266462', 'W2142611308', 'W2142628334', 'W2142716851', 'W2143013308', 'W2143035803', 'W2143070952', 'W2143122495', 'W2143188926', 'W2143238979', 'W2143307679', 'W2143357828', 'W2144022982', 'W2144112169', 'W2144138541', 'W2144291816', 'W2144377119', 'W2144409921', 'W2144554317', 'W2144718635', 'W2145084709', 'W2145272227', 'W2145388691', 'W2145434871', 'W2145972463', 'W2146124002', 'W2146143594', 'W2146395847', 'W2146731647', 'W2146860811', 'W2146961420', 'W2146973891', 'W2147056438', 'W2147062583', 'W2147890788', 'W2148050305', 'W2148281700', 'W2148340793', 'W2148483715', 'W2148709687', 'W2148863301', 'W2149332763', 'W2149469423', 'W2149472608', 'W2149566917', 'W2149624733', 'W2149761383', 'W2149761960', 'W2149840294', 'W2150027419', 'W2150168624', 'W2150196668', 'W2150273748', 'W2150550276', 'W2150723052', 'W2151084479', 'W2151456890', 'W2151485523', 'W2151582019', 'W2151817435', 'W2151840266', 'W2152051790', 'W2152063830', 'W2152148024', 'W2152243318', 'W2152281757', 'W2152315865', 'W2152331223', 'W2152389843', 'W2152545043', 'W2152874097', 'W2152950305', 'W2153013618', 'W2153071150', 'W2153123353', 'W2153188271', 'W2153232917', 'W2153472821', 'W2153784495', 'W2154106855', 'W2154249083', 'W2154365415', 'W2154500154', 'W2154516787', 'W2154663531', 'W2154748862', 'W2154834069', 'W2154836325', 'W2154846778', 'W2154887758', 'W2154993563', 'W2155009617', 'W2155015733', 'W2155219687', 'W2155313245', 'W2155330019', 'W2155613928', 'W2155701751', 'W2155810842', 'W2155835417', 'W2156144699', 'W2156171182', 'W2156264079', 'W2156356581', 'W2156679064', 'W2156827038', 'W2157053804', 'W2157100007', 'W2157104650', 'W2157197153', 'W2157270864', 'W2157381639', 'W2157638939', 'W2157661564', 'W2157699677', 'W2157766835', 'W2157783220', 'W2157873949', 'W2158115879', 'W2158132545', 'W2158411287', 'W2158520870', 'W2158535498', 'W2158565718', 'W2158609694', 'W2158714788', 'W2158971210', 'W2159007909', 'W2159042370', 'W2159364800', 'W2159570560', 'W2159636137', 'W2159664830', 'W2159714420', 'W2159796957', 'W2159871192', 'W2160099746', 'W2160615487', 'W2160936581', 'W2161035105', 'W2161050915', 'W2161082864', 'W2161199002', 'W2161234475', 'W2161408133', 'W2161520095', 'W2161594732', 'W2161803468', 'W2161830466', 'W2161925614', 'W2162108148', 'W2162139935', 'W2162190911', 'W2162528709', 'W2162843846', 'W2163047720', 'W2163185136', 'W2163354845', 'W2163508965', 'W2163788889', 'W2163798716', 'W2163832829', 'W2164081145', 'W2164126193', 'W2164681766', 'W2164707361', 'W2164778558', 'W2164785771', 'W2164943291', 'W2164945122', 'W2165146798', 'W2165191318', 'W2165218453', 'W2165355490', 'W2165451118', 'W2165454598', 'W2165914214', 'W2166156671', 'W2166169681', 'W2166218161', 'W2166601167', 'W2166611136', 'W2166630278', 'W2167033587', 'W2167088833', 'W2167141338', 'W2167154618', 'W2167430243', 'W2167434574', 'W2167656141', 'W2167816458', 'W2167887566', 'W2168263994', 'W2168326939', 'W2168427881', 'W2168825528', 'W2169118703', 'W2169448756', 'W2169583594', 'W2169770014', 'W2169855071', 'W2169859012', 'W2169903226', 'W2170240687', 'W2170275957', 'W2170281382', 'W2170423778', 'W2170489293', 'W2170661703', 'W2170708401', 'W2170776969', 'W2170973789', 'W2171155178', 'W2171417214', 'W2171704754', 'W2171958984', 'W2173437641', 'W2179514526', 'W2187796536', 'W2203973633', 'W2209816797', 'W2213248838', 'W2219963396', 'W2234668029', 'W2249259354', 'W2257572427', 'W2268386531', 'W2281014268', 'W2301131921', 'W2312621780', 'W2317848129', 'W2317934029', 'W2318353205', 'W2319761515', 'W2322171201', 'W2324371267', 'W2324929766', 'W2326190830', 'W2326959862', 'W2329233206', 'W2330533782', 'W2333839776', 'W2334687851', 'W2335393869', 'W2363118366', 'W2396850422', 'W2398457642', 'W2404905481', 'W2405502375', 'W2408634540', 'W2409058062', 'W2411667254', 'W2412080271', 'W2412194565', 'W2412229195', 'W2413317183', 'W2413607610', 'W2413616721', 'W2413657047', 'W2415074001', 'W2415539077', 'W2415728598', 'W2415814835', 'W2416207934', 'W2417691174', 'W2419604653', 'W2419985789', 'W2428745519', 'W2463885715', 'W2467883685', 'W2743254891', 'W2915381896', 'W4210968583', 'W4211001321', 'W4212939231', 'W4230670393', 'W4233734086', 'W4235522152', 'W4236585236', 'W4238843411', 'W4242463129', 'W4243182520', 'W4248341849', 'W4251249877', 'W4252544358', 'W4254283855', 'W4254630790', 'W4256220649', 'W4256376099', 'W4296215195'], 'abstract': 'In 2008 we published the first set of guidelines for standardizing research in autophagy. Since then, research on this topic has continued to accelerate, and many new scientists have entered the field. Our knowledge base and relevant new technologies have also been expanding. Accordingly, it is important to update these guidelines for monitoring autophagy in different organisms. Various reviews have described the range of assays that have been used for this purpose. Nevertheless, there continues to be confusion regarding acceptable methods to measure autophagy, especially in multicellular eukaryotes.\r\n\r\nFor example, a key point that needs to be emphasized is that there is a difference between measurements that monitor the numbers or volume of autophagic elements (e.g., autophagosomes or autolysosomes) at any stage of the autophagic process versus those that measure flux through the autophagy pathway (i.e., the complete process including the amount and rate of cargo sequestered and degraded). In particular, a block in macroautophagy that results in autophagosome accumulation must be differentiated from stimuli that increase autophagic activity, defined as increased autophagy induction coupled with increased delivery to, and degradation within, lysosomes (in most higher eukaryotes and some protists such as Dictyostelium) or the vacuole (in plants and fungi). In other words, it is especially important that investigators new to the field understand that the appearance of more autophagosomes does not necessarily equate with more autophagy. In fact, in many cases, autophagosomes accumulate because of a block in trafficking to lysosomes without a concomitant change in autophagosome biogenesis, whereas an increase in autolysosomes may reflect a reduction in degradative activity. It is worth emphasizing here that lysosomal digestion is a stage of autophagy and evaluating its competence is a crucial part of the evaluation of autophagic flux, or complete autophagy.\r\n\r\nHere, we present a set of guidelines for the selection and interpretation of methods for use by investigators who aim to examine macroautophagy and related processes, as well as for reviewers who need to provide realistic and reasonable critiques of papers that are focused on these processes. These guidelines are not meant to be a formulaic set of rules, because the appropriate assays depend in part on the question being asked and the system being used. In addition, we emphasize that no individual assay is guaranteed to be the most appropriate one in every situation, and we strongly recommend the use of multiple assays to monitor autophagy. Along these lines, because of the potential for pleiotropic effects due to blocking autophagy through genetic manipulation, it is imperative to target by gene knockout or RNA interference more than one autophagy-related protein. In addition, some individual Atg proteins, or groups of proteins, are involved in other cellular pathways implying that not all Atg proteins can be used as a specific marker for an autophagic process. In these guidelines, we consider these various methods of assessing autophagy and what information can, or cannot, be obtained from them. Finally, by discussing the merits and limits of particular assays, we hope to encourage technical innovation in the field.', 'counts_by_year': [[2022, 267], [2021, 624], [2020, 762], [2019, 737], [2018, 697], [2017, 673], [2016, 295], [2015, 11], [2014, 22], [2013, 30], [2012, 17]]}, {'id': 'W2928665623', 'doi': 'https://doi.org/10.1038/s41467-019-09234-6', 'title': 'Metascape provides a biologist-oriented resource for the analysis of systems-level datasets', 'type': 'journal-article', 'publication_date': '2019-04-03', 'host_venue': 'V64187185', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2113961186', ['I127128434']], ['A2276500107', ['I127128434']], ['A2146231314', ['I2799424032']], ['A2872675147', ['I36258959']], ['A2925645345', ['I127128434']], ['A2282636968', ['I127128434']], ['A2253315458', ['I36258959']], ['A2184218653', ['I2799424032']]], 'cited_by_count': 4062, 'concepts': [['C2778824453', '0.8386899'], ['C41008148', '0.6596538'], ['C206345919', '0.52992004'], ['C2522767166', '0.4563189'], ['C70721500', '0.38839263']], 'referenced_works': ['W953912505', 'W1968886567', 'W1979594866', 'W1990518394', 'W1991883934', 'W2003988251', 'W2053154970', 'W2070924692', 'W2074410038', 'W2086054561', 'W2096561521', 'W2097948046', 'W2100743480', 'W2102221598', 'W2103017472', 'W2104488717', 'W2123830992', 'W2124649657', 'W2130410032', 'W2130962401', 'W2131776976', 'W2133465414', 'W2134695923', 'W2135230352', 'W2136850043', 'W2139703939', 'W2148043260', 'W2149232113', 'W2152495216', 'W2158217645', 'W2158804744', 'W2159675211', 'W2159686118', 'W2170648600', 'W2191030086', 'W2191641478', 'W2242206668', 'W2246595627', 'W2345356016', 'W2509717357', 'W2517051411', 'W2545753590', 'W2557833574', 'W2558534012', 'W2559466477', 'W2577621264', 'W2611604995', 'W2611696724', 'W2765384588', 'W2767891136', 'W2915536738', 'W2918086501', 'W4233120011', 'W4233698560', 'W4237381349', 'W4250359879', 'W4294216483'], 'abstract': 'A critical component in the interpretation of systems-level studies is the inference of enriched biological pathways and protein complexes contained within OMICs datasets. Successful analysis requires the integration of a broad set of current biological databases and the application of a robust analytical pipeline to produce readily interpretable results. Metascape is a web-based portal designed to provide a comprehensive gene list annotation and analysis resource for experimental biologists. In terms of design features, Metascape combines functional enrichment, interactome analysis, gene annotation, and membership search to leverage over 40 independent knowledgebases within one integrated portal. Additionally, it facilitates comparative analyses of datasets across multiple independent and orthogonal experiments. Metascape provides a significantly simplified user experience through a one-click Express Analysis interface to generate interpretable outputs. Taken together, Metascape is an effective and efficient tool for experimental biologists to comprehensively analyze and interpret OMICs-based studies in the big data era.', 'counts_by_year': [[2022, 1605], [2021, 1563], [2020, 764], [2019, 122], [2018, 2]]}, {'id': 'W2963857521', 'doi': 'https://doi.org/10.1109/sp.2017.49', 'title': 'Towards Evaluating the Robustness of Neural Networks', 'type': 'proceedings-article', 'publication_date': '2017-05-22', 'host_venue': 'V4306418833', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A1606335232', ['I95457486']], ['A2109123731', ['I95457486']]], 'cited_by_count': 4050, 'concepts': [['C37736160', '0.86863816'], ['C63479239', '0.78373826'], ['C50644808', '0.7736325'], ['C41008148', '0.759172'], ['C154945302', '0.638161']], 'referenced_works': ['W1545528966', 'W1966948031', 'W2108598243', 'W2112796928', 'W2117539524', 'W2122646361', 'W2143612262', 'W2145339207', 'W2160815625', 'W2180612164', 'W2194775991', 'W2257979135', 'W2296673577', 'W2964082701', 'W4213329945', 'W4256462051'], 'abstract': "Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x' that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from 95% to 0.5%.In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100% probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.", 'counts_by_year': [[2022, 396], [2021, 1144], [2020, 1177], [2019, 843], [2018, 410], [2017, 75], [2016, 1]]}, {'id': 'W2463195069', 'doi': 'https://doi.org/10.1038/nmeth.3901', 'title': 'The Perseus computational platform for comprehensive analysis of (prote)omics data', 'type': 'journal-article', 'publication_date': '2016-09-01', 'host_venue': 'V127827428', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A1207563747', ['I4210150093']], ['A1259385706', ['I4210150093']], ['A298886857', ['I4210150093']], ['A2652407150', ['I4210150093']], ['A2120208697', ['I180670191']], ['A2169714569', ['I16391192']], ['A2136386296', ['I4210150093']], ['A2125152962', ['I4210150093']]], 'cited_by_count': 4020, 'concepts': [['C4924752', '0.74093676'], ['C41008148', '0.7204042'], ['C177212765', '0.5939014'], ['C2777904410', '0.57635665'], ['C170130773', '0.52906215']], 'referenced_works': ['W205153889', 'W953912505', 'W1480376833', 'W1490161904', 'W1611787727', 'W1838945435', 'W1851565937', 'W1968403457', 'W1973094248', 'W1981509058', 'W1981593008', 'W1991502938', 'W1992726958', 'W1993153599', 'W1993450321', 'W1994045445', 'W1996310767', 'W2004515150', 'W2014449669', 'W2023010129', 'W2026465178', 'W2027469236', 'W2031095666', 'W2036321220', 'W2042510382', 'W2043274355', 'W2047275456', 'W2051145352', 'W2059041864', 'W2060705109', 'W2062114704', 'W2064631610', 'W2068862934', 'W2077248448', 'W2079517684', 'W2080752012', 'W2091377998', 'W2096863518', 'W2102653059', 'W2103453943', 'W2106074866', 'W2107578201', 'W2110256992', 'W2114526351', 'W2119047401', 'W2122805769', 'W2124091237', 'W2128551987', 'W2130410032', 'W2130706354', 'W2140638448', 'W2140729960', 'W2147566730', 'W2148541040', 'W2150375898', 'W2152983548', 'W2153635508', 'W2154431984', 'W2156909104', 'W2157918066', 'W2159675211', 'W2164707361', 'W2165962849', 'W2169456326', 'W2170776626', 'W2171948041', 'W2171998392', 'W2183117764', 'W2209651181', 'W2260700459', 'W2739999456', 'W4237335579', 'W4252232449', 'W4294107304'], 'abstract': "A main bottleneck in proteomics is the downstream biological analysis of highly multivariate quantitative protein abundance data generated using mass-spectrometry-based analysis. We developed the Perseus software platform (http://www.perseus-framework.org) to support biological and biomedical researchers in interpreting protein quantification, interaction and post-translational modification data. Perseus contains a comprehensive portfolio of statistical tools for high-dimensional omics data analysis covering normalization, pattern recognition, time-series analysis, cross-omics comparisons and multiple-hypothesis testing. A machine learning module supports the classification and validation of patient groups for diagnosis and prognosis, and it also detects predictive protein signatures. Central to Perseus is a user-friendly, interactive workflow environment that provides complete documentation of computational methods used in a publication. All activities in Perseus are realized as plugins, and users can extend the software by programming their own, which can be shared through a plugin store. We anticipate that Perseus's arsenal of algorithms and its intuitive usability will empower interdisciplinary analysis of complex large data sets.", 'counts_by_year': [[2022, 771], [2021, 1019], [2020, 882], [2019, 638], [2018, 442], [2017, 235], [2016, 32]]}, {'id': 'W2963420686', 'doi': 'https://doi.org/10.1109/tpami.2019.2913372', 'title': 'Squeeze-and-Excitation Networks', 'type': 'journal-article', 'publication_date': '2018-06-18', 'host_venue': 'V199944782', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2752023548', ['I4210128818']], ['A2236987923', ['I40120149']], ['A2580435753', ['I40120149']], ['A2653808965', ['I4210112150']], ['A2238935163', ['I4210128818']]], 'cited_by_count': 4006, 'concepts': [['C41008148', '0.7723954'], ['C2777210771', '0.68228143'], ['C45347329', '0.6394483'], ['C81363708', '0.6232356'], ['C2780801425', '0.55905944']], 'referenced_works': ['W139960808', 'W1568165162', 'W1677182931', 'W1903029394', 'W1966385142', 'W1996901117', 'W2097117768', 'W2111935653', 'W2113325037', 'W2115441154', 'W2117539524', 'W2128272608', 'W2130325614', 'W2144764737', 'W2183341477', 'W2194775991', 'W2221625691', 'W2288122362', 'W2339172597', 'W2400429454', 'W2531409750', 'W2549139847', 'W2550553598', 'W2732026016', 'W2952746495', 'W2962737770', 'W2962971773', 'W2963125010', 'W2963446712', 'W2963495494', 'W2964081807', 'W2964137095', 'W2965658867', 'W4255158661'], 'abstract': 'The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%. Models and code are available at https://github.com/hujie-frank/SENet.', 'counts_by_year': [[2022, 764], [2021, 1497], [2020, 1077], [2019, 537], [2018, 109], [2017, 8], [2012, 1]]}, {'id': 'W2301656337', 'doi': 'https://doi.org/10.1039/c5ee03874j', 'title': 'Cesium-containing triple cation perovskite solar cells: improved stability, reproducibility and high efficiency', 'type': 'journal-article', 'publication_date': '2016-06-08', 'host_venue': 'V117082959', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2305790808', ['I5124864']], ['A2098897795', ['I1283155146']], ['A2940467638', ['I5124864']], ['A2120973173', ['I5124864']], ['A1082285987', ['I5124864']], ['A2004633034', ['I5124864']], ['A45966780', ['I5124864']], ['A2568784028', ['I5124864']], ['A2121380061', ['I5124864']], ['A14922024', ['I5124864']], ['A2012626572', ['I5124864']]], 'cited_by_count': 3908, 'concepts': [['C9893847', '0.9054769'], ['C155011858', '0.79266524'], ['C519659679', '0.6987058'], ['C192562407', '0.56798714'], ['C112972136', '0.5027878']], 'referenced_works': ['W1517556801', 'W1629273100', 'W1696825759', 'W1724397231', 'W1837428562', 'W1849148947', 'W1916611353', 'W1927627425', 'W1941396097', 'W1965464747', 'W1981910115', 'W1994012531', 'W1998670932', 'W2006619554', 'W2011417067', 'W2012327696', 'W2013801304', 'W2020683240', 'W2029637177', 'W2041726686', 'W2042468043', 'W2044270989', 'W2045513766', 'W2047520446', 'W2054373279', 'W2057327293', 'W2062571793', 'W2073726416', 'W2075511306', 'W2081640113', 'W2085525582', 'W2090129700', 'W2095995071', 'W2100438467', 'W2111653317', 'W2114118829', 'W2120829841', 'W2123857657', 'W2125469086', 'W2127320550', 'W2129970907', 'W2130790248', 'W2132189412', 'W2144574847', 'W2151097606', 'W2154285289', 'W2160391726', 'W2169328609', 'W2184746371', 'W2202937947', 'W2216495470', 'W2234054032', 'W2238961223', 'W2260885620', 'W2268311649', 'W2268497559', 'W2284117193', 'W2317851040', 'W2320884563', 'W2321578105', 'W2329085983', 'W2334658948', 'W2335170602', 'W2415832373'], 'abstract': "Today's best perovskite solar cells use a mixture of formamidinium and methylammonium as the monovalent cations. Adding cesium improves the compositions greatly.", 'counts_by_year': [[2022, 370], [2021, 608], [2020, 649], [2019, 738], [2018, 698], [2017, 639], [2016, 203]]}, {'id': 'W2963524571', 'doi': 'https://doi.org/10.1109/cvpr.2017.502', 'title': 'Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2429081565', ['I4210090411']], ['A2469405535', ['I40120149']]], 'cited_by_count': 3898, 'concepts': [['C41008148', '0.8461137'], ['C70437156', '0.72763044'], ['C185798385', '0.7128157'], ['C154945302', '0.6352169'], ['C153180895', '0.5336393']], 'referenced_works': ['W1522734439', 'W1578985305', 'W1923332106', 'W1923404803', 'W1926645898', 'W1944615693', 'W1947481528', 'W1983364832', 'W2016053056', 'W2102605133', 'W2105101328', 'W2126579184', 'W2135658380', 'W2158169396', 'W2165715280', 'W2194775991', 'W2342662179', 'W2462996230', 'W2962803561'], 'abstract': 'The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.2% on HMDB-51 and 97.9% on UCF-101.', 'counts_by_year': [[2022, 473], [2021, 1276], [2020, 1067], [2019, 737], [2018, 313], [2017, 25], [2016, 3], [2012, 2]]}, {'id': 'W2110256992', 'doi': 'https://doi.org/10.1093/nar/gkv1070', 'title': 'KEGG as a reference resource for gene and protein annotation', 'type': 'journal-article', 'publication_date': '2016-01-04', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2151300008', ['I22299242']], ['A2310244469', ['I2252096349']], ['A2212128787', ['I2252096349']], ['A143568440', ['I22299242']], ['A2168774161', ['I22299242']]], 'cited_by_count': 3851, 'concepts': [['C152724338', '0.9777727'], ['C86803240', '0.71851104'], ['C2776321320', '0.6916005'], ['C141231307', '0.6806058'], ['C2908923196', '0.6539043']], 'referenced_works': ['W1969404643', 'W1977025277', 'W1983922226', 'W2005568709', 'W2032936332', 'W2042160423', 'W2131848047', 'W2133579817', 'W2142678031', 'W2152127318', 'W2156125289', 'W2165770381', 'W2739999456', 'W4235121031'], 'abstract': 'KEGG (http://www.kegg.jp/ or http://www.genome.jp/kegg/) is an integrated database resource for biological interpretation of genome sequences and other high-throughput data. Molecular functions of genes and proteins are associated with ortholog groups and stored in the KEGG Orthology (KO) database. The KEGG pathway maps, BRITE hierarchies and KEGG modules are developed as networks of KO nodes, representing high-level functions of the cell and the organism. Currently, more than 4000 complete genomes are annotated with KOs in the KEGG GENES database, which can be used as a reference data set for KO assignment and subsequent reconstruction of KEGG pathways and other molecular networks. As an annotation resource, the following improvements have been made. First, each KO record is re-examined and associated with protein sequence data used in experiments of functional characterization. Second, the GENES database now includes viruses, plasmids, and the addendum category for functionally characterized proteins that are not represented in complete genomes. Third, new automatic annotation servers, BlastKOALA and GhostKOALA, are made available utilizing the non-redundant pangenome data set generated from the GENES database. As a resource for translational bioinformatics, various data sets are created for antimicrobial resistance and drug interaction networks.', 'counts_by_year': [[2022, 494], [2021, 672], [2020, 601], [2019, 613], [2018, 610], [2017, 558], [2016, 295], [2015, 5]]}, {'id': 'W2395611524', 'doi': 'https://doi.org/10.1109/tpami.2016.2572683', 'title': 'Fully Convolutional Networks for Semantic Segmentation', 'type': 'journal-article', 'publication_date': '2017-04-01', 'host_venue': 'V199944782', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A662858474', ['I95457486']], ['A2106816141', ['I95457486']], ['A2174985400', ['I95457486']]], 'cited_by_count': 3827, 'concepts': [['C41008148', '0.8072395'], ['C154945302', '0.73334354'], ['C89600930', '0.66261375'], ['C81363708', '0.61395943'], ['C75608658', '0.58659196']], 'referenced_works': ['W639708223', 'W845365781', 'W1495267108', 'W1745334888', 'W1783315696', 'W1803059841', 'W1903029394', 'W1923115158', 'W1938976761', 'W1948751323', 'W1984757472', 'W2022508996', 'W2067912884', 'W2083597815', 'W2090518410', 'W2092985495', 'W2093112233', 'W2097117768', 'W2103504761', 'W2124592697', 'W2125215748', 'W2144794286', 'W2154644822', 'W2154815154', 'W2155893237', 'W2158778629', 'W2162741153', 'W2183182206', 'W2618530766'], 'abstract': 'Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.', 'counts_by_year': [[2022, 750], [2021, 909], [2020, 794], [2019, 750], [2018, 469], [2017, 137], [2016, 13], [2015, 1]]}, {'id': 'W2557738935', 'doi': 'https://doi.org/10.1001/jama.2016.17216', 'title': 'Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs', 'type': 'journal-article', 'publication_date': '2016-12-13', 'host_venue': 'V172573765', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A95520236', ['I1291425158']], ['A2434682237', ['I1291425158']], ['A2225837016', ['I1291425158']], ['A2486974555', ['I1291425158']], ['A2565050438', ['I1291425158']], ['A1995998730', ['I1291425158']], ['A2024951123', ['I1291425158']], ['A2562619734', ['I1291425158']], ['A2965140802', ['I1291425158']], ['A2158622472', ['I896857757']], ['A2231943984', ['I2800958399']], ['A2099367384', ['I118298837']], ['A2565191535', ['I1291425158']], ['A1848698474', ['I1283280774']], ['A2101079242', ['I1291425158']]], 'cited_by_count': 3823, 'concepts': [['C71924100', '0.8843045'], ['C2779829184', '0.79753125'], ['C2776391266', '0.6416706'], ['C108583219', '0.63469833'], ['C81363708', '0.6298924']], 'referenced_works': ['W197570494', 'W1969496006', 'W1973351628', 'W1984639731', 'W1995023030', 'W2010120422', 'W2013444477', 'W2049036672', 'W2062393305', 'W2073244572', 'W2117539524', 'W2137591261', 'W2148309496', 'W2149430368', 'W2156876426', 'W2159764117', 'W2160829096', 'W2163605009', 'W2168231600', 'W2919115771'], 'abstract': 'Deep learning is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior, removing the need to specify rules explicitly. Application of these methods to medical imaging requires further assessment and validation.To apply deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs.A specific type of neural network optimized for image classification called a deep convolutional neural network was trained using a retrospective development data set of 128 175 retinal images, which were graded 3 to 7 times for diabetic retinopathy, diabetic macular edema, and image gradability by a panel of 54 US licensed ophthalmologists and ophthalmology senior residents between May and December 2015. The resultant algorithm was validated in January and February 2016 using 2 separate data sets, both graded by at least 7 US board-certified ophthalmologists with high intragrader consistency.Deep learning-trained algorithm.The sensitivity and specificity of the algorithm for detecting referable diabetic retinopathy (RDR), defined as moderate and worse diabetic retinopathy, referable diabetic macular edema, or both, were generated based on the reference standard of the majority decision of the ophthalmologist panel. The algorithm was evaluated at 2 operating points selected from the development set, one selected for high specificity and another for high sensitivity.The EyePACS-1 data set consisted of 9963 images from 4997 patients (mean age, 54.4 years; 62.2% women; prevalence of RDR, 683/8878 fully gradable images [7.8%]); the Messidor-2 data set had 1748 images from 874 patients (mean age, 57.6 years; 42.6% women; prevalence of RDR, 254/1745 fully gradable images [14.6%]). For detecting RDR, the algorithm had an area under the receiver operating curve of 0.991 (95% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95% CI, 0.986-0.995) for Messidor-2. Using the first operating cut point with high specificity, for EyePACS-1, the sensitivity was 90.3% (95% CI, 87.5%-92.7%) and the specificity was 98.1% (95% CI, 97.8%-98.5%). For Messidor-2, the sensitivity was 87.0% (95% CI, 81.1%-91.0%) and the specificity was 98.5% (95% CI, 97.7%-99.1%). Using a second operating point with high sensitivity in the development set, for EyePACS-1 the sensitivity was 97.5% and specificity was 93.4% and for Messidor-2 the sensitivity was 96.1% and specificity was 93.9%.In this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.', 'counts_by_year': [[2022, 590], [2021, 939], [2020, 924], [2019, 741], [2018, 456], [2017, 158], [2016, 4], [2015, 1], [2012, 3]]}, {'id': 'W2416799949', 'doi': 'https://doi.org/10.1109/jiot.2016.2579198', 'title': 'Edge Computing: Vision and Challenges', 'type': 'journal-article', 'publication_date': '2016-06-09', 'host_venue': 'V2480266640', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2116347407', ['I185443292']], ['A2308146352', ['I185443292']], ['A2304487019', ['I185443292']], ['A2136266848', ['I185443292']], ['A2418133457', ['I185443292']]], 'cited_by_count': 3719, 'concepts': [['C41008148', '0.75321114'], ['C2778456923', '0.46140733'], ['C31972630', '0.40233707'], ['C162307627', '0.39953455'], ['C154945302', '0.37358475']], 'referenced_works': ['W1585636656', 'W1981293710', 'W2012423440', 'W2023380813', 'W2037806787', 'W2071875983', 'W2083389856', 'W2088692353', 'W2101788345', 'W2111619626', 'W2114296561', 'W2114623221', 'W2119565742', 'W2119738171', 'W2121884932', 'W2125444986', 'W2135099885', 'W2163349370', 'W2169466806', 'W2173213060', 'W2245189809', 'W4211186987', 'W4242036132'], 'abstract': 'The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.', 'counts_by_year': [[2022, 567], [2021, 921], [2020, 905], [2019, 769], [2018, 419], [2017, 127], [2016, 6]]}, {'id': 'W2174661749', 'doi': 'https://doi.org/10.1148/radiol.2015151169', 'title': 'Radiomics: Images Are More than Pictures, They Are Data', 'type': 'journal-article', 'publication_date': '2016-02-04', 'host_venue': 'V50280174', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2200847216', ['I3019308854']], ['A2160927198', ['I201448701']], ['A14482836', ['I1334819555']]], 'cited_by_count': 3672, 'concepts': [['C2778559731', '0.8174426'], ['C71924100', '0.71424377'], ['C58489278', '0.6202328'], ['C2777466982', '0.55036545'], ['C2779974597', '0.48537648']], 'referenced_works': ['W313065440', 'W1506135535', 'W1572056027', 'W1578777125', 'W1917894041', 'W1969984544', 'W1975309478', 'W1980497534', 'W1987054640', 'W1987777080', 'W1995061416', 'W2006383231', 'W2007494001', 'W2009083663', 'W2017187984', 'W2020813095', 'W2025971174', 'W2032079626', 'W2035484878', 'W2038901755', 'W2040458067', 'W2041179993', 'W2042387032', 'W2044465660', 'W2047630467', 'W2052040374', 'W2052295501', 'W2054999256', 'W2055575746', 'W2060427373', 'W2061077502', 'W2061476066', 'W2062338927', 'W2067833766', 'W2067959276', 'W2072504371', 'W2080965354', 'W2081296990', 'W2083927153', 'W2085887106', 'W2093290197', 'W2097475056', 'W2101329867', 'W2103004421', 'W2111013601', 'W2111389142', 'W2113499049', 'W2114588884', 'W2115581335', 'W2124735791', 'W2128739912', 'W2130535356', 'W2137516955', 'W2141059777', 'W2141619730', 'W2142114378', 'W2142266080', 'W2144032281', 'W2149541891', 'W2150575159', 'W2158893731', 'W2169877598', 'W2169895148', 'W2169961966', 'W2219978888', 'W2231925625', 'W2313339984', 'W2482351703', 'W2509002782', 'W4255382556'], 'abstract': 'In the past decade, the field of medical image analysis has grown exponentially, with an increased number of pattern recognition tools and an increase in data set sizes. These advances have facilitated the development of processes for high-throughput extraction of quantitative features that result in the conversion of images into mineable data and the subsequent analysis of these data for decision support; this practice is termed radiomics. This is in contrast to the traditional practice of treating medical images as pictures intended solely for visual interpretation. Radiomic data contain first-, second-, and higher-order statistics. These data are combined with other patient data and are mined with sophisticated bioinformatics tools to develop models that may potentially improve diagnostic, prognostic, and predictive accuracy. Because radiomics analyses are intended to be conducted with standard of care images, it is conceivable that conversion of digital images to mineable data will eventually become routine practice. This report describes the process of radiomics, its challenges, and its potential power to facilitate better clinical decision making, particularly in the care of patients with cancer.', 'counts_by_year': [[2022, 795], [2021, 992], [2020, 765], [2019, 541], [2018, 352], [2017, 175], [2016, 48]]}, {'id': 'W1490421035', 'doi': 'https://doi.org/10.4324/9781315129266-7', 'title': 'Parental Investment and Sexual Selection', 'type': 'book-chapter', 'publication_date': '2017-07-12', 'host_venue': 'V4306463855', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2102238808', ['I136199984']]], 'cited_by_count': 3649, 'concepts': [['C130981225', '0.5455825'], ['C27548731', '0.5422945'], ['C81917197', '0.51784116'], ['C86803240', '0.2193793'], ['C41008148', '0.20224264']], 'referenced_works': ['W2319914034'], 'abstract': "There is a tendency among biologists studying social behavior to regard the adult sex ratio as an independent variable to which the species reacts with appropriate adaptations. D. Lack often interprets social behavior as an adaptation in part to an unbalanced (or balanced) sex ratio, and J. Verner has summarized other instances of this tendency. The only mechanism that will generate differential mortality independent of sexual differences clearly related to parental investment and sexual selection is the chromosomal mechanism, applied especially to humans and other mammals: the unguarded X chromosome of the male is presumed to predispose him to higher mortality. Each offspring can be viewed as an investment independent of other offspring, increasing investment in one offspring tending to decrease investment in others. Species can be classified according to the relative parental investment of the sexes in their young. In the vast majority of species, the male's only contribution to the survival of his offspring is his sex cells.", 'counts_by_year': [[2022, 45], [2021, 54], [2020, 65], [2019, 45], [2018, 45], [2017, 47], [2016, 89], [2015, 109], [2014, 138], [2013, 121], [2012, 179]]}, {'id': 'W2962914239', 'doi': 'https://doi.org/10.1109/3dv.2016.79', 'title': 'V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation', 'type': 'proceedings-article', 'publication_date': '2016-06-15', 'host_venue': 'V4306419358', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A1985922831', ['I62916508']], ['A572302453', ['I62916508']], ['A2110319412', ['I8204097']]], 'cited_by_count': 3619, 'concepts': [['C41008148', '0.8105537'], ['C154945302', '0.7940235'], ['C81363708', '0.779541'], ['C89600930', '0.6401964'], ['C163892561', '0.6096256']], 'referenced_works': ['W565224490', 'W1677182931', 'W1745334888', 'W1849277567', 'W1901129140', 'W1903029394', 'W2001142427', 'W2101608218', 'W2106033751', 'W2117340355', 'W2119496927', 'W2148027243', 'W2307535535'], 'abstract': 'Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.', 'counts_by_year': [[2022, 651], [2021, 1068], [2020, 877], [2019, 688], [2018, 248], [2017, 73], [2016, 5], [2012, 1]]}, {'id': 'W147232447', 'doi': 'https://doi.org/10.1051/0004-6361/201629272', 'title': 'The<i>Gaia</i>mission', 'type': 'journal-article', 'publication_date': '2016-11-01', 'host_venue': 'V205231332', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A3012521882', ['I44377176']], ['A1943248465', ['I44377176']], ['A2160904088', ['I121797337']], ['A1921976926', ['I875825670']], ['A2829230393', ['I169173203']], ['A2982917919', ['I149899117']], ['A2290290381', ['I223822909']], ['A3020319636', ['I223822909']], ['A2192977182', ['I241749']], ['A2162396141', ['I114457229']], ['A2800531618', ['I44377176']], ['A2976943547', ['I71999127']], ['A78091972', ['I78650965']], ['A2079802608', ['I2801994115']], ['A2099976791', ['I187531555']], ['A2180319449', ['I71999127']], ['A2467827460', ['I201841394']], ['A2973409670', ['I1321659569']], ['A2233478754', []], ['A2974058870', ['I103167339']], ['A336212624', ['I132053463']], ['A2139641010', ['I875825670']], ['A2327371580', ['I44377176']], ['A2806973846', ['I169173203']], ['A3103039284', ['I5101941']], ['A2527360437', ['I15057530']], ['A2994918965', []], ['A2270109691', ['I241749']], ['A2864728443', ['I241749']], ['A2009066629', ['I145872427']], ['A2778608201', ['I169173203']], ['A2017138959', ['I45129253']], ['A2309569722', ['I875825670']], ['A2568300086', ['I124055696']], ['A2996088242', ['I169173203']], ['A2061813290', ['I875825670']], ['A3212003020', ['I2801994115']], ['A2031721904', ['I223822909']], ['A2130508632', ['I204136569']], ['A1915331505', []], ['A993220370', []], ['A2863937541', ['I865339163']], ['A2810442059', ['I875825670']], ['A2303735041', ['I71999127']], ['A2560615925', []], ['A2844522754', ['I23877868']], ['A3081989128', ['I241749']], ['A2806687599', ['I71999127']], ['A2128008059', ['I2801994115']], ['A2625357129', ['I2801994115']], ['A2560266018', []], ['A2183792948', ['I71999127']], ['A3213030245', ['I865339163']], ['A3206977667', ['I114457229']], ['A2048526991', ['I114457229']], ['A361892099', ['I114457229']], ['A2827736024', ['I169173203']], ['A2614993456', ['I71999127']], ['A2615945734', ['I1286704778']], ['A3012163846', ['I241749']], ['A3028530188', ['I45129253']], ['A2038205816', ['I201841394']], ['A2976564312', ['I201841394']], ['A2316246364', ['I71999127']], ['A1997491244', ['I223822909']], ['A2859415216', ['I71999127']], ['A2995718868', ['I5101941']], ['A2515844249', ['I2801994115']], ['A3185324263', ['I2801994115']], ['A2973620798', ['I223822909']], ['A2617948198', ['I2746051580']], ['A1997090731', ['I149899117']], ['A1272823052', ['I149899117']], ['A2946515095', []], ['A2148405839', ['I45129253']], ['A2777349338', ['I2746051580']], ['A2155047780', ['I2799502024']], ['A2204987722', ['I241749']], ['A2802203867', ['I201841394']], ['A2938431246', ['I875825670']], ['A2053764693', ['I875825670']], ['A2840715968', ['I241749']], ['A2807071540', ['I201841394']], ['A2190090831', ['I2799502024']], ['A2153085531', ['I98677209']], ['A2156207578', ['I99464096']], ['A2131829759', []], ['A2197847343', ['I157674565']], ['A2558920252', ['I875825670']], ['A2422632151', ['I15057530']], ['A1995167661', ['I2799502024']], ['A2157889035', ['I95013407']], ['A2065810771', ['I157674565']], ['A2470633633', ['I68947357']], ['A40177529', ['I98677209']], ['A2433637235', ['I241749']], ['A2625020190', ['I223822909']], ['A2021156215', ['I2746051580']], ['A3067031115', ['I241749']], ['A2907650962', ['I45129253']], ['A2542767672', []], ['A1971143122', ['I19894307']], ['A2138794924', ['I223822909']], ['A2943051656', ['I200777214']], ['A3015578528', ['I123387679']], ['A2005753654', ['I39063666']], ['A2106149344', []], ['A2303389179', ['I141596103']], ['A2000799932', ['I33876163']], ['A2339803820', []], ['A2819929774', ['I59361560']], ['A2165804224', ['I2799502024']], ['A3176565609', ['I1294671590']], ['A2873728526', ['I201841394']], ['A2425046397', ['I1294671590']], ['A2046423563', ['I178450904']], ['A2351802129', ['I132053463']], ['A2845916219', ['I45129253']], ['A2432254286', ['I149899117']], ['A1648810539', ['I875825670']], ['A2949281054', ['I2746051580']], ['A2300692420', []], ['A2820346861', ['I169173203']], ['A2299211669', ['I875825670']], ['A2560707204', []], ['A2559787179', []], ['A2942592793', ['I178450904']], ['A2608100635', ['I1286704778']], ['A1241721641', []], ['A2137880595', ['I875825670']], ['A2560435705', []], ['A2129738116', ['I129774422']], ['A2616011163', ['I145311948']], ['A3189943927', ['I2746051580']], ['A2560831092', ['I23877868']], ['A2226363179', ['I71999127']], ['A2078722464', ['I44377176']], ['A2168831773', ['I141596103']], ['A1984475403', []], ['A2974744073', ['I44377176']], ['A3132628066', ['I44377176']], ['A2559940599', []], ['A3105478443', ['I45129253']], ['A368014891', ['I71999127']], ['A1987719689', ['I2746051580']], ['A2505433132', ['I141596103']], ['A2125293425', []], ['A2509527519', ['I114457229']], ['A2973285433', ['I44377176']], ['A439364690', ['I134820265']], ['A2894398941', ['I141596103']], ['A2528849801', ['I153648349']], ['A204511207', ['I875825670']], ['A2008455111', ['I875825670']], ['A2974294126', ['I889804353']], ['A2513444451', ['I165339363']], ['A1956868592', ['I241749']], ['A2061094561', ['I201841394']], ['A2070001189', ['I39147953']], ['A2138059221', []], ['A2027446853', ['I68947357']], ['A1857125947', ['I15057530']], ['A2049687442', ['I241749']], ['A2860687460', ['I15057530']], ['A2859320681', ['I68947357']], ['A2151464114', []], ['A357918510', ['I71999127']], ['A2490216925', ['I2746051580']], ['A2974545839', ['I15057530']], ['A2144603386', ['I134820265']], ['A2722823332', ['I875825670']], ['A2059361806', []], ['A2006467337', ['I15057530']], ['A3098654955', ['I223822909']], ['A2016327145', ['I875825670']], ['A2973840539', ['I1321659569']], ['A3177284169', ['I241749']], ['A2085132177', ['I204136569']], ['A2622325167', []], ['A2081633468', ['I875825670']], ['A2878356836', ['I875825670']], ['A3150029116', ['I169173203']], ['A1257587078', []], ['A2779386421', ['I241749']], ['A2095045415', ['I55143463']], ['A2908170126', ['I875825670']], ['A2816566103', ['I2746051580']], ['A2785996814', ['I71999127']], ['A2470153579', ['I875825670']], ['A3049546653', ['I15057530']], ['A3207413192', ['I114457229']], ['A2974798235', ['I103167339']], ['A3216520532', ['I103167339']], ['A2791454587', ['I201841394']], ['A2003754843', ['I71999127']], ['A2007505216', ['I875825670']], ['A2764011965', ['I98677209']], ['A2974119661', ['I1321659569']], ['A2894452972', ['I121797337']], ['A2974733369', ['I169173203']], ['A2041182160', ['I98677209']], ['A2095745443', ['I875825670']], ['A3103285008', []], ['A2146290278', []], ['A2067303399', []], ['A2761461774', []], ['A3190119414', ['I2746051580']], ['A3102439187', ['I149213910']], ['A2955477419', ['I2799502024']], ['A2303676150', ['I138689650']], ['A2004505141', ['I201841394']], ['A2163659996', ['I875825670']], ['A2560030255', ['I865339163']], ['A2158827885', ['I875825670']], ['A2156342468', ['I17974374']], ['A2112628909', ['I99464096']], ['A2847918734', []], ['A318104425', ['I201841394']], ['A3214210635', ['I241749']], ['A2523788444', ['I178450904']], ['A2623115353', ['I5101941']], ['A3110285481', ['I169173203']], ['A2892902341', ['I1294671590']], ['A2062742638', ['I875825670']], ['A2902919504', ['I45129253']], ['A2169245837', ['I17974374']], ['A434790586', ['I200777214']], ['A2560284665', []], ['A2825232213', ['I887064364']], ['A2974594563', ['I103167339']], ['A2067558470', ['I123387679']], ['A2123752291', ['I2799713863']], ['A3099665038', ['I44377176']], ['A3214338689', ['I44377176']], ['A2974183247', ['I1321659569']], ['A2207703038', ['I241749']], ['A2560842665', []], ['A2856168311', ['I170138621']], ['A2998360660', ['I59361560']], ['A2467931816', ['I16391192']], ['A2151551670', []], ['A2560259443', ['I71999127']], ['A2973250521', ['I103167339']], ['A2153558802', ['I875825670']], ['A1883301607', ['I133731052']], ['A2835978152', ['I23877868']], ['A715203453', ['I68947357']], ['A1058208318', ['I201841394']], ['A2011587163', ['I71999127']], ['A3011095733', ['I865339163']], ['A2893449149', ['I169173203']], ['A2881314998', ['I865339163']], ['A100734421', ['I149899117']], ['A2153960844', []], ['A2201816745', ['I241749']], ['A3090615387', ['I202391551']], ['A2974924818', ['I44377176']], ['A2658100067', ['I875825670']], ['A1992323407', ['I875825670']], ['A2760913645', ['I201841394']], ['A2560570222', []], ['A2841030816', ['I178450904']], ['A2057245886', ['I44377176']], ['A3082965165', ['I875825670']], ['A3103996715', ['I71999127']], ['A2618540805', ['I200777214']], ['A2828630022', ['I2799713863']], ['A2832702444', ['I78650965']], ['A2037013556', ['I241749']], ['A2572804754', ['I2799803557']], ['A2614792194', ['I59361560']], ['A3205401315', ['I141596103']], ['A2047079775', ['I168974976']], ['A3211419631', ['I6289922']], ['A3098431947', ['I71999127']], ['A2125646902', ['I133731052']], ['A2054997292', []], ['A2779770183', ['I68947357']], ['A2622774001', []], ['A2819988211', ['I71999127']], ['A3213454096', ['I5101941']], ['A2131908436', ['I114457229']], ['A3140680175', ['I169173203']], ['A2861449239', ['I200777214']], ['A3056805025', ['I169173203']], ['A2080423667', ['I123387679']], ['A1907626395', []], ['A2124744380', ['I187531555']], ['A2811834436', ['I223822909']], ['A2504372913', ['I114457229']], ['A2424076866', ['I241749']], ['A2974666472', ['I45129253']], ['A3217448137', ['I121797337']], ['A2829596647', ['I865339163']], ['A2113279216', ['I241749']], ['A3102712249', ['I114457229']], ['A2143269014', ['I241749']], ['A2055446322', ['I49206369']], ['A2020210784', ['I132053463']], ['A2257354375', ['I71999127']], ['A1976339427', ['I200777214']], ['A3101350907', ['I153976015']], ['A2288769910', ['I2801994115']], ['A2777002791', ['I136199984']], ['A2425090799', []], ['A2118250750', ['I241749']], ['A3036257690', ['I2799713863']], ['A2300612181', ['I202391551']], ['A2974085764', ['I1321659569']], ['A707964919', ['I141596103']], ['A2860191375', ['I2746051580']], ['A3213740075', ['I16391192']], ['A2212339843', ['I187531555']], ['A2559901667', []], ['A2764713076', ['I875825670']], ['A2589963544', []], ['A2019295799', ['I2746051580']], ['A2989176583', ['I169173203']], ['A3050867880', ['I129774422']], ['A1829270367', ['I875825670']], ['A3214493903', ['I169173203']], ['A2467224946', ['I114457229']], ['A1988415221', []], ['A2696706592', ['I223822909']], ['A2560732511', []], ['A2116471710', ['I19820366']], ['A2224933697', []], ['A2764374272', ['I124055696']], ['A2426494767', ['I1287402277']], ['A2069548507', ['I200777214']], ['A1991523214', ['I2799502024']], ['A2821398710', ['I223822909']], ['A3125219785', ['I134820265']], ['A2973689340', ['I889804353']], ['A2602040920', ['I129774422']], ['A2973881394', ['I1321659569']], ['A2800320328', ['I98677209']], ['A2559952146', []], ['A2560455659', []], ['A2148665006', ['I98677209']], ['A3103272864', ['I223822909']], ['A2748746957', ['I169173203']], ['A2473258969', ['I63098007']], ['A2154203018', ['I875825670']], ['A2974866338', ['I889804353']], ['A2471266545', ['I59361560']], ['A2174514421', ['I59361560']], ['A2858992183', ['I7597260']], ['A2862560749', ['I169173203']], ['A2628973555', []], ['A3174039302', ['I865339163']], ['A2569192218', []], ['A2952911899', ['I2799713863']], ['A2438413637', ['I16391192']], ['A2411459606', ['I187531555']], ['A2837118234', ['I875825670']], ['A2973554218', ['I889804353']], ['A2167074508', ['I187531555']], ['A2973947371', ['I241749']], ['A2973732668', ['I141596103']], ['A3011965363', ['I71999127']], ['A1903381034', ['I875825670']], ['A2296914234', ['I875825670']], ['A2109373066', ['I7597260']], ['A3120331750', ['I277688954']], ['A2020298332', ['I875825670']], ['A2973605858', ['I44377176']], ['A2463484837', ['I71999127']], ['A3001841472', []], ['A1842134074', ['I875825670']], ['A1937717674', ['I157674565']], ['A2159415115', ['I5124864']], ['A2973813016', ['I5101941']], ['A2119167818', ['I98677209']], ['A3176155137', ['I865339163']], ['A2633077173', ['I875825670']], ['A469002413', ['I875825670']], ['A2937379796', []], ['A2439563356', ['I145872427']], ['A2080426145', ['I875825670']], ['A2273874634', []], ['A2110184268', ['I201841394']], ['A2015866711', ['I88060688']], ['A2976457852', ['I241749']], ['A2138605715', ['I153648349']], ['A2121927168', ['I875825670']], ['A2118988984', []], ['A2279966148', []], ['A2183986719', ['I114457229']], ['A2519491506', ['I5101941']], ['A2974004742', ['I44377176']], ['A2036950765', []], ['A2560577935', []], ['A2549272364', ['I133731052']], ['A2973653569', ['I44377176']], ['A2974621708', ['I201841394']], ['A2512017004', ['I875825670']], ['A3021260211', ['I68947357']], ['A2791604391', ['I7597260']], ['A2542136686', ['I169173203']], ['A2559790920', []], ['A286868970', ['I7863295']], ['A2071664402', ['I875825670']], ['A3161003130', ['I875825670']], ['A2822803416', ['I875825670']], ['A2840179762', ['I2746051580']], ['A2520684031', []], ['A3214443867', ['I187531555']], ['A2142955422', ['I157674565']], ['A2171814091', ['I153648349']], ['A3012177245', ['I99464096']], ['A2974316635', ['I1321659569']], ['A2467385717', ['I1294671590']], ['A2127763278', []], ['A2308282099', ['I114457229']], ['A1990159702', ['I875825670']], ['A2999939681', ['I875825670']], ['A2041409742', ['I241749']], ['A2499200225', ['I114457229']], ['A3118082443', ['I71999127']], ['A2336878963', ['I98677209']], ['A2227379306', ['I169173203']], ['A2123575116', ['I1321659569']], ['A2285716558', ['I169173203']], ['A2593024873', ['I132053463']], ['A3142417170', ['I223822909']], ['A1810961832', ['I2801994115']], ['A2672619362', []], ['A2189120660', []], ['A2582363386', ['I875825670']], ['A2560837038', []], ['A2090646244', ['I44377176']], ['A2974358414', ['I201841394']], ['A2072645533', ['I875825670']], ['A2026318257', []], ['A3026904232', ['I23877868']], ['A2047793719', ['I114457229']], ['A3215151446', ['I5101941']], ['A3080146698', ['I169173203']], ['A2158350943', ['I875825670']], ['A2775883393', ['I875825670']], ['A2147751977', ['I889804353']], ['A2478660968', ['I134820265']], ['A3174109405', ['I865339163']], ['A2025704710', ['I875825670']], ['A2126988624', ['I71999127']], ['A2749969630', ['I2746051580']], ['A2085323956', ['I875825670']], ['A1997505886', ['I201841394']], ['A3098011825', ['I223822909']], ['A2182904200', ['I63098007']], ['A2540462905', ['I78650965']], ['A2798653091', ['I5101941']], ['A3096151456', ['I875825670']], ['A2860513023', ['I241749']], ['A2130471115', ['I114457229']], ['A2211771848', ['I157674565']], ['A2560617254', ['I7597260']], ['A2856746054', ['I7597260']], ['A1983542609', ['I121748325']], ['A2840946435', ['I2746051580']], ['A2559849917', []], ['A2579765795', ['I36234482']], ['A2947497898', ['I17974374']], ['A2819120486', ['I1286704778']], ['A2146697449', ['I204337017']], ['A1982376769', []], ['A2785094500', ['I169173203']], ['A680991069', ['I6289922']], ['A2615196244', []], ['A2149367218', ['I875825670']], ['A2490926393', ['I121797337']], ['A2560693642', ['I2799502024']], ['A2570584705', ['I241749']], ['A3132539031', ['I7597260']], ['A2187526214', ['I875825670']], ['A2973882180', []], ['A2539869062', []], ['A2861939069', ['I2799803557']], ['A2570926951', ['I44377176']], ['A2625585528', ['I71999127']], ['A1976729845', ['I202391551']], ['A1967702059', ['I98677209']], ['A3099643514', []], ['A2826622112', ['I71999127']], ['A3145227193', ['I2799713863']], ['A2434581417', ['I1321659569']], ['A2235398358', ['I145872427']], ['A2973907474', ['I1321659569']], ['A2039717483', ['I241749']], ['A2812226807', ['I241749']], ['A2211806597', ['I153976015']], ['A2111890041', ['I16391192']], ['A1984528686', ['I19894307']], ['A2691247408', ['I153976015']], ['A2812740206', ['I241749']], ['A3125819726', ['I44377176']], ['A2907230330', ['I134820265']], ['A2322941096', ['I141596103']], ['A2243873467', ['I71999127']], ['A2762701234', ['I141596103']], ['A2484147259', ['I44377176']], ['A2848881683', ['I5101941']], ['A3212178699', ['I114457229']], ['A2614873069', ['I223822909']], ['A3188501522', ['I201841394']], ['A2965691343', ['I201841394']], ['A2974471329', ['I44377176']], ['A2329035524', ['I875825670']], ['A2129881717', ['I875825670']], ['A2331419178', ['I149899117']], ['A2480460915', ['I138549579']], ['A3215137449', ['I241749']], ['A2560671056', []], ['A2463337845', ['I241749']], ['A2838752093', ['I875825670']], ['A2336680542', ['I78650965']], ['A2614454028', []], ['A2563292315', ['I178450904']], ['A1997561457', ['I80849659']], ['A2974775814', ['I169173203']], ['A1951293143', ['I875825670']], ['A2559994512', []], ['A2121586008', ['I135140700']], ['A2003885280', ['I23877868']], ['A2941535605', ['I1321434546']], ['A2111584555', ['I169173203']], ['A2026986992', ['I141596103']], ['A3104939860', ['I114457229']], ['A2560530443', []], ['A3022999702', []], ['A2161833974', ['I71999127']], ['A1965886636', []], ['A2045644432', ['I153648349']], ['A2864275752', ['I71999127']], ['A2874035972', ['I23877868']], ['A2047704219', ['I875825670']], ['A2302803152', ['I60770847']], ['A2973540853', ['I192455894']], ['A2857690782', ['I169173203']], ['A2867280981', ['I1294671590']], ['A2973846728', ['I169173203']], ['A2560826067', ['I98677209']], ['A2043826130', ['I2801994115']], ['A1935489518', ['I875825670']], ['A2560138825', ['I71999127']], ['A2549887343', []], ['A3099554756', ['I122411786']], ['A2827891612', ['I2799502024']], ['A3217026378', ['I180437899']], ['A3124157793', ['I149899117']], ['A2924227212', ['I7597260']], ['A2154166130', ['I2799713863']], ['A2833559481', ['I124055696']], ['A288362366', ['I123387679']], ['A2599096726', ['I2802841028']], ['A3105617120', ['I129604602']], ['A2843531592', ['I202391551']], ['A2962904376', ['I2746051580']], ['A2560114832', []], ['A2309571299', ['I149899117']], ['A1751400812', ['I875825670']], ['A2472389746', []], ['A2111560385', []], ['A2014363028', ['I2799502024']], ['A2469653208', ['I71999127']], ['A3132604486', ['I201841394']], ['A2057144239', ['I169173203']], ['A3084229351', ['I169173203']], ['A2126692109', ['I875825670']], ['A2431551185', []], ['A2144759819', ['I123387679']], ['A2973463477', ['I2799713863']], ['A2205648527', ['I44377176']], ['A2559860787', []], ['A2974231322', ['I153648349']], ['A2837493057', ['I182534213']], ['A1619395864', ['I133731052']], ['A2857435872', ['I98677209']], ['A2798718362', ['I71999127']], ['A2943177711', ['I114457229']], ['A2075430481', ['I132053463']], ['A2842498021', ['I241749']], ['A2974553458', ['I71999127']], ['A2859452200', ['I169173203']], ['A2039281476', ['I133731052']], ['A2866142360', ['I865339163']], ['A2015222337', ['I157674565']], ['A2559768716', []], ['A2275006779', ['I44377176']], ['A3046094987', ['I149899117']], ['A2303432418', ['I141596103']], ['A388393648', ['I121797337']], ['A2013238772', ['I2801766991']], ['A3062425131', ['I45129253']], ['A2882451282', ['I114457229']], ['A2338150255', ['I875825670']], ['A3094479564', ['I153648349']], ['A2559778089', []], ['A2108239667', ['I44377176']], ['A3176154149', ['I2799713863']], ['A2559898207', []], ['A2627885468', []], ['A2806350244', ['I201841394']], ['A2145430554', ['I78650965']], ['A2088929817', ['I114457229']], ['A2021220397', ['I173212132']], ['A2614407890', []], ['A2101269115', ['I1290463931']], ['A2059516223', ['I2898391981']], ['A3212554833', ['I149899117']], ['A2837309532', ['I116067653']], ['A3099629875', ['I149899117']], ['A2100308234', []], ['A2318457021', ['I869660684']], ['A2861857367', ['I2746051580']], ['A2867349197', ['I71999127']], ['A2089041025', ['I19700959']], ['A2974666704', ['I2799713863']], ['A3130499332', ['I33876163']], ['A2554818529', []], ['A2596273738', []], ['A2141845510', ['I241749']], ['A2894322180', ['I1294671590']], ['A2134126467', ['I78650965']]], 'cited_by_count': 3550, 'concepts': [['C134066672', '0.8937975'], ['C29829512', '0.72865254'], ['C207520454', '0.5939228'], ['C121332964', '0.52559876'], ['C178802073', '0.38615608']], 'referenced_works': ['W1658330429', 'W1670318221', 'W1776631870', 'W1903699604', 'W1937633401', 'W1942800319', 'W1965786206', 'W1971635174', 'W1973287615', 'W1989642169', 'W1992967771', 'W1994380566', 'W1999983341', 'W2000154485', 'W2001848365', 'W2003488746', 'W2004890371', 'W2005319355', 'W2006997511', 'W2009884854', 'W2018382173', 'W2018652200', 'W2019702364', 'W2024091859', 'W2033011260', 'W2037671434', 'W2039704429', 'W2040240968', 'W2046116465', 'W2052536974', 'W2053142512', 'W2066064700', 'W2071054355', 'W2078781759', 'W2082270302', 'W2082619949', 'W2089623245', 'W2091067330', 'W2106691198', 'W2107347629', 'W2114706748', 'W2118267472', 'W2124336192', 'W2125843554', 'W2132515606', 'W2136899486', 'W2140297256', 'W2144491297', 'W2144857429', 'W2147117410', 'W2158225758', 'W2158911876', 'W2166913410', 'W2169816089', 'W2170357631', 'W2171109965', 'W2171744771', 'W2173737456', 'W2211251945', 'W2216753448', 'W2279414948', 'W2290801896', 'W2407982734', 'W2420798534', 'W2520186054', 'W2522000147', 'W2952440869', 'W2952729586', 'W2963291350', 'W2963349706', 'W3099575558', 'W3099937896', 'W3101025511', 'W3101671570', 'W3102270993', 'W3104873006', 'W3105369632', 'W3106239471', 'W4206006736', 'W4233606492', 'W4238293310', 'W4238388699', 'W4244637093', 'W4245847993', 'W4249401967', 'W4289784996', 'W4289926186', 'W4292025404', 'W4292954804', 'W4300702671', 'W4302577828'], 'abstract': 'Gaia is a cornerstone mission in the science programme of the European Space Agency (ESA). The spacecraft construction was approved in 2006, following a study in which the original interferometric concept was changed to a direct-imaging approach. Both the spacecraft and the payload were built by European industry. The involvement of the scientific community focusses on data processing for which the international Gaia Data Processing and Analysis Consortium (DPAC) was selected in 2007. Gaia was launched on 19 December 2013 and arrived at its operating point, the second Lagrange point of the Sun-Earth-Moon system, a few weeks later. The commissioning of the spacecraft and payload was completed on 19 July 2014. The nominal five-year mission started with four weeks of special, ecliptic-pole scanning and subsequently transferred into full-sky scanning mode. We recall the scientific goals of Gaia and give a description of the as-built spacecraft that is currently (mid-2016) being operated to achieve these goals. We pay special attention to the payload module, the performance of which is closely related to the scientific performance of the mission. We provide a summary of the commissioning activities and findings, followed by a description of the routine operational mode. We summarise scientific performance estimates on the basis of in-orbit operations. Several intermediate Gaia data releases are planned and the data can be retrieved from the Gaia Archive, which is available through the Gaia home page at http://www.cosmos.esa.int/gaia.', 'counts_by_year': [[2022, 535], [2021, 665], [2020, 677], [2019, 706], [2018, 666], [2017, 275], [2016, 23], [2015, 2]]}, {'id': 'W2593815020', 'doi': 'https://doi.org/10.1038/nnano.2017.16', 'title': 'Reviving the lithium metal anode for high-energy batteries', 'type': 'journal-article', 'publication_date': '2017-03-01', 'host_venue': 'V7822423', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2100354044', ['I97018004']], ['A2268547797', ['I97018004']], ['A2100354044', ['I2801935854']]], 'cited_by_count': 3549, 'concepts': [['C89395315', '0.6704068'], ['C171250308', '0.66816765'], ['C2994426979', '0.57579046'], ['C138331895', '0.5711517'], ['C73916439', '0.5523671']], 'referenced_works': ['W1488297445', 'W1585306775', 'W1590497121', 'W1606528480', 'W1615371353', 'W1949231392', 'W1964681816', 'W1964940765', 'W1968283912', 'W1970581450', 'W1972767018', 'W1973010644', 'W1976221331', 'W1978791121', 'W1979073204', 'W1980157469', 'W1983782233', 'W1985204514', 'W1986180492', 'W1987452691', 'W1996879303', 'W2004038225', 'W2004721328', 'W2007084362', 'W2009049081', 'W2009496901', 'W2011246863', 'W2012341724', 'W2018226081', 'W2019296186', 'W2021172010', 'W2024257032', 'W2026762875', 'W2028037840', 'W2029749630', 'W2030515179', 'W2030616757', 'W2031740785', 'W2032870862', 'W2034028111', 'W2035745576', 'W2037447290', 'W2038845148', 'W2040597986', 'W2042581501', 'W2045372267', 'W2047775546', 'W2048590072', 'W2051708193', 'W2053230744', 'W2056489591', 'W2056777003', 'W2057899803', 'W2058160603', 'W2059419061', 'W2059481908', 'W2060701132', 'W2061452348', 'W2062517395', 'W2064454797', 'W2069727353', 'W2070416274', 'W2077868354', 'W2079027895', 'W2080852016', 'W2081130935', 'W2081191081', 'W2081865126', 'W2081891816', 'W2082866588', 'W2084080017', 'W2088961968', 'W2089525884', 'W2092104187', 'W2095425919', 'W2095486023', 'W2097243006', 'W2099314167', 'W2100061873', 'W2107818169', 'W2115203588', 'W2115959992', 'W2120493396', 'W2120632249', 'W2123155247', 'W2124043346', 'W2127024701', 'W2128315408', 'W2135545364', 'W2136822440', 'W2138195799', 'W2138210747', 'W2143795526', 'W2144378929', 'W2146884076', 'W2151207643', 'W2152048732', 'W2153511586', 'W2153690895', 'W2155489103', 'W2158006396', 'W2159156446', 'W2159557151', 'W2160923048', 'W2163779749', 'W2164269943', 'W2169211003', 'W2186290912', 'W2206021129', 'W2217159267', 'W2233059155', 'W2265160803', 'W2270669981', 'W2275137717', 'W2279026311', 'W2287078245', 'W2291182905', 'W2310374715', 'W2310519341', 'W2313044201', 'W2313163500', 'W2314204444', 'W2314279561', 'W2320624684', 'W2321197966', 'W2321696461', 'W2322346598', 'W2322684643', 'W2398208645', 'W2405266609', 'W2419183065', 'W2429337228', 'W2436341552', 'W2466777838', 'W2490292921', 'W2514147873', 'W2515110430', 'W2515574353', 'W2561730661'], 'abstract': 'Lithium-ion batteries have had a profound impact on our daily life, but inherent limitations make it difficult for Li-ion chemistries to meet the growing demands for portable electronics, electric vehicles and grid-scale energy storage. Therefore, chemistries beyond Li-ion are currently being investigated and need to be made viable for commercial applications. The use of metallic Li is one of the most favoured choices for next-generation Li batteries, especially Li-S and Li-air systems. After falling into oblivion for several decades because of safety concerns, metallic Li is now ready for a revival, thanks to the development of investigative tools and nanotechnology-based solutions. In this Review, we first summarize the current understanding on Li anodes, then highlight the recent key progress in materials design and advanced characterization techniques, and finally discuss the opportunities and possible directions for future development of Li anodes in applications.', 'counts_by_year': [[2022, 727], [2021, 862], [2020, 791], [2019, 652], [2018, 417], [2017, 89]]}, {'id': 'W2577537660', 'doi': 'https://doi.org/10.18637/jss.v076.i01', 'title': '<i>Stan</i>: A Probabilistic Programming Language', 'type': 'journal-article', 'publication_date': '2017-01-11', 'host_venue': 'V167961193', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2921799764', []], ['A2034095171', []], ['A2298166661', []], ['A2617741426', []], ['A2577750113', []], ['A2596872165', []], ['A1994464961', []], ['A2609459292', []], ['A2181088952', []], ['A2301189606', []]], 'cited_by_count': 3536, 'concepts': [['C41008148', '0.73904556'], ['C199360897', '0.6316138'], ['C49937458', '0.60265434'], ['C154945302', '0.23073521']], 'referenced_works': ['W68436435', 'W163279865', 'W1481653166', 'W1486064919', 'W1496599205', 'W1510659740', 'W1517555081', 'W1545319692', 'W1549853756', 'W1788054578', 'W1857053476', 'W1981457167', 'W1985715247', 'W2045656233', 'W2056760934', 'W2059448777', 'W2082752515', 'W2101687185', 'W2105090664', 'W2126879436', 'W2134012062', 'W2148534890', 'W2290761674', 'W2336987377', 'W2478027467', 'W2479909395', 'W2582743722', 'W2613361048', 'W2963977107', 'W3029645440', 'W3037265734', 'W3104580728'], 'abstract': 'Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.', 'counts_by_year': [[2022, 526], [2021, 926], [2020, 854], [2019, 606], [2018, 383], [2017, 170], [2016, 60], [2015, 5], [2014, 2], [2012, 1]]}, {'id': 'W2905811773', 'doi': 'https://doi.org/10.1108/ebr-11-2018-0203', 'title': 'When to use and how to report the results of PLS-SEM', 'type': 'journal-article', 'publication_date': '2019-01-14', 'host_venue': 'V94775995', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2131431282', []], ['A2897526438', []], ['A2107703387', []], ['A2239006834', []]], 'cited_by_count': 3492, 'concepts': [['C71104824', '0.7043147'], ['C41008148', '0.61075956'], ['C89246107', '0.6034034'], ['C63479239', '0.5404819'], ['C22354355', '0.5322855']], 'referenced_works': ['W136176785', 'W824422210', 'W953353200', 'W1530597771', 'W1591013228', 'W1970571431', 'W1983246018', 'W1985977391', 'W1987654098', 'W1988050849', 'W1988201884', 'W1997422591', 'W2005723452', 'W2008844891', 'W2021189926', 'W2024218294', 'W2028894223', 'W2033360656', 'W2039861700', 'W2041094138', 'W2049159124', 'W2062110409', 'W2066090712', 'W2074259494', 'W2090552208', 'W2100739170', 'W2105846236', 'W2108401663', 'W2118904325', 'W2126728600', 'W2130892515', 'W2131292268', 'W2137195939', 'W2138071797', 'W2161215293', 'W2167344400', 'W2168175751', 'W2170185509', 'W2173765169', 'W2206114644', 'W2251500908', 'W2294888319', 'W2332066856', 'W2342472925', 'W2460403273', 'W2473047188', 'W2528032239', 'W2561541699', 'W2588309384', 'W2734665322', 'W2754304469', 'W2766311737', 'W2768636149', 'W2798651188', 'W2804444475', 'W2804862408', 'W2811456963', 'W2889039935', 'W2896512139', 'W2902249468', 'W2955829435', 'W3049490572', 'W3121452939', 'W3121501521', 'W3122073606', 'W3123747783', 'W3125983421', 'W3147343449', 'W3150796314', 'W3151176709', 'W4235678817', 'W4242935565', 'W4251225833', 'W4252336336'], 'abstract': 'Purpose The purpose of this paper is to provide a comprehensive, yet concise, overview of the considerations and metrics required for partial least squares structural equation modeling (PLS-SEM) analysis and result reporting. Preliminary considerations are summarized first, including reasons for choosing PLS-SEM, recommended sample size in selected contexts, distributional assumptions, use of secondary data, statistical power and the need for goodness-of-fit testing. Next, the metrics as well as the rules of thumb that should be applied to assess the PLS-SEM results are covered. Besides presenting established PLS-SEM evaluation criteria, the overview includes the following new guidelines: PLSpredict (i.e., a novel approach for assessing a model’s out-of-sample prediction), metrics for model comparisons, and several complementary methods for checking the results’ robustness. Design/methodology/approach This paper provides an overview of previously and recently proposed metrics as well as rules of thumb for evaluating the research results based on the application of PLS-SEM. Findings Most of the previously applied metrics for evaluating PLS-SEM results are still relevant. Nevertheless, scholars need to be knowledgeable about recently proposed metrics (e.g. model comparison criteria) and methods (e.g. endogeneity assessment, latent class analysis and PLSpredict), and when and how to apply them to extend their analyses. Research limitations/implications Methodological developments associated with PLS-SEM are rapidly emerging. The metrics reported in this paper are useful for current applications, but must always be up to date with the latest developments in the PLS-SEM method. Originality/value In light of more recent research and methodological developments in the PLS-SEM domain, guidelines for the method’s use need to be continuously extended and updated. This paper is the most current and comprehensive summary of the PLS-SEM method and the metrics applied to assess its solutions.', 'counts_by_year': [[2022, 1579], [2021, 1288], [2020, 533], [2019, 74]]}, {'id': 'W3141151088', 'doi': 'https://doi.org/10.1017/9781108120241.010', 'title': 'Ordinary Differential Equations', 'type': 'book-chapter', 'publication_date': '2017-10-12', 'host_venue': 'V4210178667', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A3213901167', []], ['A3142971135', []], ['A3213013703', []], ['A2586486205', []], ['A2157490227', []]], 'cited_by_count': 3477, 'concepts': [['C136197465', '0.70326936'], ['C51544822', '0.5877772'], ['C2778143727', '0.5494643'], ['C121332964', '0.46472532'], ['C78045399', '0.41117322']], 'referenced_works': ['W643047613', 'W2483676704', 'W4210703431'], 'abstract': 'This handbook is the fourth volume in a series of volumes devoted to self contained and up-to-date surveys in the theory of ordinary differential equations, with an additional effort to achieve readability for mathematicians and scientists from other related fields so that the chapters have been made accessible to a wider audience. It covers a variety of problems in ordinary differential equations. It provides pure mathematical and real world applications. It is written for mathematicians and scientists of many related fields.', 'counts_by_year': [[2022, 8], [2021, 79], [2020, 110], [2019, 108], [2018, 72], [2017, 56], [2016, 87], [2015, 147], [2014, 127], [2013, 151], [2012, 139]]}, {'id': 'W2763766763', 'doi': 'https://doi.org/10.1177/1609406917733847', 'title': 'Thematic Analysis', 'type': 'journal-article', 'publication_date': '2017-10-02', 'host_venue': 'V4210182004', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2054854907', ['I168635309']], ['A2019621979', ['I168635309']], ['A2154092408', ['I168635309']], ['A2037150995', ['I168635309']]], 'cited_by_count': 3450, 'concepts': [['C93692415', '0.573255'], ['C41008148', '0.39186537'], ['C205649164', '0.17622676'], ['C58640448', '0.15075997']], 'referenced_works': ['W79229537', 'W186362352', 'W1544133109', 'W1581058145', 'W1979290264', 'W1993209814', 'W2049133117', 'W2063736169', 'W2093790434', 'W2103666798', 'W2103879113', 'W2111719303', 'W2114299733', 'W2115375428', 'W2115534757', 'W2118146027', 'W2131575393', 'W2134599069', 'W2138664283', 'W2139560185', 'W2140534260', 'W2155662067', 'W2170816415', 'W2170827698', 'W2171161819', 'W2740784540', 'W3147940169', 'W4256398075'], 'abstract': 'As qualitative research becomes increasingly recognized and valued, it is imperative that it is conducted in a rigorous and methodical manner to yield meaningful and useful results. To be accepted ...', 'counts_by_year': [[2022, 1122], [2021, 1164], [2020, 706], [2019, 367], [2018, 88], [2017, 2]]}, {'id': 'W2580035316', 'doi': 'https://doi.org/10.1186/s12859-017-1934-z', 'title': 'ImageJ2: ImageJ for the next generation of scientific image data', 'type': 'journal-article', 'publication_date': '2017-11-29', 'host_venue': 'V19032547', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2116378052', ['I135310074']], ['A2113479336', ['I135310074', 'I29680605']], ['A2211470442', ['I135310074']], ['A2277000502', ['I135310074']], ['A2582422347', ['I135310074', 'I29680605']], ['A2409547552', ['I135310074', 'I29680605']], ['A2126056381', ['I135310074', 'I29680605']]], 'cited_by_count': 3384, 'concepts': [['C4924752', '0.7824395'], ['C41008148', '0.76090544'], ['C177212765', '0.61236286'], ['C61423126', '0.53004575'], ['C115903868', '0.52661216']], 'referenced_works': ['W1524992328', 'W1580913419', 'W1853753143', 'W1969389753', 'W1970480709', 'W1974694769', 'W1989396876', 'W2000558985', 'W2006302594', 'W2016029650', 'W2020111712', 'W2031502025', 'W2037200077', 'W2037947871', 'W2099540110', 'W2104827963', 'W2107554012', 'W2121469024', 'W2129289695', 'W2131579659', 'W2136773811', 'W2137753785', 'W2154917588', 'W2166572577', 'W2167279371', 'W2168166653', 'W2170358380', 'W2268279227', 'W2344741602', 'W2410532393', 'W2419496464', 'W2474653353', 'W2501301823', 'W2529052661', 'W2560690681', 'W2560760128', 'W2589048514', 'W2592068467', 'W2601810315', 'W2603330924', 'W4210512919', 'W4248275435'], 'abstract': 'ImageJ is an image analysis program extensively used in the biological sciences and beyond. Due to its ease of use, recordable macro language, and extensible plug-in architecture, ImageJ enjoys contributions from non-programmers, amateur programmers, and professional developers alike. Enabling such a diversity of contributors has resulted in a large community that spans the biological and physical sciences. However, a rapidly growing user base, diverging plugin suites, and technical limitations have revealed a clear need for a concerted software engineering effort to support emerging imaging paradigms, to ensure the software\'s ability to handle the requirements of modern science.We rewrote the entire ImageJ codebase, engineering a redesigned plugin mechanism intended to facilitate extensibility at every level, with the goal of creating a more powerful tool that continues to serve the existing community while addressing a wider range of scientific requirements. This next-generation ImageJ, called "ImageJ2" in places where the distinction matters, provides a host of new functionality. It separates concerns, fully decoupling the data model from the user interface. It emphasizes integration with external applications to maximize interoperability. Its robust new plugin framework allows everything from image formats, to scripting languages, to visualization to be extended by the community. The redesigned data model supports arbitrarily large, N-dimensional datasets, which are increasingly common in modern image acquisition. Despite the scope of these changes, backwards compatibility is maintained such that this new functionality can be seamlessly integrated with the classic ImageJ interface, allowing users and developers to migrate to these new methods at their own pace.Scientific imaging benefits from open-source programs that advance new method development and deployment to a diverse audience. ImageJ has continuously evolved with this idea in mind; however, new and emerging scientific requirements have posed corresponding challenges for ImageJ\'s development. The described improvements provide a framework engineered for flexibility, intended to support these requirements as well as accommodate future needs. Future efforts will focus on implementing new algorithms in this framework and expanding collaborations with other popular scientific software suites.', 'counts_by_year': [[2022, 506], [2021, 1098], [2020, 928], [2019, 615], [2018, 232], [2017, 3]]}, {'id': 'W2754799758', 'doi': 'https://doi.org/10.1007/s11135-017-0574-8', 'title': 'Saturation in qualitative research: exploring its conceptualization and operationalization', 'type': 'journal-article', 'publication_date': '2018-01-01', 'host_venue': 'V102399824', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2989012653', ['I56007636']], ['A2121719731', ['I56007636']], ['A2583458443', ['I56007636']], ['A2309028937', ['I56007636']], ['A2133403638', ['I181656237']], ['A1661532459', ['I56007636']], ['A1966482260', ['I56007636']], ['A2007028664', ['I56007636']]], 'cited_by_count': 3335, 'concepts': [['C9354725', '0.89197785'], ['C90734943', '0.82966435'], ['C9930424', '0.69490635'], ['C17859611', '0.6188848'], ['C111472728', '0.44567066']], 'referenced_works': ['W606806371', 'W609609347', 'W1501631455', 'W1540110233', 'W1594096578', 'W1763376679', 'W1971765404', 'W1988440122', 'W1988602817', 'W1996004421', 'W1999009641', 'W2010266308', 'W2037053368', 'W2045529909', 'W2049012261', 'W2061361998', 'W2079243999', 'W2088734663', 'W2091327588', 'W2091847390', 'W2104000083', 'W2104016592', 'W2113869775', 'W2116864656', 'W2121638631', 'W2124471079', 'W2125416355', 'W2129660502', 'W2130332411', 'W2133778677', 'W2134316093', 'W2141495406', 'W2142617170', 'W2147147674', 'W2149466250', 'W2152974464', 'W2157131725', 'W2158242434', 'W2160157400', 'W2171044747', 'W2171161819', 'W2204364253', 'W2219193113', 'W2294985862', 'W2305278139', 'W2523074512', 'W2524597629', 'W2562473293', 'W2582336476', 'W2890624035'], 'abstract': 'Saturation has attained widespread acceptance as a methodological principle in qualitative research. It is commonly taken to indicate that, on the basis of the data that have been collected or analysed hitherto, further data collection and/or analysis are unnecessary. However, there appears to be uncertainty as to how saturation should be conceptualized, and inconsistencies in its use. In this paper, we look to clarify the nature, purposes and uses of saturation, and in doing so add to theoretical debate on the role of saturation across different methodologies. We identify four distinct approaches to saturation, which differ in terms of the extent to which an inductive or a deductive logic is adopted, and the relative emphasis on data collection, data analysis, and theorizing. We explore the purposes saturation might serve in relation to these different approaches, and the implications for how and when saturation will be sought. In examining these issues, we highlight the uncertain logic underlying saturation-as essentially a predictive statement about the unobserved based on the observed, a judgement that, we argue, results in equivocation, and may in part explain the confusion surrounding its use. We conclude that saturation should be operationalized in a way that is consistent with the research question(s), and the theoretical position and analytic framework adopted, but also that there should be some limit to its scope, so as not to risk saturation losing its coherence and potency if its conceptualization and uses are stretched too widely.', 'counts_by_year': [[2022, 980], [2021, 1035], [2020, 779], [2019, 447], [2018, 91], [2017, 2]]}, {'id': 'W2396881363', 'doi': 'https://doi.org/10.1038/sdata.2016.35', 'title': 'MIMIC-III, a freely accessible critical care database', 'type': 'journal-article', 'publication_date': '2016-05-24', 'host_venue': 'V2607323502', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2589796016', ['I63966007']], ['A2953730245', ['I63966007']], ['A2531528025', ['I1316535847']], ['A1973262018', ['I63966007']], ['A2115837136', ['I3005327000', 'I63966007']], ['A2142459951', ['I63966007']], ['A2956362836', ['I63966007']], ['A29903417', ['I63966007']], ['A2116171563', ['I1316535847', 'I63966007']], ['A2918765094', ['I1316535847', 'I63966007']]], 'cited_by_count': 3311, 'concepts': [['C2781110425', '0.6008548'], ['C77088390', '0.55096835'], ['C2987404301', '0.48958573'], ['C2991842025', '0.48366317'], ['C2779473830', '0.44938487']], 'referenced_works': ['W1992122264', 'W1995228216', 'W2044853225', 'W2046788142', 'W2048060485', 'W2075147641', 'W2131966673', 'W2162800060', 'W2273556505', 'W2288398389', 'W2315740346'], 'abstract': 'Abstract MIMIC-III (‘Medical Information Mart for Intensive Care’) is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.', 'counts_by_year': [[2022, 604], [2021, 936], [2020, 777], [2019, 550], [2018, 292], [2017, 120], [2016, 23]]}, {'id': 'W4210702584', 'doi': 'https://doi.org/10.1093/nar/gkw1099', 'title': 'UniProt: the universal protein knowledgebase', 'type': 'journal-article', 'publication_date': '2017-01-04', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A4212623071', []]], 'cited_by_count': 3307, 'concepts': [['C202264299', '0.9807228'], ['C104397665', '0.7582698'], ['C86803240', '0.7269368'], ['C41009113', '0.6168371'], ['C2776321320', '0.5318237']], 'referenced_works': ['W1568341953', 'W1982582935', 'W1989962769', 'W1999615477', 'W2037685033', 'W2051978340', 'W2053551770', 'W2055615325', 'W2064795263', 'W2076048958', 'W2098650870', 'W2102461176', 'W2105410871', 'W2107785922', 'W2118001361', 'W2129800387', 'W2135581618', 'W2142096740', 'W2152503273', 'W2164461702', 'W2169567789', 'W2179282661', 'W2269299490', 'W2280346092', 'W2409617262', 'W2461106335', 'W2739999456', 'W2914405292', 'W4237286925'], 'abstract': 'The UniProt knowledgebase is a large resource of protein sequences and associated detailed annotation. The database contains over 60 million sequences, of which over half a million sequences have been curated by experts who critically review experimental and predicted data for each protein. The remainder are automatically annotated based on rule systems that rely on the expert curated knowledge. Since our last update in 2014, we have more than doubled the number of reference proteomes to 5631, giving a greater coverage of taxonomic diversity. We implemented a pipeline to remove redundant highly similar proteomes that were causing excessive redundancy in UniProt. The initial run of this pipeline reduced the number of sequences in UniProt by 47 million. For our users interested in the accessory proteomes, we have made available sets of pan proteome sequences that cover the diversity of sequences for each species that is found in its strains and sub-strains. To help interpretation of genomic variants, we provide tracks of detailed protein information for the major genome browsers. We provide a SPARQL endpoint that allows complex queries of the more than 22 billion triples of data in UniProt (http://sparql.uniprot.org/). UniProt resources can be accessed via the website at http://www.uniprot.org/.', 'counts_by_year': [[2022, 268], [2021, 432], [2020, 585], [2019, 873], [2018, 849], [2017, 298], [2016, 1]]}, {'id': 'W2560609797', 'doi': 'https://doi.org/10.1109/cvpr.2017.16', 'title': 'PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2769334518', ['I97018004']], ['A2123144340', ['I97018004']], ['A2770391618', ['I97018004']], ['A356043702', ['I97018004']]], 'cited_by_count': 3304, 'concepts': [['C131979681', '0.8574164'], ['C41008148', '0.7961902'], ['C89600930', '0.65403616'], ['C154945302', '0.62207174'], ['C186644900', '0.59470737']], 'referenced_works': ['W156975732', 'W1644641054', 'W1955462214', 'W2007206727', 'W2021122545', 'W2077251165', 'W2091791686', 'W2098764590', 'W2099606917', 'W2100657858', 'W2160821342', 'W2165414070', 'W2211722331', 'W2254644702', 'W2293349265', 'W2460657278', 'W2553307952', 'W2962731536', 'W2963021451'], 'abstract': 'Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.', 'counts_by_year': [[2022, 299], [2021, 1058], [2020, 1017], [2019, 648], [2018, 248], [2017, 34], [2016, 1]]}, {'id': 'W2253429366', 'doi': 'https://doi.org/10.1109/tmi.2016.2528162', 'title': 'Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning', 'type': 'journal-article', 'publication_date': '2016-02-11', 'host_venue': 'V58069681', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2132207553', ['I4210154583']], ['A2609450958', ['I4210154583']], ['A2118556804', ['I1300365122']], ['A2119127965', ['I4210154583']], ['A2144854313', ['I1300365122']], ['A2416287851', ['I4210154583']], ['A2110751174', ['I4210154583']], ['A236967710', ['I1300365122']], ['A2163830057', ['I4210154583']]], 'cited_by_count': 3300, 'concepts': [['C81363708', '0.8812425'], ['C41008148', '0.85305136'], ['C150899416', '0.83471286'], ['C154945302', '0.78273153'], ['C108583219', '0.7564328']], 'referenced_works': ['W16705017', 'W22040386', 'W66427752', 'W948663339', 'W1516595210', 'W1565402342', 'W1570613334', 'W1641498739', 'W1871050032', 'W1883146913', 'W1901943601', 'W1908709347', 'W1969366022', 'W1975020933', 'W1975815454', 'W1976921161', 'W1994488211', 'W2003644909', 'W2008120479', 'W2009463667', 'W2022508996', 'W2037227137', 'W2046875449', 'W2062118960', 'W2069143585', 'W2081580037', 'W2082526668', 'W2084783417', 'W2085370032', 'W2095581756', 'W2097117768', 'W2099707769', 'W2107878631', 'W2109255472', 'W2112467442', 'W2112796928', 'W2136922672', 'W2148157540', 'W2151103935', 'W2154956324', 'W2155893237', 'W2160921898', 'W2161969291', 'W2183182206', 'W2204578866', 'W2293063104', 'W2294318823', 'W2400264455', 'W2467126091', 'W4256213178'], 'abstract': 'Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.', 'counts_by_year': [[2022, 459], [2021, 772], [2020, 794], [2019, 635], [2018, 390], [2017, 213], [2016, 31]]}, {'id': 'W2762689525', 'doi': 'https://doi.org/10.1016/s0140-6736(17)32129-3', 'title': 'Worldwide trends in body-mass index, underweight, overweight, and obesity from 1975 to 2016: a pooled analysis of 2416 population-based measurement studies in 128·9 million children, adolescents, and adults', 'type': 'journal-article', 'publication_date': '2017-12-16', 'host_venue': 'V49861241', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2682030876', ['I177969490']], ['A2068101109', ['I202697423']], ['A2286634966', ['I202697423']], ['A2989320974', []], ['A2196553093', ['I175532246']], ['A3183158556', []], ['A2473662066', []], ['A1997168069', []], ['A2275164971', []], ['A2035456903', []], ['A735953978', []], ['A2098039862', []], ['A3133084647', []], ['A3187733688', []], ['A2795184334', []], ['A2904002754', []], ['A2291443039', []], ['A2952949420', []], ['A2762959855', []], ['A2762978838', []], ['A2139383287', []], ['A3044301742', []], ['A1896914120', []], ['A3206268970', []], ['A2503366877', []], ['A833217776', []], ['A2807679382', []], ['A2038754818', []], ['A2112469477', []], ['A1977765811', []], ['A2153501837', []], ['A2951616841', []], ['A2067846893', []], ['A1965216579', []], ['A2478078282', []], ['A3212504868', []], ['A2220357105', []], ['A2802843008', []], ['A3070876202', []], ['A3211254539', []], ['A1893639545', []], ['A3114958682', []], ['A2040629458', []], ['A2760875033', []], ['A1134496739', []], ['A2133410634', []], ['A2235486240', []], ['A2063673575', []], ['A2066819014', []], ['A2132379183', []], ['A3213972889', []], ['A129692856', []], ['A1847310112', []], ['A214529110', []], ['A2123421646', []], ['A2061894800', []], ['A2155233510', []], ['A2029839498', []], ['A2141952123', []], ['A2761718840', []], ['A2578999297', []], ['A1862370273', []], ['A2131792796', []], ['A2794949000', []], ['A2117257110', []], ['A3167082168', []], ['A838444184', []], ['A2762604911', []], ['A2103998097', []], ['A2104347288', []], ['A1892310797', []], ['A1987710974', []], ['A343886324', []], ['A2612461509', []], ['A2255887854', []], ['A2806681721', []], ['A2125338727', []], ['A2015186656', []], ['A2665080130', []], ['A2121065633', []], ['A2160969986', []], ['A2002638083', []], ['A2019225622', []], ['A1884409900', []], ['A1956071217', []], ['A77440825', []], ['A2342053117', []], ['A2186495620', []], ['A2113667729', []], ['A3073734431', []], ['A1310048085', []], ['A2304224921', []], ['A2021373880', []], ['A2081197100', []], ['A260602563', []], ['A1578798336', []], ['A3213224851', []], ['A2950674779', []], ['A2078905202', []], ['A3045105087', []], ['A2102628340', []], ['A2340215395', []], ['A2611884914', []], ['A2192377111', []], ['A2587875954', []], ['A2165160393', []], ['A2130905594', []], ['A1981081183', []], ['A2119500417', []], ['A3205990544', []], ['A2165980740', []], ['A2041045907', []], ['A1983179563', []], ['A2645743040', []], ['A1951460657', []], ['A2576208543', []], ['A2949129990', []], ['A2616401253', []], ['A2135100842', []], ['A2035644276', []], ['A3175882133', []], ['A2038873493', []], ['A2138013075', []], ['A2091544932', []], ['A2764158108', []], ['A1913308175', []], ['A2128671967', []], ['A2057511986', []], ['A2439145409', []], ['A2893246086', []], ['A3001488830', []], ['A2647388827', []], ['A2703647893', []], ['A2797778640', []], ['A2155333134', []], ['A2018559943', []], ['A2071100619', []], ['A2565284032', []], ['A2480179289', []], ['A2998353955', []], ['A2886698731', []], ['A2144197545', []], ['A2101516877', []], ['A2036584519', []], ['A2762963757', []], ['A1789796914', []], ['A2597079283', []], ['A2686394006', []], ['A2498808648', []], ['A2130927895', []], ['A2342444247', []], ['A2165233316', []], ['A2033486745', []], ['A2152781988', []], ['A1590331035', []], ['A2764425075', []], ['A2010294628', []], ['A2017393386', []], ['A2188288059', []], ['A1510054053', []], ['A2140769477', []], ['A2423067293', []], ['A2169310649', []], ['A2073025913', []], ['A3082081934', []], ['A2166330915', []], ['A2689021479', []], ['A2051201434', []], ['A2710633756', []], ['A3124837688', []], ['A89168293', []], ['A135959942', []], ['A2153268528', []], ['A1947002580', []], ['A1945955134', []], ['A2203251405', []], ['A2008644747', []], ['A1978482721', []], ['A2591198880', []], ['A2015830873', []], ['A2776645449', []], ['A1977310011', []], ['A2305123453', []], ['A1916180396', []], ['A917896363', []], ['A2160364680', []], ['A2990404966', []], ['A2991135953', []], ['A2099839548', []], ['A2670239669', []], ['A2116930422', []], ['A2106252229', []], ['A1201722959', []], ['A2096160237', []], ['A2000551798', []], ['A2308321090', []], ['A2557335456', []], ['A2128289500', []], ['A2438990952', []], ['A3215649662', []], ['A2760893517', []], ['A2025574871', []], ['A1987072035', []], ['A1979976924', []], ['A2171062904', []], ['A1924982491', []], ['A3044215531', []], ['A1989414362', []], ['A805289726', []], ['A1445582362', []], ['A2763625014', []], ['A2763075088', []], ['A2021853718', []], ['A2036727904', []], ['A2177053883', []], ['A2570649224', []], ['A1564713138', []], ['A2344363948', []], ['A2303742968', []], ['A124780864', []], ['A2069801482', []], ['A2171418200', []], ['A1977630402', []], ['A2526061509', []], ['A152382697', []], ['A2015784969', []], ['A2139380368', []], ['A2145816474', []], ['A2110779941', []], ['A2618095494', ['I202697423']], ['A2145440461', []], ['A2045567931', []], ['A2170374040', []], ['A2080464777', []], ['A2971385610', []], ['A2153484246', []], ['A1648306985', []], ['A3206700454', []], ['A3082158823', []], ['A1052864352', []], ['A1957933193', []], ['A3102838185', []], ['A2116204317', []], ['A2430743913', []], ['A207088766', []], ['A2106125870', []], ['A2796856492', []], ['A143818621', []], ['A1998967213', []], ['A2121706610', []], ['A2219622667', []], ['A2780562525', []], ['A2657508536', []], ['A2043105341', []], ['A1968269790', []], ['A2797384554', []], ['A2009086613', []], ['A3037680532', []], ['A2468136293', []], ['A2139645158', []], ['A1280028882', []], ['A2003233005', []], ['A2642881519', []], ['A2129967664', []], ['A2966496759', []], ['A3146798095', []], ['A2112101663', []], ['A2762862971', []], ['A2177286373', []], ['A2218700663', []], ['A247602335', ['I27897274']], ['A141621299', []], ['A2127977879', []], ['A2570771687', []], ['A2135310240', []], ['A1280028882', []], ['A2430463121', []], ['A2147095497', []], ['A117264901', []], ['A2100868334', []], ['A1840960802', []], ['A2554716121', []], ['A1973220251', []], ['A3082947912', []], ['A2761084152', []], ['A2172974370', []], ['A387443451', []], ['A180630686', []], ['A3012861832', []], ['A2346569426', []], ['A2046913853', []], ['A2039733999', []], ['A2153277603', []], ['A2477619530', []], ['A2686462599', []], ['A2303546325', []], ['A2057627677', []], ['A333588413', []], ['A2003700512', []], ['A2435123145', []], ['A2024571312', []], ['A2145929179', []], ['A2077617762', []], ['A2112027712', []], ['A1974604176', []], ['A2146670245', []], ['A2156409221', []], ['A2675126590', []], ['A3191449112', []], ['A2188037762', []], ['A2295826387', []], ['A2767334201', []], ['A328325244', []], ['A3116579132', []], ['A2895807831', []], ['A1993970561', []], ['A2041690837', []], ['A2794845570', []], ['A2100393840', []], ['A1945154624', []], ['A2118112449', []], ['A2951345928', []], ['A2021538936', []], ['A2145615555', []], ['A2101336132', []], ['A2132203485', []], ['A2761612822', []], ['A2118428675', []], ['A2154613841', []], ['A2569168313', []], ['A2480859676', []], ['A2339518175', []], ['A2621464452', []], ['A1895832007', []], ['A234119253', []], ['A790767378', []], ['A2165643749', []], ['A2124100657', []], ['A2118724476', []], ['A2251992767', []], ['A2762290200', []], ['A2991519085', []], ['A2318503019', []], ['A1800759093', []], ['A2560032097', []], ['A2096049180', []], ['A2560303575', []], ['A2338803232', []], ['A2345056296', []], ['A2439752517', []], ['A2099691238', []], ['A2763363834', []], ['A1779392766', []], ['A2138383812', []], ['A2279641505', []], ['A1077152438', []], ['A712856414', []], ['A2763033865', []], ['A292634770', []], ['A2560346189', []], ['A2475243927', []], ['A2155646412', []], ['A2252812894', []], ['A1998563527', []], ['A3212339021', []], ['A2966505240', []], ['A2586416388', []], ['A1964885482', []], ['A2936951497', []], ['A2151747610', []], ['A2218723425', []], ['A2312117183', []], ['A2291379833', []], ['A2144640944', []], ['A2181105208', []], ['A348243547', []], ['A2616789321', []], ['A1718136763', []], ['A3014482893', []], ['A1366900122', []], ['A2190363594', []], ['A2762806479', []], ['A2160978293', []], ['A2763842508', []], ['A1432807253', []], ['A2187911621', []], ['A1900504705', []], ['A2545108090', []], ['A2560193144', []], ['A2763515223', []], ['A1980701175', []], ['A3037549405', []], ['A899984630', []], ['A2001128985', []], ['A1990801937', []], ['A3211595771', []], ['A1397134919', ['I1288882113']], ['A1443236184', []], ['A2151424940', []], ['A2638506117', []], ['A2117026891', []], ['A2132501245', []], ['A2305755298', []], ['A2027633326', ['I175532246']], ['A2464573620', []], ['A2464641877', []], ['A2687477041', []], ['A737049493', []], ['A303329535', []], ['A215102915', []], ['A2248073579', []], ['A2114341001', []], ['A3037058723', []], ['A3212358261', []], ['A2310523381', []], ['A3081866967', []], ['A2097927447', []], ['A2066511934', []], ['A2011233612', []], ['A1963193313', []], ['A2794685425', []], ['A2135531191', []], ['A1999606051', []], ['A2170267760', []], ['A514027966', []], ['A1145733466', []], ['A2890082532', []], ['A2794624509', []], ['A2560074813', []], ['A288483659', []], ['A3022395946', []], ['A2614444594', []], ['A2043920142', []], ['A3205780343', []], ['A86592531', []], ['A2617350603', []], ['A2050342510', []], ['A1848933928', []], ['A2761710968', []], ['A2074476797', []], ['A2085097665', []], ['A1979217688', []], ['A2617497374', []], ['A2092300620', []], ['A2052254941', []], ['A1984882074', []], ['A2121324034', []], ['A2018933003', []], ['A2087605637', []], ['A2433945580', []], ['A130269992', []], ['A70111183', []], ['A2109534456', []], ['A2028446660', []], ['A2578466262', []], ['A2066307789', []], ['A2026422194', []], ['A229110022', []], ['A2761866875', []], ['A2718524621', []], ['A2011372992', []], ['A1965705618', []], ['A1941170968', []], ['A2169411822', []], ['A2339969264', []], ['A1974177043', []], ['A1797394469', []], ['A1993055715', []], ['A1926373330', []], ['A2234898624', []], ['A1579031933', []], ['A2285279734', []], ['A2590413356', []], ['A2761940053', []], ['A2226546334', []], ['A2112203922', []], ['A2437730194', []], ['A572283030', []], ['A2075372943', []], ['A2333075774', []], ['A2132937021', []], ['A2053074197', []], ['A3211647045', []], ['A2138314085', []], ['A2151536729', []], ['A3207164641', []], ['A2009555889', []], ['A2058959085', []], ['A1899597493', []], ['A2135771809', []], ['A2603457569', []], ['A2097462600', []], ['A2165196495', []], ['A1859873237', []], ['A2040328808', []], ['A2715958809', []], ['A1964343506', []], ['A2050582764', []], ['A314507543', []], ['A308450643', []], ['A3116837681', []], ['A2490476035', []], ['A2049622849', []], ['A1209295092', []], ['A2035262823', []], ['A3121697714', []], ['A1995847088', []], ['A2635404924', []], ['A2762380297', []], ['A2096290711', []], ['A2102029276', []], ['A2008201426', []], ['A1631637737', []], ['A2618640900', []], ['A2560181759', []], ['A1994992763', []], ['A244958483', []], ['A2900781895', []], ['A3037605405', []], ['A179532585', []], ['A3205486634', []], ['A2135542924', []], ['A2147515193', []], ['A2776356797', []], ['A2950883404', []], ['A2045743399', []], ['A2578585759', []], ['A2294335422', []], ['A2712684284', []], ['A1953526467', []], ['A2158759188', []], ['A1967743342', []], ['A1992538613', []], ['A2404841792', []], ['A2314061884', []], ['A2250132359', []], ['A2143411937', []], ['A1633861626', []], ['A2250989391', []], ['A2183421787', []], ['A2100798807', []], ['A2762031157', []], ['A2132768750', []], ['A3186614757', []], ['A2663956315', []], ['A2011277452', []], ['A1836135584', []], ['A2715019165', []], ['A2145483092', []], ['A3102490710', []], ['A2032690607', []], ['A3190531697', []], ['A1976579537', []], ['A227540701', []], ['A2151237675', []], ['A2811033368', []], ['A2093459114', []], ['A2059735974', []], ['A2342582839', []], ['A2796089604', []], ['A2028582722', []], ['A2130328899', []], ['A2105780166', []], ['A2760985577', []], ['A2023451452', []], ['A2091112931', []], ['A2026310690', []], ['A2250613334', []], ['A44115835', []], ['A2145552405', []], ['A2120311186', []], ['A1981214840', []], ['A2151970082', []], ['A2576912562', []], ['A2169790698', []], ['A1833220684', []], ['A2744835223', []], ['A2071125561', []], ['A2154835759', []], ['A1998169968', []], ['A2047924555', []], ['A2050821743', []], ['A2302898849', []], ['A2794575528', []], ['A2781103814', []], ['A2152882147', []], ['A2105448363', []], ['A2149331555', []], ['A2064289063', []], ['A8715795', []], ['A110457895', []], ['A2105625089', []], ['A2101250160', []], ['A2057780970', []], ['A2800320522', []], ['A2118807358', []], ['A2208496761', []], ['A1987929234', []], ['A695008598', []], ['A2168516242', []], ['A1965110001', []], ['A1999192910', []], ['A1965402149', []], ['A2147465570', []], ['A749104330', []], ['A2169326384', []], ['A3187849428', []], ['A2100426135', []], ['A755535894', []], ['A2442136451', []], ['A2059048655', []], ['A1374552325', []], ['A702795633', []], ['A2628015456', ['I27897274']], ['A2063198871', []], ['A2145521599', []], ['A2675267975', []], ['A132584697', []], ['A3037693876', []], ['A2555793860', []], ['A3192228819', []], ['A2618262654', []], ['A2339608917', []], ['A2923207457', []], ['A2136617314', []], ['A1177258763', []], ['A3124651883', []], ['A2163077740', []], ['A191411400', []], ['A2761223911', []], ['A2141993948', []], ['A3212202028', []], ['A1969775378', []], ['A2761690873', []], ['A2761651053', []], ['A1975805881', []], ['A2980548766', []], ['A2048980200', []], ['A2067818254', []], ['A221474822', []], ['A2465966434', []], ['A2469809070', []], ['A2640481844', []], ['A3037488677', []], ['A3167271892', []], ['A2762038244', []], ['A2513946697', []], ['A2985600513', []], ['A2155492732', []], ['A2143285020', []], ['A1787382175', []], ['A2262954967', []], ['A2001636589', []], ['A2666523157', []], ['A2950321150', ['I177969490']], ['A2296926715', []], ['A728865362', []], ['A2089890155', []], ['A2361826624', []], ['A1194488658', []], ['A1341453186', []], ['A2008422887', []], ['A2134822587', []], ['A2648354365', []], ['A1900504705', []], ['A2252413701', []], ['A2426088', []], ['A2638177930', []], ['A2104087080', []], ['A2042780704', []], ['A2257742654', []], ['A2460211422', []], ['A2761788963', []], ['A2047281491', []], ['A1998311120', []], ['A2421821114', []], ['A2111597826', []], ['A2155516925', []], ['A2112753085', []], ['A2118575078', []], ['A2619429304', []], ['A2146713991', []], ['A2100480808', []], ['A2292911033', []], ['A2586627265', []], ['A1487221726', []], ['A2154552586', []], ['A168016521', []], ['A2277375397', []], ['A818435002', []], ['A2648372531', []], ['A2107513181', []], ['A3206221462', []], ['A333006425', []], ['A1977174171', []], ['A2571035470', []], ['A2097971413', []], ['A2302935306', []], ['A2141171231', []], ['A2054790990', []], ['A2109113567', []], ['A2072221744', []], ['A2764137119', []], ['A2462433737', []], ['A2796607963', []], ['A92641685', []], ['A2020141480', []], ['A2067846893', []], ['A2595151854', []], ['A3037698004', []], ['A2444684670', []], ['A2194727819', []], ['A2115385062', []], ['A58097363', []], ['A2063173914', []], ['A1587262111', []], ['A2130668079', []], ['A2307629186', []], ['A2462868298', []], ['A2177398213', []], ['A3042032276', []], ['A161287833', []], ['A2788397792', []], ['A2430789828', []], ['A2953638190', []], ['A214627722', []], ['A2125252707', []], ['A2165157786', []], ['A2018166838', []], ['A2023454549', []], ['A2028972797', []], ['A2106757016', []], ['A2016476708', []], ['A2305433175', []], ['A2138769414', []], ['A669902285', []], ['A1964271924', []], ['A1971470398', []], ['A1987407498', []], ['A1843583804', []], ['A2132455678', []], ['A2114951520', []], ['A1992406705', []], ['A2888198820', []], ['A2990074785', []], ['A2108081569', []], ['A1950581991', []], ['A2169314813', []], ['A339667961', []], ['A1942803212', []], ['A2090005262', []], ['A2430883769', []], ['A2893480027', []], ['A2794512620', []], ['A2106302292', []], ['A2074074111', ['I202697423']], ['A765261849', []], ['A2110834410', []], ['A2412795083', []], ['A1994187097', []], ['A1955797375', []], ['A2011147605', []], ['A3037009723', []], ['A2990636136', []], ['A1932211111', []], ['A2172308787', []], ['A2087086142', []], ['A2122716675', []], ['A2560409712', []], ['A2721352177', []], ['A2805134915', []], ['A2103482049', []], ['A3177932752', []], ['A2134064319', []], ['A2148733320', []], ['A2582633213', []], ['A2160752384', []], ['A1962774929', []], ['A2267412623', []], ['A424594893', []], ['A2800084966', []], ['A2560337598', []], ['A3141825195', []], ['A50573420', []], ['A2109035664', []], ['A3213840452', []], ['A1973908842', []], ['A2951136612', []], ['A2286709630', []], ['A2169277253', []], ['A2776525841', []], ['A1972757265', []], ['A2316272070', []], ['A2609619376', []], ['A679842097', []], ['A1920971052', []], ['A2056343833', []], ['A3014183738', []], ['A2234504887', []], ['A2101409733', []], ['A2102328879', []], ['A3174326343', []], ['A2116158397', []], ['A2085533449', []], ['A2432944910', []], ['A2013827514', []], ['A1858273019', []], ['A2163138263', []], ['A2097837126', []], ['A3037284128', []], ['A3082202103', []], ['A2777764162', []], ['A2138416964', []], ['A2066791622', []], ['A3037101040', []], ['A3082206909', []], ['A2002499485', []], ['A2090498089', []], ['A2097789201', []], ['A2953874115', []], ['A2324754014', []], ['A614647486', []], ['A2217867163', []], ['A2284599314', []], ['A351789345', []], ['A310583671', []], ['A2523451105', []], ['A2168147116', []], ['A2025768854', []], ['A2668296610', []], ['A2505556608', []], ['A2008620751', []], ['A2646790288', []], ['A2148282958', []], ['A3043903153', []], ['A928997519', []], ['A2031537160', []], ['A2150458617', ['I202697423']], ['A2082523072', []], ['A2010355516', []], ['A3150767332', []], ['A2106250063', []], ['A1965939378', []], ['A2017509771', []], ['A2133403922', []], ['A1984425242', []], ['A1928359078', []], ['A1831658207', []], ['A2151965991', []], ['A281053832', []], ['A334347042', []], ['A3206009467', []], ['A2618983276', []], ['A2004729794', []], ['A2344332385', []], ['A1921673354', []], ['A3081732661', []], ['A2625184831', []], ['A2103299422', []], ['A2717273037', []], ['A2147379698', []], ['A2739813038', []], ['A2046924277', []], ['A2761627865', []], ['A2162301576', []], ['A2003062268', []], ['A2661179584', []], ['A2012391880', []], ['A2004127916', []], ['A1981099779', []], ['A2654958273', []], ['A2640278209', []], ['A2167281429', []], ['A2029187179', []], ['A2116906693', []], ['A2755694814', []], ['A2990816277', []], ['A2110957080', []], ['A2041015711', []], ['A56649572', []], ['A2050814358', []], ['A2165382043', []], ['A2619345531', []], ['A2471663902', []], ['A2807173396', []], ['A2026682353', []], ['A3020386027', []], ['A1816015697', []], ['A2096695667', []], ['A1996305901', []], ['A2289887654', []], ['A2673810223', []], ['A2472022029', []], ['A3191343609', []], ['A2002121255', []], ['A1350871735', []], ['A2783004755', []], ['A679639046', []], ['A705222334', []], ['A1911138694', []], ['A265198179', []], ['A2430806458', []], ['A154988338', []], ['A1898219745', []], ['A2749205071', []], ['A3176120636', []], ['A1966538674', []], ['A2161176243', []], ['A1954070353', []], ['A2223239403', []], ['A2107848975', []], ['A2021082479', []], ['A590231635', []], ['A2599701426', []], ['A2095828179', []], ['A2229443397', []], ['A2527676632', []], ['A1996135846', []], ['A1389291389', []], ['A307625456', []], ['A2065093379', []], ['A2552081584', []], ['A1835411509', []], ['A288691706', []], ['A2762061158', []], ['A1990698217', []], ['A2162235912', []], ['A2065837648', []], ['A2065516453', []], ['A2116007402', []], ['A1968893660', []], ['A292056200', []], ['A2191585765', []], ['A2067715899', []], ['A252392113', []], ['A1966613976', []], ['A2097920853', []], ['A2064481922', []], ['A1999052458', []], ['A2067083332', []], ['A93586365', []], ['A2121050010', []], ['A2952374400', []], ['A323920286', []], ['A2181523962', []], ['A1599291100', []], ['A2051411245', []], ['A2794672604', []], ['A2811167731', []], ['A3003216784', []], ['A2107512337', []], ['A2793309059', []], ['A2985907302', []], ['A716274308', []], ['A2764081135', []], ['A2470248228', []], ['A2560259812', []], ['A1313171257', []], ['A689136175', []], ['A1585572293', []], ['A1669531693', []], ['A2437574034', []], ['A1907347112', []], ['A2005440687', []], ['A926888639', []], ['A267405558', []], ['A2435437510', []], ['A1851197827', []], ['A2437516851', []], ['A2100281075', []], ['A2394853576', []], ['A2104245256', []], ['A2135242463', []], ['A2148711556', []], ['A2626103670', []], ['A3089929213', []], ['A2560508205', []], ['A1987687092', []], ['A2295833828', []], ['A2442162546', []], ['A2986051582', []], ['A2466475293', []], ['A3038008707', []], ['A2002679891', []], ['A1556093357', []], ['A1992647866', []], ['A3037146121', []], ['A2004623275', []], ['A2764338174', []], ['A2762408537', []], ['A239754694', []], ['A2427773622', []], ['A3006042166', []], ['A2138270657', []], ['A2263678010', []], ['A1968989593', []], ['A151888123', []], ['A2615987305', []], ['A2139332213', []], ['A2422964059', []], ['A2110746689', []], ['A2111754571', []], ['A2764107249', []], ['A2442487362', []], ['A2714219363', []], ['A2794878162', []], ['A2125636847', []], ['A2619737239', []], ['A2105929338', ['I20581793']], ['A2122861846', []], ['A1984138050', []], ['A2120310985', []], ['A2555771270', []], ['A2143084601', []], ['A2166839008', []], ['A2129984997', []], ['A1930671346', []], ['A2251699322', []], ['A2763086413', []], ['A2190720510', []], ['A7782295', []], ['A2737505702', []], ['A2763529495', []], ['A1971062812', ['I47508984']]], 'cited_by_count': 3283, 'concepts': [['C2781121325', '0.9525954'], ['C2780586474', '0.90469015'], ['C2780221984', '0.8319968'], ['C511355011', '0.6644742'], ['C71924100', '0.6212367']], 'referenced_works': ['W1484747638', 'W1593178974', 'W1745396860', 'W1792110071', 'W1866842189', 'W1900919865', 'W1985668205', 'W1996661765', 'W2011137594', 'W2013556775', 'W2016650232', 'W2019898283', 'W2021053203', 'W2044379852', 'W2057792711', 'W2072398376', 'W2072725211', 'W2077803625', 'W2093918066', 'W2104129218', 'W2106023093', 'W2117021897', 'W2119287388', 'W2121477958', 'W2128452231', 'W2130189158', 'W2130894435', 'W2133204090', 'W2138085958', 'W2148924243', 'W2149284219', 'W2156317766', 'W2158569927', 'W2159765595', 'W2161643046', 'W2162002672', 'W2163598671', 'W2163710303', 'W2168540026', 'W2281608632', 'W2338167986', 'W2341244791', 'W2461148724', 'W2466444307', 'W2521702234', 'W2550720962', 'W2599745988', 'W2602619606', 'W2610451848', 'W2702919331', 'W4237186931'], 'abstract': "Summary  Background  Underweight, overweight, and obesity in childhood and adolescence are associated with adverse health consequences throughout the life-course. Our aim was to estimate worldwide trends in mean body-mass index (BMI) and a comprehensive set of BMI categories that cover underweight to obesity in children and adolescents, and to compare trends with those of adults.  Methods  We pooled 2416 population-based studies with measurements of height and weight on 128·9 million participants aged 5 years and older, including 31·5 million aged 5–19 years. We used a Bayesian hierarchical model to estimate trends from 1975 to 2016 in 200 countries for mean BMI and for prevalence of BMI in the following categories for children and adolescents aged 5–19 years: more than 2 SD below the median of the WHO growth reference for children and adolescents (referred to as moderate and severe underweight hereafter), 2 SD to more than 1 SD below the median (mild underweight), 1 SD below the median to 1 SD above the median (healthy weight), more than 1 SD to 2 SD above the median (overweight but not obese), and more than 2 SD above the median (obesity).  Findings  Regional change in age-standardised mean BMI in girls from 1975 to 2016 ranged from virtually no change (−0·01 kg/m 2  per decade; 95% credible interval −0·42 to 0·39, posterior probability [PP] of the observed decrease being a true decrease=0·5098) in eastern Europe to an increase of 1·00 kg/m 2  per decade (0·69–1·35, PP>0·9999) in central Latin America and an increase of 0·95 kg/m 2  per decade (0·64–1·25, PP>0·9999) in Polynesia and Micronesia. The range for boys was from a non-significant increase of 0·09 kg/m 2  per decade (−0·33 to 0·49, PP=0·6926) in eastern Europe to an increase of 0·77 kg/m 2  per decade (0·50–1·06, PP>0·9999) in Polynesia and Micronesia. Trends in mean BMI have recently flattened in northwestern Europe and the high-income English-speaking and Asia-Pacific regions for both sexes, southwestern Europe for boys, and central and Andean Latin America for girls. By contrast, the rise in BMI has accelerated in east and south Asia for both sexes, and southeast Asia for boys. Global age-standardised prevalence of obesity increased from 0·7% (0·4–1·2) in 1975 to 5·6% (4·8–6·5) in 2016 in girls, and from 0·9% (0·5–1·3) in 1975 to 7·8% (6·7–9·1) in 2016 in boys; the prevalence of moderate and severe underweight decreased from 9·2% (6·0–12·9) in 1975 to 8·4% (6·8–10·1) in 2016 in girls and from 14·8% (10·4–19·5) in 1975 to 12·4% (10·3–14·5) in 2016 in boys. Prevalence of moderate and severe underweight was highest in India, at 22·7% (16·7–29·6) among girls and 30·7% (23·5–38·0) among boys. Prevalence of obesity was more than 30% in girls in Nauru, the Cook Islands, and Palau; and boys in the Cook Islands, Nauru, Palau, Niue, and American Samoa in 2016. Prevalence of obesity was about 20% or more in several countries in Polynesia and Micronesia, the Middle East and north Africa, the Caribbean, and the USA. In 2016, 75 (44–117) million girls and 117 (70–178) million boys worldwide were moderately or severely underweight. In the same year, 50 (24–89) million girls and 74 (39–125) million boys worldwide were obese.  Interpretation  The rising trends in children's and adolescents' BMI have plateaued in many high-income countries, albeit at high levels, but have accelerated in parts of Asia, with trends no longer correlated with those of adults.  Funding  Wellcome Trust, AstraZeneca Young Health Programme.", 'counts_by_year': [[2022, 690], [2021, 886], [2020, 853], [2019, 566], [2018, 276], [2017, 7]]}, {'id': 'W2963748441', 'doi': 'https://doi.org/10.18653/v1/d16-1264', 'title': 'SQuAD: 100,000+ Questions for Machine Comprehension of Text', 'type': 'proceedings-article', 'publication_date': '2016-06-16', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2222836187', ['I97018004']], ['A2945991164', ['I97018004']], ['A2553425227', ['I97018004']], ['A2171686691', ['I97018004']]], 'cited_by_count': 3283, 'concepts': [['C41008148', '0.7822492'], ['C19768560', '0.7341411'], ['C12725497', '0.731514'], ['C511192102', '0.70146006'], ['C2778780117', '0.6542027']], 'referenced_works': ['W1525961042', 'W1544827683', 'W1632114991', 'W2028175314', 'W2033047024', 'W2057824915', 'W2104582871', 'W2105717194', 'W2108598243', 'W2115758952', 'W2121300346', 'W2125436846', 'W2167435923', 'W2171278097', 'W2197164549', 'W2240322187', 'W2250432970', 'W2250595585', 'W2251349042', 'W2251355301', 'W2251818205', 'W2252016937', 'W2361488420', 'W2407645726', 'W2962809918', 'W2963080779', 'W2964267515'], 'abstract': 'We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com', 'counts_by_year': [[2022, 266], [2021, 1087], [2020, 950], [2019, 640], [2018, 268], [2017, 68], [2016, 2]]}, {'id': 'W2927961455', 'doi': 'https://doi.org/10.1093/nar/gkz239', 'title': 'Interactive Tree Of Life (iTOL) v4: recent updates and new developments', 'type': 'journal-article', 'publication_date': '2019-07-02', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A46769561', ['I4210107206']], ['A2068107358', ['I1303691731']]], 'cited_by_count': 3243, 'concepts': [['C71901391', '0.84568'], ['C2776321320', '0.7751882'], ['C113174947', '0.64654064'], ['C41008148', '0.6356201'], ['C36464697', '0.5830471']], 'referenced_works': ['W2068187483', 'W2111647009', 'W2113708808', 'W2124967403', 'W2125230147', 'W2132247062', 'W2148698435', 'W2149057166', 'W2150297520', 'W2162371815', 'W2277953252', 'W2311203695', 'W2336330109', 'W2343887005', 'W2560181459', 'W2763775416'], 'abstract': 'Abstract The Interactive Tree Of Life (https://itol.embl.de) is an online tool for the display, manipulation and annotation of phylogenetic and other trees. It is freely available and open to everyone. The current version introduces four new dataset types, together with numerous new features. Annotation options have been expanded and new control options added for many display elements. An interactive spreadsheet-like editor has been implemented, providing dataset creation and editing directly in the web interface. Font support has been rewritten with full support for UTF-8 character encoding throughout the user interface. Google Web Fonts are now fully supported in the tree text labels. iTOL v4 is the first tool which supports direct visualization of Qiime 2 trees and associated annotations. The user account system has been streamlined and expanded with new navigation options, and currently handles &gt;700 000 trees from more than 40 000 individual users. Full batch access has been implemented allowing programmatic upload and export of trees and annotations.', 'counts_by_year': [[2022, 767], [2021, 1394], [2020, 922], [2019, 154], [2018, 1], [2017, 1]]}, {'id': 'W2470052926', 'doi': 'https://doi.org/10.1093/nar/gkw569', 'title': 'NCBI prokaryotic genome annotation pipeline', 'type': 'journal-article', 'publication_date': '2016-08-19', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2053101140', ['I4210109390']], ['A1964015497', ['I4210109390']], ['A269266212', ['I4210109390']], ['A1582603586', ['I4210109390']], ['A1993195152', ['I4210109390']], ['A2045354893', ['I4210109390']], ['A1972088427', ['I2802612298']], ['A1877008227', ['I4210109390']], ['A2289823242', ['I2802612298']], ['A2089396775', ['I4210109390']]], 'cited_by_count': 3242, 'concepts': [['C2776321320', '0.8733969'], ['C141231307', '0.71526223'], ['C86803240', '0.694213'], ['C89566754', '0.66039824'], ['C70721500', '0.63000715']], 'referenced_works': ['W1519815462', 'W1965144037', 'W1969455484', 'W1971403296', 'W2013447902', 'W2017608260', 'W2045745647', 'W2051838613', 'W2060573994', 'W2070517644', 'W2078969200', 'W2096093282', 'W2106090237', 'W2108230379', 'W2117486996', 'W2120172561', 'W2120799151', 'W2124351063', 'W2127391962', 'W2133253129', 'W2141152740', 'W2142428670', 'W2142678478', 'W2148245360', 'W2149429041', 'W2150550043', 'W2160053034', 'W2163241233', 'W2165770381', 'W2167922509', 'W2169929748', 'W2170382524', 'W2233192706', 'W2461608401', 'W2739999456', 'W4250052351'], 'abstract': "Recent technological advances have opened unprecedented opportunities for large-scale sequencing and analysis of populations of pathogenic species in disease outbreaks, as well as for large-scale diversity studies aimed at expanding our knowledge across the whole domain of prokaryotes. To meet the challenge of timely interpretation of structure, function and meaning of this vast genetic information, a comprehensive approach to automatic genome annotation is critically needed. In collaboration with Georgia Tech, NCBI has developed a new approach to genome annotation that combines alignment based methods with methods of predicting protein-coding and RNA genes and other functional elements directly from sequence. A new gene finding tool, GeneMarkS+, uses the combined evidence of protein and RNA placement by homology as an initial map of annotation to generate and modify ab initio gene predictions across the whole genome. Thus, the new NCBI's Prokaryotic Genome Annotation Pipeline (PGAP) relies more on sequence similarity when confident comparative data are available, while it relies more on statistical predictions in the absence of external evidence. The pipeline provides a framework for generation and analysis of annotation on the full breadth of prokaryotic taxonomy. For additional information on PGAP see https://www.ncbi.nlm.nih.gov/genome/annotation_prok/ and the NCBI Handbook, https://www.ncbi.nlm.nih.gov/books/NBK174280/.", 'counts_by_year': [[2022, 655], [2021, 824], [2020, 800], [2019, 530], [2018, 263], [2017, 155], [2016, 14], [2015, 1]]}, {'id': 'W2964309882', 'doi': 'https://doi.org/10.1007/978-3-030-01234-2_49', 'title': 'Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation', 'type': 'book-chapter', 'publication_date': '2018-09-08', 'host_venue': 'V106296714', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2127898042', ['I1291425158']], ['A2127971021', ['I1291425158']], ['A2466861874', ['I1291425158']], ['A1966707555', ['I1291425158']], ['A2107777167', ['I1291425158']]], 'cited_by_count': 3225, 'concepts': [['C41008148', '0.8015584'], ['C70710897', '0.6544782'], ['C118505674', '0.6297112'], ['C154945302', '0.5768146'], ['C31972630', '0.509225']], 'referenced_works': ['W1745334888', 'W1861492603', 'W1901129140', 'W1903029394', 'W1905829557', 'W1923115158', 'W1938976761', 'W1948751323', 'W1949049686', 'W1953465585', 'W1964772475', 'W2022508996', 'W2037227137', 'W2054279472', 'W2092985495', 'W2097117768', 'W2104978738', 'W2112796928', 'W2116877738', 'W2117539524', 'W2124592697', 'W2125215748', 'W2144794286', 'W2179352600', 'W2194775991', 'W2221898772', 'W2264432461', 'W2307770531', 'W2333621733', 'W2340897893', 'W2412782625', 'W2473131906', 'W2531409750', 'W2535516436', 'W2536208356', 'W2549139847', 'W2557406251', 'W2560023338', 'W2561196672', 'W2563705555', 'W2565639579', 'W2592939477', 'W2598666589', 'W2601564443', 'W2606492274', 'W2737258237', 'W2743554749', 'W2753588254', 'W2776638780', 'W2777737607', 'W2792926444', 'W2962843773', 'W2962891704', 'W2963073398', 'W2963108253', 'W2963125010', 'W2963563573', 'W2963674086', 'W2963815618', 'W2963881378', 'W2964081807', 'W4251033893'], 'abstract': 'Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89% and 82.1% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https://github.com/tensorflow/models/tree/master/research/deeplab.', 'counts_by_year': [[2022, 741], [2021, 1030], [2020, 817], [2019, 528], [2018, 93], [2017, 2]]}, {'id': 'W2173732482', 'doi': 'https://doi.org/10.1093/nar/gkv1189', 'title': 'Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation', 'type': 'journal-article', 'publication_date': '2016-01-04', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2395020792', ['I1299303238']], ['A2304578043', ['I1299303238']], ['A1984504140', ['I1299303238']], ['A1908388427', ['I1299303238']], ['A2172550985', ['I1299303238']], ['A2256538809', ['I1299303238']], ['A1975143245', ['I1299303238']], ['A1810852608', ['I1299303238']], ['A2409466817', ['I1299303238']], ['A46466112', ['I1299303238']], ['A2331516065', ['I1299303238']], ['A269266212', ['I1299303238']], ['A2132633766', ['I1299303238']], ['A1998794321', ['I1299303238']], ['A2325039351', ['I1299303238']], ['A1582603586', ['I1299303238']], ['A2329776818', ['I1299303238']], ['A2516661885', ['I1299303238']], ['A1963757467', ['I1299303238']], ['A2429722641', ['I1299303238']], ['A2162075402', ['I1299303238']], ['A2114725044', ['I1299303238']], ['A2033734346', ['I1299303238']], ['A2324900004', ['I1299303238']], ['A1966310512', ['I1299303238']], ['A2160817217', ['I1299303238']], ['A1975158788', ['I1299303238']], ['A3207295112', ['I1299303238']], ['A370127644', ['I1299303238']], ['A2328104537', ['I1299303238']], ['A2089600412', ['I1299303238']], ['A2253130503', ['I1299303238']], ['A2495227867', ['I1299303238']], ['A2297191951', ['I1299303238']], ['A1973227959', ['I1299303238']], ['A2315662538', ['I1299303238']], ['A1975186878', ['I1299303238']], ['A2949590097', ['I1299303238']], ['A2070305097', ['I1299303238']], ['A2335763193', ['I1299303238']], ['A2111154689', ['I1299303238']], ['A2307368259', ['I1299303238']], ['A1877866882', ['I1299303238']], ['A2307242731', ['I1299303238']], ['A2318554617', ['I1299303238']], ['A2054177139', ['I1299303238']], ['A2421473300', ['I1299303238']], ['A2227006312', ['I1299303238']], ['A2085829377', ['I1299303238']], ['A2307500684', ['I1299303238']], ['A2053101140', ['I1299303238']], ['A1964015497', ['I1299303238']], ['A2030396006', ['I1299303238']], ['A1877008227', ['I1299303238']]], 'cited_by_count': 3224, 'concepts': [['C151810110', '0.9976145'], ['C2776321320', '0.66340685'], ['C86803240', '0.5770488'], ['C141674004', '0.54576004'], ['C105176652', '0.48155382']], 'referenced_works': ['W1216137606', 'W1220385903', 'W1491910236', 'W1519962630', 'W1591180101', 'W1767470961', 'W1895291674', 'W1917495954', 'W1964480889', 'W1965100590', 'W1974312918', 'W1975182819', 'W1986836541', 'W1989943248', 'W1992155194', 'W1993679825', 'W1995839193', 'W2015232409', 'W2021135205', 'W2026345779', 'W2029854306', 'W2034076729', 'W2042459970', 'W2042811033', 'W2058615001', 'W2061803918', 'W2065405603', 'W2067508599', 'W2070634275', 'W2073569767', 'W2077327925', 'W2078210797', 'W2081651733', 'W2084160423', 'W2085377666', 'W2097233732', 'W2099477841', 'W2106090237', 'W2107903949', 'W2108230379', 'W2108982334', 'W2110375946', 'W2117936709', 'W2118834857', 'W2119339090', 'W2119837484', 'W2135993011', 'W2139658526', 'W2141327598', 'W2144268209', 'W2152375430', 'W2152860087', 'W2158242638', 'W2160345222', 'W2163038531', 'W2164598907', 'W2165770381', 'W2167406239', 'W2167898988', 'W2168909179', 'W2169767349', 'W2426512432', 'W2739999456', 'W2915667579', 'W4234094145'], 'abstract': 'The RefSeq project at the National Center for Biotechnology Information (NCBI) maintains and curates a publicly available database of annotated genomic, transcript, and protein sequence records (http://www.ncbi.nlm.nih.gov/refseq/). The RefSeq project leverages the data submitted to the International Nucleotide Sequence Database Collaboration (INSDC) against a combination of computation, manual curation, and collaboration to produce a standard set of stable, non-redundant reference sequences. The RefSeq project augments these reference sequences with current knowledge including publications, functional features and informative nomenclature. The database currently represents sequences from more than 55,000 organisms (>4800 viruses, >40,000 prokaryotes and >10,000 eukaryotes; RefSeq release 71), ranging from a single record to complete genomes. This paper summarizes the current status of the viral, prokaryotic, and eukaryotic branches of the RefSeq project, reports on improvements to data access and details efforts to further expand the taxonomic representation of the collection. We also highlight diverse functional curation initiatives that support multiple uses of RefSeq data including taxonomic validation, genome annotation, comparative genomics, and clinical testing. We summarize our approach to utilizing available RNA-Seq and other data types in our manual curation process for vertebrate, plant, and other species, and describe a new direction for prokaryotic genomes and protein name management.', 'counts_by_year': [[2022, 514], [2021, 779], [2020, 699], [2019, 532], [2018, 403], [2017, 221], [2016, 65], [2015, 2], [2014, 2], [2012, 1]]}, {'id': 'W2341539131', 'doi': 'https://doi.org/10.1093/nar/gkw257', 'title': 'deepTools2: a next generation web server for deep-sequencing data analysis', 'type': 'journal-article', 'publication_date': '2016-07-08', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2119774666', ['I4210164321']], ['A2149069042', ['I4210164321']], ['A2327525373', ['I161046081']], ['A2096411765', ['I4210164321']], ['A2581325839', ['I4210164321']], ['A2792898511', ['I4210164321']], ['A2317147626', ['I4210164321']], ['A2140016247', ['I205783295']], ['A1862394181', ['I4210164321']]], 'cited_by_count': 3215, 'concepts': [['C79581498', '0.6737051'], ['C36464697', '0.66404223'], ['C11392498', '0.58206284'], ['C177212765', '0.566587'], ['C41008148', '0.56634414']], 'referenced_works': ['W1518437340', 'W2014677321', 'W2040909468', 'W2064374160', 'W2076154138', 'W2093146349', 'W2097464861', 'W2108234281', 'W2124514674', 'W2135937351', 'W2146303939', 'W2150090219', 'W2154164376', 'W2158019017', 'W2158789637', 'W2163789882', 'W2176686025', 'W2259938310', 'W4233630467'], 'abstract': 'We present an update to our Galaxy-based web server for processing and visualizing deeply sequenced data. Its core tool set, deepTools, allows users to perform complete bioinformatic workflows ranging from quality controls and normalizations of aligned reads to integrative analyses, including clustering and visualization approaches. Since we first described our deepTools Galaxy server in 2014, we have implemented new solutions for many requests from the community and our users. Here, we introduce significant enhancements and new tools to further improve data visualization and interpretation. deepTools continue to be open to all users and freely available as a web service at deeptools.ie-freiburg.mpg.de The new deepTools2 suite can be easily deployed within any Galaxy framework via the toolshed repository, and we also provide source code for command line usage under Linux and Mac OS X. A public and documented API for access to deepTools functionality is also available.', 'counts_by_year': [[2022, 723], [2021, 960], [2020, 712], [2019, 435], [2018, 274], [2017, 106], [2016, 5]]}, {'id': 'W2554773181', 'doi': 'https://doi.org/10.1038/nrc.2016.108', 'title': 'Cancer nanomedicine: progress, challenges and opportunities', 'type': 'journal-article', 'publication_date': '2017-01-01', 'host_venue': 'V160464432', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2125415226', ['I1283280774']], ['A701384459', ['I1334819555']], ['A2804434788', ['I4210153832']], ['A237198337', ['I1283280774', 'I185163786']]], 'cited_by_count': 3207, 'concepts': [['C15083742', '0.9273405'], ['C2780625559', '0.66009045'], ['C121608353', '0.59825623'], ['C2983331546', '0.516308'], ['C171250308', '0.47409853']], 'referenced_works': ['W853016887', 'W947799404', 'W1008067630', 'W1170002600', 'W1497889772', 'W1512493964', 'W1514256570', 'W1781132531', 'W1802495158', 'W1871308790', 'W1874889006', 'W1902159756', 'W1903419358', 'W1921986114', 'W1937451961', 'W1945220000', 'W1964146045', 'W1965235573', 'W1965636780', 'W1966501740', 'W1966597723', 'W1967748130', 'W1968583232', 'W1968898129', 'W1970168078', 'W1970682511', 'W1972461780', 'W1972756117', 'W1975592104', 'W1977457835', 'W1978139448', 'W1978183755', 'W1978389424', 'W1980052502', 'W1980419364', 'W1982228392', 'W1982248450', 'W1982637919', 'W1983085370', 'W1985233863', 'W1987406340', 'W1989195046', 'W1989230986', 'W1989401206', 'W1990293640', 'W1990641613', 'W1990796256', 'W1995770815', 'W1996139192', 'W1996993122', 'W1997203755', 'W1998346656', 'W2000584529', 'W2000941201', 'W2001679453', 'W2002050544', 'W2003103971', 'W2003690177', 'W2005082473', 'W2005771030', 'W2006250811', 'W2007987850', 'W2008226030', 'W2010723976', 'W2012247886', 'W2012809310', 'W2012867417', 'W2012867962', 'W2014509855', 'W2014668585', 'W2015821654', 'W2018204959', 'W2020552396', 'W2021521632', 'W2023115975', 'W2023202045', 'W2023882012', 'W2024102779', 'W2024253505', 'W2025628768', 'W2026250154', 'W2028745823', 'W2029749273', 'W2030574023', 'W2032760352', 'W2033278321', 'W2034717709', 'W2038781914', 'W2039759758', 'W2042345402', 'W2043009832', 'W2044090247', 'W2044598213', 'W2046851174', 'W2047358259', 'W2047826409', 'W2048777642', 'W2050085273', 'W2050732886', 'W2050886209', 'W2051053191', 'W2051271385', 'W2052580681', 'W2052987547', 'W2053938092', 'W2054728198', 'W2054745605', 'W2054981131', 'W2055244270', 'W2055631278', 'W2055818374', 'W2056380758', 'W2057848113', 'W2057910069', 'W2064699522', 'W2064861225', 'W2066374585', 'W2066616821', 'W2066796083', 'W2069422976', 'W2069564782', 'W2069650869', 'W2071030419', 'W2071404720', 'W2074544404', 'W2076043169', 'W2076754526', 'W2076922024', 'W2078147660', 'W2078502914', 'W2079332722', 'W2082143317', 'W2082218425', 'W2083710155', 'W2084478449', 'W2087414776', 'W2087824631', 'W2087914076', 'W2088223764', 'W2088536552', 'W2088659772', 'W2089220360', 'W2091922990', 'W2092527323', 'W2092699829', 'W2094288422', 'W2096996795', 'W2097121410', 'W2097203662', 'W2097261404', 'W2101599998', 'W2101830709', 'W2102258001', 'W2102869104', 'W2103364597', 'W2104550697', 'W2106088947', 'W2108882170', 'W2109552892', 'W2110634891', 'W2111605241', 'W2112927879', 'W2113676592', 'W2117630026', 'W2118291545', 'W2119423559', 'W2121322725', 'W2122099856', 'W2125284193', 'W2125460615', 'W2126163864', 'W2128061275', 'W2129462207', 'W2129917557', 'W2130166533', 'W2131388477', 'W2131430454', 'W2132920287', 'W2133209277', 'W2135486162', 'W2136451551', 'W2136532800', 'W2136606745', 'W2138151577', 'W2141518860', 'W2144350652', 'W2145479354', 'W2145756312', 'W2146757262', 'W2149597677', 'W2149896139', 'W2149959721', 'W2150375936', 'W2150594642', 'W2151957316', 'W2153017356', 'W2153771870', 'W2155170820', 'W2155239981', 'W2155685502', 'W2157979200', 'W2158558808', 'W2158787062', 'W2159098445', 'W2160843021', 'W2161611906', 'W2161684993', 'W2161946327', 'W2163326536', 'W2164841185', 'W2164918134', 'W2165555463', 'W2166797582', 'W2166885734', 'W2167204367', 'W2169515956', 'W2170315635', 'W2171227711', 'W2171361466', 'W2173147935', 'W2173359203', 'W2176456200', 'W2176599934', 'W2190141802', 'W2235491852', 'W2245319613', 'W2252729201', 'W2279217276', 'W2280549490', 'W2284550715', 'W2285408387', 'W2297128620', 'W2298943387', 'W2306899626', 'W2312592011', 'W2321763390', 'W2322194440', 'W2323147899', 'W2335076883', 'W2340906166', 'W2346576309', 'W2403806215', 'W2504151231', 'W2565453440', 'W4244004143', 'W4252712324', 'W4252941190'], 'abstract': 'The intrinsic limits of conventional cancer therapies prompted the development and application of various nanotechnologies for more effective and safer cancer treatment, herein referred to as cancer nanomedicine. Considerable technological success has been achieved in this field, but the main obstacles to nanomedicine becoming a new paradigm in cancer therapy stem from the complexities and heterogeneity of tumour biology, an incomplete understanding of nano-bio interactions and the challenges regarding chemistry, manufacturing and controls required for clinical translation and commercialization. This Review highlights the progress, challenges and opportunities in cancer nanomedicine and discusses novel engineering approaches that capitalize on our growing understanding of tumour biology and nano-bio interactions to develop more effective nanotherapeutics for cancer patients.', 'counts_by_year': [[2022, 543], [2021, 722], [2020, 664], [2019, 611], [2018, 463], [2017, 197], [2016, 1]]}, {'id': 'W2611772571', 'doi': 'https://doi.org/10.1175/jcli-d-16-0758.1', 'title': 'The Modern-Era Retrospective Analysis for Research and Applications, Version 2 (MERRA-2)', 'type': 'journal-article', 'publication_date': '2017-06-20', 'host_venue': 'V80591372', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2557455857', ['I1306266525']], ['A2534117248', ['I1306266525']], ['A2106447018', ['I1306266525', 'I1329765538']], ['A2054113247', ['I1306266525']], ['A1994159590', ['I1306266525']], ['A2609352842', ['I1306266525']], ['A1970403743', ['I1306266525', 'I83909951']], ['A2045743203', ['I1306266525']], ['A1585321837', ['I1306266525']], ['A1994659243', ['I1306266525']], ['A1639412569', ['I1306266525']], ['A2137977172', ['I1306266525']], ['A1992278624', ['I4210144776', 'I1306266525']], ['A2128236943', ['I1306266525', 'I1329765538']], ['A2318779832', ['I1306266525']], ['A2427582702', ['I1306266525', 'I1329765538']], ['A1988887419', ['I1306266525']], ['A2189758836', ['I1306266525']], ['A2497122638', ['I1306266525']], ['A2152461521', ['I1306266525']], ['A2163856772', ['I1306266525']], ['A2055707233', ['I1306266525']], ['A1017004719', ['I1306266525']], ['A2474052015', ['I1306266525']], ['A2086134102', ['I1306266525']], ['A1741173208', ['I1306266525']], ['A2772962851', ['I1306266525']], ['A2230903946', ['I1306266525']], ['A2099291881', ['I1306266525']], ['A2057278800', ['I1306266525']], ['A2602474071', ['I1306266525', 'I880747138']]], 'cited_by_count': 3192, 'concepts': [['C39432304', '0.61703885'], ['C24552861', '0.57956284'], ['C153294291', '0.5784391'], ['C2779067591', '0.55448127'], ['C49204034', '0.5247022']], 'referenced_works': ['W1544454855', 'W1616087035', 'W1638421591', 'W1907125670', 'W1973926943', 'W1974038018', 'W1978648105', 'W1982548951', 'W1984613928', 'W1986637952', 'W1987640666', 'W1989120577', 'W1993065427', 'W1997826306', 'W1998040038', 'W2000809920', 'W2001981020', 'W2002348323', 'W2011181634', 'W2015175770', 'W2019066705', 'W2019234852', 'W2022548072', 'W2027986230', 'W2036807018', 'W2040778825', 'W2047852430', 'W2051416171', 'W2051607933', 'W2055698954', 'W2056115835', 'W2056444889', 'W2058179313', 'W2061685147', 'W2061889742', 'W2066992347', 'W2068002167', 'W2069705567', 'W2071825174', 'W2072019789', 'W2074355870', 'W2077868956', 'W2079002606', 'W2080475879', 'W2082854428', 'W2083284462', 'W2084564677', 'W2084571691', 'W2085672189', 'W2085742763', 'W2089433206', 'W2099255394', 'W2100671367', 'W2105103805', 'W2106396115', 'W2107057810', 'W2108896901', 'W2112335693', 'W2115577276', 'W2121745948', 'W2125855972', 'W2131070146', 'W2131901831', 'W2136408596', 'W2139937607', 'W2143357393', 'W2145265567', 'W2145910073', 'W2152831301', 'W2153438006', 'W2159502524', 'W2162953324', 'W2165242200', 'W2165889513', 'W2169825433', 'W2172819965', 'W2173128646', 'W2175066699', 'W2175400375', 'W2176202464', 'W2178389963', 'W2260089446', 'W2273717001', 'W2339405093', 'W2339767585', 'W2494806942', 'W2511997790', 'W2519418212', 'W2533922453', 'W2543820470', 'W2556661975', 'W2566089477', 'W2566381813', 'W2588787202', 'W2605203627', 'W2607030186'], 'abstract': 'The Modern-Era Retrospective Analysis for Research and Applications, version 2 (MERRA-2), is the latest atmospheric reanalysis of the modern satellite era produced by NASA’s Global Modeling and Assimilation Office (GMAO). MERRA-2 assimilates observation types not available to its predecessor, MERRA, and includes updates to the Goddard Earth Observing System (GEOS) model and analysis scheme so as to provide a viable ongoing climate analysis beyond MERRA’s terminus. While addressing known limitations of MERRA, MERRA-2 is also intended to be a development milestone for a future integrated Earth system analysis (IESA) currently under development at GMAO. This paper provides an overview of the MERRA-2 system and various performance metrics. Among the advances in MERRA-2 relevant to IESA are the assimilation of aerosol observations, several improvements to the representation of the stratosphere including ozone, and improved representations of cryospheric processes. Other improvements in the quality of MERRA-2 compared with MERRA include the reduction of some spurious trends and jumps related to changes in the observing system and reduced biases and imbalances in aspects of the water cycle. Remaining deficiencies are also identified. Production of MERRA-2 began in June 2014 in four processing streams and converged to a single near-real-time stream in mid-2015. MERRA-2 products are accessible online through the NASA Goddard Earth Sciences Data Information Services Center (GES DISC).', 'counts_by_year': [[2022, 725], [2021, 918], [2020, 765], [2019, 466], [2018, 268], [2017, 45], [2016, 1]]}, {'id': 'W2242218935', 'doi': 'https://doi.org/10.1109/cvpr.2016.182', 'title': 'Accurate Image Super-Resolution Using Very Deep Convolutional Networks', 'type': 'proceedings-article', 'publication_date': '2016-06-27', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2293552381', ['I139264467']], ['A2598630691', ['I139264467']], ['A2163009075', ['I139264467']]], 'cited_by_count': 3166, 'concepts': [['C41008148', '0.7922158'], ['C108583219', '0.7020615'], ['C2776848632', '0.7015749'], ['C154945302', '0.67236763'], ['C2777303404', '0.57135814']], 'referenced_works': ['W1677182931', 'W1791560514', 'W1950594372', 'W2035677848', 'W2087380704', 'W2098506229', 'W2107878631', 'W2112796928', 'W2118963448', 'W2121927366', 'W2137577104', 'W2149760002', 'W2150081556', 'W2534320940'], 'abstract': 'We present a highly accurate single-image superresolution (SR) method. Our method uses a very deep convolutional network inspired by VGG-net used for ImageNet classification [19]. We find increasing our network depth shows a significant improvement in accuracy. Our final model uses 20 weight layers. By cascading small filters many times in a deep network structure, contextual information over large image regions is exploited in an efficient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates (104 times higher than SRCNN [6]) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable.', 'counts_by_year': [[2022, 363], [2021, 734], [2020, 805], [2019, 688], [2018, 380], [2017, 173], [2016, 18]]}, {'id': 'W3008696669', 'doi': 'https://doi.org/10.1001/jama.2020.2565', 'title': 'Presumed Asymptomatic Carrier Transmission of COVID-19', 'type': 'journal-article', 'publication_date': '2020-04-14', 'host_venue': 'V172573765', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2592937736', ['I4210133534']], ['A3008568565', ['I4210166889']], ['A3006796015', ['I4210096186']], ['A3006844970', ['I2802573037']], ['A3006718468', ['I1322227051']], ['A2951120447', ['I4210133534']], ['A2695156826', ['I4210133534']]], 'cited_by_count': 3145, 'concepts': [['C71924100', '0.91553736'], ['C2777910003', '0.8685075'], ['C3008058167', '0.86305606'], ['C3007834351', '0.7436695'], ['C137916694', '0.7324754']], 'referenced_works': ['W3002539152'], 'abstract': 'This study describes possible transmission of novel coronavirus disease 2019 (COVID-19) from an asymptomatic Wuhan resident to 5 family members in Anyang, a Chinese city in the neighboring province of Hubei.', 'counts_by_year': [[2022, 362], [2021, 979], [2020, 1798], [2019, 3]]}, {'id': 'W3101998545', 'doi': 'https://doi.org/10.1109/lsp.2016.2603342', 'title': 'Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks', 'type': 'journal-article', 'publication_date': '2016-08-26', 'host_venue': 'V120629676', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2481619292', ['I4210145761']], ['A2721069345', ['I177725633']], ['A2107648850', ['I4210145761']], ['A2411627636', ['I4210145761']]], 'cited_by_count': 3098, 'concepts': [['C41008148', '0.8724848'], ['C185798385', '0.8689989'], ['C154945302', '0.7493408'], ['C2779304628', '0.6555434'], ['C165696696', '0.6018141']], 'referenced_works': ['W204612701', 'W345900524', 'W1677182931', 'W1834627138', 'W1849007038', 'W1934410531', 'W1970456555', 'W1990937109', 'W2012885984', 'W2056025798', 'W2111372597', 'W2113120414', 'W2124750300', 'W2152826865', 'W2153461700', 'W2157285372', 'W2166694921', 'W2209882149', 'W2963721882', 'W3097096317'], 'abstract': 'Face detection and alignment in unconstrained environment are challenging due to various poses, illuminations and occlusions. Recent studies show that deep learning approaches can achieve impressive performance on these two tasks. In this paper, we propose a deep cascaded multi-task framework which exploits the inherent correlation between them to boost up their performance. In particular, our framework adopts a cascaded structure with three stages of carefully designed deep convolutional networks that predict face and landmark location in a coarse-to-fine manner. In addition, in the learning process, we propose a new online hard sample mining strategy that can improve the performance automatically without manual sample selection. Our method achieves superior accuracy over the state-of-the-art techniques on the challenging FDDB and WIDER FACE benchmark for face detection, and AFLW benchmark for face alignment, while keeps real time performance.', 'counts_by_year': [[2022, 462], [2021, 835], [2020, 727], [2019, 609], [2018, 350], [2017, 110], [2016, 3]]}, {'id': 'W2559085405', 'doi': 'https://doi.org/10.1109/cvpr.2017.143', 'title': 'Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2488671715', ['I74973139']], ['A2132231410', ['I74973139']], ['A2157229433', ['I74973139']], ['A2146369081', ['I74973139']]], 'cited_by_count': 3087, 'concepts': [['C185798385', '0.73933935'], ['C41008148', '0.7148664'], ['C2779343474', '0.68249315'], ['C154945302', '0.6345791'], ['C52102323', '0.6287592']], 'referenced_works': ['W602397586', 'W1936750108', 'W1993149133', 'W1996478295', 'W1997500560', 'W2013640163', 'W2030536784', 'W2031004336', 'W2073246097', 'W2080873731', 'W2097151019', 'W2113325037', 'W2131263044', 'W2135533529', 'W2157939923', 'W2175012183', 'W2194775991', 'W2222512263', 'W2509865052', 'W2578797046', 'W2963448913', 'W2964304707'], 'abstract': 'We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency.', 'counts_by_year': [[2022, 322], [2021, 709], [2020, 873], [2019, 816], [2018, 334], [2017, 31]]}, {'id': 'W2475287302', 'doi': 'https://doi.org/10.1109/cvpr.2016.265', 'title': 'Image Style Transfer Using Convolutional Neural Networks', 'type': 'proceedings-article', 'publication_date': '2016-06-27', 'host_venue': 'V4306417987', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A1990517559', ['I8087733']], ['A2126410521', ['I4210112925']], ['A1898836778', ['I4210112925']]], 'cited_by_count': 3046, 'concepts': [['C41008148', '0.8039952'], ['C81363708', '0.77846265'], ['C154945302', '0.7359377'], ['C205711294', '0.6412637'], ['C115961682', '0.5510526']], 'referenced_works': ['W1715013381', 'W1756731880', 'W1909952827', 'W1973399149', 'W1999360130', 'W2005126631', 'W2040036684', 'W2058616551', 'W2078790577', 'W2109253138', 'W2116013899', 'W2119798818', 'W2127006916', 'W2155893237', 'W2166206801', 'W2170653751', 'W2232702494', 'W2292976057', 'W4236692733'], 'abstract': 'Rendering the semantic content of an image in different styles is a difficult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic information and, thus, allow to separate image content from style. Here we use image representations derived from Convolutional Neural Networks optimised for object recognition, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can separate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an arbitrary photograph with the appearance of numerous wellknown artworks. Our results provide new insights into the deep image representations learned by Convolutional Neural Networks and demonstrate their potential for high level image synthesis and manipulation.', 'counts_by_year': [[2022, 278], [2021, 738], [2020, 716], [2019, 668], [2018, 412], [2017, 194], [2016, 35], [2015, 2], [2012, 1]]}, {'id': 'W1689711448', 'doi': 'https://doi.org/10.1109/tnnls.2016.2582924', 'title': 'LSTM: A Search Space Odyssey', 'type': 'journal-article', 'publication_date': '2017-10-01', 'host_venue': 'V4210175523', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A1700852547', ['I2614128279']], ['A2104638231', ['I2614128279']], ['A2246005422', ['I2614128279']], ['A2557237635', ['I2614128279']], ['A2116333191', ['I2614128279']]], 'cited_by_count': 3035, 'concepts': [['C8642999', '0.79605985'], ['C41008148', '0.78168255'], ['C2780451532', '0.59910977'], ['C154945302', '0.5566832'], ['C147168706', '0.5541286']], 'referenced_works': ['W1971129545', 'W2018970719', 'W2040940879', 'W2064675550', 'W2079735306', 'W2088286197', 'W2100649405', 'W2116261113', 'W2122585011', 'W2123859926', 'W2127141656', 'W2147107577', 'W2152550252', 'W2157331557', 'W2951183276', 'W4232288880'], 'abstract': 'Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs ($\\approx 15$ years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.', 'counts_by_year': [[2022, 523], [2021, 716], [2020, 688], [2019, 576], [2018, 301], [2017, 111], [2016, 82], [2015, 30]]}, {'id': 'W3017185871', 'doi': 'https://doi.org/10.1016/j.ijsu.2020.04.018', 'title': 'The socio-economic implications of the coronavirus pandemic (COVID-19): A review', 'type': 'journal-article', 'publication_date': '2020-06-01', 'host_venue': 'V67965910', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A3015880366', ['I153355300']], ['A2301139071', ['I45129253']], ['A2963645491', ['I166337079']], ['A2927621227', ['I4210119896']], ['A2501138612', ['I183935753']], ['A2983744233', ['I166337079']], ['A2125609616', ['I71178462']], ['A1940434747', ['I225661044']]], 'cited_by_count': 3034, 'concepts': [['C89623803', '0.78340423'], ['C195742910', '0.6929083'], ['C3008058167', '0.64593583'], ['C172656115', '0.64339405'], ['C2775941552', '0.53250086']], 'referenced_works': ['W1982797147', 'W3010223921', 'W3015323620', 'W3016048745'], 'abstract': 'The COVID-19 pandemic has resulted in over 4.3 million confirmed cases and over 290,000 deaths globally. It has also sparked fears of an impending economic crisis and recession. Social distancing, self-isolation and travel restrictions have lead to a reduced workforce across all economic sectors and caused many jobs to be lost. Schools have closed down, and the need for commodities and manufactured products has decreased. In contrast, the need for medical supplies has significantly increased. The food sector is also facing increased demand due to panic-buying and stockpiling of food products. In response to this global outbreak, we summarise the socio-economic effects of COVID-19 on individual aspects of the world economy. • Central banks globally commit to a ‘Whatever it takes’ approach in an attempt to save the global economy. • Europe pledges a €1.7tn rescue package. • The road to economic recovery is predicted to be a long one, with a period of economic inactivity for years to come.', 'counts_by_year': [[2022, 876], [2021, 1533], [2020, 619], [2019, 1]]}, {'id': 'W2962770929', 'doi': 'https://doi.org/10.1109/cvpr.2019.00453', 'title': 'A Style-Based Generator Architecture for Generative Adversarial Networks', 'type': 'proceedings-article', 'publication_date': '2019-06-15', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2153729192', ['I1304085615']], ['A2158163072', ['I1304085615']], ['A1975563680', ['I1304085615']]], 'cited_by_count': 3027, 'concepts': [['C2780992000', '0.76224923'], ['C41008148', '0.7552583'], ['C137800194', '0.6821568'], ['C123657996', '0.6795011'], ['C39890363', '0.5824238']], 'referenced_works': ['W1994618660', 'W2087681821', 'W2112594540', 'W2475287302', 'W2583638424', 'W2603777577', 'W2879390606', 'W2962785568', 'W2962873296', 'W2963525668', 'W4213187341'], 'abstract': 'We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.', 'counts_by_year': [[2022, 512], [2021, 1342], [2020, 936], [2019, 216], [2018, 13], [2012, 1]]}, {'id': 'W2587625522', 'doi': 'https://doi.org/10.1038/nmeth.4169', 'title': 'cryoSPARC: algorithms for rapid unsupervised cryo-EM structure determination', 'type': 'journal-article', 'publication_date': '2017-03-01', 'host_venue': 'V127827428', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A761067650', ['I185261750']], ['A2110301261', ['I2801317318']], ['A2157904231', ['I185261750']], ['A1994464961', ['I192455969']]], 'cited_by_count': 3025, 'concepts': [['C20702342', '0.86747026'], ['C41008148', '0.6737927'], ['C2780513914', '0.6355362'], ['C11413529', '0.5322995'], ['C65293305', '0.5185112']], 'referenced_works': ['W114517082', 'W1508637318', 'W1964654828', 'W1976664710', 'W1983751391', 'W2005240547', 'W2007351279', 'W2010193340', 'W2011665065', 'W2027986941', 'W2048826637', 'W2051925784', 'W2053681566', 'W2054770020', 'W2056101326', 'W2075824676', 'W2077874772', 'W2084306157', 'W2097832352', 'W2115101918', 'W2120396539', 'W2123241324', 'W2123738125', 'W2129908184', 'W2132038446', 'W2133442602', 'W2138390185', 'W2143205611', 'W2148941629', 'W2152562710', 'W2160559764', 'W2164659462', 'W2169160979', 'W2259068415', 'W2294124032', 'W2299689158', 'W2346206575', 'W2432427150', 'W2587280520', 'W2786313719', 'W2950967305', 'W2963188324', 'W3098623566', 'W4252814261', 'W4293775970'], 'abstract': 'Single-particle electron cryomicroscopy (cryo-EM) is a powerful method for determining the structures of biological macromolecules. With automated microscopes, cryo-EM data can often be obtained in a few days. However, processing cryo-EM image data to reveal heterogeneity in the protein structure and to refine 3D maps to high resolution frequently becomes a severe bottleneck, requiring expert intervention, prior structural knowledge, and weeks of calculations on expensive computer clusters. Here we show that stochastic gradient descent (SGD) and branch-and-bound maximum likelihood optimization algorithms permit the major steps in cryo-EM structure determination to be performed in hours or minutes on an inexpensive desktop computer. Furthermore, SGD with Bayesian marginalization allows ab initio 3D classification, enabling automated analysis and discovery of unexpected structures without bias from a reference map. These algorithms are combined in a user-friendly computer program named cryoSPARC (http://www.cryosparc.com).', 'counts_by_year': [[2022, 893], [2021, 929], [2020, 662], [2019, 304], [2018, 191], [2017, 43], [2016, 1]]}, {'id': 'W2793435880', 'doi': 'https://doi.org/10.1016/j.compositesb.2018.02.012', 'title': 'Additive manufacturing (3D printing): A review of materials, methods, applications and challenges', 'type': 'journal-article', 'publication_date': '2018-06-15', 'host_venue': 'V112674977', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2005349518', ['I165779595']], ['A3091366950', ['I165779595']], ['A2339819791', ['I165779595']], ['A2889714145', ['I165779595']], ['A2145865238', ['I192396691']]], 'cited_by_count': 3012, 'concepts': [['C524769229', '0.79644275'], ['C192562407', '0.4235507'], ['C21880701', '0.4182996'], ['C117671659', '0.40228364'], ['C171250308', '0.38715664']], 'referenced_works': ['W566602208', 'W815080081', 'W820971853', 'W844871572', 'W899193282', 'W1254488977', 'W1495687927', 'W1541360842', 'W1601804848', 'W1605669984', 'W1642712619', 'W1680054435', 'W1711375714', 'W1782215331', 'W1833975585', 'W1861395413', 'W1920395887', 'W1950162932', 'W1955801659', 'W1964715689', 'W1966049388', 'W1966624513', 'W1967222161', 'W1968712604', 'W1970065157', 'W1971260403', 'W1972102832', 'W1977598475', 'W1980005235', 'W1981587183', 'W1982113688', 'W1982397697', 'W1983413949', 'W1984025219', 'W1984299494', 'W1984670836', 'W1987081318', 'W1991611811', 'W1995001112', 'W1995203458', 'W1995781421', 'W1998876948', 'W2000207204', 'W2000363645', 'W2002062356', 'W2010223088', 'W2011517547', 'W2011593317', 'W2012202321', 'W2018566369', 'W2019879210', 'W2022501408', 'W2023758491', 'W2029291714', 'W2030362608', 'W2030443718', 'W2030488271', 'W2033284075', 'W2035083376', 'W2037117868', 'W2040984241', 'W2041831698', 'W2042890720', 'W2045172928', 'W2045279221', 'W2047273065', 'W2048644480', 'W2049993169', 'W2050723568', 'W2051582134', 'W2052322051', 'W2057376613', 'W2057520596', 'W2060092079', 'W2060361770', 'W2060742044', 'W2064299621', 'W2065239150', 'W2065613288', 'W2065844243', 'W2066737743', 'W2068131056', 'W2069460965', 'W2074324068', 'W2083804071', 'W2085667630', 'W2088660377', 'W2088692394', 'W2093311332', 'W2095105964', 'W2095576930', 'W2099006292', 'W2100664667', 'W2108633327', 'W2110695739', 'W2113042424', 'W2113539547', 'W2122701995', 'W2130273506', 'W2134775670', 'W2138664833', 'W2142721422', 'W2142777919', 'W2143021786', 'W2145738152', 'W2146395928', 'W2146452742', 'W2152602280', 'W2159383593', 'W2163566443', 'W2168690273', 'W2176669455', 'W2187725579', 'W2191438511', 'W2192772080', 'W2200270215', 'W2218105324', 'W2218190479', 'W2219253321', 'W2220612361', 'W2230487954', 'W2255904181', 'W2286363733', 'W2286771936', 'W2290704272', 'W2290852131', 'W2296075010', 'W2300453277', 'W2302965792', 'W2308316586', 'W2318683224', 'W2323685424', 'W2337961396', 'W2342362794', 'W2344513838', 'W2346498379', 'W2350168938', 'W2362504809', 'W2368840718', 'W2401932504', 'W2404768192', 'W2406557342', 'W2410677060', 'W2411353178', 'W2415066110', 'W2418318792', 'W2440873476', 'W2463296366', 'W2463829795', 'W2485826029', 'W2492188007', 'W2502401296', 'W2503161491', 'W2508366439', 'W2512650113', 'W2513564242', 'W2520364314', 'W2522881937', 'W2523921343', 'W2524341787', 'W2525027146', 'W2532024257', 'W2533214642', 'W2533394841', 'W2548523010', 'W2549754722', 'W2553221934', 'W2553474548', 'W2557885270', 'W2560213782', 'W2560850791', 'W2563060352', 'W2565129197', 'W2571382270', 'W2572622980', 'W2574444007', 'W2577332743', 'W2580111650', 'W2581140098', 'W2583537599', 'W2584346011', 'W2587206813', 'W2589206433', 'W2589770449', 'W2590431556', 'W2590564174', 'W2592265313', 'W2592516380', 'W2594051872', 'W2594144057', 'W2594211122', 'W2594676313', 'W2595379635', 'W2597065619', 'W2599000378', 'W2599236166', 'W2602959962', 'W2604130305', 'W2606387782', 'W2606732416', 'W2610613156', 'W2612325350', 'W2612672723', 'W2613577497', 'W2615049565', 'W2615275341', 'W2618122237', 'W2618596241', 'W2623782272', 'W2624701134', 'W2639701622', 'W2645080030', 'W2707966572', 'W2718420221', 'W2735545461', 'W2737820621', 'W2738392348', 'W2741181432', 'W2741250353', 'W2744056527', 'W2747578198', 'W2750470740', 'W2751373768', 'W2751695816', 'W2753445875', 'W2753713612', 'W2753859052', 'W2756307179', 'W2756776264', 'W2757829412', 'W2758567842', 'W2761021450', 'W2763894472', 'W2765449187', 'W2766167362', 'W2766927515', 'W2767242325', 'W2767563999', 'W2768673761', 'W2770941516', 'W2773400987', 'W2773756286', 'W2773901049', 'W2779072772', 'W2780260941', 'W2788067549', 'W2792180005', 'W4243517792', 'W4249375129'], 'abstract': 'Abstract   Freedom of design, mass customisation, waste minimisation and the ability to manufacture complex structures, as well as fast prototyping, are the main benefits of additive manufacturing (AM) or 3D printing. A comprehensive review of the main 3D printing methods, materials and their development in trending applications was carried out. In particular, the revolutionary applications of AM in biomedical, aerospace, buildings and protective structures were discussed. The current state of materials development, including metal alloys, polymer composites, ceramics and concrete, was presented. In addition, this paper discussed the main processing challenges with void formation, anisotropic behaviour, the limitation of computer design and layer-by-layer appearance. Overall, this paper gives an overview of 3D printing, including a survey on its benefits and drawbacks as a benchmark for future research and development.', 'counts_by_year': [[2022, 853], [2021, 1029], [2020, 737], [2019, 330], [2018, 58]]}, {'id': 'W3103171760', 'doi': 'https://doi.org/10.1016/j.ijsu.2020.10.034', 'title': 'The SCARE 2020 Guideline: Updating Consensus Surgical CAse REport (SCARE) Guidelines', 'type': 'journal-article', 'publication_date': '2020-11-09', 'host_venue': 'V67965910', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A1940434747', ['I225661044']], ['A3010309524', ['I91136226']], ['A2963645491', ['I166337079']], ['A2597623965', ['I4210113900']], ['A2927621227', ['I4210122718']]], 'cited_by_count': 3008, 'concepts': [['C2779356329', '0.78377086'], ['C71924100', '0.73594326'], ['C2779495148', '0.6598473'], ['C2780182762', '0.6554523'], ['C60641444', '0.64666593']], 'referenced_works': ['W1968468300', 'W2252491773', 'W2513329101', 'W2741989018', 'W2897748473'], 'abstract': 'The SCARE Guidelines were first published in 2016 and were last updated in 2018. They provide a structure for reporting surgical case reports and are used and endorsed by authors, journal editors and reviewers, in order to increase robustness and transparency in reporting surgical cases. They must be kept up to date in order to drive forwards reporting quality. As such, we have updated these guidelines via a DELPHI consensus exercise. The updated guidelines were produced via a DELPHI consensus exercise. Members were invited from the previous DELPHI group, as well as editorial board members and peer reviewers of the International Journal of Surgery Case Reports. The expert group completed an online survey to indicate their agreement with proposed changes to the checklist items. A total of 54 surgical experts agreed to participate and 53 (98%) completed the survey. The responses and suggested modifications were incorporated into the new 2020 guideline. There was a high degree of agreement amongst the SCARE Group, with all modified SCARE items receiving over 70% scores 7–9. A DELPHI consensus exercise was completed and an updated and improved SCARE Checklist is now presented. • This was a DELPHI consensus exercise to update the SCARE guidelines. • Of the invited surgical experts, 53 (98%) completed the survey. There was a high level of agreement within the SCARE Group. • The survey responses were incorporated as modifications, and an improved SCARE Checklist is now presented for use.', 'counts_by_year': [[2022, 1630], [2021, 1375]]}, {'id': 'W2883251903', 'doi': 'https://doi.org/10.1093/bioinformatics/bty633', 'title': 'ape 5.0: an environment for modern phylogenetics and evolutionary analyses in R', 'type': 'journal-article', 'publication_date': '2019-02-01', 'host_venue': 'V52395412', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2127630164', ['I1280045555']], ['A2252430138', ['I33434090']]], 'cited_by_count': 3000, 'concepts': [['C2984074130', '0.70767146'], ['C2780598303', '0.7032783'], ['C2779343474', '0.6935228'], ['C3020440742', '0.61727405'], ['C41008148', '0.53521943']], 'referenced_works': ['W585085135', 'W1973094248', 'W2097734528', 'W2151409320'], 'abstract': "After more than fifteen years of existence, the R package ape has continuously grown its contents, and has been used by a growing community of users. The release of version 5.0 has marked a leap towards a modern software for evolutionary analyses. Efforts have been put to improve efficiency, flexibility, support for 'big data' (R's long vectors), ease of use and quality check before a new release. These changes will hopefully make ape a useful software for the study of biodiversity and evolution in a context of increasing data quantity.ape is distributed through the Comprehensive R Archive Network: http://cran.r-project.org/package=ape. Further information may be found at http://ape-package.ird.fr/.", 'counts_by_year': [[2022, 923], [2021, 1144], [2020, 717], [2019, 202], [2018, 6]]}, {'id': 'W2964849163', 'doi': 'https://doi.org/10.1038/s41587-019-0201-4', 'title': 'Graph-based genome alignment and genotyping with HISAT2 and HISAT-genotype', 'type': 'journal-article', 'publication_date': '2019-08-01', 'host_venue': 'V106963461', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2340097888', ['I867280407']], ['A2753216895', ['I97018004']], ['A2966181565', ['I867280407']], ['A2966833032', ['I867280407']], ['A1904680277', ['I145311948']]], 'cited_by_count': 2972, 'concepts': [['C31467283', '0.6663859'], ['C141231307', '0.6269217'], ['C70721500', '0.59493536'], ['C192953774', '0.53055006'], ['C197754878', '0.5305063']], 'referenced_works': ['W1546528060', 'W1964305251', 'W1971924684', 'W2011210839', 'W2018838463', 'W2025943989', 'W2041391522', 'W2062872865', 'W2096791516', 'W2097341408', 'W2102278945', 'W2102853758', 'W2103441770', 'W2112113834', 'W2112752664', 'W2113312463', 'W2116041602', 'W2122732537', 'W2124362779', 'W2124985265', 'W2129933858', 'W2131104106', 'W2133212095', 'W2136145671', 'W2139760555', 'W2149992227', 'W2160344363', 'W2168909179', 'W2170551349', 'W2171777347', 'W2194172909', 'W2529924676', 'W2796069077', 'W2888300707', 'W2950122868', 'W2951160681', 'W2951529834'], 'abstract': 'The human reference genome represents only a small number of individuals, which limits its usefulness for genotyping. We present a method named HISAT2 (hierarchical indexing for spliced alignment of transcripts 2) that can align both DNA and RNA sequences using a graph Ferragina Manzini index. We use HISAT2 to represent and search an expanded model of the human reference genome in which over 14.5 million genomic variants in combination with haplotypes are incorporated into the data structure used for searching and alignment. We benchmark HISAT2 using simulated and real datasets to demonstrate that our strategy of representing a population of genomes, together with a fast, memory-efficient search algorithm, provides more detailed and accurate variant analyses than other methods. We apply HISAT2 for HLA typing and DNA fingerprinting; both applications form part of the HISAT-genotype software that enables analysis of haplotype-resolved genes or genomic regions. HISAT-genotype outperforms other computational methods and matches or exceeds the performance of laboratory-based assays.', 'counts_by_year': [[2022, 1304], [2021, 1145], [2020, 482], [2019, 35], [2018, 1], [2012, 1]]}, {'id': 'W3036319615', 'doi': 'https://doi.org/10.1016/j.molp.2020.06.009', 'title': 'TBtools: An Integrative Toolkit Developed for Interactive Analyses of Big Biological Data', 'type': 'journal-article', 'publication_date': '2020-08-03', 'host_venue': 'V150334665', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2890996152', ['I101479585']], ['A3036667674', ['I52180223']], ['A3035981606', ['I101479585']], ['A2919663646', ['I205783295']], ['A2160950239', ['I205783295']], ['A2725520083', ['I101479585']], ['A2104843590', ['I101479585']]], 'cited_by_count': 2969, 'concepts': [['C86803240', '0.8166524'], ['C2522767166', '0.5432466'], ['C75684735', '0.44331986'], ['C70721500', '0.38626617'], ['C41008148', '0.23512673']], 'referenced_works': ['W2002900187', 'W2009271040', 'W2037993016', 'W2086561953', 'W2102652793', 'W2103017472', 'W2111647009', 'W2115888213', 'W2119205648', 'W2132926880', 'W2134196182', 'W2142678478', 'W2157009395', 'W2158804744', 'W2406250479', 'W2408483783', 'W2529838505', 'W2792475317', 'W4294216483'], 'abstract': 'Abstract   The rapid development of high-throughput sequencing\xa0techniques has led biology into the big-data era. Data analyses using various bioinformatics tools rely on programming and command-line environments, which are challenging and time-consuming for most wet-lab biologists. Here, we present TBtools (a  T oolkit for  B iologists integrating various biological data-handling  tools ), a stand-alone software with a user-friendly interface. The toolkit incorporates over 130 functions, which are designed to meet the increasing demand for big-data analyses, ranging from bulk sequence processing to interactive data visualization. A wide variety of graphs can be prepared in TBtools\xa0using a new plotting engine (“JIGplot”) developed to maximize their interactive ability; this engine allows quick point-and-click modification of almost every graphic feature. TBtools is platform-independent software that can be run under all operating systems with Java Runtime Environment 1.6 or newer. It is freely available to non-commercial users at  https://github.com/CJ-Chen/TBtools/releases .', 'counts_by_year': [[2022, 1743], [2021, 1053], [2020, 156]]}, {'id': 'W1971788485', 'doi': 'https://doi.org/10.4135/9781483381411.n575', 'title': 'Social Network Analysis', 'type': 'reference-entry', 'publication_date': '2017-01-01', 'host_venue': 'V79135273', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2785026101', []]], 'cited_by_count': 2966, 'concepts': [['C144024400', '0.68418795'], ['C4727928', '0.6382951'], ['C114713312', '0.586641'], ['C144348335', '0.5739333'], ['C137753397', '0.5319531']], 'referenced_works': ['W21867500', 'W1978274937', 'W1981410982', 'W2003065134', 'W2006060642', 'W2011924621', 'W2023662408', 'W2024821007', 'W2029231161', 'W2034013712', 'W2034124120', 'W2036423364', 'W2042929680', 'W2045374636', 'W2056350085', 'W2056944867', 'W2060935923', 'W2098061691', 'W2106169604', 'W2109469951', 'W2109743555', 'W2124969489', 'W2132092465', 'W2146025191', 'W2333923417', 'W3023815068', 'W4232323869'], 'abstract': 'This paper reports on the development of social network analysis, tracing its origins in classical sociology and its more recent formulation in social scientific and mathematical work. It is argued that the concept of social network provides a powerful model for social structure, and that a number of important formal methods of social network analysis can be discerned. Social network analysis has been used in studies of kinship structure, social mobility, science citations, contacts among members of deviant groups, corporate power, international trade exploitation, class structure, and many other areas. A review of the formal models proposed in graph theory, multidimensional scaling, and algebraic topology is followed by extended illustrations of social network analysis in the study of community structure and interlocking directorships.', 'counts_by_year': [[2022, 6], [2021, 67], [2020, 86], [2019, 71], [2018, 80], [2017, 88], [2016, 151], [2015, 203], [2014, 201], [2013, 226], [2012, 221]]}, {'id': 'W2938574745', 'doi': 'https://doi.org/10.1093/nar/gkz268', 'title': 'The EMBL-EBI search and sequence analysis tools APIs in 2019', 'type': 'journal-article', 'publication_date': '2019-07-02', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2026996585', ['I1303153112']], ['A2429193772', ['I1303153112']], ['A2937590915', ['I1303153112']], ['A2014391113', ['I1303153112']], ['A2097025435', ['I1303153112']], ['A2938888379', ['I1303153112']], ['A2939058585', ['I1303153112']], ['A2940017017', ['I1303153112']], ['A2124824206', ['I1303153112']], ['A2120643991', ['I1303153112']], ['A2146468246', ['I1303153112']]], 'cited_by_count': 2961, 'concepts': [['C2778531742', '0.68471426'], ['C20136886', '0.68154424'], ['C136764020', '0.62157524'], ['C41008148', '0.5494612'], ['C23123220', '0.47056007']], 'referenced_works': ['W183866246', 'W1973234126', 'W1996423252', 'W2014975375', 'W2015292449', 'W2038605324', 'W2068187483', 'W2101291993', 'W2112752664', 'W2120772351', 'W2120908187', 'W2125079066', 'W2131386482', 'W2142678478', 'W2145374174', 'W2160378127', 'W2211193336', 'W2558999090', 'W2606700140', 'W2610786720', 'W2613847904', 'W2766195206', 'W2766848373', 'W2768838077', 'W2769449671', 'W2898210859', 'W2900359059', 'W2900674118', 'W2903835444', 'W2918063357'], 'abstract': 'Abstract The EMBL-EBI provides free access to popular bioinformatics sequence analysis applications as well as to a full-featured text search engine with powerful cross-referencing and data retrieval capabilities. Access to these services is provided via user-friendly web interfaces and via established RESTful and SOAP Web Services APIs (https://www.ebi.ac.uk/seqdb/confluence/display/JDSAT/EMBL-EBI+Web+Services+APIs+-+Data+Retrieval). Both systems have been developed with the same core principles that allow them to integrate an ever-increasing volume of biological data, making them an integral part of many popular data resources provided at the EMBL-EBI. Here, we describe the latest improvements made to the frameworks which enhance the interconnectivity between public EMBL-EBI resources and ultimately enhance biological data discoverability, accessibility, interoperability and reusability.', 'counts_by_year': [[2022, 751], [2021, 1136], [2020, 919], [2019, 153], [2012, 1]]}, {'id': 'W2898402099', 'doi': 'https://doi.org/10.1093/nar/gky995', 'title': 'The Pfam protein families database in 2019', 'type': 'journal-article', 'publication_date': '2019-01-08', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2559864363', ['I1303153112']], ['A2230831812', ['I1303153112']], ['A2120643991', ['I1303153112']], ['A1417685655', ['I136199984']], ['A2765692903', ['I1303153112']], ['A2124824206', ['I1303153112']], ['A2298931247', ['I1303153112']], ['A2097871067', ['I1303153112']], ['A2955079620', ['I1303153112']], ['A2898293081', ['I1303153112']], ['A474265672', ['I2800139495']], ['A2491862705', ['I138689650', 'I65285256']], ['A2234409473', ['I138689650']], ['A1972212482', ['I138689650']], ['A245468651', ['I138689650']]], 'cited_by_count': 2937, 'concepts': [['C2776321320', '0.6588744'], ['C86803240', '0.60320485'], ['C70721500', '0.58375263'], ['C202264299', '0.5369482'], ['C171897839', '0.46240857']], 'referenced_works': ['W1499198785', 'W1965827393', 'W1995808589', 'W2024708232', 'W2109872885', 'W2121520117', 'W2124871329', 'W2128653811', 'W2129448726', 'W2136280642', 'W2138122982', 'W2145931647', 'W2224056471', 'W2302501749', 'W2557496587', 'W2558272290', 'W2558945136', 'W2559738700', 'W2562773385', 'W2610786720', 'W2769449671', 'W4210702584', 'W4255192132'], 'abstract': "The last few years have witnessed significant changes in Pfam (https://pfam.xfam.org). The number of families has grown substantially to a total of 17,929 in release 32.0. New additions have been coupled with efforts to improve existing families, including refinement of domain boundaries, their classification into Pfam clans, as well as their functional annotation. We recently began to collaborate with the RepeatsDB resource to improve the definition of tandem repeat families within Pfam. We carried out a significant comparison to the structural classification database, namely the Evolutionary Classification of Protein Domains (ECOD) that led to the creation of 825 new families based on their set of uncharacterized families (EUFs). Furthermore, we also connected Pfam entries to the Sequence Ontology (SO) through mapping of the Pfam type definitions to SO terms. Since Pfam has many community contributors, we recently enabled the linking between authorship of all Pfam entries with the corresponding authors' ORCID identifiers. This effectively permits authors to claim credit for their Pfam curation and link them to their ORCID record.", 'counts_by_year': [[2022, 532], [2021, 1028], [2020, 996], [2019, 371], [2018, 8]]}, {'id': 'W2964081807', 'doi': 'https://doi.org/10.1109/cvpr.2018.00907', 'title': 'Learning Transferable Architectures for Scalable Image Recognition', 'type': 'proceedings-article', 'publication_date': '2018-06-18', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2655758810', ['I1291425158']], ['A2089062156', ['I1291425158']], ['A2343055381', ['I1291425158']], ['A2148448995', ['I1291425158']]], 'cited_by_count': 2907, 'concepts': [['C41008148', '0.80272794'], ['C3826847', '0.76014835'], ['C81363708', '0.6739149'], ['C48044578', '0.59644896'], ['C2777210771', '0.577544']], 'referenced_works': ['W2064675550', 'W2097117768', 'W2101926813', 'W2108598243', 'W2112796928', 'W2119717200', 'W2119814172', 'W2125303539', 'W2144161366', 'W2171658832', 'W2183341477', 'W2194775991', 'W2531409750', 'W2549139847', 'W2557728737', 'W2565639579', 'W2962737770', 'W2963351448', 'W2963446712', 'W3099206234', 'W4255158661', 'W4256520039'], 'abstract': 'Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the  search space) which enables transferability. In our experiments, we search for the best convolutional layer (or cell) on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a  architecture. We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4% error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.', 'counts_by_year': [[2022, 341], [2021, 933], [2020, 881], [2019, 565], [2018, 166], [2017, 18]]}, {'id': 'W2135581618', 'doi': 'https://doi.org/10.1093/nar/gkv1145', 'title': '2016 update of the PRIDE database and its related tools', 'type': 'journal-article', 'publication_date': '2016-01-04', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2137679054', ['I1303153112']], ['A2049020796', ['I1303153112']], ['A2003389809', ['I1303153112']], ['A1584058292', ['I1303153112']], ['A2159810253', ['I76134821']], ['A2276987194', ['I1303153112']], ['A2717130303', ['I1303153112']], ['A1712178497', ['I1303153112']], ['A2051042321', ['I1303153112']], ['A1977635502', ['I1303153112']], ['A2267583821', ['I1303153112']], ['A3201701784', ['I1303153112']], ['A27377996', ['I1303153112']]], 'cited_by_count': 2859, 'concepts': [['C2779728303', '0.9671287'], ['C77088390', '0.49143302'], ['C41008148', '0.4650973'], ['C136764020', '0.44338176'], ['C17744445', '0.086988896']], 'referenced_works': ['W1003536350', 'W1852508338', 'W1945748241', 'W1964336313', 'W1971091652', 'W1972247888', 'W1992436488', 'W1994045445', 'W1996992222', 'W2000619947', 'W2005086972', 'W2013955377', 'W2017077050', 'W2023010129', 'W2028526557', 'W2028673251', 'W2029849110', 'W2031095666', 'W2037744323', 'W2040312226', 'W2046872827', 'W2051319314', 'W2051810299', 'W2053573662', 'W2054391202', 'W2056499061', 'W2057761395', 'W2064295826', 'W2069860553', 'W2073022274', 'W2076688076', 'W2079061207', 'W2080752012', 'W2081990960', 'W2091377998', 'W2091754843', 'W2098932404', 'W2104764521', 'W2106380482', 'W2109993774', 'W2110017950', 'W2111846704', 'W2118001361', 'W2118721685', 'W2130706354', 'W2138563170', 'W2139096442', 'W2144504965', 'W2149384852', 'W2150457655', 'W2151893314', 'W2157918066', 'W2166377314', 'W2167089719', 'W2170255714', 'W2171814265', 'W2234991979', 'W2322683223', 'W2531506996', 'W2739999456'], 'abstract': "The PRoteomics IDEntifications (PRIDE) database is one of the world-leading data repositories of mass spectrometry (MS)-based proteomics data. Since the beginning of 2014, PRIDE Archive (http://www.ebi.ac.uk/pride/archive/) is the new PRIDE archival system, replacing the original PRIDE database. Here we summarize the developments in PRIDE resources and related tools since the previous update manuscript in the Database Issue in 2013. PRIDE Archive constitutes a complete redevelopment of the original PRIDE, comprising a new storage backend, data submission system and web interface, among other components. PRIDE Archive supports the most-widely used PSI (Proteomics Standards Initiative) data standard formats (mzML and mzIdentML) and implements the data requirements and guidelines of the ProteomeXchange Consortium. The wide adoption of ProteomeXchange within the community has triggered an unprecedented increase in the number of submitted data sets (around 150 data sets per month). We outline some statistics on the current PRIDE Archive data contents. We also report on the status of the PRIDE related stand-alone tools: PRIDE Inspector, PRIDE Converter 2 and the ProteomeXchange submission tool. Finally, we will give a brief update on the resources under development 'PRIDE Cluster' and 'PRIDE Proteomes', which provide a complementary view and quality-scored information of the peptide and protein identification data available in PRIDE Archive.", 'counts_by_year': [[2022, 117], [2021, 234], [2020, 349], [2019, 665], [2018, 765], [2017, 499], [2016, 228], [2015, 1]]}, {'id': 'W2790166049', 'doi': 'https://doi.org/10.1126/science.aap9559', 'title': 'The spread of true and false news online', 'type': 'journal-article', 'publication_date': '2018-03-09', 'host_venue': 'V3880285', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2001188003', ['I63966007']], ['A2101508713', ['I63966007']], ['A2138429372', ['I63966007']]], 'cited_by_count': 2841, 'concepts': [['C2780469804', '0.8546314'], ['C2778738651', '0.68701065'], ['C2779756789', '0.6686605'], ['C177264268', '0.45705697'], ['C518677369', '0.45503306']], 'referenced_works': ['W1105550512', 'W1532503642', 'W1569507287', 'W1638051351', 'W1796766288', 'W1804621450', 'W1965555277', 'W1975879668', 'W1980868027', 'W1995875735', 'W2007212082', 'W2028055861', 'W2035654630', 'W2039546655', 'W2040467972', 'W2040938924', 'W2050619059', 'W2057026632', 'W2076718192', 'W2084591134', 'W2091084672', 'W2118836230', 'W2149197198', 'W2153148122', 'W2164082612', 'W2168332560', 'W2170490457', 'W2178843456', 'W2232384272', 'W2286465138', 'W2289587777', 'W2295893477', 'W2398287226', 'W2470894770', 'W2535105469', 'W2735898654', 'W2741616044', 'W2964269387', 'W3098040575', 'W3098249847', 'W3121315632', 'W3122000667', 'W3123436322', 'W3125182500', 'W4246506621', 'W4301424608'], 'abstract': 'Lies spread faster than the truth There is worldwide concern over false news and the possibility that it can influence political, economic, and social well-being. To understand how false news spreads, Vosoughi et al. used a data set of rumor cascades on Twitter from 2006 to 2017. About 126,000 rumors were spread by ∼3 million people. False news reached more people than the truth; the top 1% of false news cascades diffused to between 1000 and 100,000 people, whereas the truth rarely diffused to more than 1000 people. Falsehood also diffused faster than the truth. The degree of novelty and the emotional reactions of recipients may be responsible for the differences observed. Science , this issue p. 1146', 'counts_by_year': [[2022, 535], [2021, 878], [2020, 736], [2019, 516], [2018, 165], [2017, 2], [2015, 1]]}, {'id': 'W2239306219', 'doi': 'https://doi.org/10.11648/j.ajtas.20160501.11', 'title': 'Comparison of Convenience Sampling and Purposive Sampling', 'type': 'journal-article', 'publication_date': '2016-01-01', 'host_venue': 'V2764541676', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A309997187', []], ['A2337695016', []], ['A2414702360', []]], 'cited_by_count': 2820, 'concepts': [['C100363876', '0.8391552'], ['C140779682', '0.62735814'], ['C105795698', '0.5273714'], ['C41008148', '0.45934516'], ['C33923547', '0.34977502']], 'referenced_works': ['W352168851', 'W364871431', 'W1495403436', 'W1525778942', 'W1527054660', 'W1556808170', 'W1564194297', 'W1727534288', 'W1820505434', 'W2020675527', 'W2023971322', 'W2085028749', 'W2292294910', 'W2790841064'], 'abstract': 'This article studied and compared the two nonprobability sampling techniques namely, Convenience Sampling and Purposive Sampling. Convenience Sampling and Purposive Sampling are Nonprobability Sampling Techniques that a researcher uses to choose a sample of subjects/units from a population. Although, Nonprobability sampling has a lot of limitations due to the subjective nature in choosing the sample and thus it is not good representative of the population, but it is useful especially when randomization is impossible like when the population is very large. It can be useful when the researcher has limited resources, time and workforce. It can also be used when the research does not aim to generate results that will be used to create generalizations pertaining to the entire population. Therefore, there is a need to use nonprobability sampling techniques. The aim of this study is to compare among the two nonrandom sampling techniques in order to know whether one technique is better or useful than the other. Different articles were reviewed to compare between Convenience Sampling and Purposive Sampling and it is concluded that the choice of the techniques (Convenience Sampling and Purposive Sampling) depends on the nature and type of the research.', 'counts_by_year': [[2022, 676], [2021, 799], [2020, 610], [2019, 451], [2018, 208], [2017, 64], [2016, 10]]}, {'id': 'W3102327032', 'doi': 'https://doi.org/10.1109/tro.2017.2705103', 'title': 'ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras', 'type': 'journal-article', 'publication_date': '2017-06-12', 'host_venue': 'V144620930', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A116760373', ['I255234318']], ['A2019354557', ['I255234318']]], 'cited_by_count': 2802, 'concepts': [['C31972630', '0.76277745'], ['C86369673', '0.756801'], ['C154945302', '0.7558942'], ['C65909025', '0.6340883'], ['C41008148', '0.63312787']], 'referenced_works': ['W49679257', 'W1987648924', 'W1989484209', 'W2021851106', 'W2064451896', 'W2069479606', 'W2082312667', 'W2082313107', 'W2101648351', 'W2115579991', 'W2117228865', 'W2118428504', 'W2143769815', 'W2154280780', 'W2202897526', 'W2218842719', 'W2396274919', 'W2527142681', 'W2538522345', 'W3103648783', 'W4236769309'], 'abstract': 'We present ORB-SLAM2 a complete SLAM system for monocular, stereo and RGB-D cameras, including map reuse, loop closing and relocalization capabilities. The system works in real-time on standard CPUs in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our back-end based on bundle adjustment with monocular and stereo observations allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches to map points that allow for zero-drift localization. The evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields.', 'counts_by_year': [[2022, 433], [2021, 721], [2020, 704], [2019, 590], [2018, 300], [2017, 48], [2016, 1]]}, {'id': 'W2954996726', 'doi': 'https://doi.org/10.1186/s40537-019-0197-0', 'title': 'A survey on Image Data Augmentation for Deep Learning', 'type': 'journal-article', 'publication_date': '2019-07-06', 'host_venue': 'V2737955091', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2953545650', ['I63772739']], ['A3186327443', ['I63772739']]], 'cited_by_count': 2795, 'concepts': [['C41008148', '0.8022493'], ['C68597687', '0.64558244'], ['C108583219', '0.53366995'], ['C115961682', '0.5120793'], ['C154945302', '0.50051546']], 'referenced_works': ['W54257720', 'W1901129140', 'W2041616772', 'W2069143585', 'W2103018059', 'W2112796928', 'W2148143831', 'W2163345210', 'W2171943915', 'W2395579298', 'W2581082771', 'W2592929672', 'W2604262106', 'W2765253680', 'W2767106145', 'W2789588857', 'W2806118840', 'W2899434936', 'W2964185501'], 'abstract': 'Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.', 'counts_by_year': [[2022, 985], [2021, 1225], [2020, 552], [2019, 19], [2018, 1]]}, {'id': 'W2899714098', 'doi': 'https://doi.org/10.7554/elife.42166', 'title': 'New tools for automated high-resolution cryo-EM structure determination in RELION-3', 'type': 'journal-article', 'publication_date': '2018-11-09', 'host_venue': 'V1336409049', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2886584488', ['I170203145']], ['A2108756930', ['I170203145']], ['A1970827502', ['I2800139495']], ['A2467028581', ['I2800139495']], ['A2410773886', ['I4210153543']], ['A2056355882', ['I86987016', 'I2800139495']], ['A93034870', ['I170203145']]], 'cited_by_count': 2774, 'concepts': [['C41008148', '0.6527679'], ['C519991488', '0.6198446'], ['C459310', '0.5118174'], ['C11413529', '0.50766766'], ['C195065555', '0.4617259']], 'referenced_works': ['W1810581179', 'W1885115521', 'W1890431460', 'W1974978266', 'W1975552457', 'W1977064288', 'W1978288593', 'W1982295411', 'W1987420415', 'W2005835351', 'W2006676401', 'W2006826332', 'W2011665065', 'W2028470175', 'W2032347855', 'W2034527511', 'W2038743966', 'W2039739820', 'W2048826637', 'W2051925784', 'W2053185887', 'W2056713455', 'W2071166164', 'W2073949924', 'W2075824676', 'W2077219861', 'W2079583250', 'W2086812659', 'W2097604487', 'W2100455255', 'W2104234755', 'W2120396539', 'W2142161929', 'W2146349883', 'W2149360962', 'W2152562710', 'W2171074980', 'W2299689158', 'W2340685884', 'W2397429809', 'W2432427150', 'W2519618428', 'W2564545111', 'W2587625522', 'W2593511418', 'W2594023905', 'W2614431660', 'W2728385286', 'W2738200264', 'W2755784633', 'W2783350531', 'W2791354025', 'W2798457309', 'W2798733184', 'W2799415291', 'W2801961940', 'W2808303925', 'W2950455172', 'W2950967305', 'W2951384153', 'W2951611668', 'W2953161116', 'W2953320394', 'W2963188324', 'W4210352507'], 'abstract': 'Here, we describe the third major release of RELION. CPU-based vector acceleration has been added in addition to GPU support, which provides flexibility in use of resources and avoids memory limitations. Reference-free autopicking with Laplacian-of-Gaussian filtering and execution of jobs from python allows non-interactive processing during acquisition, including 2D-classification, de novo model generation and 3D-classification. Per-particle refinement of CTF parameters and correction of estimated beam tilt provides higher resolution reconstructions when particles are at different heights in the ice, and/or coma-free alignment has not been optimal. Ewald sphere curvature correction improves resolution for large particles. We illustrate these developments with publicly available data sets: together with a Bayesian approach to beam-induced motion correction it leads to resolution improvements of 0.2-0.7 Å compared to previous RELION versions.', 'counts_by_year': [[2022, 714], [2021, 903], [2020, 800], [2019, 343], [2018, 12]]}, {'id': 'W2470673105', 'doi': 'https://doi.org/10.18653/v1/n16-1174', 'title': 'Hierarchical Attention Networks for Document Classification', 'type': 'proceedings-article', 'publication_date': '2016-06-13', 'host_venue': 'V4306420633', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2596307046', ['I74973139']], ['A2127567756', ['I74973139']], ['A2119216958', ['I74973139']], ['A2122755126', ['I1290206253']], ['A1972291593', ['I74973139']], ['A2046588481', ['I74973139']]], 'cited_by_count': 2719, 'concepts': [['C41008148', '0.7871784'], ['C774472', '0.73829186'], ['C2777530160', '0.63863635'], ['C154945302', '0.5722438'], ['C2776359362', '0.5626264']], 'referenced_works': ['W1514535095', 'W1544827683', 'W1648885110', 'W1832693441', 'W1899794420', 'W2064675550', 'W2097726431', 'W2112796928', 'W2113459411', 'W2115613106', 'W2120615054', 'W2123442489', 'W2131876387', 'W2142972908', 'W2149684865', 'W2153579005', 'W2154359981', 'W2156413587', 'W2209647458', 'W2250879510', 'W2250966211', 'W2251008987', 'W2251849926', 'W2251939518', 'W2265846598', 'W2284289336', 'W2949563612', 'W2951008357', 'W2963012544', 'W2963069010', 'W2963355447', 'W2963921497', 'W2963954913', 'W2964091467', 'W2964308564'], 'abstract': 'We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.', 'counts_by_year': [[2022, 270], [2021, 563], [2020, 739], [2019, 675], [2018, 339], [2017, 103], [2016, 14], [2015, 12]]}, {'id': 'W2296283641', 'doi': 'https://doi.org/10.18653/v1/n16-1030', 'title': 'Neural Architectures for Named Entity Recognition', 'type': 'proceedings-article', 'publication_date': '2016-03-04', 'host_venue': 'V4306420633', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2798455563', ['I2252078561']], ['A2159534029', ['I170486558']], ['A2553540387', ['I74973139']], ['A2252601387', ['I1291425158']], ['A2119216958', ['I74973139']]], 'cited_by_count': 2691, 'concepts': [['C41008148', '0.6346665'], ['C154945302', '0.46829194'], ['C2984842247', '0.4384562'], ['C142853389', '0.43816766'], ['C50644808', '0.42473328']], 'referenced_works': ['W6908809', 'W11298561', 'W296583332', 'W1531333757', 'W1899794420', 'W1904365287', 'W1940872118', 'W1951325712', 'W1984186257', 'W1995249715', 'W2004763266', 'W2030904529', 'W2056451646', 'W2064675550', 'W2096953947', 'W2107878631', 'W2116983617', 'W2120844411', 'W2130903752', 'W2144578941', 'W2147880316', 'W2153579005', 'W2158139315', 'W2158899491', 'W2159107349', 'W2168596788', 'W2251599843', 'W2251830157', 'W2520117834', 'W2786972369', 'W2949777170', 'W2949952998', 'W2950577311', 'W2951559648', 'W2963012544', 'W2963208801', 'W2963625095', 'W2964121744'], 'abstract': 'Comunicacio presentada a la 2016 Conference of the North American Chapter of the Association for Computational Linguistics, celebrada a San Diego (CA, EUA) els dies 12 a 17 de juny 2016.', 'counts_by_year': [[2022, 164], [2021, 517], [2020, 641], [2019, 644], [2018, 437], [2017, 207], [2016, 79]]}, {'id': 'W2266175888', 'doi': 'https://doi.org/10.1038/nature16521', 'title': 'Fully integrated wearable sensor arrays for multiplexed in situ perspiration analysis', 'type': 'journal-article', 'publication_date': '2016-01-28', 'host_venue': 'V137773608', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2154259907', ['I148283060', 'I95457486']], ['A2026146142', ['I148283060', 'I97018004', 'I95457486']], ['A2255584316', ['I148283060', 'I95457486']], ['A2225876008', ['I97018004']], ['A2900600952', ['I95457486', 'I148283060']], ['A2610044360', ['I95457486']], ['A2507951085', ['I148283060', 'I95457486']], ['A2131007051', ['I148283060', 'I95457486']], ['A1998073590', ['I148283060', 'I95457486']], ['A119581241', ['I148283060', 'I95457486']], ['A3213498200', ['I4210117802']], ['A2129903503', ['I95457486']], ['A2145114538', ['I97018004']], ['A111265778', ['I148283060', 'I95457486']]], 'cited_by_count': 2688, 'concepts': [['C2779528915', '0.8787867'], ['C150594956', '0.76494014'], ['C41008148', '0.581272'], ['C19275194', '0.5660052'], ['C160756335', '0.52335066']], 'referenced_works': ['W1548680965', 'W1963528082', 'W1969530683', 'W1980864800', 'W1982085124', 'W1984297076', 'W1993634572', 'W1994964334', 'W1997166311', 'W1997425475', 'W1998544072', 'W2000694408', 'W2010683549', 'W2016461817', 'W2048479060', 'W2057104805', 'W2061453480', 'W2066415272', 'W2068896164', 'W2070038049', 'W2074448281', 'W2079026833', 'W2085296840', 'W2090514156', 'W2093381236', 'W2098534191', 'W2124158393', 'W2129547735', 'W2131368080', 'W2144506268', 'W2146553545', 'W2158417770', 'W2160524585', 'W2161552938', 'W2166680787', 'W2167696419', 'W2167972453', 'W2176890785', 'W4213360791'], 'abstract': "Wearable sensor technologies are essential to the realization of personalized medicine through continuously monitoring an individual's state of health. Sampling human sweat, which is rich in physiological information, could enable non-invasive monitoring. Previously reported sweat-based and other non-invasive biosensors either can only monitor a single analyte at a time or lack on-site signal processing circuitry and sensor calibration mechanisms for accurate analysis of the physiological state. Given the complexity of sweat secretion, simultaneous and multiplexed screening of target biomarkers is critical and requires full system integration to ensure the accuracy of measurements. Here we present a mechanically flexible and fully integrated (that is, no external analysis is needed) sensor array for multiplexed in situ perspiration analysis, which simultaneously and selectively measures sweat metabolites (such as glucose and lactate) and electrolytes (such as sodium and potassium ions), as well as the skin temperature (to calibrate the response of the sensors). Our work bridges the technological gap between signal transduction, conditioning (amplification and filtering), processing and wireless transmission in wearable biosensors by merging plastic-based sensors that interface with the skin with silicon integrated circuits consolidated on a flexible circuit board for complex signal processing. This application could not have been realized using either of these technologies alone owing to their respective inherent limitations. The wearable system is used to measure the detailed sweat profile of human subjects engaged in prolonged indoor and outdoor physical activities, and to make a real-time assessment of the physiological state of the subjects. This platform enables a wide range of personalized diagnostic and physiological monitoring applications.", 'counts_by_year': [[2022, 390], [2021, 554], [2020, 509], [2019, 472], [2018, 390], [2017, 258], [2016, 107], [2015, 1]]}, {'id': 'W2753051611', 'doi': 'https://doi.org/10.1016/s0140-6736(17)32152-9', 'title': 'Global, regional, and national age-sex specific mortality for 264 causes of death, 1980–2016: a systematic analysis for the Global Burden of Disease Study 2016', 'type': 'journal-article', 'publication_date': '2017-09-16', 'host_venue': 'V49861241', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2113265520', ['I201448701']], ['A2336267196', ['I165143802']], ['A1261223370', ['I861853513']], ['A2104043555', ['I145487455']], ['A2167087174', ['I145487455']], ['A2026292620', []], ['A275226674', []], ['A2755910248', ['I26092322']], ['A2337146604', ['I201448701']], ['A2151274743', ['I181547552']], ['A2246075794', ['I28166907']], ['A2132116411', ['I1443707']], ['A2755240926', []], ['A2755349297', []], ['A2756237734', []], ['A2754942991', ['I201448701']], ['A1968471205', []], ['A2343865831', ['I201448701']], ['A2279206299', ['I129604602']], ['A2525984107', []], ['A2754876412', ['I201448701']], ['A1991399651', ['I118347636']], ['A2396300262', []], ['A2628602411', []], ['A2251799706', ['I4871159']], ['A2464387091', ['I201726411']], ['A1896914120', []], ['A2709848888', ['I90183372']], ['A109783649', ['I28166907']], ['A2968762908', ['I201448701']], ['A2952949420', []], ['A290882514', ['I75951250']], ['A2318996425', ['I28022161']], ['A2075525165', ['I153390918']], ['A2116969515', ['I8764889']], ['A2291839830', ['I70640408']], ['A2018956803', []], ['A2658307528', ['I28046988']], ['A2096370698', ['I159247623']], ['A2139054913', ['I891191580']], ['A2766346432', []], ['A2305148106', ['I145722265']], ['A2917564806', []], ['A2109138503', ['I103911934']], ['A3213889487', []], ['A174217572', ['I28166907']], ['A2707513552', ['I201448701']], ['A1949092674', ['I46247651']], ['A3214426403', ['I184942183']], ['A1974818969', []], ['A2754942240', []], ['A2607074190', []], ['A116052509', []], ['A2515935880', []], ['A2424735554', []], ['A2754137033', []], ['A2053519999', []], ['A2289255636', []], ['A2026359138', []], ['A2268693013', []], ['A660088416', []], ['A1981440893', []], ['A2220936201', []], ['A2170988738', []], ['A308811693', []], ['A2108298034', []], ['A1895530724', ['I19630809']], ['A2238440639', []], ['A2754195866', []], ['A2135382290', []], ['A2616542806', []], ['A2082872497', []], ['A2255678021', []], ['A2483563741', []], ['A2527205794', []], ['A3212010293', []], ['A2116982717', []], ['A2762951047', []], ['A1719445272', []], ['A1954828074', []], ['A1996812315', []], ['A2139622386', []], ['A678399540', []], ['A2755867325', []], ['A2755448035', []], ['A3176002406', []], ['A138047837', []], ['A2273134870', []], ['A2124342525', []], ['A2148183577', []], ['A2345520266', []], ['A2105920767', []], ['A2570973330', []], ['A2610112066', []], ['A2306491945', []], ['A2064694482', []], ['A2123807283', []], ['A2509173075', []], ['A781531929', []], ['A2494376796', []], ['A2559996202', []], ['A2128665858', []], ['A2266351856', []], ['A2609800051', []], ['A2424892588', []], ['A2470792078', []], ['A2140769477', []], ['A2110727687', []], ['A935523974', []], ['A3019388298', []], ['A344498437', []], ['A2076714150', []], ['A2568267441', []], ['A1661044825', []], ['A2015830873', []], ['A2006538287', []], ['A256209369', ['I70931966']], ['A2061034448', []], ['A2754324203', []], ['A4431029', []], ['A1993463829', []], ['A2305493226', []], ['A3149196549', []], ['A2023264609', []], ['A2092117536', []], ['A2789395593', []], ['A2014559487', []], ['A2755506828', []], ['A2171790375', []], ['A2162480515', []], ['A2304245645', []], ['A2797132218', []], ['A2225675537', []], ['A2030608638', []], ['A2419696075', []], ['A2444505863', []], ['A2024673501', []], ['A1485388775', []], ['A1991156927', []], ['A2139978954', []], ['A2525638264', []], ['A2147768707', []], ['A2170374040', []], ['A130753112', []], ['A2599128148', []], ['A2719099999', []], ['A3156606865', []], ['A2637625157', []], ['A3216312492', []], ['A2766221262', []], ['A2239987130', []], ['A2433616363', []], ['A2058142950', []], ['A1989487947', []], ['A2766989059', []], ['A2096790775', []], ['A248647053', []], ['A2180474783', []], ['A2160364156', []], ['A2309765197', []], ['A3213016336', []], ['A2039479126', []], ['A2732752638', []], ['A2517658965', []], ['A2345001411', []], ['A2177286373', []], ['A2245432073', []], ['A2476985414', []], ['A1971808073', []], ['A2661338970', []], ['A2527676953', []], ['A726144794', []], ['A1991954648', ['I19630809']], ['A2336558418', []], ['A2614872320', []], ['A2138432658', []], ['A2045839628', []], ['A2409117092', []], ['A2143292742', []], ['A2136108649', []], ['A2562978351', []], ['A3206248490', []], ['A2147249232', []], ['A2190367765', []], ['A2765465807', []], ['A2767572814', []], ['A1934246530', []], ['A2155623130', []], ['A2527252649', []], ['A2003644413', []], ['A2703605013', []], ['A2170943676', []], ['A2752875094', []], ['A2951901137', []], ['A3212987995', []], ['A2087097567', []], ['A2236383320', []], ['A2008056385', []], ['A261494640', []], ['A1999104575', []], ['A1975482828', []], ['A2159017802', []], ['A2767141309', []], ['A1987290921', []], ['A2754028391', []], ['A2702278152', []], ['A2237597403', []], ['A2159703735', []], ['A2498133086', []], ['A2063878478', []], ['A1684365439', []], ['A110332668', []], ['A500290227', []], ['A2145691735', []], ['A2425928874', []], ['A1933433957', []], ['A2755112910', []], ['A2577354962', []], ['A2615697546', []], ['A2756137649', []], ['A2144421627', ['I183874623']], ['A2165675880', []], ['A2510023083', []], ['A2090105886', []], ['A2112064235', []], ['A2135094426', []], ['A2230245500', []], ['A2123248410', []], ['A2614702463', []], ['A2145525382', []], ['A2614554465', []], ['A1900504705', []], ['A2048675361', []], ['A2133846596', []], ['A2755115224', []], ['A2755509893', []], ['A2097761319', []], ['A2058870430', []], ['A2258601719', []], ['A2310999867', []], ['A2132501245', []], ['A2754735122', []], ['A2098444608', []], ['A2146891038', []], ['A1975324464', []], ['A2525534507', []], ['A2576156219', []], ['A2310038103', []], ['A2066511934', []], ['A2468155649', []], ['A1963193313', []], ['A2615402321', []], ['A2104754581', []], ['A2298408023', []], ['A1875942229', []], ['A242742814', ['I187531555']], ['A2107678894', []], ['A2584263659', []], ['A3177409340', []], ['A2632825197', []], ['A3120043664', []], ['A954872853', []], ['A2064570782', []], ['A1969029766', []], ['A2192301710', []], ['A3214522656', []], ['A2160882886', []], ['A2591497774', []], ['A2087695571', []], ['A2529094164', []], ['A2137545149', []], ['A2804334075', []], ['A2560788212', []], ['A2402779843', []], ['A3213668464', []], ['A2038807793', []], ['A2767134970', []], ['A2126648517', []], ['A1989521457', []], ['A3190491666', []], ['A2304074508', []], ['A2095606354', []], ['A2139270141', []], ['A2101549818', []], ['A2618847948', []], ['A2755614889', []], ['A2140496955', []], ['A2755975474', []], ['A2106144155', []], ['A2102161365', []], ['A2755571491', []], ['A2776297621', []], ['A3213225612', []], ['A3160619284', []], ['A2562465928', []], ['A2040328808', []], ['A2792833490', []], ['A2629503440', []], ['A2109274299', []], ['A2585818880', []], ['A2756296023', []], ['A2426028861', []], ['A2598709781', []], ['A1727635955', []], ['A2133394427', []], ['A1631637737', []], ['A2945866049', []], ['A1964422715', []], ['A2756291383', []], ['A2668447760', []], ['A2508058704', []], ['A2560597097', []], ['A2040913804', []], ['A2462423758', []], ['A1286028393', []], ['A2755117306', []], ['A3213448936', []], ['A2756325217', []], ['A2106301102', []], ['A2269532386', []], ['A2664379405', []], ['A2559829694', []], ['A3037227835', []], ['A202526925', []], ['A2943224566', []], ['A2754436038', []], ['A2615554322', []], ['A2165933833', []], ['A2598037195', []], ['A1311980873', []], ['A2573321743', []], ['A2109432494', []], ['A2641135659', []], ['A2644442497', []], ['A2023451452', []], ['A2217126065', []], ['A3212072964', []], ['A2766847779', []], ['A2124962734', []], ['A2011409888', []], ['A2126042519', []], ['A2287785407', []], ['A2168798418', []], ['A2755166487', []], ['A2292854733', []], ['A224987146', []], ['A2692574153', []], ['A1860009983', []], ['A2796883702', []], ['A2072898194', []], ['A2696008654', []], ['A2078422342', []], ['A2577134405', []], ['A2156641128', []], ['A2754774276', []], ['A2756148125', []], ['A2755060371', []], ['A2173061812', []], ['A2134948551', []], ['A797305362', []], ['A2018982985', []], ['A1728224484', []], ['A2059048655', []], ['A2154526963', []], ['A243988749', []], ['A2755691198', []], ['A1809145585', []], ['A2013758883', []], ['A2133215214', []], ['A2949180028', []], ['A2756247572', []], ['A2164729008', []], ['A3090495659', []], ['A2755026186', []], ['A2709541152', []], ['A3211503600', []], ['A2040971506', []], ['A2529893633', []], ['A2598867768', []], ['A2727664498', []], ['A2201699675', []], ['A2527901436', []], ['A2045444862', []], ['A2527585475', []], ['A2756037193', []], ['A1859188192', []], ['A1990140758', []], ['A2288782178', []], ['A2509393853', []], ['A2007049068', []], ['A3018650095', []], ['A2109200798', []], ['A2804216529', []], ['A2623549860', []], ['A2754216068', []], ['A1900504705', []], ['A2113374822', []], ['A2212815995', []], ['A2024441155', []], ['A2128235974', []], ['A2623221864', []], ['A2755021302', []], ['A2157298618', []], ['A1984789547', []], ['A138278537', ['I19630809']], ['A1955349671', []], ['A3094348129', []], ['A2780731423', []], ['A2113647411', []], ['A2049665584', []], ['A1976173630', []], ['A2797166399', []], ['A2306642447', []], ['A2145634610', []], ['A2673842419', []], ['A2766555974', []], ['A1587262111', []], ['A2413366599', []], ['A2162568637', []], ['A2112367615', []], ['A251309730', []], ['A2303031201', []], ['A2177398213', []], ['A2572175195', []], ['A2026017327', []], ['A2544894299', []], ['A2528034545', []], ['A2630255269', []], ['A395835951', []], ['A2755118126', []], ['A1975406000', []], ['A2126571783', []], ['A2117136343', []], ['A2183182747', ['I19630809']], ['A2251493392', []], ['A2252065202', []], ['A2472487767', []], ['A601196031', []], ['A2133495838', []], ['A2169176658', []], ['A76907487', []], ['A2034393689', []], ['A2765551780', []], ['A2162078611', []], ['A2122470004', []], ['A2606161589', []], ['A2596156213', []], ['A2266022754', []], ['A2694501703', []], ['A2165210302', []], ['A2279835831', []], ['A2099944687', []], ['A2465701595', []], ['A2104279826', []], ['A2151569403', []], ['A2251827248', []], ['A2288782178', []], ['A2483337482', []], ['A2082872497', []], ['A3214555152', []], ['A2089080248', []], ['A2043004940', []], ['A2103273789', []], ['A2126457318', []], ['A2765528675', []], ['A1972757265', []], ['A1890927917', []], ['A3214281271', []], ['A1920971052', []], ['A798807942', []], ['A2054013772', []], ['A1849683455', []], ['A2560759403', []], ['A2034445190', []], ['A2263369703', []], ['A2653082139', []], ['A2997373551', []], ['A2121364159', []], ['A2084661986', []], ['A2095352456', []], ['A2157030991', []], ['A2755742307', []], ['A2121553805', []], ['A2085533449', []], ['A2755071492', []], ['A2985894433', []], ['A2663722086', []], ['A341443032', []], ['A329907494', []], ['A2526931825', []], ['A2758241083', []], ['A2167506362', []], ['A2319995488', []], ['A1907465466', []], ['A2524435981', []], ['A2627886379', []], ['A2489864996', []], ['A2754570444', []], ['A2755255890', []], ['A1977245344', []], ['A1978294612', []], ['A2165142774', []], ['A2308578969', []], ['A2754950566', []], ['A2012074336', []], ['A2040371662', []], ['A2311673811', []], ['A2108967599', []], ['A2187417062', []], ['A2811276577', []], ['A2112093150', []], ['A2154281469', []], ['A2754092363', []], ['A2604742042', []], ['A2617635383', []], ['A1480813768', []], ['A2527981122', []], ['A2115643939', []], ['A2304636401', []], ['A1259793373', []], ['A2151397838', []], ['A2235522935', []], ['A2650374411', []], ['A2023584877', []], ['A2012391880', []], ['A2166148018', []], ['A1985531105', []], ['A2395891654', []], ['A3193264458', []], ['A2804812000', []], ['A1997328309', []], ['A2124281492', []], ['A267907075', []], ['A2224875839', []], ['A2669981133', []], ['A2707543684', []], ['A2132834798', []], ['A1980880360', []], ['A1239443135', []], ['A2619345531', []], ['A2804786586', []], ['A2099142247', []], ['A3103523630', []], ['A2171619173', []], ['A2531047643', []], ['A2104826947', []], ['A2754255124', []], ['A1784439412', []], ['A2952451616', []], ['A2607599903', []], ['A512278449', []], ['A2604723853', []], ['A284438857', []], ['A2063877301', []], ['A1976008987', []], ['A3165398205', []], ['A3049497679', []], ['A2040123927', []], ['A1973406440', []], ['A2888942052', []], ['A2894260413', []], ['A2685385024', []], ['A2123033560', []], ['A2103680419', []], ['A2025574871', []], ['A1967961136', []], ['A2599664222', []], ['A772519725', []], ['A2299946479', []], ['A2766997715', []], ['A2182410343', []], ['A2019219434', []], ['A1965223436', []], ['A2615580838', []], ['A2105129513', []], ['A2243130012', []], ['A2709168440', []], ['A2144154959', []], ['A2951998377', []], ['A2112328861', []], ['A2931535131', []], ['A1760979461', []], ['A2108032137', []], ['A2016704907', []], ['A2716683888', []], ['A2560293163', []], ['A2169155597', []], ['A1827643105', []], ['A2439370755', []], ['A2238864982', []], ['A2112779362', []], ['A2691973723', []], ['A2705705456', []], ['A1983496294', []], ['A2309348756', []], ['A2756138928', []], ['A2530073150', []], ['A1852087164', []], ['A2128434403', []], ['A2316804223', []]], 'cited_by_count': 2688, 'concepts': [['C2781410689', '0.6688925'], ['C133925201', '0.66665256'], ['C29374701', '0.5900237'], ['C149923435', '0.5626555'], ['C71924100', '0.518895']], 'referenced_works': ['W1729017034', 'W1913039393', 'W1991379729', 'W2005743411', 'W2025675035', 'W2043272963', 'W2054753282', 'W2075010294', 'W2093403846', 'W2096228078', 'W2107999364', 'W2108021927', 'W2108107087', 'W2112976419', 'W2123547995', 'W2125065061', 'W2126524595', 'W2133919899', 'W2147222003', 'W2149759828', 'W2150278133', 'W2150561315', 'W2150950302', 'W2151253584', 'W2154180993', 'W2164490662', 'W2166610853', 'W2166809911', 'W2168630917', 'W2169220022', 'W2175751433', 'W2175954218', 'W2258607771', 'W2285745055', 'W2288320856', 'W2337316715', 'W2413356385', 'W2491572170', 'W2515236995', 'W2527824850', 'W2530569707', 'W2537291095', 'W2606956625', 'W2614578122', 'W2614986146', 'W2743318307', 'W2758561236'], 'abstract': 'Monitoring levels and trends in premature mortality is crucial to understanding how societies can address prominent sources of early death. The Global Burden of Disease 2016 Study (GBD 2016) provides a comprehensive assessment of cause-specific mortality for 264 causes in 195 locations from 1980 to 2016. This assessment includes evaluation of the expected epidemiological transition with changes in development and where local patterns deviate from these trends.We estimated cause-specific deaths and years of life lost (YLLs) by age, sex, geography, and year. YLLs were calculated from the sum of each death multiplied by the standard life expectancy at each age. We used the GBD cause of death database composed of: vital registration (VR) data corrected for under-registration and garbage coding; national and subnational verbal autopsy (VA) studies corrected for garbage coding; and other sources including surveys and surveillance systems for specific causes such as maternal mortality. To facilitate assessment of quality, we reported on the fraction of deaths assigned to GBD Level 1 or Level 2 causes that cannot be underlying causes of death (major garbage codes) by location and year. Based on completeness, garbage coding, cause list detail, and time periods covered, we provided an overall data quality rating for each location with scores ranging from 0 stars (worst) to 5 stars (best). We used robust statistical methods including the Cause of Death Ensemble model (CODEm) to generate estimates for each location, year, age, and sex. We assessed observed and expected levels and trends of cause-specific deaths in relation to the Socio-demographic Index (SDI), a summary indicator derived from measures of average income per capita, educational attainment, and total fertility, with locations grouped into quintiles by SDI. Relative to GBD 2015, we expanded the GBD cause hierarchy by 18 causes of death for GBD 2016.The quality of available data varied by location. Data quality in 25 countries rated in the highest category (5 stars), while 48, 30, 21, and 44 countries were rated at each of the succeeding data quality levels. Vital registration or verbal autopsy data were not available in 27 countries, resulting in the assignment of a zero value for data quality. Deaths from non-communicable diseases (NCDs) represented 72·3% (95% uncertainty interval [UI] 71·2-73·2) of deaths in 2016 with 19·3% (18·5-20·4) of deaths in that year occurring from communicable, maternal, neonatal, and nutritional (CMNN) diseases and a further 8·43% (8·00-8·67) from injuries. Although age-standardised rates of death from NCDs decreased globally between 2006 and 2016, total numbers of these deaths increased; both numbers and age-standardised rates of death from CMNN causes decreased in the decade 2006-16-age-standardised rates of deaths from injuries decreased but total numbers varied little. In 2016, the three leading global causes of death in children under-5 were lower respiratory infections, neonatal preterm birth complications, and neonatal encephalopathy due to birth asphyxia and trauma, combined resulting in 1·80 million deaths (95% UI 1·59 million to 1·89 million). Between 1990 and 2016, a profound shift toward deaths at older ages occurred with a 178% (95% UI 176-181) increase in deaths in ages 90-94 years and a 210% (208-212) increase in deaths older than age 95 years. The ten leading causes by rates of age-standardised YLL significantly decreased from 2006 to 2016 (median annualised rate of change was a decrease of 2·89%); the median annualised rate of change for all other causes was lower (a decrease of 1·59%) during the same interval. Globally, the five leading causes of total YLLs in 2016 were cardiovascular diseases; diarrhoea, lower respiratory infections, and other common infectious diseases; neoplasms; neonatal disorders; and HIV/AIDS and tuberculosis. At a finer level of disaggregation within cause groupings, the ten leading causes of total YLLs in 2016 were ischaemic heart disease, cerebrovascular disease, lower respiratory infections, diarrhoeal diseases, road injuries, malaria, neonatal preterm birth complications, HIV/AIDS, chronic obstructive pulmonary disease, and neonatal encephalopathy due to birth asphyxia and trauma. Ischaemic heart disease was the leading cause of total YLLs in 113 countries for men and 97 countries for women. Comparisons of observed levels of YLLs by countries, relative to the level of YLLs expected on the basis of SDI alone, highlighted distinct regional patterns including the greater than expected level of YLLs from malaria and from HIV/AIDS across sub-Saharan Africa; diabetes mellitus, especially in Oceania; interpersonal violence, notably within Latin America and the Caribbean; and cardiomyopathy and myocarditis, particularly in eastern and central Europe. The level of YLLs from ischaemic heart disease was less than expected in 117 of 195 locations. Other leading causes of YLLs for which YLLs were notably lower than expected included neonatal preterm birth complications in many locations in both south Asia and southeast Asia, and cerebrovascular disease in western Europe.The past 37 years have featured declining rates of communicable, maternal, neonatal, and nutritional diseases across all quintiles of SDI, with faster than expected gains for many locations relative to their SDI. A global shift towards deaths at older ages suggests success in reducing many causes of early death. YLLs have increased globally for causes such as diabetes mellitus or some neoplasms, and in some locations for causes such as drug use disorders, and conflict and terrorism. Increasing levels of YLLs might reflect outcomes from conditions that required high levels of care but for which effective treatments remain elusive, potentially increasing costs to health systems.Bill & Melinda Gates Foundation.', 'counts_by_year': [[2022, 393], [2021, 584], [2020, 639], [2019, 620], [2018, 434], [2017, 14], [2016, 1]]}, {'id': 'W2499800833', 'doi': 'https://doi.org/10.1038/nature18933', 'title': 'A multi-modal parcellation of human cerebral cortex', 'type': 'journal-article', 'publication_date': '2016-08-11', 'host_venue': 'V137773608', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2112537157', ['I204465549']], ['A2067847740', ['I204465549']], ['A2151623894', ['I47508984', 'I40120149']], ['A2423450540', ['I204465549']], ['A2102291221', ['I204465549']], ['A1976264487', ['I4210158702']], ['A1996768038', ['I4210158702']], ['A2946638410', ['I2802742124']], ['A1957692837', ['I40120149']], ['A2151338116', ['I2802742124']], ['A3010433618', ['I2802742124']], ['A2025321686', ['I204465549']]], 'cited_by_count': 2662, 'concepts': [['C97820695', '0.79392254'], ['C2777670902', '0.6014445'], ['C2781041448', '0.5873409'], ['C169760540', '0.54613096'], ['C95623464', '0.5143484']], 'referenced_works': ['W1549290346', 'W1973000876', 'W1975192282', 'W1975821649', 'W1983208069', 'W1983306600', 'W1985728900', 'W1993053013', 'W1999135091', 'W2012766927', 'W2015963096', 'W2022801762', 'W2024729467', 'W2028033087', 'W2034826897', 'W2041325253', 'W2062819254', 'W2066722744', 'W2067456724', 'W2071608556', 'W2073588997', 'W2085561705', 'W2093845300', 'W2096672020', 'W2098580305', 'W2107301202', 'W2107499714', 'W2108223231', 'W2116506376', 'W2118366819', 'W2122066592', 'W2124698428', 'W2137240152', 'W2151130155', 'W2157446241', 'W2162261751', 'W2314423215', 'W2319534434', 'W2468819447', 'W4241074797', 'W4295750005'], 'abstract': "Understanding the amazingly complex human cerebral cortex requires a map (or parcellation) of its major subdivisions, known as cortical areas. Making an accurate areal map has been a century-old objective in neuroscience. Using multi-modal magnetic resonance images from the Human Connectome Project (HCP) and an objective semi-automated neuroanatomical approach, we delineated 180 areas per hemisphere bounded by sharp changes in cortical architecture, function, connectivity, and/or topography in a precisely aligned group average of 210 healthy young adults. We characterized 97 new areas and 83 areas previously reported using post-mortem microscopy or other specialized study-specific approaches. To enable automated delineation and identification of these areas in new HCP subjects and in future studies, we trained a machine-learning classifier to recognize the multi-modal 'fingerprint' of each cortical area. This classifier detected the presence of 96.6% of the cortical areas in new subjects, replicated the group parcellation, and could correctly locate areas in individuals with atypical parcellations. The freely available parcellation and classifier will enable substantially improved neuroanatomical precision for studies of the structural and functional organization of human cerebral cortex and its variation across individuals and in development, aging, and disease.", 'counts_by_year': [[2022, 417], [2021, 643], [2020, 521], [2019, 431], [2018, 355], [2017, 259], [2016, 35]]}, {'id': 'W2963420272', 'doi': 'https://doi.org/10.1109/cvpr.2016.278', 'title': 'Context Encoders: Feature Learning by Inpainting', 'type': 'proceedings-article', 'publication_date': '2016-06-27', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2135830645', ['I95457486']], ['A1978515735', ['I95457486']], ['A2133088636', ['I95457486']], ['A2174985400', ['I95457486']], ['A2088536091', ['I95457486']]], 'cited_by_count': 2660, 'concepts': [['C41008148', '0.78404677'], ['C118505674', '0.76142776'], ['C11727466', '0.7296134'], ['C154945302', '0.7147807'], ['C2779343474', '0.6383559']], 'referenced_works': ['W219040644', 'W318042436', 'W343636949', 'W1520997877', 'W1532257412', 'W1536680647', 'W1836533770', 'W1893585201', 'W1903029394', 'W1993120651', 'W2011181254', 'W2025768430', 'W2055132753', 'W2074551195', 'W2100495367', 'W2101926813', 'W2102605133', 'W2117130368', 'W2117539524', 'W2144794286', 'W2147800946', 'W2155893237', 'W2171011251', 'W2198618282', 'W2295936755', 'W4231109964'], 'abstract': 'We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders – a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.', 'counts_by_year': [[2022, 286], [2021, 685], [2020, 648], [2019, 567], [2018, 325], [2017, 133], [2016, 14]]}, {'id': 'W3101095095', 'doi': 'https://doi.org/10.1088/1361-648x/aa8f79', 'title': 'Advanced capabilities for materials modelling with Quantum ESPRESSO', 'type': 'journal-article', 'publication_date': '2017-09-28', 'host_venue': 'V190924190', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A375802989', ['I129043915']], ['A1997777474', ['I5124864']], ['A1981924263', ['I926574661']], ['A2642754819', ['I204730241']], ['A2059091679', ['I123534392']], ['A1967701062', ['I204730241']], ['A2327022495', ['I20089843']], ['A210245081', []], ['A612177845', ['I197604219']], ['A1754607735', ['I5124864']], ['A1007530055', ['I5124864']], ['A163354706', ['I129043915']], ['A2762603534', ['I138549579']], ['A3098878442', ['I138549579']], ['A43957696', ['I138549579']], ['A2135892815', ['I205783295']], ['A2685884548', []], ['A2076849651', ['I51532219']], ['A2052786457', ['I189158943']], ['A1973338463', ['I142476485']], ['A2133208304', ['I12323705']], ['A2252266001', ['I206945453']], ['A2250278280', ['I40120149']], ['A2328453184', ['I138549579']], ['A2514474450', ['I205783295']], ['A2618876692', []], ['A2243447129', ['I20089843']], ['A846658246', ['I3006985408']], ['A39885787', ['I138549579']], ['A2075856783', ['I204730241']], ['A2140699594', ['I138689650']], ['A696519555', ['I5124864']], ['A2105824089', ['I861853513']], ['A2953404978', ['I5124864']], ['A2760987593', ['I70349855']], ['A1513135255', ['I141945490']], ['A275483508', ['I204730241']], ['A2124994872', ['I40120149']], ['A2156755998', ['I90183372']], ['A2920130556', []], ['A1944391784', ['I20089843']], ['A2028319183', ['I40120149']], ['A1984757537', ['I29607241']], ['A1209011253', ['I277688954']], ['A2020857591', ['I5124864']], ['A1942411008', ['I47251452']], ['A1876162838', ['I138689650']], ['A2300112068', ['I277688954']], ['A2203206169', ['I84392919']], ['A1820338028', ['I138549579']]], 'cited_by_count': 2658, 'concepts': [['C41008148', '0.6741659'], ['C20136886', '0.60999525'], ['C79581498', '0.58973825'], ['C2777904410', '0.49122947'], ['C459310', '0.48047987']], 'referenced_works': ['W43109728', 'W121524231', 'W658301530', 'W1517708868', 'W1542444890', 'W1550186202', 'W1795257549', 'W1831756368', 'W1867466875', 'W1905988234', 'W1952152691', 'W1958729574', 'W1968944490', 'W1969381876', 'W1970127494', 'W1970623278', 'W1971426850', 'W1973029646', 'W1973746288', 'W1975212458', 'W1976008838', 'W1977135913', 'W1978061754', 'W1979048990', 'W1979129451', 'W1979773515', 'W1980179156', 'W1981199510', 'W1981368803', 'W1981491642', 'W1986731219', 'W1986986500', 'W1987101971', 'W1988206859', 'W1988320534', 'W1988447421', 'W1988762313', 'W1994569185', 'W1996875333', 'W1997084402', 'W1997151511', 'W1999406166', 'W2000536749', 'W2001429541', 'W2003193361', 'W2004049569', 'W2004546742', 'W2004706090', 'W2005069114', 'W2005470678', 'W2007580576', 'W2007833519', 'W2008227056', 'W2009596258', 'W2010852687', 'W2011327487', 'W2011506619', 'W2013685438', 'W2014477095', 'W2015561435', 'W2015564053', 'W2016805737', 'W2017347525', 'W2017684018', 'W2017726476', 'W2018073061', 'W2018390828', 'W2019465613', 'W2020072134', 'W2021268706', 'W2023154011', 'W2023166289', 'W2023949464', 'W2024816704', 'W2026195773', 'W2026936385', 'W2029010401', 'W2029437009', 'W2029741576', 'W2030601089', 'W2030976617', 'W2033666799', 'W2035454388', 'W2035490689', 'W2035853859', 'W2036330668', 'W2037232512', 'W2039671322', 'W2042206426', 'W2042345808', 'W2042938036', 'W2043535587', 'W2043691039', 'W2044591029', 'W2045986522', 'W2046001409', 'W2048634023', 'W2050155328', 'W2051427315', 'W2052115731', 'W2052867835', 'W2053499780', 'W2054139194', 'W2055698188', 'W2056269736', 'W2056565905', 'W2058398316', 'W2059527006', 'W2059885388', 'W2064779126', 'W2066218785', 'W2066907462', 'W2068182120', 'W2068631314', 'W2069322345', 'W2069740276', 'W2070668052', 'W2071955309', 'W2073727874', 'W2074950413', 'W2075493118', 'W2076400712', 'W2076702475', 'W2081381112', 'W2081525551', 'W2082050320', 'W2082347469', 'W2084429590', 'W2084566663', 'W2085921055', 'W2086354538', 'W2087585288', 'W2089524679', 'W2090135568', 'W2090190744', 'W2091135029', 'W2092157292', 'W2113449409', 'W2115085145', 'W2117857532', 'W2120145199', 'W2121981730', 'W2127909111', 'W2132644412', 'W2141360791', 'W2142707065', 'W2148678374', 'W2149751265', 'W2151800339', 'W2155433909', 'W2157058142', 'W2158043072', 'W2163681071', 'W2164683989', 'W2164805481', 'W2230728100', 'W2270536778', 'W2271095341', 'W2300449814', 'W2300665413', 'W2307804041', 'W2310390902', 'W2310703973', 'W2312684626', 'W2312725846', 'W2315843238', 'W2315878088', 'W2318572885', 'W2319447390', 'W2321639181', 'W2322479756', 'W2323029128', 'W2323315173', 'W2324194559', 'W2327989486', 'W2328943287', 'W2339223279', 'W2429155608', 'W2474739281', 'W2575211403', 'W2580375713', 'W2587570528', 'W2594251464', 'W2594341860', 'W2950239415', 'W3013380380', 'W3099461197', 'W3102306075', 'W3102994515', 'W3178245316', 'W4240489779'], 'abstract': 'Abstract Q uantum ESPRESSO is an integrated suite of open-source computer codes for quantum simulations of materials using state-of-the-art electronic-structure techniques, based on density-functional theory, density-functional perturbation theory, and many-body perturbation theory, within the plane-wave pseudopotential and projector-augmented-wave approaches. Q uantum ESPRESSO owes its popularity to the wide variety of properties and processes it allows to simulate, to its performance on an increasingly broad array of hardware architectures, and to a community of researchers that rely on its capabilities as a core open-source development platform to implement their ideas. In this paper we describe recent extensions and improvements, covering new methodologies and property calculators, improved parallelization, code modularization, and extended interoperability both within the distribution and with external software.', 'counts_by_year': [[2022, 637], [2021, 811], [2020, 650], [2019, 394], [2018, 159], [2017, 1]]}, {'id': 'W2177317049', 'doi': 'https://doi.org/10.1093/nar/gkv951', 'title': 'PubChem Substance and Compound databases', 'type': 'journal-article', 'publication_date': '2016-01-04', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2617071591', ['I4210109390']], ['A3133680586', ['I4210109390']], ['A2137313570', ['I4210109390']], ['A2674214034', ['I4210109390']], ['A2306090952', ['I4210109390']], ['A106891110', ['I4210109390']], ['A2122116004', ['I4210109390']], ['A2105213383', ['I4210109390']], ['A2155512249', ['I4210109390']], ['A2085205320', ['I4210109390']], ['A2307665859', ['I4210109390']], ['A2710640738', ['I4210109390']], ['A2595284240', ['I4210109390']], ['A2102937343', ['I4210109390']]], 'cited_by_count': 2638, 'concepts': [['C158180186', '0.9981948'], ['C77088390', '0.6026066'], ['C206345919', '0.5268216'], ['C3017942907', '0.5074701'], ['C71901391', '0.47827232']], 'referenced_works': ['W150699991', 'W1505362105', 'W1508604947', 'W1509844421', 'W1601495365', 'W1852306145', 'W1969142598', 'W1975147762', 'W1975875968', 'W1999638776', 'W2013068027', 'W2018518196', 'W2019538911', 'W2023624916', 'W2023839852', 'W2027065525', 'W2038702914', 'W2045873232', 'W2051810242', 'W2060097376', 'W2077004593', 'W2080886752', 'W2084727433', 'W2087179087', 'W2087563523', 'W2092818812', 'W2094493457', 'W2095663170', 'W2096560421', 'W2096864392', 'W2112990058', 'W2113310608', 'W2116345848', 'W2117057011', 'W2120211169', 'W2127553917', 'W2135812506', 'W2143392414', 'W2144659448', 'W2146844279', 'W2153057704', 'W2160345387', 'W2169546341', 'W2170481351', 'W2260675179', 'W4230466483', 'W4237629094'], 'abstract': 'PubChem (https://pubchem.ncbi.nlm.nih.gov) is a public repository for information on chemical substances and their biological activities, launched in 2004 as a component of the Molecular Libraries Roadmap Initiatives of the US National Institutes of Health (NIH). For the past 11 years, PubChem has grown to a sizable system, serving as a chemical information resource for the scientific research community. PubChem consists of three inter-linked databases, Substance, Compound and BioAssay. The Substance database contains chemical information deposited by individual data contributors to PubChem, and the Compound database stores unique chemical structures extracted from the Substance database. Biological activity data of chemical substances tested in assay experiments are contained in the BioAssay database. This paper provides an overview of the PubChem Substance and Compound databases, including data sources and contents, data organization, data submission using PubChem Upload, chemical structure standardization, web-based interfaces for textual and non-textual searches, and programmatic access. It also gives a brief description of PubChem3D, a resource derived from theoretical three-dimensional structures of compounds in PubChem, as well as PubChemRDF, Resource Description Framework (RDF)-formatted PubChem data for data sharing, analysis and integration with information contained in other databases.', 'counts_by_year': [[2022, 366], [2021, 517], [2020, 460], [2019, 482], [2018, 412], [2017, 258], [2016, 138], [2015, 1]]}, {'id': 'W2749069611', 'doi': 'https://doi.org/10.18637/jss.v080.i01', 'title': '<b>brms</b>: An <i>R</i> Package for Bayesian Multilevel Models Using <i>Stan</i>', 'type': 'journal-article', 'publication_date': '2017-08-29', 'host_venue': 'V167961193', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2020703739', []]], 'cited_by_count': 2587, 'concepts': [['C107673813', '0.6440458'], ['C2984074130', '0.56047094'], ['C41008148', '0.50868994'], ['C154945302', '0.33760846'], ['C199360897', '0.12026328']], 'referenced_works': ['W214995755', 'W260994251', 'W656838156', 'W1490324987', 'W1517555081', 'W1532756434', 'W1585773866', 'W1587094587', 'W1933293707', 'W1951724000', 'W1965717892', 'W1966576448', 'W1981457167', 'W1987411318', 'W2000619651', 'W2009499611', 'W2013646242', 'W2013885224', 'W2014652454', 'W2019720066', 'W2020999234', 'W2051039162', 'W2056760934', 'W2057765075', 'W2059448777', 'W2059665864', 'W2068754041', 'W2083875149', 'W2084045976', 'W2086206379', 'W2104232463', 'W2104351701', 'W2110161405', 'W2114169935', 'W2119160928', 'W2123222757', 'W2128981260', 'W2138309709', 'W2144673831', 'W2148534890', 'W2149719430', 'W2153882688', 'W2157801062', 'W2160624840', 'W2165390490', 'W2181311031', 'W2478027467', 'W2478429860', 'W2577537660', 'W2911147930', 'W2963977107', 'W3037265734', 'W3146697253'], 'abstract': 'The brms package implements Bayesian multilevel models in R using the probabilistic programming language Stan. A wide range of distributions and link functions are supported, allowing users to fit  -  among others  -  linear, robust linear, binomial, Poisson, survival, ordinal, zero-inflated, hurdle, and even non-linear models all in a multilevel context. Further modeling options include autocorrelation of the response variable, user defined covariance structures, censored data, as well as meta-analytic standard errors. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. In addition, model fit can easily be assessed and compared with the Watanabe-Akaike information criterion and leave-one-out cross-validation.', 'counts_by_year': [[2022, 706], [2021, 861], [2020, 559], [2019, 323], [2018, 112], [2017, 21]]}, {'id': 'W2432815617', 'doi': 'https://doi.org/10.1093/bioinformatics/btw354', 'title': 'MultiQC: summarize analysis results for multiple tools and samples in a single report', 'type': 'journal-article', 'publication_date': '2016-10-01', 'host_venue': 'V52395412', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A1988543389', ['I2800139495']], ['A2133601716', ['I2800139495']], ['A2096195755', ['I86987016']], ['A2303541411', ['I86987016']]], 'cited_by_count': 2586, 'concepts': [['C41008148', '0.8129877'], ['C519991488', '0.8103819'], ['C174183944', '0.6415581'], ['C124101348', '0.560216'], ['C56666940', '0.5431347']], 'referenced_works': ['W2011301426', 'W2096192437', 'W2099698839', 'W2111281047', 'W2137586531', 'W2141119617'], 'abstract': 'Fast and accurate quality control is essential for studies involving next-generation sequencing data. Whilst numerous tools exist to quantify QC metrics, there is no common approach to flexibly integrate these across tools and large sample sets. Assessing analysis results across an entire project can be time consuming and error prone; batch effects and outlier samples can easily be missed in the early stages of analysis.We present MultiQC, a tool to create a single report visualising output from multiple tools across many samples, enabling global trends and biases to be quickly identified. MultiQC can plot data from many common bioinformatics tools and is built to allow easy extension and customization.MultiQC is available with an GNU GPLv3 license on GitHub, the Python Package Index and Bioconda. Documentation and example reports are available at http://multiqc.infophil.ewels@scilifelab.se.', 'counts_by_year': [[2022, 792], [2021, 783], [2020, 505], [2019, 315], [2018, 135], [2017, 48], [2016, 5]]}, {'id': 'W2963372104', 'doi': 'https://doi.org/10.1109/cvprw.2017.151', 'title': 'Enhanced Deep Residual Networks for Single Image Super-Resolution', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2302862921', ['I139264467']], ['A2736056154', ['I139264467']], ['A2617480700', ['I139264467']], ['A2292174981', ['I139264467']], ['A2163009075', ['I139264467']]], 'cited_by_count': 2579, 'concepts': [['C155512373', '0.8716669'], ['C185798385', '0.7556269'], ['C41008148', '0.7522812'], ['C108583219', '0.74938464'], ['C81363708', '0.7368456']], 'referenced_works': ['W1910615622', 'W1930824406', 'W1976416062', 'W2047920195', 'W2049237558', 'W2053186076', 'W2088254198', 'W2098506229', 'W2103844245', 'W2118963448', 'W2121058967', 'W2121927366', 'W2157190232', 'W2172128189', 'W2194775991', 'W2214802144', 'W2242218935', 'W2263468737', 'W2476548250', 'W2534320940', 'W2739757502', 'W3105700508'], 'abstract': 'Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance. In this paper, we develop an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods. The significant performance improvement of our model is due to optimization by removing unnecessary modules in conventional residual networks. The performance is further improved by expanding the model size while we stabilize the training procedure. We also propose a new multi-scale deep super-resolution system (MDSR) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model. The proposed methods show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the NTIRE2017 Super-Resolution Challenge[26].', 'counts_by_year': [[2022, 428], [2021, 842], [2020, 703], [2019, 451], [2018, 143], [2017, 5]]}, {'id': 'W2899283552', 'doi': 'https://doi.org/10.1016/j.jcp.2018.10.045', 'title': 'Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations', 'type': 'journal-article', 'publication_date': '2019-02-01', 'host_venue': 'V148709879', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2586840170', ['I27804330']], ['A2103013942', ['I79576946']], ['A117170032', ['I27804330']]], 'cited_by_count': 2571, 'concepts': [['C158622935', '0.69387966'], ['C93779851', '0.65107954'], ['C50644808', '0.6357811'], ['C28826006', '0.5464127'], ['C135252773', '0.53750324']], 'referenced_works': ['W1019830208', 'W1528439235', 'W1856502440', 'W1968326286', 'W1981743146', 'W2027355161', 'W2051434435', 'W2076110561', 'W2110418811', 'W2137983211', 'W2194321275', 'W2239232218', 'W2261689926', 'W2465244584', 'W2507348356', 'W2525748878', 'W2534240011', 'W2573864470', 'W3101260193', 'W3102380997', 'W3105385755', 'W3105432754', 'W4247680473', 'W4255975175'], 'abstract': 'Abstract   We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.', 'counts_by_year': [[2022, 947], [2021, 1038], [2020, 462], [2019, 103]]}, {'id': 'W2950967305', 'doi': 'https://doi.org/10.1016/j.jsb.2015.11.003', 'title': 'Gctf: Real-time CTF determination and correction', 'type': 'journal-article', 'publication_date': '2016-01-01', 'host_venue': 'V89954039', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2595208761', ['I170203145']]], 'cited_by_count': 2564, 'concepts': [['C164508769', '0.7985477'], ['C41008148', '0.6533857'], ['C2777904410', '0.5564142'], ['C65293305', '0.50810695'], ['C11413529', '0.4725185']], 'referenced_works': ['W1195318580', 'W1520649887', 'W1810581179', 'W1967004359', 'W1968698743', 'W1973838566', 'W1981124257', 'W1984533345', 'W2006035995', 'W2006158980', 'W2006676401', 'W2021429932', 'W2025503045', 'W2033422581', 'W2036949137', 'W2048826637', 'W2060388198', 'W2061126550', 'W2072404529', 'W2073949924', 'W2079583250', 'W2100455255', 'W2104234755', 'W2120181445', 'W2126753599', 'W2132568091', 'W2138440954', 'W2140092984', 'W2142158597', 'W2144639425', 'W2148085578', 'W2149360962', 'W2151819406', 'W2160559764', 'W2163932369', 'W2220334305', 'W2953320394'], 'abstract': "Accurate estimation of the contrast transfer function (CTF) is critical for a near-atomic resolution cryo electron microscopy (cryoEM) reconstruction. Here, a GPU-accelerated computer program, Gctf, for accurate and robust, real-time CTF determination is presented. The main target of Gctf is to maximize the cross-correlation of a simulated CTF with the logarithmic amplitude spectra (LAS) of observed micrographs after background subtraction. Novel approaches in Gctf improve both speed and accuracy. In addition to GPU acceleration (e.g. 10-50×), a fast '1-dimensional search plus 2-dimensional refinement (1S2R)' procedure further speeds up Gctf. Based on the global CTF determination, the local defocus for each particle and for single frames of movies is accurately refined, which improves CTF parameters of all particles for subsequent image processing. Novel diagnosis method using equiphase averaging (EPA) and self-consistency verification procedures have also been implemented in the program for practical use, especially for aims of near-atomic reconstruction. Gctf is an independent program and the outputs can be easily imported into other cryoEM software such as Relion (Scheres, 2012) and Frealign (Grigorieff, 2007). The results from several representative datasets are shown and discussed in this paper.", 'counts_by_year': [[2022, 419], [2021, 593], [2020, 621], [2019, 457], [2018, 324], [2017, 115], [2016, 31], [2015, 1]]}, {'id': 'W2624989916', 'doi': 'https://doi.org/10.1109/comst.2017.2745201', 'title': 'A Survey on Mobile Edge Computing: The Communication Perspective', 'type': 'journal-article', 'publication_date': '2017-08-25', 'host_venue': 'V23688054', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2102367285', ['I200769079']], ['A2283230239', ['I889458895']], ['A2626012158', ['I200769079']], ['A2128475894', ['I889458895']], ['A349968709', ['I200769079']]], 'cited_by_count': 2542, 'concepts': [['C41008148', '0.81024635'], ['C144543869', '0.6008937'], ['C12713177', '0.5467332'], ['C2776061582', '0.54611605'], ['C95491727', '0.5256934']], 'referenced_works': ['W595252221', 'W628685110', 'W631335369', 'W914916633', 'W974042900', 'W981825776', 'W999812600', 'W1508380032', 'W1513379583', 'W1557386445', 'W1560991659', 'W1706836455', 'W1834967612', 'W1874281995', 'W1888730636', 'W1904504745', 'W1964231657', 'W1966060873', 'W1967910577', 'W1969303351', 'W1970750044', 'W1977110635', 'W1984924886', 'W1985614579', 'W1988032138', 'W1988302931', 'W1989097819', 'W1989433363', 'W1989480108', 'W1990116184', 'W1996260199', 'W2001027366', 'W2002227246', 'W2009657469', 'W2009708702', 'W2010302594', 'W2012423440', 'W2014427578', 'W2020196954', 'W2022501047', 'W2025658534', 'W2027067237', 'W2037128422', 'W2045371716', 'W2046672513', 'W2048294994', 'W2051773775', 'W2052996919', 'W2053167769', 'W2054692642', 'W2056867625', 'W2058329212', 'W2059687185', 'W2060364689', 'W2066938247', 'W2067398275', 'W2068499153', 'W2068849277', 'W2071686479', 'W2073765234', 'W2076478806', 'W2080017588', 'W2081565715', 'W2082011829', 'W2082081352', 'W2086574394', 'W2093798919', 'W2094000142', 'W2096220631', 'W2101184588', 'W2101788345', 'W2107080958', 'W2109159967', 'W2110052350', 'W2114102402', 'W2114296561', 'W2114440979', 'W2114623221', 'W2116175219', 'W2116520301', 'W2116983828', 'W2118955868', 'W2120881346', 'W2121707393', 'W2125895608', 'W2126462381', 'W2131646073', 'W2133664094', 'W2134295053', 'W2134807578', 'W2135099885', 'W2136530738', 'W2136683442', 'W2140656373', 'W2141682101', 'W2142442427', 'W2143488688', 'W2144256528', 'W2145873277', 'W2146002389', 'W2148459868', 'W2150166076', 'W2150516182', 'W2153084067', 'W2154206403', 'W2155174176', 'W2155368331', 'W2159081109', 'W2159473903', 'W2162956682', 'W2165943096', 'W2168347318', 'W2168953284', 'W2170081842', 'W2173226165', 'W2184640302', 'W2195423816', 'W2216734013', 'W2243451334', 'W2253361322', 'W2259391824', 'W2260634261', 'W2262390119', 'W2263893300', 'W2291839894', 'W2293247967', 'W2295849380', 'W2298256337', 'W2315975036', 'W2342657743', 'W2343050074', 'W2344423009', 'W2344695221', 'W2345084762', 'W2364089413', 'W2392395307', 'W2397723600', 'W2400861403', 'W2401898190', 'W2404643895', 'W2408346876', 'W2471324813', 'W2471647275', 'W2472333518', 'W2472738043', 'W2482293012', 'W2486013602', 'W2486454425', 'W2486687030', 'W2493746814', 'W2508648747', 'W2516263363', 'W2516316490', 'W2521209746', 'W2521824271', 'W2521876175', 'W2524383155', 'W2527459892', 'W2528462506', 'W2538426395', 'W2539332121', 'W2547727199', 'W2548556240', 'W2560083971', 'W2560321617', 'W2561715741', 'W2574063084', 'W2586541704', 'W2603810864', 'W2608659040', 'W2608841173', 'W2615459164', 'W2761545465', 'W2962804709', 'W2962882352', 'W2962983540', 'W2963477597', 'W2963878829', 'W2964076956', 'W2964077788', 'W2964335916', 'W3098856588', 'W3104952626', 'W3105072513', 'W3106445841', 'W3112184936', 'W3122087971', 'W3142930705', 'W4243947286', 'W4247231796'], 'abstract': 'Driven by the visions of Internet of Things and 5G communications, recent years have seen a paradigm shift in mobile computing, from the centralized mobile cloud computing toward mobile edge computing (MEC). The main feature of MEC is to push mobile computing, network control and storage to the network edges (e.g., base stations and access points) so as to enable computation-intensive and latency-critical applications at the resource-limited mobile devices. MEC promises dramatic reduction in latency and mobile energy consumption, tackling the key challenges for materializing 5G vision. The promised gains of MEC have motivated extensive efforts in both academia and industry on developing the technology. A main thrust of MEC research is to seamlessly merge the two disciplines of wireless communications and mobile computing, resulting in a wide-range of new designs ranging from techniques for computation offloading to network architectures. This paper provides a comprehensive survey of the state-of-the-art MEC research with a focus on joint radio-and-computational resource management. We also discuss a set of issues, challenges, and future research directions for MEC research, including MEC system deployment, cache-enabled MEC, mobility management for MEC, green MEC, as well as privacy-aware MEC. Advancements in these directions will facilitate the transformation of MEC from theory to practice. Finally, we introduce recent standardization efforts on MEC as well as some typical MEC application scenarios.', 'counts_by_year': [[2022, 448], [2021, 653], [2020, 660], [2019, 503], [2018, 259], [2017, 14], [2016, 1], [2012, 1]]}, {'id': 'W2578528809', 'doi': 'https://doi.org/10.1038/nature21349', 'title': 'Elements of cancer immunity and the cancer–immune set point', 'type': 'journal-article', 'publication_date': '2017-01-19', 'host_venue': 'V137773608', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2262740105', ['I183934855']], ['A2216573318', ['I183934855']]], 'cited_by_count': 2530, 'concepts': [['C8891405', '0.75936186'], ['C2779341262', '0.7402244'], ['C2779343474', '0.7113077'], ['C121608353', '0.6392882'], ['C2777701055', '0.6306014']], 'referenced_works': ['W1536532709', 'W1585125753', 'W1606491212', 'W1806981800', 'W1938515409', 'W1940241680', 'W1944707476', 'W1964598084', 'W1966801715', 'W1971208896', 'W1971254854', 'W1980137951', 'W1982488143', 'W1984830992', 'W1985594917', 'W1985852759', 'W1994416926', 'W1994993231', 'W1996863944', 'W2006759360', 'W2010121426', 'W2011458181', 'W2015767395', 'W2016983859', 'W2021472146', 'W2028433016', 'W2028789734', 'W2033577259', 'W2037342985', 'W2039123767', 'W2042699075', 'W2044363258', 'W2044835625', 'W2049553585', 'W2050407789', 'W2054965969', 'W2059128083', 'W2060469376', 'W2062398277', 'W2062889582', 'W2063264144', 'W2066671159', 'W2068868177', 'W2071255079', 'W2073924079', 'W2076948707', 'W2077116159', 'W2077717095', 'W2079018735', 'W2079696222', 'W2086834665', 'W2094366129', 'W2094481139', 'W2095242372', 'W2096385955', 'W2097855813', 'W2097995306', 'W2101637500', 'W2101653483', 'W2101912928', 'W2102994107', 'W2103066464', 'W2103876938', 'W2104885590', 'W2105359173', 'W2107321795', 'W2108706633', 'W2109485845', 'W2110957620', 'W2111217091', 'W2112378746', 'W2120813964', 'W2122495232', 'W2123739134', 'W2124427232', 'W2124632458', 'W2124837959', 'W2127734125', 'W2130819559', 'W2132923173', 'W2135530652', 'W2136731993', 'W2137158234', 'W2143772132', 'W2146810078', 'W2148564980', 'W2151246207', 'W2155615155', 'W2157203934', 'W2160834915', 'W2162222767', 'W2164221986', 'W2165456770', 'W2166662937', 'W2168020604', 'W2171352366', 'W2173303337', 'W2175632373', 'W2185075416', 'W2210453860', 'W2233818050', 'W2275606382', 'W2289712604', 'W2289851792', 'W2291298003', 'W2292383635', 'W2293531514', 'W2301131921', 'W2338576144', 'W2339442385', 'W2402433991', 'W2417857349', 'W2463547628', 'W2465860350', 'W2497442705', 'W2501034558', 'W2506616866', 'W2518431739', 'W2560367415', 'W2572174216', 'W2625718262'], 'abstract': "Immunotherapy is proving to be an effective therapeutic approach in a variety of cancers. But despite the clinical success of antibodies against the immune regulators CTLA4 and PD-L1/PD-1, only a subset of people exhibit durable responses, suggesting that a broader view of cancer immunity is required. Immunity is influenced by a complex set of tumour, host and environmental factors that govern the strength and timing of the anticancer response. Clinical studies are beginning to define these factors as immune profiles that can predict responses to immunotherapy. In the context of the cancer-immunity cycle, such factors combine to represent the inherent immunological status - or 'cancer-immune set point' - of an individual.", 'counts_by_year': [[2022, 581], [2021, 609], [2020, 553], [2019, 376], [2018, 304], [2017, 105]]}, {'id': 'W2593768305', 'doi': 'https://doi.org/10.1109/cvpr.2017.316', 'title': 'Adversarial Discriminative Domain Adaptation', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2120688692', ['I95457486']], ['A2129908635', ['I97018004']], ['A1580821723', ['I111088046']], ['A2174985400', ['I95457486']]], 'cited_by_count': 2522, 'concepts': [['C97931131', '0.931976'], ['C41008148', '0.7802013'], ['C37736160', '0.6922287'], ['C154945302', '0.67686045'], ['C177148314', '0.6078607']], 'referenced_works': ['W2031342017', 'W2096943734', 'W2112796928', 'W2214409633', 'W2478454054', 'W2963275094'], 'abstract': 'Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They can also improve recognition despite the presence of domain shift or dataset bias: recent adversarial approaches to unsupervised domain adaptation reduce the difference between the training and test domain distributions and thus improve generalization performance. However, while generative adversarial networks (GANs) show compelling visualizations, they are not optimal on discriminative tasks and can be limited to smaller shifts. On the other hand, discriminative approaches can handle larger domain shifts, but impose tied weights on the model and do not exploit a GAN-based loss. In this work, we first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and use this generalized view to better relate prior approaches. We then propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard domain adaptation tasks as well as a difficult cross-modality object classification task.', 'counts_by_year': [[2022, 289], [2021, 747], [2020, 725], [2019, 574], [2018, 170], [2017, 17]]}, {'id': 'W2305278139', 'doi': 'https://doi.org/10.1177/1049732315617444', 'title': 'Sample Size in Qualitative Interview Studies', 'type': 'journal-article', 'publication_date': '2016-07-10', 'host_venue': 'V32648145', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2140646968', ['I124055696']], ['A2119986426', ['I124055696']], ['A2101088227', ['I124055696']]], 'cited_by_count': 2518, 'concepts': [['C129848803', '0.74238104'], ['C198531522', '0.72820145'], ['C190248442', '0.6224878'], ['C133462117', '0.52056175'], ['C87156501', '0.4860116']], 'referenced_works': ['W2010266308', 'W2013335946', 'W2078904464', 'W2082605365', 'W2104000083', 'W2124890572', 'W2125462933', 'W2129660502', 'W2149285103', 'W2149466250', 'W2155332470', 'W2232983300', 'W2484418755'], 'abstract': 'Sample sizes must be ascertained in qualitative studies like in quantitative studies but not by the same means. The prevailing concept for sample size in qualitative studies is “saturation.” Saturation is closely tied to a specific methodology, and the term is inconsistently applied. We propose the concept “information power” to guide adequate sample size for qualitative studies. Information power indicates that the more information the sample holds, relevant for the actual study, the lower amount of participants is needed. We suggest that the size of a sample with sufficient information power depends on (a) the aim of the study, (b) sample specificity, (c) use of established theory, (d) quality of dialogue, and (e) analysis strategy. We present a model where these elements of information and their relevant dimensions are related to information power. Application of this model in the planning and during data collection of a qualitative study is discussed.', 'counts_by_year': [[2022, 713], [2021, 658], [2020, 505], [2019, 326], [2018, 217], [2017, 74], [2016, 22], [2014, 1]]}, {'id': 'W3005235420', 'doi': 'https://doi.org/10.1093/molbev/msaa015', 'title': 'IQ-TREE 2: New Models and Efficient Methods for Phylogenetic Inference in the Genomic Era', 'type': 'journal-article', 'publication_date': '2020-05-01', 'host_venue': 'V57552105', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2141006206', ['I118347636']], ['A2169898543', ['I2802916955']], ['A211652668', ['I2802916955']], ['A564542596', ['I2802916955', 'I106118109']], ['A2288677834', ['I129801699']], ['A261659319', ['I129774422', 'I2802916955']], ['A1595473854', ['I118347636']]], 'cited_by_count': 2516, 'concepts': [['C193252679', '0.8111058'], ['C2776214188', '0.7806883'], ['C86803240', '0.7715974'], ['C113174947', '0.6509446'], ['C53208351', '0.49877286']], 'referenced_works': ['W755194005', 'W1900937478', 'W1983102293', 'W1994597060', 'W1996835973', 'W2021600621', 'W2026551295', 'W2031611770', 'W2051161693', 'W2067136705', 'W2095523744', 'W2098448352', 'W2100030044', 'W2102424972', 'W2110059512', 'W2111211467', 'W2111647009', 'W2112956791', 'W2116099439', 'W2119900532', 'W2122082385', 'W2124790653', 'W2126917299', 'W2133870991', 'W2139146574', 'W2140689424', 'W2141052558', 'W2145562754', 'W2150270014', 'W2165379901', 'W2170996473', 'W2344940607', 'W2394958679', 'W2558419142', 'W2614081736', 'W2744971808', 'W2761939397', 'W2902439909', 'W2917207851', 'W2949367299', 'W2949867654', 'W2950009653', 'W2951464304', 'W2951691340', 'W2952045272', 'W2952596786', 'W2963276645', 'W2966384155', 'W4299995088'], 'abstract': 'Abstract IQ-TREE (http://www.iqtree.org, last accessed February 6, 2020) is a user-friendly and widely used software package for phylogenetic inference using maximum likelihood. Since the release of version 1 in 2014, we have continuously expanded IQ-TREE to integrate a plethora of new models of sequence evolution and efficient computational approaches of phylogenetic inference to deal with genomic data. Here, we describe notable features of IQ-TREE version 2 and highlight the key advantages over other software.', 'counts_by_year': [[2022, 1296], [2021, 964], [2020, 243], [2019, 2], [2018, 1], [2012, 1]]}, {'id': 'W2437571239', 'doi': 'https://doi.org/10.1103/physrevlett.116.241103', 'title': 'GW151226: Observation of Gravitational Waves from a 22-Solar-Mass Binary Black Hole Coalescence', 'type': 'journal-article', 'publication_date': '2016-06-15', 'host_venue': 'V24807848', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A3178754429', ['I122411786']], ['A2931258442', ['I122411786']], ['A2898869343', ['I121820613']], ['A2047267321', ['I181401687']], ['A1989600986', ['I131729948']], ['A2102662472', ['I33213144']], ['A3130519413', ['I122411786']], ['A2209450666', ['I2799516425']], ['A2420842909', ['I16337185']], ['A2120816080', ['I122411786']], ['A2643273335', ['I1285433949']], ['A3169239100', ['I149899117']], ['A2063771739', []], ['A2196045220', []], ['A2149021947', ['I63966007']], ['A2603756616', ['I80849659']], ['A2013867193', []], ['A3188632010', ['I59781447']], ['A2590089022', ['I11947397']], ['A3131221433', ['I149899117']], ['A3187237540', ['I108290504']], ['A1977554423', ['I118347636']], ['A2690326216', ['I122411786']], ['A2582221634', ['I43579087']], ['A2730442849', ['I122411786']], ['A3192504475', ['I122411786']], ['A1984389928', ['I368840534']], ['A3187769929', ['I142934699']], ['A2304354506', ['I102197404']], ['A2259678854', ['I19149307']], ['A2947746710', ['I116067653']], ['A2113463528', ['I43439940']], ['A3184327155', ['I149899117']], ['A2031868467', ['I122411786']], ['A2022335298', []], ['A1986532585', ['I149899117']], ['A354908839', ['I149899117']], ['A2624902964', ['I149899117']], ['A2776483237', ['I169173203']], ['A2980672605', []], ['A3037946954', ['I23732399']], ['A404213974', ['I27483092']], ['A2079318338', []], ['A2431706484', ['I70983195']], ['A3192154922', ['I122411786']], ['A2596778389', ['I7882870']], ['A2951079984', ['I122411786']], ['A2161639735', ['I1311060795']], ['A2136366644', ['I131729948']], ['A2580889767', ['I7882870']], ['A2041338530', ['I63966007']], ['A2862977381', []], ['A3190637119', ['I7597260']], ['A2106758119', ['I1311060795']], ['A2061148210', ['I78577930']], ['A2030123867', ['I97018004']], ['A2440901623', ['I108290504']], ['A2137004321', ['I1311060795']], ['A2974589133', ['I1285433949']], ['A2065817332', []], ['A2463697456', ['I138689650']], ['A2943352481', []], ['A2561527376', ['I7882870']], ['A3149347399', ['I122411786']], ['A3188724226', ['I149899117']], ['A2167365184', ['I79619799']], ['A416185457', ['I83816512']], ['A2902171395', []], ['A3187809538', ['I122411786']], ['A2294285147', ['I70983195']], ['A2222729917', ['I1289303365']], ['A2305878762', ['I19880235']], ['A2592016208', ['I122411786']], ['A2061228020', ['I122411786']], ['A2916176969', ['I868834043']], ['A252456976', ['I1285433949']], ['A1685694851', ['I63966007']], ['A2242844166', ['I1285433949']], ['A1977286476', []], ['A2784445771', ['I70983195']], ['A2191991431', ['I102197404']], ['A2146764893', ['I122411786']], ['A2573281761', ['I177877127']], ['A2110062665', ['I177877127']], ['A2479864622', ['I1311060795']], ['A3036941851', ['I145872427']], ['A2878600144', ['I149899117']], ['A2510367362', ['I114983960']], ['A3187225073', []], ['A3214559794', ['I149899117']], ['A3212008331', ['I149899117']], ['A3212603273', ['I79619799']], ['A2656910661', ['I56067802']], ['A401219682', ['I2799516425']], ['A3176056021', []], ['A2245142053', ['I122411786']], ['A2950710615', ['I108290504']], ['A2752961157', ['I59781447']], ['A2781016760', []], ['A2161572305', []], ['A3187368144', []], ['A2133056318', ['I43579087']], ['A2579247217', ['I19880235']], ['A1649922120', ['I190397597']], ['A2590013777', ['I181233156']], ['A2762264107', ['I2746051580']], ['A3213443375', []], ['A2987127593', ['I1285433949']], ['A1664647747', ['I102197404']], ['A2266672070', ['I43579087']], ['A3177457120', ['I188497080']], ['A2461687777', ['I122411786']], ['A2271853632', ['I70983195']], ['A2828582320', ['I79619799']], ['A2262935243', ['I63966007']], ['A2947399517', ['I122411786']], ['A2188286533', ['I121820613']], ['A1920643890', ['I63966007']], ['A2049455156', ['I4654613']], ['A1908833969', ['I865915315']], ['A1949674659', ['I149899117']], ['A2955044138', ['I2799516425']], ['A2989870371', []], ['A2047849238', ['I97018004']], ['A2816657102', ['I1285433949']], ['A1985904051', ['I130701444']], ['A2595927603', ['I100532134']], ['A1971896208', ['I122411786']], ['A2131239672', ['I130701444']], ['A3161920399', ['I122411786']], ['A2284239319', ['I71267560']], ['A2107701862', ['I1306266525']], ['A2946402245', ['I74801974']], ['A2119394733', ['I99065089']], ['A3208452685', ['I149899117']], ['A2633311535', []], ['A2005944322', []], ['A2958567712', ['I12315562']], ['A2253635596', ['I102197404']], ['A3183896882', ['I116067653']], ['A2853512116', ['I43579087']], ['A2428212893', ['I368840534']], ['A2215987423', ['I102197404']], ['A2134872530', []], ['A2095557066', []], ['A2593215993', ['I122411786']], ['A3012343862', ['I190397597']], ['A2258186976', ['I108290504']], ['A2993381879', ['I116067653']], ['A2552948661', ['I130769515']], ['A3081522446', ['I7882870']], ['A2293492264', ['I25846049']], ['A2158094367', ['I153230381']], ['A2355490837', []], ['A3212324755', ['I12097938']], ['A2550028113', ['I40347166']], ['A3175291849', ['I122411786']], ['A2096700839', ['I25846049']], ['A1966111217', []], ['A2026320856', []], ['A2439091654', ['I878022262']], ['A2267524564', ['I66946132']], ['A1996499331', ['I118347636']], ['A2165216277', ['I188497080']], ['A2302273048', ['I177877127']], ['A3022631697', ['I2746051580']], ['A2252018950', ['I177877127']], ['A2471669862', ['I33213144']], ['A2113263115', ['I1311060795']], ['A2104149412', ['I130701444']], ['A3192959517', []], ['A2793250799', ['I116067653']], ['A2629010976', ['I2746051580']], ['A2050847768', ['I861853513']], ['A1993090682', []], ['A2946504086', ['I158011677']], ['A2091580292', ['I80849659']], ['A2509097593', ['I861853513']], ['A2345183096', []], ['A2169864109', ['I1311060795']], ['A2466503393', ['I121820613']], ['A2343198265', ['I23732399']], ['A2104489708', ['I12315562']], ['A1997433525', []], ['A2984511297', ['I80849659']], ['A2794293094', ['I188497080']], ['A3088771138', ['I111979921']], ['A3182150140', []], ['A3015068072', ['I78577930']], ['A2781364731', ['I122411786']], ['A2252855434', ['I130701444']], ['A2008373628', ['I177877127']], ['A2017395160', ['I122411786']], ['A2578295048', ['I122411786']], ['A2868493386', ['I12315562']], ['A2574034277', ['I7882870']], ['A2277296262', ['I43579087']], ['A2324162412', ['I121820613']], ['A1964852944', ['I130238516']], ['A2144671710', ['I7882870']], ['A2577600451', ['I7882870']], ['A2002723579', []], ['A2044934989', ['I1285433949']], ['A3197011930', ['I7882870']], ['A2042635623', []], ['A1979152311', ['I149899117']], ['A2254161542', ['I165779595']], ['A3183164914', []], ['A2516309085', ['I33213144']], ['A2146738652', []], ['A2153454514', ['I1289303365']], ['A2168664669', ['I102197404']], ['A3083064634', ['I7882870']], ['A2015763309', ['I91136226']], ['A2141759182', []], ['A2828709917', ['I70983195']], ['A1988922277', ['I97018004']], ['A2462136909', ['I7597260']], ['A3010691662', ['I149899117']], ['A2304156027', ['I71267560']], ['A1418452008', ['I2746051580']], ['A2116577483', ['I79619799']], ['A437422485', ['I1285433949']], ['A2569824804', ['I149899117']], ['A2789344678', ['I122411786']], ['A2595328712', ['I122411786']], ['A2988043121', ['I122411786']], ['A2880828452', ['I16337185']], ['A2249106444', ['I12097938']], ['A678187721', ['I59781447']], ['A2977241735', ['I2802326326']], ['A2120516790', []], ['A3147297631', ['I193223587']], ['A2793899711', ['I71267560']], ['A2512693162', ['I108290504']], ['A2265221633', ['I861853513']], ['A3212506750', ['I861853513']], ['A2144142868', []], ['A2797796199', []], ['A2147618017', ['I63966007']], ['A2598058840', ['I149899117']], ['A1744413271', ['I1285433949']], ['A2424897147', ['I7882870']], ['A2305700984', ['I43579087']], ['A2241179355', ['I1285433949']], ['A2026288400', ['I122411786']], ['A2598542268', ['I1311060795']], ['A2761346687', ['I2799516425']], ['A2107317751', ['I1311060795']], ['A2317069749', ['I91136226']], ['A2565748458', ['I188497080']], ['A3197649087', ['I122411786']], ['A2778232883', ['I1285433949']], ['A2056008936', ['I122411786']], ['A3119048600', ['I33213144']], ['A3016124112', ['I33213144']], ['A2309452646', ['I122411786']], ['A2467688294', ['I63966007']], ['A2601060555', ['I122411786']], ['A2917506697', ['I63966007']], ['A2948178927', ['I122411786']], ['A2317536582', ['I130769515']], ['A1806296415', ['I78577930']], ['A2914472670', ['I116067653']], ['A2592053980', ['I70983195']], ['A2307450509', ['I79510175']], ['A2147865693', ['I99065089']], ['A2136363211', ['I177877127']], ['A2876331988', []], ['A2641626098', ['I40347166']], ['A2641626098', ['I79619799']], ['A2037446865', ['I166088655']], ['A1999419386', ['I79510175']], ['A2773897234', ['I149899117']], ['A2149221475', ['I97018004']], ['A2609143453', ['I106118109']], ['A1833794549', ['I108290504']], ['A2116510464', ['I80849659']], ['A1998320771', []], ['A1965779292', ['I108290504']], ['A3066893051', []], ['A3103283984', []], ['A3152383050', ['I70983195']], ['A2792463995', []], ['A2681436121', ['I7882870']], ['A3213279077', ['I185261750']], ['A2591496726', []], ['A2139140847', ['I861853513']], ['A420234118', []], ['A1924538289', ['I106118109']], ['A2026851826', ['I79619799']], ['A2133367798', ['I181233156']], ['A2580419259', ['I102197404']], ['A2112263148', ['I63966007']], ['A2893006435', ['I122411786']], ['A648929643', ['I33213144']], ['A438297632', ['I122411786']], ['A3000397202', ['I368840534']], ['A2340803116', ['I98677209']], ['A2608772347', ['I27483092']], ['A2215402173', ['I59781447']], ['A2848021311', ['I71267560']], ['A2976546574', ['I64295750']], ['A2419685366', ['I1306266525']], ['A2544974243', []], ['A2466218759', ['I118430337']], ['A2953778706', []], ['A2034408762', []], ['A3196982953', ['I1289303365']], ['A2436983654', ['I227486990']], ['A2472445730', ['I2799516425']], ['A2224979328', ['I11947397']], ['A3213670258', ['I11947397']], ['A3210093540', ['I145872427']], ['A2894592520', ['I121820613']], ['A1637814842', ['I122411786']], ['A1931051767', []], ['A2978132810', ['I84475105']], ['A2265051445', ['I7882870']], ['A2588028427', ['I149899117']], ['A2120371615', ['I33213144']], ['A1598052457', ['I106118109']], ['A2128477673', ['I121820613']], ['A2267511333', ['I108290504']], ['A2021276475', ['I11947397']], ['A2524013334', ['I7882870']], ['A1968023185', ['I19880235']], ['A2116344843', ['I122411786']], ['A2430134528', []], ['A3193155114', ['I2799516425']], ['A2982931411', []], ['A2616057256', ['I149899117']], ['A3184238689', ['I66946132']], ['A1859510812', []], ['A2137278808', ['I7882870']], ['A2951105902', ['I63966007']], ['A2765051707', ['I1311060795']], ['A2614624528', ['I190397597']], ['A2470660414', ['I79619799']], ['A2134513389', ['I145872427']], ['A3018296462', ['I149899117']], ['A2895613899', ['I149899117']], ['A2460934134', ['I190397597']], ['A2488799353', ['I99065089']], ['A2580259573', ['I59781447']], ['A3037040773', []], ['A2082107602', ['I122411786']], ['A2018645323', ['I122411786']], ['A2592154046', ['I27837315']], ['A2579983090', ['I142934699']], ['A2302614570', ['I72951846']], ['A2946415084', ['I122411786']], ['A2558837474', ['I60205797']], ['A2420107187', ['I7882870']], ['A3164587974', ['I11947397']], ['A2806435661', ['I1285433949']], ['A2275643125', ['I1311060795']], ['A2776268936', ['I130769515']], ['A2332457619', ['I79510175']], ['A2765542636', ['I122411786']], ['A1977732146', ['I121820613']], ['A2157477235', ['I149899117']], ['A2503627946', ['I181401687']], ['A2042136083', ['I1285433949']], ['A2190268344', ['I7882870']], ['A2035800947', ['I33213144']], ['A2973622029', ['I79619799']], ['A1836536504', ['I7882870']], ['A2135192745', ['I155173764']], ['A2236585722', ['I2746051580']], ['A1980042119', ['I122411786']], ['A2344571799', ['I1294671590']], ['A3116516404', ['I102197404']], ['A2151255333', []], ['A2303437377', ['I7882870']], ['A2083397653', ['I7882870']], ['A2287281526', ['I7882870']], ['A2935802216', ['I155173764']], ['A396076215', ['I122411786']], ['A2042955919', ['I149899117']], ['A2046589782', ['I7882870']], ['A2138917923', []], ['A2013597997', []], ['A2517767075', ['I122411786']], ['A2067057196', ['I40347166']], ['A2149005230', ['I79510175']], ['A1920053454', ['I7882870']], ['A2138217739', ['I7882870']], ['A2595902692', ['I177877127']], ['A2966114616', ['I1285433949']], ['A2628746823', ['I25846049']], ['A2771330512', ['I157725225']], ['A2761304767', ['I102197404']], ['A2846634780', ['I84475105']], ['A2053798579', ['I50441567']], ['A2580008531', ['I7882870']], ['A1994649786', ['I122411786']], ['A2781178463', ['I1285433949']], ['A2157686660', ['I1311060795']], ['A1581467037', ['I12315562']], ['A2334165722', ['I7882870']], ['A2273340091', ['I2746051580']], ['A2763511771', ['I122411786']], ['A2137481263', ['I63966007']], ['A1997196459', ['I11947397']], ['A2651934103', ['I1311060795']], ['A2260744060', ['I2746051580']], ['A2112631292', ['I878022262']], ['A2112891699', ['I130701444']], ['A2075323785', ['I245364917']], ['A2096689809', ['I181647926']], ['A2649313790', ['I177877127']], ['A371636951', ['I50441567']], ['A2893095224', ['I121820613']], ['A470399586', ['I11947397']], ['A2129196187', ['I43439940']], ['A2468505319', ['I7882870']], ['A1995530748', []], ['A2137027242', ['I177877127']], ['A2982748272', ['I111979921']], ['A2474110466', ['I79510175']], ['A2998692115', ['I111979921']], ['A2567819330', ['I368840534']], ['A2150621295', ['I878022262']], ['A2974510088', ['I122411786']], ['A2903634466', ['I1285433949']], ['A2957347368', ['I181233156']], ['A2324032376', ['I1285433949']], ['A2763762051', ['I121820613']], ['A2775102250', ['I63966007']], ['A1971122323', ['I122411786']], ['A2169656381', []], ['A2718779615', ['I177877127']], ['A2946076134', ['I1311060795']], ['A2144846099', []], ['A2271062393', ['I185261750']], ['A2375511350', ['I50441567']], ['A2596826241', ['I70983195']], ['A707522805', ['I122411786']], ['A2332964699', ['I91136226']], ['A3213303994', ['I118430337']], ['A2150745972', ['I19880235']], ['A2963947177', []], ['A2949655747', ['I79510175']], ['A2117989385', []], ['A2973371795', []], ['A3144260476', ['I1311060795']], ['A3009430813', ['I878022262']], ['A3205387863', ['I878022262']], ['A2428479581', ['I4921948']], ['A2896102104', ['I4575257']], ['A2600239339', ['I97018004']], ['A2569516458', ['I5681781']], ['A3037258224', ['I4921948']], ['A1978017669', ['I130701444']], ['A2158194427', ['I5681781']], ['A2980724654', ['I1311060795']], ['A3137194014', ['I1311060795']], ['A2810618958', ['I111979921']], ['A2257335508', ['I159176309']], ['A2024514381', ['I33213144']], ['A2779389584', ['I1285433949']], ['A3190285517', []], ['A2114391012', ['I122411786']], ['A2431304288', ['I63966007']], ['A2002141085', ['I159176309']], ['A2586345805', ['I122411786']], ['A1685051018', ['I4654613']], ['A2590008495', ['I122411786']], ['A2816449937', ['I1285433949']], ['A2198090973', ['I149899117']], ['A2426512568', ['I99542240']], ['A2166327144', []], ['A2780020188', ['I149899117']], ['A2234325469', ['I185261750']], ['A2809203402', []], ['A2254396243', ['I25846049']], ['A387860427', []], ['A2834120332', ['I70983195']], ['A2605199855', ['I1311060795']], ['A2791156957', ['I155173764']], ['A2306815261', ['I97018004']], ['A1973573521', ['I56590836']], ['A2433897488', ['I122411786']], ['A2408956840', ['I122411786']], ['A1826541738', []], ['A2021249184', ['I149899117']], ['A2549372245', ['I7882870']], ['A2511818535', ['I99065089']], ['A2809015571', ['I4921948']], ['A2990015612', ['I4575257']], ['A2153355722', ['I139264467']], ['A3001018922', ['I7882870']], ['A3121789463', ['I70983195']], ['A2119509461', ['I193223587']], ['A2105730005', ['I1285433949']], ['A2546432156', ['I102197404']], ['A3131819562', ['I2799516425']], ['A2795157789', ['I56590836']], ['A2420593293', ['I122411786']], ['A2884995774', ['I177725633']], ['A2223239043', ['I63966007']], ['A2682071947', ['I82495205']], ['A425630834', ['I181647926']], ['A2318381896', ['I24603500']], ['A2027578991', ['I79510175']], ['A2626309292', ['I70983195']], ['A2135441924', []], ['A2953771372', ['I1294671590']], ['A1978127905', ['I122411786']], ['A2031087335', []], ['A3101521172', ['I1285433949']], ['A2615368805', ['I155173764']], ['A2849484998', ['I149899117']], ['A2826849062', ['I149899117']], ['A2984096928', ['I63966007']], ['A2131331663', ['I177877127']], ['A357350526', ['I149899117']], ['A2063177140', ['I63966007']], ['A2442311525', ['I121820613']], ['A1499064879', ['I70983195']], ['A2425517206', ['I70983195']], ['A3164034524', ['I72951846']], ['A1954864400', []], ['A3190282054', ['I1294671590']], ['A2215735041', ['I116067653']], ['A2764146561', []], ['A2426567721', ['I79619799']], ['A2530083674', ['I130238516']], ['A3170207227', ['I7882870']], ['A2973259274', ['I118347636']], ['A2440179721', ['I43579087']], ['A2209475002', []], ['A2072248821', ['I112859197']], ['A2788508077', ['I2799516425']], ['A2432693565', ['I78577930']], ['A3165376653', ['I78577930']], ['A2770679456', ['I97018004']], ['A379122203', ['I122411786']], ['A2597186433', ['I190397597']], ['A201038113', ['I1294671590']], ['A2102079205', ['I7882870']], ['A2163983367', ['I63966007']], ['A2576349919', ['I122411786']], ['A2986016436', ['I63966007']], ['A3188738125', ['I2799516425']], ['A3172795602', ['I70983195']], ['A2263492593', ['I7882870']], ['A3187908163', ['I861853513']], ['A1663768879', ['I63966007']], ['A2203868986', ['I78577930']], ['A2469800559', ['I63966007']], ['A3183609872', ['I72951846']], ['A2465526937', ['I1311060795']], ['A1973105240', ['I118347636']], ['A2125448681', ['I122411786']], ['A2591893922', ['I145610796']], ['A2580450533', ['I122411786']], ['A2121893416', ['I122411786']], ['A2607098049', ['I118347636']], ['A2993123622', ['I118347636']], ['A2650871170', ['I12097938']], ['A2976632078', ['I130769515']], ['A2975777987', ['I1285433949']], ['A406107958', []], ['A2001351290', ['I165779595']], ['A2218083907', ['I1311060795']], ['A2948494050', ['I43579087']], ['A2270695881', ['I1311060795']], ['A3170098099', []], ['A2574097560', ['I122411786']], ['A2468204009', ['I7882870']], ['A2428639699', ['I130769515']], ['A3018553469', ['I2746051580']], ['A2304030444', ['I130238516']], ['A356943668', ['I861853513']], ['A2798531098', ['I79619799']], ['A2761792207', []], ['A2910186728', ['I79619799']], ['A2070676417', ['I16285277']], ['A3088851510', ['I71267560']], ['A2596795233', ['I33213144']], ['A2118056577', ['I111979921']], ['A2562441733', ['I111979921']], ['A2634403777', ['I63966007']], ['A2780469136', ['I23732399']], ['A2012213346', []], ['A2779087887', ['I1285433949']], ['A1563263078', ['I879563668']], ['A2420206282', ['I11947397']], ['A2117529880', ['I59781447']], ['A2136639717', ['I19880235']], ['A2899302788', ['I33213144']], ['A2123327683', ['I63966007']], ['A2608603388', []], ['A2099520814', []], ['A3211526149', ['I63966007']], ['A2576892988', ['I190397597']], ['A2481071036', ['I166088655']], ['A3014210212', ['I241749']], ['A2012427454', ['I1311060795']], ['A2784806291', ['I1311060795']], ['A2082008669', ['I118430337']], ['A2148346173', ['I149899117']], ['A2952436053', ['I2799516425']], ['A2893928364', ['I149899117']], ['A2108725442', ['I33213144']], ['A2763198395', ['I79510175']], ['A3185592983', ['I11947397']], ['A2595464485', ['I43579087']], ['A3135606165', ['I118430337']], ['A2266688588', ['I59781447']], ['A2919488214', ['I122411786']], ['A1982961642', ['I5681781']], ['A2340083009', ['I78577930']], ['A2818447739', ['I7882870']], ['A2469540581', ['I33213144']], ['A2957965052', ['I116067653']], ['A2005208586', ['I861853513']], ['A2599763636', ['I127439422']], ['A1989082292', ['I24603500']], ['A2439563356', ['I145872427']], ['A2921463104', ['I122411786']], ['A2598550388', ['I83816512']], ['A2547973123', ['I27837315']], ['A2162966607', ['I7882870']], ['A2171044953', ['I118347636']], ['A2781815556', ['I1285433949']], ['A2272353480', ['I145872427']], ['A2023299128', ['I1285433949']], ['A2073685276', []], ['A2097564241', ['I122411786']], ['A3121491794', ['I118430337']], ['A2871160851', ['I70983195']], ['A2272207797', ['I1311060795']], ['A1973156962', ['I43579087']], ['A2553195835', ['I1286704778']], ['A2469545255', ['I63966007']], ['A2849436941', ['I138943879']], ['A2305605118', []], ['A3121867565', []], ['A3101351426', ['I79510175']], ['A2975609283', ['I50441567']], ['A2985611607', ['I1285433949']], ['A3214157093', ['I122411786']], ['A2579244377', ['I122411786']], ['A2108716544', ['I155173764']], ['A2579844043', ['I5681781']], ['A2056767313', ['I122411786']], ['A2104041286', ['I12315562']], ['A2976108310', ['I122996671']], ['A2780831007', ['I1289303365']], ['A2258687281', ['I181233156']], ['A2055008422', []], ['A2309663433', []], ['A2840184033', ['I159176309']], ['A2127597866', ['I25846049']], ['A2521298114', ['I111979921']], ['A1967288566', ['I79510175']], ['A2427216197', ['I1289303365']], ['A2024848060', []], ['A2316050742', []], ['A2766819460', ['I149899117']], ['A2777411521', ['I97018004']], ['A3039043906', ['I122411786']], ['A2253053690', ['I7882870']], ['A388307725', []], ['A381923915', ['I108290504']], ['A685353151', []], ['A2611783856', ['I108290504']], ['A2605182216', ['I97018004']], ['A2269450469', ['I7882870']], ['A2587175082', ['I122411786']], ['A1975865521', []], ['A2084820644', ['I70983195']], ['A2142834634', ['I122411786']], ['A2070179702', ['I143104139']], ['A2954703215', ['I122411786']], ['A2574613773', ['I111979921']], ['A2155309926', ['I185261750']], ['A2833209380', ['I7882870']], ['A2272538144', ['I861853513']], ['A3187223402', []], ['A415854352', ['I190397597']], ['A2151351881', ['I16337185']], ['A403887412', []], ['A3189702922', []], ['A2091317358', ['I16337185']], ['A2147448641', ['I7882870']], ['A2617729782', ['I43579087']], ['A395586697', ['I108290504']], ['A1654852912', []], ['A2790661800', ['I1285433949']], ['A2140290691', ['I7882870']], ['A2554014841', ['I59781447']], ['A1915066629', ['I79510175']], ['A1981690962', ['I130238516']], ['A2304879075', ['I122411786']], ['A2075222951', ['I149899117']], ['A2575409521', ['I16337185']], ['A3103146582', ['I149899117']], ['A1975441365', ['I149899117']], ['A2025654431', ['I193223587']], ['A1839436355', ['I19880235']], ['A3100080145', ['I149899117']], ['A1856306342', []], ['A2469000994', []], ['A2976081679', ['I149899117']], ['A2983684962', ['I43579087']], ['A2920643673', ['I177877127']], ['A2510288784', ['I56590836']], ['A3197230867', ['I118430337']], ['A2118448810', ['I122411786']], ['A1963716262', ['I181233156']], ['A2972396686', ['I1311060795']], ['A2614875211', ['I118347636']], ['A355872337', ['I1311060795']], ['A2306114148', ['I106118109']], ['A2134809853', ['I1289303365']], ['A2515858894', ['I1289303365']], ['A3003397437', ['I118430337']], ['A362084126', ['I861853513']], ['A2918218855', ['I149899117']], ['A1884058422', ['I108290504']], ['A3214583590', ['I116067653']], ['A3173173182', ['I142934699']], ['A2590508016', ['I1311060795']], ['A3207885158', []], ['A2098395136', []], ['A2462241501', ['I868834043']], ['A2089513116', ['I33213144']], ['A2077388583', ['I16285277']], ['A2323751444', ['I70983195']], ['A2461163460', ['I861853513']], ['A2941682954', ['I27837315']], ['A3205324612', ['I155173764']], ['A2805807800', ['I7882870']], ['A2108733834', ['I7882870']], ['A3188567202', ['I102197404']], ['A3007455155', []], ['A2878446963', ['I2799516425']], ['A2089289871', ['I122411786']], ['A2100500661', ['I181233156']], ['A2517624868', ['I118430337']], ['A2100129471', ['I131729948']], ['A2563662130', ['I16285277']], ['A2777681260', ['I122411786']], ['A2792340923', ['I46305939']], ['A2469241872', ['I7882870']], ['A2497839463', ['I149899117']], ['A354099228', []], ['A2147981533', ['I1311060795']], ['A2773400098', ['I122411786']], ['A372378086', ['I1311060795']], ['A1454296234', ['I43579087']], ['A2222685358', ['I183935753']], ['A2271697176', []], ['A2957078916', ['I122996671']], ['A3134223752', ['I1285433949']], ['A3135876474', ['I127439422']], ['A2030525815', ['I56590836']], ['A2596842744', ['I122411786']], ['A2793876488', ['I1311060795']], ['A2268325361', ['I111979921']], ['A2924289912', ['I70983195']], ['A3094318201', []], ['A2917981810', ['I79510175']], ['A1994600236', ['I70983195']], ['A2923680840', ['I27837315']], ['A2528509270', ['I1311060795']], ['A2075208945', []], ['A2256275616', ['I181233156']], ['A2131426749', ['I1285433949']], ['A2629193305', ['I1285433949']], ['A2263080307', ['I122411786']], ['A2132247024', ['I149899117']], ['A2262095931', ['I181233156']], ['A1928098798', ['I159176309']], ['A2780652161', ['I1285433949']], ['A2777934897', ['I1285433949']], ['A2170966916', ['I149899117']], ['A2473231171', ['I7882870']], ['A3002678630', ['I118347636']], ['A2304026211', ['I122411786']], ['A2868184314', ['I64295750']], ['A2837016479', []], ['A1908375455', ['I116067653']], ['A2954896656', []], ['A2267468254', ['I145872427']], ['A2311189936', ['I118347636']], ['A2610678376', ['I1311060795']], ['A2169968594', ['I111979921']], ['A2866031454', ['I1285433949']], ['A2576581198', ['I97018004']], ['A3132031979', ['I66946132']], ['A2143397848', ['I43579087']], ['A2791879238', ['I63966007']], ['A2997064189', ['I130701444']], ['A3147198915', ['I130701444']], ['A2590111591', ['I43579087']], ['A2610409491', []], ['A2683727804', ['I1311060795']], ['A2575681443', ['I80849659']], ['A2287315714', ['I122411786']], ['A3042780887', ['I1306266525']], ['A3013619970', ['I149899117']], ['A3205612424', ['I121820613']], ['A3140490383', []], ['A3165812064', ['I50441567']], ['A2059989788', ['I118347636']], ['A3159832477', ['I142934699']], ['A3021249316', ['I122411786']], ['A2973327313', ['I122411786']], ['A2311932885', []], ['A2038388809', ['I7882870']], ['A2924118733', []], ['A2029544047', ['I59781447']], ['A2225816550', []], ['A2597221171', ['I78577930']], ['A2976094966', ['I1285433949']], ['A1541186076', ['I7882870']], ['A2976816135', ['I7882870']], ['A2778070955', ['I1285433949']], ['A2428665566', ['I43579087']], ['A2591107197', ['I79619799']], ['A3083537935', ['I118430337']], ['A2580845160', ['I7882870']], ['A2762826624', []], ['A400490453', ['I190397597']], ['A2202548565', ['I188497080']], ['A2065534013', ['I19880235']], ['A2975410111', ['I879563668']], ['A3176384363', ['I122411786']], ['A3213747351', ['I102298084']], ['A2789987291', ['I165779595']], ['A2212093568', []], ['A2121561893', ['I79510175']], ['A2042879894', []], ['A2762642778', ['I84475105']], ['A2282471978', []], ['A2816315007', ['I181233156']], ['A2040393559', ['I33213144']], ['A1983407470', ['I227486990']], ['A2305489452', ['I1285433949']], ['A2995618979', ['I1285433949']], ['A2134195473', ['I122411786']], ['A2777362391', ['I1285433949']], ['A3214199245', ['I122411786']], ['A3213845937', ['I79619799']], ['A2162845826', ['I122411786']], ['A2781019202', ['I1311060795']], ['A2774501621', ['I122411786']], ['A2109240833', ['I56590836']], ['A3107409158', []], ['A2953014576', ['I79510175']], ['A443913643', ['I181647926']], ['A2460100039', ['I7882870']], ['A2594138872', ['I91136226']], ['A2115179272', ['I108290504']], ['A2317330835', ['I7882870']], ['A2129842365', ['I118430337']], ['A3139912005', ['I122411786']], ['A2256758043', ['I79619799']], ['A2478539643', ['I27483092']], ['A2087966639', ['I122411786']], ['A2209246135', ['I368840534']], ['A3095261960', ['I193223587']], ['A2271925699', ['I102064193']], ['A2135571423', ['I63966007']], ['A2258344664', []], ['A2853962417', ['I118430337']], ['A735303264', ['I60060512']], ['A355478588', ['I11947397']], ['A3212031215', ['I43579087']], ['A3118211531', ['I70983195']], ['A2087571514', ['I149899117']], ['A420791329', ['I122411786']], ['A2310694307', ['I118430337']], ['A2072883853', ['I122411786']], ['A2358827619', []], ['A3089223803', []], ['A2139670350', ['I865915315']], ['A2164731926', []], ['A2248622543', ['I70983195']], ['A2258435089', []], ['A2122667816', []], ['A2013806816', ['I7882870']], ['A2255402467', ['I138689650']], ['A2589417994', ['I122411786']], ['A1976376432', ['I7597260']], ['A1972270540', ['I63966007']], ['A2232612787', ['I79619799']], ['A515372773', []], ['A2617753731', ['I79619799']], ['A2422158808', ['I5681781']], ['A2049394008', ['I201448701']], ['A3190535392', ['I2799516425']], ['A2074258572', ['I190397597']], ['A2076617041', ['I190397597']], ['A3206956357', ['I79619799']], ['A2779629177', ['I868834043']], ['A3198816670', []], ['A2794769002', ['I63966007']], ['A2829398367', ['I70983195']], ['A1990234647', ['I27483092']], ['A1968203033', ['I1311060795']], ['A2596783131', ['I33213144']], ['A1825043761', ['I79619799']], ['A2554150283', ['I19880235']], ['A2190794208', ['I118347636']], ['A2869745026', ['I166972335']], ['A2163347761', ['I166972335']], ['A3198547128', ['I121820613']], ['A2563614108', ['I122411786']], ['A2997733971', ['I1285433949']], ['A2763702278', []], ['A2306205450', ['I79619799']], ['A3160097155', ['I79619799']], ['A2154798004', ['I99065089']], ['A3152281508', ['I177877127']], ['A2195821694', ['I118347636']], ['A2163404454', ['I1311060795']], ['A2806399133', ['I2799516425']], ['A2763069523', ['I1311060795']], ['A2763611573', []], ['A2779871521', ['I1285433949']], ['A3197332458', ['I122411786']], ['A2405613006', ['I63966007']], ['A2993470130', ['I177877127']], ['A3133253250', ['I1285433949']], ['A3102057580', ['I1285433949']], ['A2014299767', ['I1285433949']], ['A2097961538', ['I155173764']], ['A2175238087', ['I122411786']], ['A2942710400', ['I79510175']], ['A2518612237', ['I60205797']], ['A2893230900', ['I149899117']], ['A2697219847', ['I149899117']], ['A2157982768', ['I1285433949']], ['A2590603314', ['I122411786']], ['A2090992818', ['I149899117']], ['A1914572088', ['I7882870']], ['A2596947182', ['I1285433949']], ['A2793555429', ['I1311060795']], ['A2588464670', ['I7882870']], ['A2609393006', ['I1285433949']], ['A2328863242', ['I122411786']], ['A2467072921', ['I111979921']], ['A2250629068', ['I63966007']], ['A2810030146', ['I122411786']], ['A2161109715', ['I66946132']], ['A2563109565', ['I63966007']], ['A2762480273', ['I2799516425']], ['A2076993918', []], ['A2089872507', []], ['A2211178064', ['I84475105']], ['A3056523529', ['I122411786']], ['A2983522804', ['I16285277']], ['A3125218237', ['I122411786']], ['A2560496473', ['I155173764']], ['A2295400593', ['I111979921']], ['A2988171828', ['I111979921']], ['A2610356264', ['I111979921']], ['A2983436265', ['I63966007']]], 'cited_by_count': 2497, 'concepts': [['C121332964', '0.938009'], ['C190330329', '0.7617104'], ['C2780688901', '0.75448895'], ['C44870925', '0.70902604'], ['C107195408', '0.537503']], 'referenced_works': ['W1134494461', 'W1510355813', 'W1930959531', 'W1974046066', 'W1978411982', 'W1978525979', 'W1989170879', 'W1989374967', 'W1990978371', 'W1992161867', 'W1995114597', 'W1996004845', 'W1997367763', 'W2014522010', 'W2015844661', 'W2018944415', 'W2032765055', 'W2036334887', 'W2037939815', 'W2044542193', 'W2047446109', 'W2049663113', 'W2051539854', 'W2051952627', 'W2054109161', 'W2061930234', 'W2065913386', 'W2070717869', 'W2071126256', 'W2075152653', 'W2095105526', 'W2103931654', 'W2104863352', 'W2105234305', 'W2106723786', 'W2115305599', 'W2116108818', 'W2121098569', 'W2139823471', 'W2140515550', 'W2140973976', 'W2147286576', 'W2148994792', 'W2152377128', 'W2153135464', 'W2162520084', 'W2169650135', 'W2198581161', 'W2214545089', 'W2245922284', 'W2252795400', 'W2254226187', 'W2256095203', 'W2256216577', 'W2266202083', 'W2273686191', 'W2273772273', 'W2273892807', 'W2277521171', 'W2277666539', 'W2277737850', 'W2339792708', 'W3098069234', 'W3100017147', 'W3100839224', 'W3104489202', 'W3105874581', 'W3106384049', 'W3106401843', 'W4234856688'], 'abstract': 'We report the observation of a gravitational-wave signal produced by the coalescence of two stellar-mass black holes. The signal, GW151226, was observed by the twin detectors of the Laser Interferometer Gravitational-Wave Observatory (LIGO) on December 26, 2015 at 03:38:53 UTC. The signal was initially identified within 70 s by an online matched-filter search targeting binary coalescences. Subsequent off-line analyses recovered GW151226 with a network signal-to-noise ratio of 13 and a significance greater than 5 $\\sigma$. The signal persisted in the LIGO frequency band for approximately 1 s, increasing in frequency and amplitude over about 55 cycles from 35 to 450 Hz, and reached a peak gravitational strain of $3.4_{-0.9}^{+0.7} \\times 10^{-22}$. The inferred source-frame initial black hole masses are $14.2_{-3.7}^{+8.3} M_{\\odot}$ and $7.5_{-2.3}^{+2.3} M_{\\odot}$ and the final black hole mass is $20.8_{-1.7}^{+6.1} M_{\\odot}$. We find that at least one of the component black holes has spin greater than 0.2. This source is located at a luminosity distance of $440_{-190}^{+180}$ Mpc corresponding to a redshift $0.09_{-0.04}^{+0.03}$. All uncertainties define a 90 % credible interval. This second gravitational-wave observation provides improved constraints on stellar populations and on deviations from general relativity.', 'counts_by_year': [[2022, 126], [2021, 280], [2020, 307], [2019, 447], [2018, 619], [2017, 553], [2016, 161], [2015, 1], [2013, 1], [2012, 1]]}, {'id': 'W2295124130', 'doi': 'https://doi.org/10.1098/rsta.2015.0202', 'title': 'Principal component analysis: a review and recent developments', 'type': 'journal-article', 'publication_date': '2016-04-13', 'host_venue': 'V99517592', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A1984535706', ['I23923803']], ['A2080255396', ['I141596103', 'I137805890']]], 'cited_by_count': 2489, 'concepts': [['C27438332', '0.91659427'], ['C2781067378', '0.8570858'], ['C75553542', '0.6710065'], ['C111030470', '0.66615516'], ['C41008148', '0.6146845']], 'referenced_works': ['W158759942', 'W391578156', 'W1513013675', 'W1523985187', 'W1975900269', 'W1994938797', 'W1998528700', 'W1999773372', 'W2011430124', 'W2033728423', 'W2034024435', 'W2034400748', 'W2041465237', 'W2057367433', 'W2061473054', 'W2071128523', 'W2084871150', 'W2091691993', 'W2097714737', 'W2098235563', 'W2098290597', 'W2104146802', 'W2113600901', 'W2133178176', 'W2140296035', 'W2145962650', 'W2169546127', 'W2294798173', 'W2323312366', 'W2489822048', 'W2503993098', 'W2615253071', 'W3106160359', 'W4205806204', 'W4237838738'], 'abstract': 'Large datasets are increasingly common and are often difficult to interpret. Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance. Finding such new variables, the principal components, reduces to solving an eigenvalue/eigenvector problem, and the new variables are defined by the dataset at hand, not a priori , hence making PCA an adaptive data analysis technique. It is adaptive in another sense too, since variants of the technique have been developed that are tailored to various different data types and structures. This article will begin by introducing the basic ideas of PCA, discussing what it can and cannot do. It will then describe some variants of PCA and their application.', 'counts_by_year': [[2022, 689], [2021, 793], [2020, 530], [2019, 281], [2018, 117], [2017, 59], [2016, 15]]}, {'id': 'W2803664483', 'doi': 'https://doi.org/10.1093/nar/gky310', 'title': 'MetaboAnalyst 4.0: towards more transparent and integrative metabolomics analysis', 'type': 'journal-article', 'publication_date': '2018-07-02', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2575647863', ['I5023651']], ['A2173024314', ['I5023651']], ['A2767389256', ['I154425047']], ['A2082545160', ['I5023651']], ['A2764092285', ['I150468666']], ['A2103626381', ['I5023651']], ['A2118994065', ['I154425047']], ['A2162961539', ['I5023651']]], 'cited_by_count': 2482, 'concepts': [['C21565614', '0.5496786'], ['C116834253', '0.516756'], ['C41008148', '0.45231998'], ['C86803240', '0.30770266'], ['C60644358', '0.24181253']], 'referenced_works': ['W1480161928', 'W1566831845', 'W1629202041', 'W1976327004', 'W1979597489', 'W1982185581', 'W1982390234', 'W1989299822', 'W1992549770', 'W1997062226', 'W1998712640', 'W2004694842', 'W2025944550', 'W2079529928', 'W2081277484', 'W2084375915', 'W2097185378', 'W2106770586', 'W2110350344', 'W2123055140', 'W2130410032', 'W2134469465', 'W2135607659', 'W2140611297', 'W2145265944', 'W2146488171', 'W2149309843', 'W2167540852', 'W2177317049', 'W2179544415', 'W2277322264', 'W2301389244', 'W2336169222', 'W2341820027', 'W2411272983', 'W2502710489', 'W2507788101', 'W2517051411', 'W2557169014', 'W2559588208', 'W2584556111', 'W2593402511', 'W2600252353', 'W2608815200', 'W2767683865', 'W2770707670', 'W2781939187', 'W2789599157'], 'abstract': "We present a new update to MetaboAnalyst (version 4.0) for comprehensive metabolomic data analysis, interpretation, and integration with other omics data. Since the last major update in 2015, MetaboAnalyst has continued to evolve based on user feedback and technological advancements in the field. For this year's update, four new key features have been added to MetaboAnalyst 4.0, including: (1) real-time R command tracking and display coupled with the release of a companion MetaboAnalystR package; (2) a MS Peaks to Pathways module for prediction of pathway activity from untargeted mass spectral data using the mummichog algorithm; (3) a Biomarker Meta-analysis module for robust biomarker identification through the combination of multiple metabolomic datasets and (4) a Network Explorer module for integrative analysis of metabolomics, metagenomics, and/or transcriptomics data. The user interface of MetaboAnalyst 4.0 has been reengineered to provide a more modern look and feel, as well as to give more space and flexibility to introduce new functions. The underlying knowledgebases (compound libraries, metabolite sets, and metabolic pathways) have also been updated based on the latest data from the Human Metabolome Database (HMDB). A Docker image of MetaboAnalyst is also available to facilitate download and local installation of MetaboAnalyst. MetaboAnalyst 4.0 is freely available at http://metaboanalyst.ca.", 'counts_by_year': [[2022, 378], [2021, 718], [2020, 806], [2019, 508], [2018, 69]]}, {'id': 'W2950595506', 'doi': 'https://doi.org/10.1093/nar/gky1055', 'title': 'The Gene Ontology Resource: 20 years and still GOing strong', 'type': 'journal-article', 'publication_date': '2019-01-08', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2114289465', ['I148283060']], ['A2750377352', ['I148283060']], ['A2251287563', ['I148283060']], ['A3207549578', ['I148283060']], ['A2149860172', ['I148283060']], ['A2103470418', ['I148283060']], ['A575195435', ['I148283060']], ['A3193022397', ['I111979921']], ['A2034093627', []], ['A2152918853', ['I111979921']], ['A2971163615', ['I111979921']], ['A1989027375', ['I111979921']], ['A2104490322', ['I1174212']], ['A2039311181', ['I1174212']], ['A2900412192', ['I1174212']], ['A2970483955', ['I1174212']], ['A2963603248', ['I1174212']], ['A2171473469', ['I1174212']], ['A2970225809', ['I1174212']], ['A2898929944', ['I1174212']], ['A2572891903', ['I91045830']], ['A2809875347', ['I91045830']], ['A2113397775', ['I91045830']], ['A2295538888', ['I241749']], ['A2030088619', ['I241749']], ['A2519465724', ['I241749']], ['A2971244193', ['I241749']], ['A1991820280', ['I241749']], ['A2970804477', ['I241749']], ['A2569344664', ['I241749']], ['A2005847006', ['I241749']], ['A2548156007', ['I241749']], ['A270136109', ['I241749']], ['A2970219518', ['I136199984']], ['A2094915095', ['I136199984']], ['A2427629602', ['I136199984']], ['A2128145532', ['I136199984']], ['A2128227068', ['I592451']], ['A2572802478', ['I592451']], ['A2515388064', ['I592451']], ['A2225386764', []], ['A2970489512', []], ['A2046771686', []], ['A228665215', []], ['A2152709814', ['I204778367']], ['A2625880200', ['I204778367']], ['A2970403116', ['I204778367']], ['A2098528895', ['I145872427']], ['A2282936248', ['I45129253']], ['A1941215654', ['I45129253']], ['A2602281274', ['I45129253']], ['A2185425330', ['I45129253']], ['A2806517773', ['I45129253']], ['A2970656549', ['I45129253']], ['A2970497270', ['I45129253']], ['A2970064242', ['I45129253']], ['A2971140140', ['I45129253']], ['A1580516435', ['I126744593']], ['A2158970578', ['I126744593']], ['A2107371585', ['I126744593']], ['A2606782483', ['I126744593']], ['A2011898283', []], ['A2970627451', []], ['A752721745', []], ['A2027712896', []], ['A2105588655', []], ['A2065847026', []], ['A2799992048', []], ['A2970141683', []], ['A2098987889', []], ['A2120643991', []], ['A2195817632', []], ['A2109762330', []], ['A2097871067', []], ['A134561875', []], ['A2240988444', []], ['A1966188278', []], ['A2306958208', []], ['A2950380766', []], ['A2134030691', []], ['A2312115845', []], ['A2120243547', []], ['A2123787574', ['I241749']], ['A2141347178', ['I241749']], ['A2405965528', ['I241749']], ['A2144585081', ['I241749']], ['A2316060410', ['I2801081054']], ['A1971305419', ['I45129253']], ['A1978084368', ['I45129253']], ['A2907158543', ['I204308271']], ['A2971158461', ['I204308271']], ['A2048467408', ['I204308271']], ['A2655974606', ['I204308271']], ['A1997429610', ['I204308271']], ['A2131585879', ['I204308271']], ['A1985627787', ['I204308271']], ['A2111931728', ['I204308271']], ['A2468387294', ['I57206974']], ['A2166835320', ['I57206974']], ['A2003054790', ['I114027177']], ['A2974996217', ['I97018004']], ['A1985121917', ['I97018004']], ['A2970067126', ['I97018004']], ['A2134652321', ['I97018004']], ['A2149777856', ['I97018004']], ['A2970906106', ['I97018004']], ['A1996463597', ['I97018004']], ['A2546795093', ['I97018004']], ['A2341712383', ['I97018004']], ['A2123600478', ['I97018004']], ['A3049500420', ['I97018004']], ['A2484566621', ['I97018004']], ['A2970178968', ['I97018004']], ['A1992372804', ['I97018004']], ['A1221497429', ['I97018004']], ['A2100007277', ['I97018004']], ['A2097127287', ['I97018004']], ['A1982239957', ['I12708293']], ['A2419836275', ['I12708293']], ['A2970641288', []], ['A345007406', []], ['A2051054773', []], ['A2158643951', []], ['A201055255', []], ['A3207109021', ['I63190737']], ['A2970231912', ['I63190737']], ['A2971352950', ['I63190737']], ['A2970693706', ['I63190737']], ['A2120643991', ['I63190737']], ['A2970540172', ['I63190737']], ['A3128654300', ['I63190737']], ['A2970301252', ['I63190737']], ['A2970489937', ['I63190737']], ['A2970213480', ['I63190737']], ['A2977353792', ['I63190737']], ['A2971247442', ['I63190737']], ['A2970938113', ['I63190737']], ['A2970830488', ['I63190737']], ['A2970782301', ['I63190737']], ['A2970888512', ['I63190737']], ['A2970484720', ['I63190737']], ['A2729731401', ['I63190737']], ['A2970989281', ['I63190737']], ['A3045987692', ['I63190737']], ['A3111087856', ['I63190737']], ['A2970423427', ['I63190737']], ['A2970772038', ['I63190737']], ['A3158465692', ['I63190737']], ['A2970484192', ['I63190737']], ['A2970453655', ['I63190737']], ['A2970475055', ['I63190737']], ['A2970290789', ['I63190737']], ['A2971104482', ['I63190737']], ['A2971031268', ['I63190737']], ['A2970288419', ['I63190737']], ['A3207103092', ['I63190737']], ['A2137837491', ['I63190737']], ['A2971311909', ['I63190737']], ['A3206402610', ['I63190737']], ['A2971264486', ['I63190737']], ['A3188441460', ['I63190737']], ['A2971062535', ['I63190737']], ['A2971161258', ['I63190737']], ['A2971089376', ['I63190737']], ['A2970774453', ['I63190737']], ['A2971254257', ['I63190737']], ['A2970313165', ['I63190737']], ['A2970152132', ['I63190737']], ['A2971037125', ['I63190737']], ['A2970127911', ['I63190737']], ['A2970541986', ['I63190737']], ['A2971171138', ['I63190737']], ['A2970202591', ['I63190737']], ['A2232118364', ['I63190737']], ['A2970167167', ['I63190737']], ['A2163884142', []], ['A2970096241', []], ['A2519984035', []], ['A1979332326', []], ['A2971082308', []], ['A2152099196', []], ['A2104912896', []], ['A2098377754', []], ['A2190004771', []], ['A2212667938', []], ['A2106328922', []], ['A2280075228', []], ['A2095792263', []], ['A2128134571', []], ['A2103005038', ['I181233156']], ['A2157287673', ['I181233156']], ['A2090949039', ['I181233156']]], 'cited_by_count': 2468, 'concepts': [['C86803240', '0.6615993'], ['C206345919', '0.62589175'], ['C14036430', '0.5980236'], ['C2987395477', '0.5843159'], ['C25810664', '0.5711311']], 'referenced_works': ['W1806630989', 'W1935434993', 'W2100980426', 'W2103260426', 'W2109899437', 'W2112589398', 'W2116034137', 'W2142678031', 'W2148084209', 'W2261812172', 'W2543238710', 'W2546530173', 'W2547565465', 'W2557399158', 'W2604808360', 'W2767088268', 'W2770091635', 'W2786189816', 'W2788534090', 'W2788752194', 'W2803427942', 'W2898458979', 'W2950894235', 'W4233586970', 'W4233698560'], 'abstract': "The Gene Ontology resource (GO; http://geneontology.org) provides structured, computable knowledge regarding the functions of genes and gene products. Founded in 1998, GO has become widely adopted in the life sciences, and its contents are under continual improvement, both in quantity and in quality. Here, we report the major developments of the GO resource during the past two years. Each monthly release of the GO resource is now packaged and given a unique identifier (DOI), enabling GO-based analyses on a specific release to be reproduced in the future. The molecular function ontology has been refactored to better represent the overall activities of gene products, with a focus on transcription regulator activities. Quality assurance efforts have been ramped up to address potentially out-of-date or inaccurate annotations. New evidence codes for high-throughput experiments now enable users to filter out annotations obtained from these sources. GO-CAM, a new framework for representing gene function that is more expressive than standard GO annotations, has been released, and users can now explore the growing repository of these models. We also provide the 'GO ribbon' widget for visualizing GO annotations to a gene; the widget can be easily embedded in any web page.", 'counts_by_year': [[2022, 481], [2021, 797], [2020, 917], [2019, 270], [2018, 2]]}, {'id': 'W2134305330', 'doi': 'https://doi.org/10.1073/pnas.1602413113', 'title': 'Cluster failure: Why fMRI inferences for spatial extent have inflated false-positive rates', 'type': 'journal-article', 'publication_date': '2016-07-12', 'host_venue': 'V125754415', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2427555444', ['I102134673']], ['A2112294658', ['I39555362']], ['A1996709181', ['I102134673']]], 'cited_by_count': 2462, 'concepts': [['C193244246', '0.7858331'], ['C64869954', '0.7165691'], ['C58693492', '0.6062683'], ['C183905921', '0.5688591'], ['C112789634', '0.54560065']], 'referenced_works': ['W1967629764', 'W1976954310', 'W1983273559', 'W1987509220', 'W1988193785', 'W1994436732', 'W2008607322', 'W2018102971', 'W2024729467', 'W2039570780', 'W2044634376', 'W2050717100', 'W2052337396', 'W2052341330', 'W2053239079', 'W2066641849', 'W2071190900', 'W2072522618', 'W2074315794', 'W2078524519', 'W2084942536', 'W2095182071', 'W2102121277', 'W2104524472', 'W2113620950', 'W2113916316', 'W2117101773', 'W2117140276', 'W2119848633', 'W2126693856', 'W2128521145', 'W2138790588', 'W2144981148', 'W2145431709', 'W2145680887', 'W2150469595', 'W2150903265', 'W2155513557', 'W2169428430', 'W2169463832', 'W2171652727', 'W2977883299', 'W3184801616', 'W4243685837'], 'abstract': 'Significance Functional MRI (fMRI) is 25 years old, yet surprisingly its most common statistical methods have not been validated using real data. Here, we used resting-state fMRI data from 499 healthy controls to conduct 3 million task group analyses. Using this null data with different experimental designs, we estimate the incidence of significant results. In theory, we should find 5% false positives (for a significance threshold of 5%), but instead we found that the most common software packages for fMRI analysis (SPM, FSL, AFNI) can result in false-positive rates of up to 70%. These results question the validity of a number of fMRI studies and may have a large impact on the interpretation of weakly significant neuroimaging results.', 'counts_by_year': [[2022, 186], [2021, 346], [2020, 370], [2019, 462], [2018, 512], [2017, 496], [2016, 87], [2015, 1], [2014, 1]]}, {'id': 'W3101479050', 'doi': 'https://doi.org/10.1038/s41586-019-1666-5', 'title': 'Quantum supremacy using a programmable superconducting processor', 'type': 'journal-article', 'publication_date': '2019-10-23', 'host_venue': 'V137773608', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2979388321', []], ['A2745692253', []], ['A687077366', []], ['A2154881235', []], ['A2066476036', []], ['A2048918837', []], ['A2135397521', []], ['A202787524', []], ['A1691380916', []], ['A2979863458', []], ['A2637902202', []], ['A2428318826', []], ['A2128566827', []], ['A693119210', []], ['A2979728325', []], ['A2981372574', []], ['A2037753533', []], ['A2161618647', []], ['A2582798817', []], ['A2123297389', []], ['A2806171682', []], ['A2950621494', []], ['A2718987920', []], ['A2981988637', []], ['A2981677601', []], ['A2117782720', []], ['A2778846226', []], ['A3003486721', []], ['A3211432309', []], ['A2747056463', []], ['A1772380555', []], ['A2305351692', []], ['A2951261647', []], ['A2308708173', []], ['A2779085400', []], ['A2527925175', []], ['A2481757337', []], ['A1987885433', []], ['A1989577753', []], ['A2171244792', []], ['A2955182425', []], ['A2979413663', []], ['A2981397203', []], ['A2112379496', []], ['A180412069', []], ['A2479109033', []], ['A2005649937', []], ['A2918107174', []], ['A2077367134', []], ['A2299267868', []], ['A2089150609', []], ['A2151547598', []], ['A3101761151', []], ['A2707907125', []], ['A2792425482', []], ['A2760814594', []], ['A2491180371', []], ['A2981445283', []], ['A2579471798', []], ['A2116698004', []], ['A2846403092', []], ['A2941454245', []], ['A2169770857', []], ['A2112890422', []], ['A2028454155', []], ['A2644729087', []], ['A1712229757', []], ['A2599566518', []], ['A2997462704', []], ['A1985457869', []], ['A2662726326', []], ['A3131172212', []], ['A3005271560', []], ['A2980040937', []], ['A2980081033', []], ['A733516151', []], ['A2119670038', []]], 'cited_by_count': 2444, 'concepts': [['C2777146004', '0.8400841'], ['C88006597', '0.72106546'], ['C2776436953', '0.5394666'], ['C84114770', '0.5055454'], ['C41008148', '0.5018265']], 'referenced_works': ['W1530691225', 'W1532834996', 'W1590419653', 'W1963734567', 'W1965628368', 'W1968850365', 'W1988808284', 'W1993292204', 'W1993688167', 'W1999706070', 'W2002372750', 'W2016321815', 'W2020018917', 'W2023727353', 'W2030867075', 'W2037354050', 'W2038356993', 'W2052891002', 'W2066500891', 'W2068163719', 'W2092458348', 'W2160661582', 'W2161685427', 'W2168676717', 'W2170620327', 'W2179731956', 'W2211717363', 'W2463723514', 'W2482126025', 'W2488179145', 'W2502126115', 'W2530247960', 'W2593019179', 'W2604467189', 'W2758259983', 'W2781738013', 'W2789231139', 'W2794444783', 'W2795017070', 'W2800906530', 'W2889557380', 'W2896712926', 'W2897579646', 'W2980017474', 'W3099335517'], 'abstract': 'This is an updated version of supplementary information to accompany "Quantum supremacy using a programmable superconducting processor", an article published in the October 24, 2019 issue of Nature. The main article is freely available at https://www.nature.com/articles/s41586-019-1666-5. Summary of changes since arXiv:1910.11333v1 (submitted 23 Oct 2019): added URL for qFlex source code; added Erratum section; added Figure S41 comparing statistical and total uncertainty for log and linear XEB; new References [1,65]; miscellaneous updates for clarity and style consistency; miscellaneous typographical and formatting corrections.', 'counts_by_year': [[2022, 776], [2021, 981], [2020, 659], [2019, 25]]}, {'id': 'W2120575449', 'doi': 'https://doi.org/10.1137/141000671', 'title': 'Julia: A Fresh Approach to Numerical Computing', 'type': 'journal-article', 'publication_date': '2017-02-07', 'host_venue': 'V160107561', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2108918533', []], ['A2472388376', []], ['A2473177951', []], ['A2137361932', []]], 'cited_by_count': 2442, 'concepts': [['C41008148', '0.5501526'], ['C459310', '0.35254502'], ['C28826006', '0.3305611'], ['C33923547', '0.2641948']], 'referenced_works': ['W1982582946', 'W1988425770', 'W2028713366', 'W2079474739', 'W2105198154', 'W2168242361'], 'abstract': 'Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast and questions notions generally held to be “laws of nature by practitioners of numerical computing: \\beginlist \\item High-level dynamic programs have to be slow. \\item One must prototype in one language and then rewrite in another language for speed or deployment. \\item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. \\endlist We introduce the Julia programming language and its design---a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after dif...', 'counts_by_year': [[2022, 533], [2021, 676], [2020, 533], [2019, 389], [2018, 204], [2017, 90], [2016, 10], [2015, 3], [2013, 1]]}, {'id': 'W2185564569', 'doi': 'https://doi.org/10.1016/j.jpha.2015.11.005', 'title': 'Methods for in vitro evaluating antimicrobial activity: A review', 'type': 'journal-article', 'publication_date': '2016-04-01', 'host_venue': 'V2765050281', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2764529832', ['I81605866']], ['A2609449853', ['I81605866']], ['A3205484048', ['I81605866']]], 'cited_by_count': 2441, 'concepts': [['C4937899', '0.80726886'], ['C183696295', '0.63677895'], ['C2780493024', '0.6335369'], ['C185592680', '0.58190876'], ['C188087704', '0.5372847']], 'referenced_works': ['W109280662', 'W135540365', 'W1486807157', 'W1576427887', 'W1869062106', 'W1891149729', 'W1972343132', 'W1975358903', 'W1981404071', 'W1981699940', 'W1983866749', 'W1989780701', 'W1995304732', 'W2000396521', 'W2002848763', 'W2005047318', 'W2009156161', 'W2009365107', 'W2010678726', 'W2011444187', 'W2012818739', 'W2014409681', 'W2014738223', 'W2024846774', 'W2029641265', 'W2030316650', 'W2030826638', 'W2032417358', 'W2041915819', 'W2045662593', 'W2051236163', 'W2053042399', 'W2054495423', 'W2059099956', 'W2060736646', 'W2063086831', 'W2063771413', 'W2063921870', 'W2070342011', 'W2081355108', 'W2081542706', 'W2081779552', 'W2087555148', 'W2095415622', 'W2096827639', 'W2098507707', 'W2098804733', 'W2102009379', 'W2102656675', 'W2103517584', 'W2104463629', 'W2105678060', 'W2108989058', 'W2109684694', 'W2111853444', 'W2112141450', 'W2122548201', 'W2125347310', 'W2128658496', 'W2131552016', 'W2137161479', 'W2137375011', 'W2142832600', 'W2142917899', 'W2143087972', 'W2145542366', 'W2146611080', 'W2148572274', 'W2149053252', 'W2149220964', 'W2158750459', 'W2159836659', 'W2160355967', 'W2164150789', 'W2164806808', 'W2167375423', 'W2170997336', 'W2171496709', 'W2171867168', 'W2335656257', 'W2417966066', 'W2616307005', 'W2884177588', 'W3140922844'], 'abstract': "In recent years, there has been a growing interest in researching and developing new antimicrobial agents from various sources to combat microbial resistance. Therefore, a greater attention has been paid to antimicrobial activity screening and evaluating methods. Several bioassays such as disk-diffusion, well diffusion and broth or agar dilution are well known and commonly used, but others such as flow cytofluorometric and bioluminescent methods are not widely used because they require specified equipment and further evaluation for reproducibility and standardization, even if they can provide rapid results of the antimicrobial agent's effects and a better understanding of their impact on the viability and cell damage inflicted to the tested microorganism. In this review article, an exhaustive list of in vitro antimicrobial susceptibility testing methods and detailed information on their advantages and limitations are reported.", 'counts_by_year': [[2022, 464], [2021, 599], [2020, 520], [2019, 408], [2018, 253], [2017, 155], [2016, 31], [2014, 1]]}, {'id': 'W3035524453', 'doi': 'https://doi.org/10.1109/cvpr42600.2020.00975', 'title': 'Momentum Contrast for Unsupervised Visual Representation Learning', 'type': 'proceedings-article', 'publication_date': '2020-06-14', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2164292938', ['I2252078561']], ['A2642346059', ['I2252078561']], ['A2464472631', ['I2252078561']], ['A2551560347', ['I2252078561']], ['A2473549963', ['I2252078561']]], 'cited_by_count': 2439, 'concepts': [['C154945302', '0.7388259'], ['C41008148', '0.7093883'], ['C75608658', '0.7057284'], ['C8038995', '0.67291737'], ['C2776502983', '0.61390746']], 'referenced_works': ['W219040644', 'W343636949', 'W1536680647', 'W1903029394', 'W1976921161', 'W2025768430', 'W2031489346', 'W2102605133', 'W2108598243', 'W2131846894', 'W2138621090', 'W2144794286', 'W2194775991', 'W2340897893', 'W2549139847', 'W2558661413', 'W2565639579', 'W2575671312', 'W2797977484', 'W2798991696', 'W2913939497', 'W2948672349', 'W2962824366', 'W2963016543', 'W2963150697', 'W2963420272', 'W2987741655', 'W2990873191', 'W2991391304', 'W2998388430'], 'abstract': 'We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.', 'counts_by_year': [[2022, 580], [2021, 1493], [2020, 353], [2019, 11], [2018, 1]]}, {'id': 'W2746562707', 'doi': 'https://doi.org/10.1159/000480347', 'title': "Can Treatment Adherence Be Improved by Using Rubin's Four Tendencies Framework to Understand a Patient's Response to Expectations", 'type': 'journal-article', 'publication_date': '2017-05-01', 'host_venue': 'V4210170157', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2254625331', ['I4210100347']], ['A1973390764', ['I4210100347']], ['A2748504421', ['I4210162602']], ['A2285751563', ['I4210111611']], ['A2749926064', ['I135428043']]], 'cited_by_count': 2429, 'concepts': [['C160798450', '0.6817646'], ['C27415008', '0.6166306'], ['C2779343474', '0.56498915'], ['C15744967', '0.49438214'], ['C3019806175', '0.48709774']], 'referenced_works': ['W28775834', 'W148658413', 'W189599441', 'W1236637892', 'W1480729244', 'W1536786309', 'W1539546175', 'W1547375638', 'W1594567624', 'W1622015792', 'W1651579527', 'W1948234906', 'W1964461123', 'W1967128028', 'W1967392692', 'W1968362340', 'W1970280637', 'W1972074578', 'W1974487308', 'W1978841871', 'W1979809530', 'W1982975611', 'W1983235507', 'W1985041468', 'W1986215651', 'W1986450807', 'W1986977853', 'W1987949056', 'W1988077799', 'W1991106348', 'W1991221654', 'W1992966219', 'W1994315894', 'W1996658188', 'W1998778493', 'W1999753168', 'W2002793586', 'W2005195223', 'W2008490878', 'W2011366209', 'W2011932878', 'W2016323291', 'W2016576941', 'W2017058166', 'W2019223138', 'W2019634771', 'W2020649530', 'W2021604300', 'W2024856457', 'W2024980521', 'W2028447469', 'W2029915685', 'W2030271226', 'W2030407036', 'W2032169170', 'W2032217963', 'W2034039469', 'W2034932164', 'W2035304784', 'W2035471619', 'W2035499945', 'W2037655103', 'W2037732769', 'W2038693293', 'W2039555502', 'W2040106495', 'W2041713396', 'W2042041854', 'W2043561880', 'W2046080025', 'W2046323884', 'W2047245093', 'W2052810066', 'W2055966099', 'W2058188004', 'W2059476796', 'W2061320251', 'W2061906064', 'W2063639182', 'W2067111100', 'W2067524677', 'W2067967005', 'W2070835554', 'W2071222708', 'W2072773013', 'W2076729007', 'W2078717479', 'W2081430061', 'W2082000510', 'W2089129061', 'W2090404676', 'W2093299634', 'W2095914662', 'W2095946061', 'W2096428332', 'W2097624629', 'W2100835521', 'W2104829234', 'W2105730563', 'W2109608360', 'W2109728564', 'W2111591671', 'W2112401517', 'W2113033841', 'W2113893776', 'W2116567300', 'W2117405519', 'W2119700561', 'W2122141898', 'W2122999152', 'W2125168028', 'W2125212733', 'W2125763669', 'W2125948338', 'W2128541546', 'W2130547012', 'W2134338262', 'W2134740567', 'W2134859409', 'W2138939303', 'W2139844483', 'W2140465984', 'W2148349675', 'W2152095411', 'W2153855118', 'W2155086604', 'W2155441883', 'W2157156054', 'W2158216255', 'W2159792496', 'W2161570090', 'W2161580601', 'W2231159098', 'W2265362832', 'W2328222875', 'W2398887742', 'W2536914091', 'W2586958921', 'W2612984952', 'W2746562707', 'W4239443057'], 'abstract': "Within the context of poorer patient outcomes and rising healthcare costs, we need to better understand why many patients do not engage fully with their treatment plan. Movement away from talking about “compliance” towards “adherence” and “concordance” is evidence of a recognition that this is a two-way process. Whilst healthcare professionals expect patients to engage in treatment, equally, patients have expectations (whether positive or negative) of their treatment and their need for engagement. There is a need for an effective method that can specifically target those interventions that will provide the most benefit to individual patients and which, crucially, is easy and inexpensive to administer in everyday practice and widely applicable. Rubin's Four Tendencies model identifies a patient's “response to outer and inner expectations” as a key factor in adherence. The model therefore provides an opportunity to test such a targeted, patient-specific strategy and we present a call to action for research in this area.", 'counts_by_year': [[2022, 262], [2021, 188], [2020, 212], [2019, 309], [2018, 332], [2017, 337], [2016, 321], [2015, 254], [2014, 68], [2013, 5], [2012, 4]]}, {'id': 'W2901669506', 'doi': 'https://doi.org/10.1186/s12874-018-0611-x', 'title': 'Systematic review or scoping review? Guidance for authors when choosing between a systematic or scoping review approach', 'type': 'journal-article', 'publication_date': '2018-11-19', 'host_venue': 'V185874209', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2023216196', ['I124515143']], ['A2149778953', ['I124515143']], ['A2331239720', ['I124515143']], ['A1968983690', ['I124515143']], ['A2044735199', ['I124515143']], ['A2284053755', ['I124515143']]], 'cited_by_count': 2415, 'concepts': [['C189708586', '0.8995365'], ['C2778012447', '0.65490294'], ['C158154518', '0.5170327'], ['C539667460', '0.5054281'], ['C2779473830', '0.4423683']], 'referenced_works': ['W1703903452', 'W1803981184', 'W1911361640', 'W1952264060', 'W1974019033', 'W1980776335', 'W1981119137', 'W2019022980', 'W2030418439', 'W2063616523', 'W2067000730', 'W2067384918', 'W2068264290', 'W2071941436', 'W2075950485', 'W2079283960', 'W2084154288', 'W2085076230', 'W2091538045', 'W2091984855', 'W2092255211', 'W2098348335', 'W2103862981', 'W2105321265', 'W2122543210', 'W2126264646', 'W2127468653', 'W2129009311', 'W2134833483', 'W2146505975', 'W2146668368', 'W2156100542', 'W2162242768', 'W2163834848', 'W2191739185', 'W2253148421', 'W2261406464', 'W2294894920', 'W2411357912', 'W2525828948', 'W2550859085', 'W2783040364', 'W2787898157', 'W2884766878', 'W2891378911', 'W2892590175'], 'abstract': 'Scoping reviews are a relatively new approach to evidence synthesis and currently there exists little guidance regarding the decision to choose between a systematic review or scoping review approach when synthesising evidence. The purpose of this article is to clearly describe the differences in indications between scoping reviews and systematic reviews and to provide guidance for when a scoping review is (and is not) appropriate.Researchers may conduct scoping reviews instead of systematic reviews where the purpose of the review is to identify knowledge gaps, scope a body of literature, clarify concepts or to investigate research conduct. While useful in their own right, scoping reviews may also be helpful precursors to systematic reviews and can be used to confirm the relevance of inclusion criteria and potential questions.Scoping reviews are a useful tool in the ever increasing arsenal of evidence synthesis approaches. Although conducted for different purposes compared to systematic reviews, scoping reviews still require rigorous and transparent methods in their conduct to ensure that the results are trustworthy. Our hope is that with clear guidance available regarding whether to conduct a scoping review or a systematic review, there will be less scoping reviews being performed for inappropriate indications better served by a systematic review, and vice-versa.', 'counts_by_year': [[2022, 978], [2021, 932], [2020, 406], [2019, 95], [2018, 1]]}, {'id': 'W1987219442', 'doi': 'https://doi.org/10.21832/9781783095681-004', 'title': '3. English as a Global Language', 'type': 'book-chapter', 'publication_date': '2016-12-31', 'host_venue': 'V2764375719', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2910433791', []]], 'cited_by_count': 2388, 'concepts': [['C2987496018', '0.6506442'], ['C2780966255', '0.61373764'], ['C41895202', '0.5575397'], ['C2777382242', '0.5456023'], ['C2779343474', '0.51399606']], 'referenced_works': ['W233744722', 'W586206560', 'W587010976', 'W591689233', 'W594515912', 'W641939724', 'W642003523', 'W649966906', 'W656309685', 'W659550629', 'W1040071417', 'W1480637425', 'W1495582000', 'W1529057280', 'W1536550012', 'W1570342070', 'W1970978492', 'W1973635371', 'W1975161010', 'W1976404812', 'W1977892422', 'W1980208933', 'W1983248018', 'W1984063916', 'W1994830040', 'W2004182376', 'W2007381401', 'W2014731689', 'W2018971072', 'W2020981387', 'W2022851986', 'W2028321841', 'W2030758276', 'W2035450969', 'W2041334335', 'W2048034056', 'W2048439333', 'W2051750264', 'W2055580368', 'W2065883000', 'W2070220514', 'W2074833876', 'W2078350191', 'W2080397935', 'W2086690907', 'W2091387799', 'W2098747623', 'W2101193288', 'W2102886383', 'W2104422539', 'W2109872976', 'W2112250349', 'W2115526458', 'W2115658060', 'W2126432003', 'W2134215246', 'W2142022026', 'W2143928726', 'W2154142338', 'W2166558298', 'W2167397460', 'W2289593144', 'W2315267976', 'W2485037838', 'W2499687140', 'W2503352971', 'W2788129639', 'W2796493717', 'W2798525786', 'W2798643241', 'W2807488068', 'W2990206487', 'W3135224407'], 'abstract': 'Preface 1. Why a global language? 2. Why English? The historical context 3. Why English? The cultural foundation 4. Why English? The cultural legacy 5. The future of global English References Index List of tables.', 'counts_by_year': [[2022, 1], [2021, 92], [2020, 131], [2019, 120], [2018, 109], [2017, 116], [2016, 160], [2015, 230], [2014, 214], [2013, 210], [2012, 171]]}, {'id': 'W2962785568', 'doi': 'https://doi.org/10.1109/cvpr.2018.00068', 'title': 'The Unreasonable Effectiveness of Deep Features as a Perceptual Metric', 'type': 'proceedings-article', 'publication_date': '2018-01-11', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2167242762', ['I95457486']], ['A2077136294', ['I2747134083', 'I95457486']], ['A2088536091', ['I95457486']], ['A269985891', ['I1306409833']], ['A2121368759', ['I1306409833']]], 'cited_by_count': 2386, 'concepts': [['C26760741', '0.78736204'], ['C154945302', '0.7678356'], ['C103278499', '0.7347871'], ['C176217482', '0.7334384'], ['C41008148', '0.7290204']], 'referenced_works': ['W219040644', 'W343636949', 'W1520997877', 'W1905052409', 'W1919542679', 'W1974013408', 'W1979104110', 'W2055745001', 'W2059975159', 'W2078807908', 'W2080211902', 'W2104974755', 'W2117539524', 'W2133665775', 'W2147253850', 'W2150734399', 'W2156566307', 'W2161907179', 'W2242218935', 'W2274405424', 'W2475287302', 'W2575671312', 'W2585702364', 'W2586275201', 'W2737134362', 'W2738579427', 'W2741137940', 'W2963037581', 'W2963073614', 'W2963289467', 'W2963372104', 'W2963420272', 'W2963522749', 'W2963661589', 'W2964251418', 'W2964345931', 'W3103635814'], 'abstract': 'While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.', 'counts_by_year': [[2022, 405], [2021, 1032], [2020, 613], [2019, 288], [2018, 42], [2012, 1]]}, {'id': 'W1564897360', 'doi': 'https://doi.org/10.1201/9781315136370', 'title': 'A Mathematical Introduction to Robotic Manipulation', 'type': 'book', 'publication_date': '2017-12-14', 'host_venue': 'V4306463075', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2160665213', []], ['A2149692785', []], ['A2282478908', []]], 'cited_by_count': 2379, 'concepts': [['C41008148', '0.50975436'], ['C107457646', '0.43121505'], ['C154945302', '0.37656626']], 'referenced_works': ['W22545347', 'W129751876', 'W133767100', 'W134451811', 'W155808269', 'W185128714', 'W635503458', 'W1484515782', 'W1485633210', 'W1490678064', 'W1506846531', 'W1510918013', 'W1514432699', 'W1516079644', 'W1519963105', 'W1537482913', 'W1555601014', 'W1555993017', 'W1557450088', 'W1565212980', 'W1570598801', 'W1582353822', 'W1587069887', 'W1596615372', 'W1597009130', 'W1612551514', 'W1654807447', 'W1680323303', 'W1864838508', 'W1869418976', 'W1887006513', 'W1891021866', 'W1961716932', 'W1964373008', 'W1973363841', 'W1975382117', 'W1977186231', 'W1991803036', 'W1996630649', 'W1996660445', 'W1998439873', 'W2010704362', 'W2013833162', 'W2017444601', 'W2022592354', 'W2023285936', 'W2023398310', 'W2026892575', 'W2033623440', 'W2038533008', 'W2042038080', 'W2047993072', 'W2048517616', 'W2052996471', 'W2053471784', 'W2061631683', 'W2062110458', 'W2062691475', 'W2070678224', 'W2071355416', 'W2071572887', 'W2083143597', 'W2083376752', 'W2083483041', 'W2086983719', 'W2089482383', 'W2093013148', 'W2099761232', 'W2100134099', 'W2101196304', 'W2101915367', 'W2106737440', 'W2107156812', 'W2112474089', 'W2122443012', 'W2130676727', 'W2133361850', 'W2136621952', 'W2136628844', 'W2140173255', 'W2140998418', 'W2142517568', 'W2143769694', 'W2153525410', 'W2164067102', 'W2164528954', 'W2167133624', 'W2246023205', 'W2565505720', 'W2569641373', 'W2569769452', 'W2570268714', 'W2718457082', 'W2795963767', 'W2913248487', 'W2967492447', 'W3094547799', 'W3094663703'], 'abstract': "INTRODUCTION: Brief History. Multifingered Hands and Dextrous Manipulation. Outline of the Book. Bibliography. RIGID BODY MOTION: Rigid Body Transformations. Rotational Motion in R3. Rigid Motion in R3. Velocity of a Rigid Body. Wrenches and Reciprocal Screws. MANIPULATOR KINEMATICS: Introduction. Forward Kinematics. Inverse Kinematics. The Manipulator Jacobian. Redundant and Parallel Manipulators. ROBOT DYNAMICS AND CONTROL: Introduction. Lagrange's Equations. Dynamics of Open-Chain Manipulators. Lyapunov Stability Theory. Position Control and Trajectory Tracking. Control of Constrained Manipulators. MULTIFINGERED HAND KINEMATICS: Introduction to Grasping. Grasp Statics. Force-Closure. Grasp Planning. Grasp Constraints. Rolling Contact Kinematics. HAND DYNAMICS AND CONTROL: Lagrange's Equations with Constraints. Robot Hand Dynamics. Redundant and Nonmanipulable Robot Systems. Kinematics and Statics of Tendon Actuation. Control of Robot Hands. NONHOLONOMIC BEHAVIOR IN ROBOTIC SYSTEMS: Introduction. Controllability and Frobenius' Theorem. Examples of Nonholonomic Systems. Structure of Nonholonomic Systems. NONHOLONOMIC MOTION PLANNING: Introduction. Steering Model Control Systems Using Sinusoids. General Methods for Steering. Dynamic Finger Repositioning. FUTURE PROSPECTS: Robots in Hazardous Environments. Medical Applications for Multifingered Hands. Robots on a Small Scale: Microrobotics. APPENDICES: Lie Groups and Robot Kinematics. A Mathematica Package for Screw Calculus. Bibliography. Index Each chapter also includes a Summary, Bibliography, and Exercises", 'counts_by_year': [[2022, 38], [2021, 102], [2020, 163], [2019, 135], [2018, 110], [2017, 103], [2016, 124], [2015, 122], [2014, 113], [2013, 97], [2012, 97]]}]
[{'id': 'W2888532099', 'doi': 'https://doi.org/10.3847/1538-3881/aabc4f', 'title': 'The Astropy Project: Building an Open-science Project and Status of the v2.0 Core Package', 'type': 'journal-article', 'publication_date': '2018-08-24', 'host_venue': 'V100695177', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2076069237', ['I149899117']], ['A3098754281', ['I149899117']], ['A2185584058', ['I149899117']], ['A3011844608', ['I149899117']], ['A2164082912', ['I149899117']], ['A2987025019', ['I149899117']], ['A3035776718', ['I149899117']], ['A3011822651', ['I149899117']], ['A3011970472', ['I149899117']], ['A2299776155', ['I149899117']], ['A2050754209', ['I149899117']], ['A2107568166', ['I149899117']], ['A2216220436', ['I149899117']], ['A618688186', ['I149899117']], ['A3012199075', ['I149899117']], ['A2127110638', ['I149899117']], ['A2888588487', ['I149899117']], ['A3019204785', ['I149899117']], ['A2888073876', ['I149899117']], ['A2898921759', ['I149899117']], ['A3011073544', ['I149899117']], ['A1499993589', ['I149899117']], ['A2888453887', ['I149899117']], ['A2139577183', ['I149899117']], ['A2069228914', ['I149899117']], ['A1310101665', ['I149899117']], ['A2828195353', ['I149899117']], ['A2171297354', ['I149899117']], ['A2888061221', ['I149899117']], ['A2191528683', ['I149899117']], ['A3195771984', ['I149899117']], ['A3011589871', ['I149899117']], ['A3208214512', ['I149899117']], ['A2888263516', ['I149899117']], ['A3011502766', ['I149899117']], ['A3106539025', ['I149899117']], ['A3011382414', ['I149899117']], ['A2888529987', ['I149899117']], ['A2888747923', ['I149899117']], ['A2888019445', ['I149899117']], ['A2106714917', ['I149899117']], ['A2888162869', ['I149899117']], ['A2311970711', ['I149899117']], ['A2337951011', ['I149899117']], ['A2119660159', ['I149899117']], ['A2888474216', ['I149899117']], ['A2302789307', ['I149899117']], ['A3012008894', ['I149899117']], ['A3012298478', ['I149899117']], ['A2524865632', ['I149899117']], ['A3012376473', ['I149899117']], ['A2808787059', ['I149899117']], ['A3010959162', ['I149899117']], ['A1966242802', ['I149899117']], ['A2139538485', ['I149899117']], ['A2888275264', ['I149899117']], ['A2888341614', ['I149899117']], ['A2495627175', ['I149899117']], ['A2888049157', ['I149899117']], ['A2549677643', ['I149899117']], ['A2693365242', ['I149899117']], ['A2689071840', ['I149899117']], ['A3011349855', ['I149899117']], ['A2888401401', ['I149899117']], ['A222145511', ['I149899117']], ['A3011264461', ['I149899117']], ['A3012520978', ['I149899117']], ['A2913765178', ['I149899117']], ['A2568179652', ['I149899117']], ['A2433270329', ['I149899117']], ['A2888058702', ['I149899117']], ['A2888551068', ['I149899117']], ['A3172704675', ['I149899117']], ['A2888119552', ['I149899117']], ['A3011142407', ['I149899117']], ['A2888660435', ['I149899117']], ['A2258560517', ['I149899117']], ['A2550731082', ['I149899117']], ['A1984752803', ['I149899117']], ['A2888135781', ['I149899117']], ['A3214455245', ['I149899117']], ['A2060775928', ['I149899117']], ['A2888740152', ['I149899117']], ['A2798520857', ['I149899117']], ['A2134252991', ['I149899117']], ['A2427649979', ['I149899117']], ['A2014308189', ['I149899117']], ['A2499628843', ['I149899117']], ['A2442311525', ['I149899117']], ['A2888676543', ['I149899117']], ['A2133179609', ['I149899117']], ['A2888054716', ['I149899117']], ['A2719301614', ['I149899117']], ['A2888606962', ['I149899117']], ['A2130082638', ['I149899117']], ['A3012071172', ['I149899117']], ['A2122961489', ['I149899117']], ['A2888500496', ['I149899117']], ['A2888347903', ['I149899117']], ['A3011045857', ['I149899117']], ['A2888537898', ['I149899117']], ['A3011549490', ['I149899117']], ['A3121867565', ['I149899117']], ['A3011586487', ['I149899117']], ['A1958634152', ['I149899117']], ['A2145991452', ['I149899117']], ['A3012375695', ['I149899117']], ['A3012228036', ['I149899117']], ['A2043510355', ['I149899117']], ['A2950487254', ['I149899117']], ['A3011689065', ['I149899117']], ['A3011422704', ['I149899117']], ['A2562354648', ['I149899117']], ['A136555400', ['I149899117']], ['A2971598034', ['I149899117']], ['A3011193342', ['I149899117']], ['A3011623836', ['I149899117']], ['A3010766499', ['I149899117']], ['A3011268301', ['I149899117']], ['A3011705484', ['I149899117']], ['A2895863058', ['I149899117']], ['A3042780887', ['I149899117']], ['A3012302398', ['I149899117']], ['A3012041599', ['I149899117']], ['A3010913770', ['I149899117']], ['A2010341489', ['I149899117']], ['A117683099', ['I149899117']], ['A2145513586', ['I149899117']], ['A2125477309', ['I149899117']], ['A2915680082', ['I149899117']], ['A3011630087', ['I149899117']], ['A2082954650', ['I149899117']], ['A2475616109', ['I149899117']], ['A2558220219', ['I149899117']], ['A3147262687', ['I149899117']], ['A2001556281', ['I149899117']], ['A3045581181', ['I149899117']], ['A3049645008', ['I149899117']], ['A3011827247', ['I149899117']]], 'cited_by_count': 2368, 'concepts': [['C519991488', '0.7369826'], ['C2164484', '0.48409712'], ['C26517878', '0.48291227'], ['C201995342', '0.4701657'], ['C110354214', '0.4680628']], 'referenced_works': ['W1620478973', 'W1910823972', 'W1974618482', 'W1981452559', 'W1986316936', 'W2008150142', 'W2062511015', 'W2065236031', 'W2066365557', 'W2077005128', 'W2093899622', 'W2104266030', 'W2105212639', 'W2121480729', 'W2135625048', 'W2146292423', 'W2281272944', 'W2496031908', 'W2766897459', 'W3099155372', 'W3104579003', 'W3105187411', 'W3105959567', 'W3106234471', 'W4211153246', 'W4229685370', 'W4292309267', 'W4292875581', 'W4300851986'], 'abstract': 'The Astropy project supports and fosters the development of open-source and openly-developed Python packages that provide commonly-needed functionality to the astronomical community. A key element of the Astropy project is the core package Astropy, which serves as the foundation for more specialized projects and packages. In this article, we provide an overview of the organization of the Astropy project and summarize key features in the core package as of the recent major release, version 2.0. We then describe the project infrastructure designed to facilitate and support development for a broader ecosystem of inter-operable packages. We conclude with a future outlook of planned new features and directions for the broader Astropy project.', 'counts_by_year': [[2022, 597], [2021, 592], [2020, 689], [2019, 423], [2018, 65]]}, {'id': 'W2268751503', 'doi': 'https://doi.org/10.1109/mcom.2016.7470933', 'title': 'Wireless communications with unmanned aerial vehicles: opportunities and challenges', 'type': 'journal-article', 'publication_date': '2016-05-18', 'host_venue': 'V158797327', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2118498887', ['I165932596']], ['A2413204075', ['I165932596']], ['A2614640898', ['I165932596']]], 'cited_by_count': 2360, 'concepts': [['C41008148', '0.7878829'], ['C555944384', '0.73768884'], ['C108037233', '0.48946917'], ['C31258907', '0.48264462'], ['C26517878', '0.4815593']], 'referenced_works': ['W1540107614', 'W1904495647', 'W2020199432', 'W2031834036', 'W2052190282', 'W2101871422', 'W2103472722', 'W2136478830', 'W3098915991', 'W4241021229'], 'abstract': 'Wireless communication systems that include unmanned aerial vehicles (UAVs) promise to provide cost-effective wireless connectivity for devices without infrastructure coverage. Compared to terrestrial communications or those based on high-altitude platforms (HAPs), on-demand wireless systems with low-altitude UAVs are in general faster to deploy, more flexibly re-configured, and are likely to have better communication channels due to the presence of short-range line-of-sight (LoS) links. However, the utilization of highly mobile and energy-constrained UAVs for wireless communications also introduces many new challenges. In this article, we provide an overview of UAV-aided wireless communications, by introducing the basic networking architecture and main channel characteristics, highlighting the key design considerations as well as the new opportunities to be exploited.', 'counts_by_year': [[2022, 281], [2021, 446], [2020, 553], [2019, 557], [2018, 395], [2017, 112], [2016, 13], [2015, 1], [2012, 1]]}, {'id': 'W2392113277', 'doi': 'https://doi.org/10.1109/access.2016.2566339', 'title': 'Blockchains and Smart Contracts for the Internet of Things', 'type': 'journal-article', 'publication_date': '2016-05-10', 'host_venue': 'V2485537415', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2431315483', ['I137902535']], ['A175482417', ['I137902535']]], 'cited_by_count': 2346, 'concepts': [['C81860439', '0.68692654'], ['C41008148', '0.6373791'], ['C38652104', '0.56911075'], ['C108827166', '0.5297875'], ['C110875604', '0.41958922']], 'referenced_works': ['W179922057', 'W1520914943', 'W1794148987', 'W2022932831', 'W2335755599', 'W2503779953', 'W3137092842'], 'abstract': 'Motivated by the recent explosion of interest around blockchains, we examine whether they make a good fit for the Internet of Things (IoT) sector. Blockchains allow us to have a distributed peer-to-peer network where non-trusting members can interact with each other without a trusted intermediary, in a verifiable manner. We review how this mechanism works and also look into smart contracts—scripts that reside on the blockchain that allow for the automation of multi-step processes. We then move into the IoT domain, and describe how a blockchain-IoT combination: 1) facilitates the sharing of services and resources leading to the creation of a marketplace of services between devices and 2) allows us to automate in a cryptographically verifiable manner several existing, time-consuming workflows. We also point out certain issues that should be considered before the deployment of a blockchain network in an IoT setting: from transactional privacy to the expected value of the digitized assets traded on the network. Wherever applicable, we identify solutions and workarounds. Our conclusion is that the blockchain-IoT combination is powerful and can cause significant transformations across several industries, paving the way for new business models and novel, distributed applications.', 'counts_by_year': [[2022, 287], [2021, 505], [2020, 560], [2019, 564], [2018, 325], [2017, 99], [2016, 3]]}, {'id': 'W2555870966', 'doi': 'https://doi.org/10.1038/nmeth.4067', 'title': 'CHARMM36m: an improved force field for folded and intrinsically disordered proteins', 'type': 'journal-article', 'publication_date': '2017-01-01', 'host_venue': 'V127827428', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2619972578', ['I126744593']], ['A2070482497', ['I4210131661']], ['A2518163540', ['I87216513']], ['A2104331820', ['I126744593']], ['A678123104', ['I87216513']], ['A2105469777', ['I4210131661']], ['A700801705', ['I4210131661']], ['A2568130925', ['I126744593']]], 'cited_by_count': 2338, 'concepts': [['C2778815515', '0.83290994'], ['C10803110', '0.59677166'], ['C59593255', '0.5089965'], ['C9652623', '0.42673844'], ['C185592680', '0.41233417']], 'referenced_works': ['W153292298', 'W1908381757', 'W1966078827', 'W1968790827', 'W1971842458', 'W1974030600', 'W1976203248', 'W1976499671', 'W1979706337', 'W1979716756', 'W2007829343', 'W2010753316', 'W2012005320', 'W2015115860', 'W2037410278', 'W2039155848', 'W2059671784', 'W2060757799', 'W2060872117', 'W2061969981', 'W2064274102', 'W2074986801', 'W2089943412', 'W2093878302', 'W2109154308', 'W2117238666', 'W2119677527', 'W2132262459', 'W2144062946', 'W2149407603', 'W2171697771', 'W2234602429', 'W2296553342', 'W2322866202', 'W2327000477', 'W2327880765', 'W3030183899', 'W4238663480'], 'abstract': 'The all-atom additive CHARMM36 protein force field is widely used in molecular modeling and simulations. We present its refinement, CHARMM36m (http://mackerell.umaryland.edu/charmm_ff.shtml), with improved accuracy in generating polypeptide backbone conformational ensembles for intrinsically disordered peptides and proteins.', 'counts_by_year': [[2022, 648], [2021, 654], [2020, 523], [2019, 312], [2018, 151], [2017, 49]]}, {'id': 'W2899773405', 'doi': 'https://doi.org/10.1016/s0140-6736(18)32225-6', 'title': 'Global, regional, and national comparative risk assessment of 84 behavioural, environmental and occupational, and metabolic risks or clusters of risks for 195 countries and territories, 1990–2017: a systematic analysis for the Global Burden of Disease Study 2017', 'type': 'journal-article', 'publication_date': '2018-11-10', 'host_venue': 'V49861241', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2012074336', ['I201448701']], ['A2337146604', ['I201448701']], ['A2309765197', ['I201448701']], ['A2106144155', ['I201448701']], ['A2893263134', ['I1443707']], ['A2526738623', []], ['A1261223370', ['I70640408']], ['A2899664525', ['I5023651']], ['A2899975135', ['I70640408']], ['A2167087174', ['I145487455']], ['A2803759457', ['I153487414']], ['A2268894588', ['I145487455']], ['A317332464', ['I35510649']], ['A2755016573', []], ['A2123093268', ['I8764889']], ['A2804173468', ['I8764889']], ['A2026292620', ['I74813324']], ['A2900275153', ['I301615471']], ['A2765264519', ['I74813324']], ['A2900082293', ['I153487414']], ['A1950551245', ['I205783295']], ['A2989320974', ['I94800806']], ['A2338237595', []], ['A2801097294', ['I47881588']], ['A2985340466', []], ['A2921733902', ['I10947320']], ['A2049154532', ['I8764889']], ['A2993900584', ['I2800598127']], ['A1980273966', ['I45711476']], ['A2248880431', ['I79510175']], ['A1806559747', ['I242743682']], ['A2755910248', ['I10947320']], ['A2899700362', []], ['A2900310040', []], ['A2121581757', ['I80606768']], ['A2293937502', []], ['A1828904284', []], ['A2163139810', ['I98251732']], ['A2165518812', ['I201448701']], ['A2788635087', ['I196551433']], ['A2151274743', ['I150037166']], ['A2209012582', ['I74653059']], ['A2246075794', ['I39642139']], ['A2243815348', []], ['A2994386133', ['I119939603']], ['A2132116411', ['I1443707']], ['A2755240926', []], ['A2756237734', []], ['A2755349297', []], ['A2251297020', []], ['A1865820898', ['I143302722']], ['A2562109587', ['I185261750']], ['A2343865831', []], ['A2396300262', ['I28022161']], ['A2056562052', ['I170535673']], ['A1968471205', []], ['A2279206299', ['I177877127']], ['A2186295676', ['I129902397']], ['A2754876412', ['I3143470655']], ['A2326545844', ['I1316902750']], ['A2054255639', ['I120514687']], ['A1991399651', ['I118347636']], ['A2899528386', ['I3143470655']], ['A2521350420', ['I1305457654']], ['A2804468851', ['I64532579']], ['A2251799706', ['I4871159']], ['A2568822994', ['I885383172']], ['A1896914120', []], ['A2709848888', []], ['A290882514', []], ['A2318996425', ['I28022161']], ['A2075525165', []], ['A2116969515', ['I57092466']], ['A2018956803', ['I98635879']], ['A2096370698', ['I159247623']], ['A2899698624', ['I3143470655']], ['A2766346432', ['I162051807']], ['A2305148106', ['I145722265']], ['A2899751559', []], ['A2899762222', []], ['A2615465384', ['I1282700336']], ['A2033215675', ['I170486558']], ['A2109138503', ['I14243506']], ['A1902482174', []], ['A2068374777', []], ['A1607629799', ['I8204097']], ['A3037635098', ['I161106909']], ['A2138273497', ['I12870472']], ['A174217572', []], ['A1949092674', ['I46247651']], ['A3211254539', []], ['A1974818969', ['I184468904']], ['A2794043151', ['I153487414']], ['A2190077523', ['I153648349']], ['A2515935880', []], ['A2424735554', []], ['A2413366599', []], ['A2800400051', ['I74801974']], ['A2804274014', ['I165390105']], ['A2807634735', ['I145902186']], ['A2612786428', []], ['A1205674922', []], ['A2016239449', ['I177235860']], ['A2289255636', ['I51452335']], ['A2899958025', ['I153487414']], ['A2120748957', ['I149704539']], ['A2075268468', []], ['A1847310112', ['I1296604144']], ['A2899849040', []], ['A2026359138', ['I4068193']], ['A660088416', ['I154130895']], ['A3213045101', ['I136199984']], ['A2170988738', []], ['A2273669580', ['I97018004']], ['A308811693', ['I124357947']], ['A79898004', ['I161318765']], ['A2108298034', []], ['A1895530724', []], ['A2176008495', []], ['A2059558069', []], ['A2238440639', ['I177064439']], ['A2754195866', ['I267325792']], ['A2899796624', []], ['A2899926919', ['I74813324']], ['A2803666751', ['I916065895']], ['A2135382290', ['I32971472']], ['A2113092053', ['I154425047']], ['A2155770780', ['I40120149']], ['A2082872497', ['I17974374']], ['A2708373616', []], ['A3204928383', ['I4537092']], ['A2483563741', []], ['A2991567530', ['I2801649442']], ['A1680462067', ['I162051807']], ['A2892779575', ['I161106909']], ['A2029878285', ['I79619799']], ['A2141097400', ['I45294948']], ['A2899820352', []], ['A2015186656', ['I118185606']], ['A2204367295', ['I8764889']], ['A1247365415', []], ['A1954828074', []], ['A1984138050', ['I47508984']], ['A2893918937', []], ['A2064131949', ['I205746353']], ['A1996812315', ['I142263535']], ['A2095654434', []], ['A1967176581', ['I4432739']], ['A2808580211', ['I3143470655']], ['A2053014239', ['I919571938']], ['A2090103492', ['I165779595']], ['A2755232509', []], ['A2010446097', ['I31746571']], ['A2139622386', ['I51216347']], ['A2011161419', ['I124357947']], ['A2158980598', ['I141945490']], ['A2045993194', ['I74788687']], ['A678399540', ['I52357470']], ['A2130905594', ['I17937529']], ['A2980431408', []], ['A2803914527', ['I121100312']], ['A621791706', ['I153648349']], ['A2144710852', []], ['A2144113992', []], ['A2044165096', ['I39343248']], ['A3176002406', []], ['A2150892176', ['I136199984']], ['A138047837', []], ['A2050079315', []], ['A2345520266', ['I200362191']], ['A2161230630', []], ['A2105920767', ['I28166907']], ['A2132407021', ['I182534213']], ['A2306491945', ['I36243813']], ['A2161699002', []], ['A2766350609', ['I121100312']], ['A2064694482', ['I2801357902']], ['A2899841467', ['I3143470655']], ['A2366254680', ['I3143470655']], ['A148669243', ['I889458895']], ['A2900168728', ['I134359838']], ['A2854778701', ['I165441096']], ['A2571089986', ['I16733864']], ['A2784364142', []], ['A2123807283', ['I165143802']], ['A1984139223', ['I197882922']], ['A319497597', ['I137799390']], ['A2772293512', ['I165932596']], ['A2101516877', ['I165932596']], ['A2492004282', []], ['A1964628093', []], ['A781531929', ['I188700360']], ['A2254492411', ['I242743682']], ['A2559996202', []], ['A2527551833', ['I2802835388']], ['A2273107552', ['I241749']], ['A2589248567', ['I2802567020']], ['A2128665858', ['I45129253']], ['A2980420995', ['I91807558']], ['A353686740', ['I242743682']], ['A2609800051', ['I131729948']], ['A2424892588', ['I1303043848']], ['A1755537152', []], ['A2140769477', ['I43439940']], ['A2130564229', ['I188538660']], ['A2212624663', ['I145311948']], ['A2528576217', ['I3143470655']], ['A2912226948', ['I66752286']], ['A2619617878', []], ['A3013849647', ['I1289490764']], ['A2804530546', ['I130442723']], ['A935523974', ['I36258959']], ['A2011184550', ['I201448701']], ['A2941127014', ['I1174212']], ['A2804069036', ['I8764889']], ['A2178846950', ['I165143802']], ['A2010731171', ['I169541294']], ['A2153268528', ['I16904388']], ['A3019388298', ['I74653059']], ['A344498437', ['I74653059']], ['A2121435976', ['I40120149']], ['A2076714150', []], ['A2606395416', ['I4871159']], ['A2559927947', []], ['A2568267441', ['I182534213']], ['A2192228371', ['I112471378']], ['A1661044825', []], ['A2015830873', ['I191378831']], ['A2051175785', ['I36243813']], ['A2141754734', []], ['A2146772820', ['I11701301']], ['A2105998383', ['I223822909']], ['A2061034448', ['I31746571']], ['A2754324203', ['I3143470655']], ['A692807803', ['I2799886695']], ['A2804315373', ['I78545622']], ['A2439543002', []], ['A4431029', ['I71637028']], ['A3212789648', ['I21370196']], ['A3094181563', ['I3143470655']], ['A1082508886', ['I98704320']], ['A2898713432', ['I916065895']], ['A1627884688', ['I185261750']], ['A2305493226', []], ['A1944064886', ['I111199411']], ['A2194955151', []], ['A2899826552', ['I1282700336']], ['A2023264609', ['I136199984']], ['A3047459777', ['I153487414']], ['A2092117536', []], ['A2789395593', ['I170238339']], ['A2036512884', []], ['A2014559487', ['I166825849']], ['A2528323918', ['I82951845']], ['A2171790375', ['I129604602']], ['A2162480515', ['I1295768896']], ['A1967608775', ['I4068193']], ['A2900305718', ['I301615471']], ['A2304245645', ['I130442723']], ['A2084326502', []], ['A2997551774', ['I39343248']], ['A2134754651', ['I70640408']], ['A2240583472', []], ['A9775719', ['I90267481']], ['A2900340119', ['I177877127']], ['A2153629207', ['I78037679']], ['A108152018', ['I201448701']], ['A2225675537', ['I28166907']], ['A2572132342', []], ['A2419696075', ['I4871159']], ['A2526237631', []], ['A2899714723', []], ['A2024673501', ['I33618826']], ['A1485388775', ['I70640408']], ['A1991156927', ['I70640408']], ['A3211344098', ['I3143470655']], ['A3045561045', ['I38476204']], ['A2310149165', []], ['A2081665819', ['I240666556']], ['A1992818988', ['I142740786']], ['A2525638264', []], ['A1998511410', ['I9360294']], ['A2147768707', ['I190085865']], ['A2052207220', ['I136199984']], ['A1994927033', ['I39642139']], ['A2656424704', ['I118185606']], ['A3010334873', ['I3143470655']], ['A2298397821', []], ['A130753112', ['I39854758']], ['A2051789221', []], ['A3207448617', ['I12789410']], ['A2160926455', ['I182534213']], ['A2719099999', ['I84470341']], ['A2899962804', ['I3143470655']], ['A3156606865', ['I33618826']], ['A2133453768', ['I129604602']], ['A2517957986', ['I1343052199']], ['A2168220090', ['I24359323']], ['A2239987130', ['I20121455']], ['A1989487947', ['I1302978242']], ['A2029234275', ['I3143470655']], ['A2138052936', ['I65837984']], ['A248647053', ['I3143470655']], ['A2180474783', ['I1850255']], ['A2160364156', ['I17974374']], ['A686031631', ['I201448701']], ['A2114831371', ['I124944309']], ['A2091590686', []], ['A2184690000', []], ['A2098038954', ['I70640408']], ['A419432919', []], ['A2171034739', ['I3143470655']], ['A2803225923', ['I4871159']], ['A1963799814', ['I205640436']], ['A2899938806', ['I74813324']], ['A2899913050', ['I153487414']], ['A2177286373', ['I913481162']], ['A2900212817', ['I42869670']], ['A1971808073', ['I40120149']], ['A2899708718', []], ['A2135880570', []], ['A2135025260', ['I39642139']], ['A2585718815', ['I161106909']], ['A2339031754', []], ['A2799429686', ['I146399215']], ['A2900037740', ['I180670191']], ['A2109725859', []], ['A3032946704', ['I12315562']], ['A2174002162', ['I39555362']], ['A2127977879', ['I5681781']], ['A3211804909', ['I136199984']], ['A2893333756', []], ['A1991954648', []], ['A2700788008', []], ['A2336558418', ['I201448701']], ['A2195636746', ['I152429107']], ['A210497552', []], ['A2045839628', ['I2802841742']], ['A2409117092', ['I111088046']], ['A2136108649', ['I17974374']], ['A2970147794', ['I111088046']], ['A2153277603', []], ['A1987498663', []], ['A2257103434', ['I242743682']], ['A2802928962', ['I12097938']], ['A2295826387', []], ['A2147249232', ['I129975664']], ['A2122588423', []], ['A1995486516', []], ['A2767572814', ['I2801952686']], ['A2789775992', ['I169381384']], ['A439920156', ['I125749732']], ['A1934246530', ['I145311948']], ['A2892500531', ['I74813324']], ['A2905406165', ['I153487414']], ['A2527252649', ['I74813324']], ['A3037616358', []], ['A1058117426', ['I145311948']], ['A2003644413', ['I53218197']], ['A2170943676', ['I102239671']], ['A69356327', ['I169521973']], ['A2087097567', []], ['A2663029974', []], ['A2008056385', []], ['A3132660212', ['I1315953007']], ['A2991944912', ['I71999127']], ['A2434796725', ['I41832843']], ['A2614752668', ['I149213910']], ['A1975482828', ['I1335113835']], ['A2754965555', ['I3143470655']], ['A2159017802', ['I201448701']], ['A2602378559', ['I4871159']], ['A2074661978', ['I161106909']], ['A2120158151', ['I70640408']], ['A3012136236', []], ['A2059841237', ['I205640436']], ['A2893158037', []], ['A2702278152', []], ['A2237597403', ['I88491126']], ['A2883371677', ['I39642139']], ['A2063134924', ['I39642139']], ['A2803950921', ['I193649603']], ['A2581498565', ['I22759111']], ['A2766600251', []], ['A2159703735', ['I78577930']], ['A2103604644', ['I1299303238']], ['A2161050559', ['I86519309']], ['A2103229673', ['I877176835']], ['A2894101377', ['I164861460']], ['A1684365439', ['I129975664']], ['A2127024479', ['I70640408']], ['A2021199147', []], ['A2000709273', []], ['A110332668', ['I884809797']], ['A2145691735', ['I129604602']], ['A2091436485', []], ['A3142245586', ['I3143470655']], ['A2113762876', ['I139660479']], ['A2125926674', []], ['A2964652680', ['I32971472']], ['A2899993008', ['I1443707']], ['A2425928874', ['I3143470655']], ['A1933433957', ['I204337017']], ['A2252812894', []], ['A2118386970', []], ['A2111317264', ['I47519274']], ['A2804510925', []], ['A3213880252', ['I3143470655']], ['A2144421627', ['I235686326']], ['A2018198572', ['I1288198617']], ['A2991555852', ['I110263422']], ['A2165675880', ['I162714631']], ['A2989573781', ['I41832843']], ['A2510023083', []], ['A2177076244', []], ['A2090105886', ['I14245010']], ['A2142404985', ['I3143470655']], ['A2164570179', ['I36258959']], ['A2094353866', ['I90678064']], ['A2230245500', ['I1315953007']], ['A2803303234', ['I91357014']], ['A2123248410', ['I40120149']], ['A2170837810', ['I170897317']], ['A1900504705', ['I183519381']], ['A2132490811', ['I31746571']], ['A2305332739', []], ['A2176990831', ['I63739035']], ['A1980701175', ['I170583851']], ['A2048675361', ['I56085075']], ['A2133846596', ['I27577105']], ['A2755509893', ['I74813324']], ['A2056724151', ['I201448701']], ['A52183390', ['I63739035']], ['A2167359899', ['I280994']], ['A2803779426', []], ['A2122161300', []], ['A1965717446', ['I39642139']], ['A2058870430', []], ['A1967675247', []], ['A2175111212', ['I4871159']], ['A2258601719', ['I201448701']], ['A2132501245', ['I70640408']], ['A2803683186', ['I916065895']], ['A2412726901', ['I916065895']], ['A2804126342', ['I74813324']], ['A2098444608', ['I1295499534']], ['A2146891038', ['I7882870']], ['A2108302257', ['I87208437']], ['A1975324464', ['I74801974']], ['A2156122380', []], ['A1747172840', ['I39642139']], ['A2892887256', ['I267325792']], ['A2525534507', ['I12859529']], ['A2804806068', ['I87216513']], ['A2737505702', ['I157614274']], ['A2618130511', ['I197251160']], ['A2310038103', ['I66760702']], ['A2066511934', ['I156983542']], ['A2899614381', ['I110525433']], ['A292435705', ['I119939603']], ['A2426973567', []], ['A2305436410', []], ['A3157605970', ['I3143470655']], ['A2173787770', ['I201726411']], ['A2166364980', ['I39727005']], ['A2999364391', ['I32389192']], ['A1963193313', ['I139264467']], ['A2803581596', ['I145487455']], ['A2112720645', []], ['A2288821606', ['I39642139']], ['A2615402321', ['I240666556']], ['A2298408023', []], ['A2104754581', ['I120514687']], ['A242742814', ['I187531555']], ['A2803498930', []], ['A2583376270', []], ['A2632825197', ['I12912129']], ['A2102746403', ['I191208505']], ['A3177409340', ['I191208505']], ['A3120043664', ['I179245896']], ['A954872853', []], ['A706716812', ['I114832834']], ['A3037833216', ['I126596746']], ['A1969029766', []], ['A1169616326', ['I165143802']], ['A2192301710', ['I1333353642']], ['A2309303682', ['I2801952686']], ['A3211412615', []], ['A2900358254', ['I1282700336']], ['A2251044169', []], ['A2160882886', []], ['A2591497774', ['I206040828']], ['A2087695571', []], ['A2221155485', []], ['A1901047912', []], ['A3213778004', ['I3143470655']], ['A2131372770', ['I193662353']], ['A256209369', ['I70931966']], ['A2123150680', ['I66514158']], ['A2137545149', ['I74653059']], ['A2141983841', ['I12859529']], ['A2804165313', []], ['A2402779843', ['I201448701']], ['A1965705618', ['I32597200']], ['A2112018775', ['I45294948']], ['A2900230580', ['I45294948']], ['A2073232406', ['I66752286']], ['A2038807793', ['I165143802']], ['A188612772', []], ['A2199730884', []], ['A2317931897', ['I177235860']], ['A1989521457', []], ['A2902857582', ['I3143470655']], ['A2803595268', ['I143318147']], ['A2440267422', []], ['A2304074508', ['I14243506']], ['A2095606354', ['I129604602']], ['A2558711093', []], ['A2615854391', ['I916065895']], ['A2139270141', ['I165143802']], ['A2985188262', ['I861853513']], ['A2028568714', ['I40120149']], ['A3191976838', ['I242743682']], ['A2805183671', []], ['A2126589205', []], ['A2140496955', ['I184490438']], ['A2766590822', []], ['A2755975474', []], ['A2398601361', ['I177725633']], ['A2565902731', ['I91203450']], ['A2148447523', []], ['A1243347398', ['I63739035']], ['A2174321888', []], ['A2128434403', ['I3143470655']], ['A2562465928', []], ['A2040328808', ['I17974374']], ['A2792833490', ['I201448701']], ['A2629503440', ['I40120149']], ['A1830555489', ['I146655781']], ['A2109274299', ['I165932596']], ['A2585818880', ['I136199984']], ['A2154846529', []], ['A2622390099', ['I66752286']], ['A2899825127', ['I170238339']], ['A2598709781', ['I144246526']], ['A1727635955', ['I70640408']], ['A2133394427', ['I47508984']], ['A1631637737', ['I166459259']], ['A1964422715', []], ['A2103078029', ['I165143802']], ['A2924877840', []], ['A2756291383', ['I3143470655']], ['A2137998209', ['I70640408']], ['A2937964638', ['I66752286']], ['A2352135930', ['I165143802']], ['A3015021429', ['I183935753']], ['A2899831046', ['I3143470655']], ['A2146427514', ['I129902397']], ['A2143960410', []], ['A1286028393', []], ['A1452385511', ['I202276237']], ['A2036673486', []], ['A2500278846', ['I201448701']], ['A2107937508', ['I45129253']], ['A2116643131', ['I37048141']], ['A2171983489', ['I145311948']], ['A2192121978', ['I31746571']], ['A1847310112', ['I66862912']], ['A1908212773', []], ['A2106301102', ['I204337017']], ['A2250989391', ['I124357947']], ['A2005455829', []], ['A2131922514', ['I180670191']], ['A2295124587', []], ['A2664379405', ['I68956291']], ['A2750062981', ['I8764889']], ['A1969880502', ['I8764889']], ['A2679217618', []], ['A719148084', ['I8764889']], ['A3037227835', ['I2801649442']], ['A202526925', []], ['A2102508135', ['I1286959531']], ['A2754436038', ['I74813324']], ['A2165933833', ['I157614274']], ['A1935550320', ['I24359323']], ['A2320193999', ['I1443707']], ['A1311980873', []], ['A2598037195', []], ['A1919879098', []], ['A2701840363', []], ['A2900380420', ['I79833322']], ['A2419348531', ['I79833322']], ['A2573321743', ['I3143470655']], ['A2109432494', ['I1296851301']], ['A2811241807', ['I3143470655']], ['A2594237171', ['I1315953007']], ['A2644442497', []], ['A2805022591', []], ['A2023451452', []], ['A2217126065', ['I201448701']], ['A2110954104', ['I153487414']], ['A1987651379', ['I114090438']], ['A2552374679', ['I70640408']], ['A2124962734', ['I908820301']], ['A3016943249', ['I4871159']], ['A1981214840', ['I39268498']], ['A789665590', []], ['A2126042519', []], ['A2899875364', ['I70640408']], ['A2287785407', ['I201448701']], ['A2292854733', ['I124357947']], ['A1894786206', ['I161106909']], ['A224987146', []], ['A2578048617', ['I95023434']], ['A2099977418', ['I3122832768']], ['A2072898194', ['I161106909']], ['A1900208697', ['I39642139']], ['A2696008654', ['I67415387']], ['A2078422342', ['I160993911']], ['A2900096527', []], ['A2156641128', ['I201448701']], ['A2118027197', ['I206127235']], ['A2633131436', []], ['A2099881019', ['I70640408']], ['A20044890', ['I1336096307']], ['A2899577901', ['I1282700336']], ['A2750972282', ['I8764889']], ['A2519176381', ['I8764889']], ['A2991251739', ['I135140700']], ['A2900361261', ['I4537092']], ['A2222871926', []], ['A3213002013', ['I3143470655']], ['A2134948551', ['I139322472']], ['A2991188895', []], ['A1998687911', ['I107720978']], ['A2113265520', ['I201448701']], ['A2018982985', []], ['A2171501933', ['I70640408']], ['A2151390696', ['I32389192']], ['A2153808732', ['I63739035']], ['A3177463724', ['I39642139']], ['A2059048655', []], ['A2511179059', ['I1331070479']], ['A2154526963', []], ['A1979208720', []], ['A2899986828', []], ['A1809145585', []], ['A878870270', []], ['A2119565903', ['I166825849']], ['A2594807885', ['I2841861']], ['A2013758883', []], ['A2900424441', ['I56311857']], ['A2164729008', []], ['A2899727972', ['I56311857']], ['A2570005848', ['I170238339']], ['A3209789068', []], ['A3024215855', []], ['A2900118983', []], ['A2755026186', ['I3143470655']], ['A3089153368', ['I87445476']], ['A2709541152', ['I148158369']], ['A2889639047', ['I74813324']], ['A3180214943', []], ['A2900026058', ['I3143470655']], ['A2205359854', ['I161106909']], ['A2167229687', []], ['A650968989', ['I4432739']], ['A2802103136', ['I166825849']], ['A2028028389', ['I187531555']], ['A2040971506', ['I157614274']], ['A2080851943', []], ['A2173690811', ['I4871159']], ['A2201761951', ['I70640408']], ['A2217915910', ['I201448701']], ['A2422382554', ['I26092322']], ['A2989587545', ['I21491767']], ['A2899594322', ['I3143470655']], ['A2171437401', ['I56590836']], ['A2529893633', ['I63525965']], ['A2598867768', ['I35928602']], ['A2602401922', ['I217085601']], ['A2201699675', ['I197610006']], ['A2799989877', ['I98251732']], ['A2558895513', []], ['A2527901436', ['I3143470655']], ['A2045444862', []], ['A2527585475', []], ['A3212714120', ['I3143470655']], ['A3167271892', []], ['A1990140758', ['I26538001']], ['A1074637623', ['I153718931']], ['A2288782178', []], ['A2007049068', []], ['A2900342250', ['I153845743']], ['A1993002079', ['I1333353642']], ['A2130487955', ['I181631907']], ['A2894288472', ['I104658917']], ['A2109200798', ['I63739035']], ['A1957732233', ['I63739035']], ['A2045510129', ['I64532579']], ['A2623549860', []], ['A1900504705', ['I223822909']], ['A2212815995', ['I90735388']], ['A2098591421', []], ['A2990953286', ['I1326872476']], ['A2253105221', ['I1326872476']], ['A2112193247', []], ['A2804488304', []], ['A2804261331', []], ['A2128235974', ['I165779595']], ['A2623221864', ['I1290748110']], ['A2755021302', ['I3143470655']], ['A2899710203', []], ['A2269989558', ['I124357947']], ['A2118575078', ['I136199984']], ['A2157298618', ['I153390918']], ['A138278537', []], ['A1991284286', ['I192619145']], ['A3094348129', ['I192619145']], ['A3145615489', ['I183067930']], ['A2780731423', ['I183067930']], ['A2526120338', ['I177181097']], ['A1714729726', []], ['A1890082838', ['I39642139']], ['A1918267710', ['I205582932']], ['A2133877409', ['I70640408']], ['A1252535804', ['I204337017']], ['A1976173630', []], ['A2306642447', ['I2801952686']], ['A2051195365', ['I242743682']], ['A2145634610', []], ['A2075798095', ['I80281795']], ['A92641685', ['I70640408']], ['A2020141480', ['I70640408']], ['A2113447463', ['I124357947']], ['A2580055384', ['I161370692']], ['A2559374945', ['I161370692']], ['A2766555974', ['I3143470655']], ['A2212570595', []], ['A1587262111', []], ['A2162568637', []], ['A2112367615', []], ['A3216493157', ['I4871159']], ['A2151331507', ['I39642139']], ['A2072183333', ['I39642139']], ['A1224992126', ['I70640408']], ['A251309730', ['I70640408']], ['A2177398213', []], ['A2303031201', ['I197882922']], ['A2156989373', ['I74656192']], ['A2572175195', ['I74656192']], ['A2677238551', ['I39642139']], ['A2525102276', ['I143397708']], ['A2105047663', []], ['A2436480564', ['I197882922']], ['A2026017327', ['I193775966']], ['A2899795515', ['I32389192']], ['A2630255269', ['I63739035']], ['A2770400936', ['I1340918713']], ['A395835951', []], ['A2132745090', ['I74653059']], ['A2152681086', ['I129975664']], ['A1975406000', ['I185261750']], ['A2126571783', ['I201448701']], ['A2117136343', ['I3143470655']], ['A2183182747', ['I19630809']], ['A689632033', ['I63525965']], ['A2952209971', []], ['A299447943', []], ['A2251493392', ['I39642139']], ['A2116791245', []], ['A2132455678', []], ['A2796797587', ['I153487414']], ['A2308680639', []], ['A2645102116', []], ['A2564633991', ['I3143470655']], ['A601196031', []], ['A2133495838', ['I198873322']], ['A2427189664', []], ['A2169176658', ['I201448701']], ['A76907487', ['I196349391']], ['A2124543573', ['I63739035']], ['A2755582526', ['I3045169105']], ['A1976543349', ['I47508984']], ['A1994187097', ['I165932596']], ['A2122470004', []], ['A2014528437', ['I29891158']], ['A2168757528', ['I39642139']], ['A2158943272', ['I70640408']], ['A2900621010', []], ['A2442149409', ['I39642139']], ['A2896852189', ['I39642139']], ['A2596156213', ['I161106909']], ['A2266022754', []], ['A3205494996', ['I143318147']], ['A2165210302', ['I63739035']], ['A1976725761', []], ['A712021154', ['I70640408']], ['A1979308812', ['I70640408']], ['A2127606781', ['I2801517398']], ['A2465701595', ['I192455969']], ['A2125650111', ['I51452335']], ['A1924108618', ['I39642139']], ['A1985869573', ['I97018004']], ['A2104279826', ['I97018004']], ['A2748111248', ['I111088046']], ['A2899674649', ['I107720978']], ['A2151569403', ['I107720978']], ['A2251827248', ['I58956616']], ['A3213106162', []], ['A2205227065', []], ['A1910999609', ['I86695891']], ['A3037636737', ['I33618826']], ['A2483337482', ['I33618826']], ['A2153559775', []], ['A2101248102', ['I4068193']], ['A2900148085', []], ['A2339755247', ['I166722992']], ['A2340641108', []], ['A2217838924', []], ['A424594893', ['I39268498']], ['A2089080248', ['I95023434']], ['A2075365644', ['I4871159']], ['A106833059', ['I9300472']], ['A2043004940', []], ['A2899648065', []], ['A1968728195', ['I102149020']], ['A2034845917', ['I67348948']], ['A2060280414', ['I119939603']], ['A2070413459', ['I39343248']], ['A2103273789', ['I130442723']], ['A2126457318', ['I4104125']], ['A1981914947', ['I17937529']], ['A1972757265', ['I27765905']], ['A1890927917', ['I39343248']], ['A2013181484', ['I39343248']], ['A2592477237', ['I33618826']], ['A1570052640', []], ['A1090877881', []], ['A1920971052', ['I166459259']], ['A2167836184', ['I114027177']], ['A798807942', []], ['A1643765926', []], ['A2602007624', ['I161106909']], ['A119010230', ['I23923803']], ['A2803702816', ['I57206974']], ['A2602275910', []], ['A2148901667', []], ['A2560759403', []], ['A2656209814', []], ['A3138996089', ['I39642139']], ['A2011795471', ['I39642139']], ['A2989718610', ['I39642139']], ['A1940959876', ['I39642139']], ['A3206699635', ['I110525433']], ['A1040700576', ['I70640408']], ['A1905544546', []], ['A2653082139', []], ['A2306204790', ['I37048141']], ['A2997373551', ['I863896202']], ['A2121364159', ['I24943067']], ['A2180025489', ['I98677209']], ['A2950880494', ['I1336096307']], ['A2102328879', ['I74801974']], ['A2756321290', ['I153487414']], ['A2121553805', ['I1299303238']], ['A2221776659', ['I197347611']], ['A2085533449', ['I70640408']], ['A2082798115', ['I70640408']], ['A3213723387', ['I68956291']], ['A2602280519', ['I142263535']], ['A2663722086', ['I47508984']], ['A341443032', ['I136199984']], ['A2789330937', ['I141945490']], ['A2913554443', ['I242743682']], ['A2271697240', ['I114017466']], ['A1858273019', ['I98635879']], ['A2598054235', ['I39727005']], ['A329907494', ['I123185442']], ['A1995962597', ['I123185442']], ['A2097837126', ['I150729083']], ['A2758241083', []], ['A2526438301', []], ['A2167506362', ['I32389192']], ['A2755061759', []], ['A2788832255', []], ['A2163382776', []], ['A1907465466', []], ['A185842758', ['I1333353642']], ['A2627886379', ['I201448701']], ['A2754570444', ['I3143470655']], ['A2755255890', ['I82952536']], ['A2568049821', ['I70640408']], ['A2078993706', ['I168635309']], ['A2340193354', ['I39642139']], ['A1978294612', ['I201448701']], ['A2165142774', []], ['A2768323355', ['I98677209']], ['A2008620751', []], ['A1852563117', ['I125749732']], ['A2308578969', ['I62370553']], ['A3212954370', ['I3143470655']], ['A2888981946', []], ['A1999976374', []], ['A2108967599', ['I157614274']], ['A1981348977', ['I36258959']], ['A2013718801', ['I4068193']], ['A295081252', ['I881427289']], ['A2112093150', ['I149704539']], ['A2014464262', []], ['A2604742042', ['I3143470655']], ['A2479167966', ['I3131389504']], ['A2617635383', ['I12789410']], ['A1981610748', ['I1333353642']], ['A1480813768', ['I154840374']], ['A2527981122', ['I103635307']], ['A2304636401', ['I204250578']], ['A2001912647', ['I86695891']], ['A1259793373', ['I2801357902']], ['A2061012529', []], ['A2151397838', ['I11701301']], ['A2235522935', []], ['A2126302603', ['I63739035']], ['A2900092585', ['I74813324']], ['A2968671683', ['I78577930']], ['A1985531105', ['I141596103']], ['A3193264458', ['I161106909']], ['A2900059160', ['I153487414']], ['A2899732530', ['I74813324']], ['A2893518454', ['I153487414']], ['A2900249171', ['I134359838']], ['A2804812000', ['I134359838']], ['A2899833211', ['I134359838']], ['A1997328309', []], ['A2564109398', ['I8764889']], ['A2899924775', ['I78545622']], ['A2224875839', ['I1315953007']], ['A2753949767', ['I194744927']], ['A2899741645', ['I194744927']], ['A2142921883', ['I33618826']], ['A2556729383', ['I51601045']], ['A2122378306', []], ['A2579735154', ['I57206974']], ['A2107372407', ['I8764889']], ['A2991491615', ['I160993911']], ['A1982721321', ['I160993911']], ['A2044166451', []], ['A2591217870', ['I168635309']], ['A2619345531', ['I126596746']], ['A2804786586', ['I3143470655']], ['A3213341235', []], ['A1575105442', ['I154526488']], ['A646358098', ['I17974374']], ['A3214681165', ['I160606119']], ['A2171619173', ['I189643551']], ['A2800308158', []], ['A2104826947', ['I2802567020']], ['A2826434173', ['I170238339']], ['A2900377508', ['I74813324']], ['A2989792118', ['I1316902750']], ['A1658669325', ['I1316902750']], ['A2604766324', ['I3143470655']], ['A2952451616', []], ['A2607599903', []], ['A2803757361', []], ['A2604723853', ['I3143470655']], ['A2156256324', ['I39727005']], ['A284438857', ['I39555362']], ['A2126867937', ['I136199984']], ['A3028773664', ['I4871159']], ['A2804301087', []], ['A66691543', ['I129902397']], ['A2888993594', []], ['A2237400685', []], ['A3165398205', []], ['A2892883214', ['I136199984']], ['A373515349', []], ['A2615083598', ['I80606768']], ['A2040123927', ['I9360294']], ['A2963961637', ['I17877952']], ['A2712299672', ['I118501908']], ['A2888942052', ['I201448701']], ['A2894260413', ['I201448701']], ['A2899542227', []], ['A3135428756', ['I4068193']], ['A2074135114', ['I4068193']], ['A2412136040', ['I916065895']], ['A1837427614', []], ['A2286669759', ['I148314036']], ['A1865990060', ['I201448701']], ['A2962075517', ['I17974374']], ['A2199045158', ['I24943067']], ['A2103680419', ['I28166907']], ['A2025574871', ['I28166907']], ['A1967961136', ['I1321649718']], ['A425144802', ['I153487414']], ['A772519725', []], ['A2721094784', ['I114027177']], ['A2125838075', ['I114027177']], ['A2299946479', []], ['A2182410343', ['I165143802']], ['A294027109', ['I79833322']], ['A2019219434', ['I165779595']], ['A2709040469', ['I62916508']], ['A2797516633', ['I10947320']], ['A1965223436', ['I10947320']], ['A2105129513', ['I183935753']], ['A2394853576', ['I2799299286']], ['A3043558172', []], ['A2144154959', ['I142823887']], ['A2112328861', ['I83519826']], ['A2796586280', ['I3143470655']], ['A2341457108', ['I119939603']], ['A2899825995', []], ['A2100829425', ['I74801974']], ['A2275319484', ['I170897317']], ['A2108032137', ['I111979921']], ['A2949845258', []], ['A2900251229', []], ['A2900102795', ['I74813324']], ['A2893884787', ['I74813324']], ['A2139945721', ['I889458895']], ['A2893357419', ['I4537092']], ['A2560293163', ['I22299242']], ['A2169155597', ['I139264467']], ['A1827643105', ['I34942010']], ['A2439370755', ['I99065089']], ['A1905705299', ['I161106909']], ['A3088485169', ['I37461747']], ['A2238864982', []], ['A2610303064', []], ['A3157096520', []], ['A2755317805', []], ['A2309348756', ['I82951845']], ['A2538038556', ['I24943067']], ['A3174808869', ['I919571938']], ['A2701680434', ['I3143470655']], ['A2803173578', ['I3143470655']], ['A1852087164', ['I74653059']], ['A2316804223', ['I201448701']]], 'cited_by_count': 2298, 'concepts': [['C12174686', '0.66576505'], ['C99454951', '0.61123514'], ['C82789193', '0.57045776'], ['C71924100', '0.5472722'], ['C201903717', '0.43950418']], 'referenced_works': ['W1979080764', 'W1979918427', 'W1987313755', 'W1992484917', 'W2017602185', 'W2041827408', 'W2056180219', 'W2101997013', 'W2116404316', 'W2116515517', 'W2118808703', 'W2122520859', 'W2127287605', 'W2148044501', 'W2154137484', 'W2158480275', 'W2162775913', 'W2169428949', 'W2267159071', 'W2313998826', 'W2440172825', 'W2462161029', 'W2470481127', 'W2509334837', 'W2546882876', 'W2582137666', 'W2582602949', 'W2599301087', 'W2751792491', 'W2751884637', 'W2753051611', 'W2756248476', 'W2765097496', 'W2795776218', 'W2887966018', 'W3025238321', 'W3178295071', 'W4253369196'], 'abstract': 'The Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) 2017 comparative risk assessment (CRA) is a comprehensive approach to risk factor quantification that offers a useful tool for synthesising evidence on risks and risk-outcome associations. With each annual GBD study, we update the GBD CRA to incorporate improved methods, new risks and risk-outcome pairs, and new data on risk exposure levels and risk-outcome associations.We used the CRA framework developed for previous iterations of GBD to estimate levels and trends in exposure, attributable deaths, and attributable disability-adjusted life-years (DALYs), by age group, sex, year, and location for 84 behavioural, environmental and occupational, and metabolic risks or groups of risks from 1990 to 2017. This study included 476 risk-outcome pairs that met the GBD study criteria for convincing or probable evidence of causation. We extracted relative risk and exposure estimates from 46 749 randomised controlled trials, cohort studies, household surveys, census data, satellite data, and other sources. We used statistical models to pool data, adjust for bias, and incorporate covariates. Using the counterfactual scenario of theoretical minimum risk exposure level (TMREL), we estimated the portion of deaths and DALYs that could be attributed to a given risk. We explored the relationship between development and risk exposure by modelling the relationship between the Socio-demographic Index (SDI) and risk-weighted exposure prevalence and estimated expected levels of exposure and risk-attributable burden by SDI. Finally, we explored temporal changes in risk-attributable DALYs by decomposing those changes into six main component drivers of change as follows: (1) population growth; (2) changes in population age structures; (3) changes in exposure to environmental and occupational risks; (4) changes in exposure to behavioural risks; (5) changes in exposure to metabolic risks; and (6) changes due to all other factors, approximated as the risk-deleted death and DALY rates, where the risk-deleted rate is the rate that would be observed had we reduced the exposure levels to the TMREL for all risk factors included in GBD 2017.In 2017, 34·1 million (95% uncertainty interval [UI] 33·3-35·0) deaths and 1·21 billion (1·14-1·28) DALYs were attributable to GBD risk factors. Globally, 61·0% (59·6-62·4) of deaths and 48·3% (46·3-50·2) of DALYs were attributed to the GBD 2017 risk factors. When ranked by risk-attributable DALYs, high systolic blood pressure (SBP) was the leading risk factor, accounting for 10·4 million (9·39-11·5) deaths and 218 million (198-237) DALYs, followed by smoking (7·10 million [6·83-7·37] deaths and 182 million [173-193] DALYs), high fasting plasma glucose (6·53 million [5·23-8·23] deaths and 171 million [144-201] DALYs), high body-mass index (BMI; 4·72 million [2·99-6·70] deaths and 148 million [98·6-202] DALYs), and short gestation for birthweight (1·43 million [1·36-1·51] deaths and 139 million [131-147] DALYs). In total, risk-attributable DALYs declined by 4·9% (3·3-6·5) between 2007 and 2017. In the absence of demographic changes (ie, population growth and ageing), changes in risk exposure and risk-deleted DALYs would have led to a 23·5% decline in DALYs during that period. Conversely, in the absence of changes in risk exposure and risk-deleted DALYs, demographic changes would have led to an 18·6% increase in DALYs during that period. The ratios of observed risk exposure levels to exposure levels expected based on SDI (O/E ratios) increased globally for unsafe drinking water and household air pollution between 1990 and 2017. This result suggests that development is occurring more rapidly than are changes in the underlying risk structure in a population. Conversely, nearly universal declines in O/E ratios for smoking and alcohol use indicate that, for a given SDI, exposure to these risks is declining. In 2017, the leading Level 4 risk factor for age-standardised DALY rates was high SBP in four super-regions: central Europe, eastern Europe, and central Asia; north Africa and Middle East; south Asia; and southeast Asia, east Asia, and Oceania. The leading risk factor in the high-income super-region was smoking, in Latin America and Caribbean was high BMI, and in sub-Saharan Africa was unsafe sex. O/E ratios for unsafe sex in sub-Saharan Africa were notably high, and those for alcohol use in north Africa and the Middle East were notably low.By quantifying levels and trends in exposures to risk factors and the resulting disease burden, this assessment offers insight into where past policy and programme efforts might have been successful and highlights current priorities for public health action. Decreases in behavioural, environmental, and occupational risks have largely offset the effects of population growth and ageing, in relation to trends in absolute burden. Conversely, the combination of increasing metabolic risks and population ageing will probably continue to drive the increasing trends in non-communicable diseases at the global level, which presents both a public health challenge and opportunity. We see considerable spatiotemporal heterogeneity in levels of risk exposure and risk-attributable burden. Although levels of development underlie some of this heterogeneity, O/E ratios show risks for which countries are overperforming or underperforming relative to their level of development. As such, these ratios provide a benchmarking tool to help to focus local decision making. Our findings reinforce the importance of both risk exposure monitoring and epidemiological research to assess causal connections between risks and health outcomes, and they highlight the usefulness of the GBD study in synthesising data to draw comprehensive and robust conclusions that help to inform good policy and strategic health planning.Bill & Melinda Gates Foundation.', 'counts_by_year': [[2022, 528], [2021, 759], [2020, 726], [2019, 280], [2018, 2], [2014, 1]]}, {'id': 'W2560647685', 'doi': 'https://doi.org/10.1073/pnas.1611835114', 'title': 'Overcoming catastrophic forgetting in neural networks', 'type': 'journal-article', 'publication_date': '2017-03-28', 'host_venue': 'V125754415', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2425779167', ['I4210090411']], ['A662855036', ['I4210090411']], ['A2669278182', ['I4210090411']], ['A2007821246', ['I4210090411']], ['A1969031609', ['I4210090411']], ['A2518520521', ['I4210090411']], ['A2603843632', ['I4210090411']], ['A2564593992', ['I4210090411']], ['A2600927655', ['I4210090411']], ['A2703777005', ['I4210090411']], ['A4302276', ['I4210090411']], ['A177472547', ['I47508984']], ['A2196286353', ['I4210090411']], ['A2154781858', ['I4210090411']]], 'cited_by_count': 2275, 'concepts': [['C7149132', '0.84229'], ['C41008148', '0.74070823'], ['C154945302', '0.6857472'], ['C97541855', '0.6585736'], ['C50644808', '0.57852423']], 'referenced_works': ['W1440335865', 'W1884390770', 'W1971037450', 'W1979019425', 'W1981184437', 'W2036963181', 'W2047057213', 'W2047125104', 'W2060277733', 'W2061304498', 'W2066783383', 'W2067840211', 'W2077052576', 'W2078179989', 'W2088321899', 'W2101493843', 'W2110048843', 'W2111051539', 'W2111534506', 'W2129386380', 'W2145339207', 'W2151137320', 'W2168342951', 'W2174227032', 'W2424347275', 'W2919115771', 'W3103780890'], 'abstract': 'Significance Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially.', 'counts_by_year': [[2022, 308], [2021, 801], [2020, 596], [2019, 332], [2018, 161], [2017, 57], [2016, 3], [2015, 13]]}, {'id': 'W2261059368', 'doi': 'https://doi.org/10.1016/j.isprsjprs.2016.01.011', 'title': 'Random forest in remote sensing: A review of applications and future directions', 'type': 'journal-article', 'publication_date': '2016-04-01', 'host_venue': 'V4210188629', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2186900402', ['I182212641']], ['A3206116870', ['I3121153395']]], 'cited_by_count': 2268, 'concepts': [['C169258074', '0.68578935'], ['C62649853', '0.61686146'], ['C41008148', '0.42124513'], ['C39432304', '0.4138956'], ['C205649164', '0.32875922']], 'referenced_works': ['W13188192', 'W793093014', 'W1483589200', 'W1485545181', 'W1520812622', 'W1554190159', 'W1555328055', 'W1565635109', 'W1580493526', 'W1589181030', 'W1605688901', 'W1828991496', 'W1948133483', 'W1964789567', 'W1971653980', 'W1973644502', 'W1980547211', 'W1982621055', 'W1986522259', 'W1988790447', 'W1989022844', 'W1989919782', 'W1990653740', 'W1991259147', 'W1994414533', 'W1996061706', 'W1998281138', 'W1998979050', 'W2000175358', 'W2001014393', 'W2002021232', 'W2011287807', 'W2012838735', 'W2013973135', 'W2018732570', 'W2019504071', 'W2019802908', 'W2020708554', 'W2026952547', 'W2030645206', 'W2035419249', 'W2036061791', 'W2037507183', 'W2039067795', 'W2045804185', 'W2045875127', 'W2046708214', 'W2047825499', 'W2052190325', 'W2052331316', 'W2055848082', 'W2059056443', 'W2059217921', 'W2061574900', 'W2063907334', 'W2064843184', 'W2067751945', 'W2068733856', 'W2074448420', 'W2078841663', 'W2079454091', 'W2081562328', 'W2081620141', 'W2084166106', 'W2085527057', 'W2089716607', 'W2089806346', 'W2090715229', 'W2093045504', 'W2094708176', 'W2099206930', 'W2101711129', 'W2103699041', 'W2112716550', 'W2119387367', 'W2121734470', 'W2128207034', 'W2130627644', 'W2132424470', 'W2134346724', 'W2135120009', 'W2138499468', 'W2145862305', 'W2150757437', 'W2154636369', 'W2155632266', 'W2156665896', 'W2161848987', 'W2166307050', 'W2167753478', 'W2168144621', 'W2168341506', 'W2168809519', 'W3004732066', 'W4212883601'], 'abstract': 'Abstract   A random forest (RF) classifier is an ensemble classifier that produces multiple decision trees, using a randomly selected subset of training samples and variables. This classifier has become popular within the remote sensing community due to the accuracy of its classifications. The overall objective of this work was to review the utilization of RF classifier in remote sensing. This review has revealed that RF classifier can successfully handle high data dimensionality and multicolinearity, being both fast and insensitive to overfitting. It is, however, sensitive to the sampling design. The variable importance (VI) measurement provided by the RF classifier has been extensively exploited in different scenarios, for example to reduce the number of dimensions of hyperspectral data, to identify the most relevant multisource remote sensing and geographic data, and to select the most suitable season to classify particular target classes. Further investigations are required into less commonly exploited uses of this classifier, such as for sample proximity analysis to detect and remove outliers in the training samples.', 'counts_by_year': [[2022, 511], [2021, 568], [2020, 520], [2019, 359], [2018, 192], [2017, 96], [2016, 17]]}, {'id': 'W2606974598', 'doi': 'https://doi.org/10.18653/v1/p17-1099', 'title': 'Get To The Point: Summarization with Pointer-Generator Networks', 'type': 'proceedings-article', 'publication_date': '2017-07-01', 'host_venue': 'V4306420508', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2778789062', ['I97018004']], ['A2551433269', ['I1291425158']], ['A2149153931', ['I97018004']]], 'cited_by_count': 2244, 'concepts': [['C170858558', '0.93886477'], ['C150202949', '0.8641689'], ['C41008148', '0.8253876'], ['C2780992000', '0.67287034'], ['C2778112365', '0.5743542']], 'referenced_works': ['W1514535095', 'W1544827683', 'W1916559533', 'W1964326564', 'W2088802729', 'W2101390659', 'W2130942839', 'W2133459682', 'W2146502635', 'W2154652894', 'W2164755781', 'W2238199890', 'W2467173223', 'W2507756961', 'W2509593407', 'W2526471240', 'W2561360547', 'W2567525733', 'W2573425638', 'W2574535369', 'W2586041121', 'W2950178297', 'W2962944953', 'W2962965405', 'W2963248296', 'W2963260202', 'W2963494889', 'W2963699608', 'W2963929190', 'W2964165364', 'W2964308564'], 'abstract': 'Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.', 'counts_by_year': [[2022, 150], [2021, 646], [2020, 643], [2019, 538], [2018, 241], [2017, 25]]}, {'id': 'W2252568502', 'doi': 'https://doi.org/10.1038/nbt.3437', 'title': 'Optimized sgRNA design to maximize activity and minimize off-target effects of CRISPR-Cas9', 'type': 'journal-article', 'publication_date': '2016-02-01', 'host_venue': 'V106963461', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A1975127990', ['I107606265']], ['A2295768917', ['I1290206253']], ['A2808366892', ['I107606265']], ['A2114794494', ['I107606265']], ['A2536092473', ['I107606265']], ['A2535191973', ['I107606265']], ['A2103383736', ['I107606265']], ['A114064242', ['I107606265', 'I4210117453']], ['A2004103966', ['I204465549']], ['A2127278898', ['I204465549']], ['A2090564267', ['I204465549']], ['A700773976', ['I1290206253']], ['A2005259742', ['I107606265']]], 'cited_by_count': 2242, 'concepts': [['C98108389', '0.8738499'], ['C132455925', '0.8013871'], ['C189819185', '0.75346714'], ['C97702854', '0.7378813'], ['C144501496', '0.6771649']], 'referenced_works': ['W1635615463', 'W1869792624', 'W1879165674', 'W1934704664', 'W1959032726', 'W1964895626', 'W1968342623', 'W1970196890', 'W1971336136', 'W1971947883', 'W1973260880', 'W1974343902', 'W1981160614', 'W1982846895', 'W1986028255', 'W1990149668', 'W2003171404', 'W2003797386', 'W2006373538', 'W2010457001', 'W2025412492', 'W2031211461', 'W2036176224', 'W2039897098', 'W2043398720', 'W2045435533', 'W2064815984', 'W2074716149', 'W2077659966', 'W2097137621', 'W2097664721', 'W2097936772', 'W2100122648', 'W2103123883', 'W2106070783', 'W2114031931', 'W2114850508', 'W2115603949', 'W2127084386', 'W2128758977', 'W2130410032', 'W2131395804', 'W2138164217', 'W2141674733', 'W2142515819', 'W2152505144', 'W2153344788', 'W2157634182', 'W2162147795', 'W2164594351', 'W2167380395', 'W2168420784', 'W2170551349'], 'abstract': 'CRISPR-Cas9-based genetic screens are a powerful new tool in biology. By simply altering the sequence of the single-guide RNA (sgRNA), one can reprogram Cas9 to target different sites in the genome with relative ease, but the on-target activity and off-target effects of individual sgRNAs can vary widely. Here, we use recently devised sgRNA design rules to create human and mouse genome-wide libraries, perform positive and negative selection screens and observe that the use of these rules produced improved results. Additionally, we profile the off-target activity of thousands of sgRNAs and develop a metric to predict off-target sites. We incorporate these findings from large-scale, empirical data to improve our computational design rules and create optimized sgRNA libraries that maximize on-target activity and minimize off-target effects to enable more effective and efficient genetic screens and genome engineering.', 'counts_by_year': [[2022, 394], [2021, 493], [2020, 482], [2019, 344], [2018, 280], [2017, 172], [2016, 74], [2015, 1]]}, {'id': 'W1773016195', 'doi': 'https://doi.org/10.4324/9781315506173-8', 'title': 'Perception and Communication', 'type': 'book-chapter', 'publication_date': '2016-06-23', 'host_venue': 'V4306463855', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A3212284846', ['I90344618']]], 'cited_by_count': 2240, 'concepts': [['C26760741', '0.76169324'], ['C144559511', '0.58089775'], ['C9652623', '0.51924133'], ['C28719098', '0.5087171'], ['C188147891', '0.5004101']], 'referenced_works': ['W1643989757', 'W2020895662', 'W2098329305'], 'abstract': "First published in 1958, this book has become recognized as a classic in its field. It marked a transition between behaviourist learning theory and the modern 'information processing' or 'cognitive' approach to perception and communication skills. It continues to provide a principal starting point for theoretical and experimental work on selective attention. As Professor Posner writes in his Foreword to the reissue: 'it remains of great interest to view the work in its original form and to ponder those creative moments when the mind first grasps a new insight and then struggles to work out its consequences.", 'counts_by_year': [[2022, 9], [2021, 19], [2020, 23], [2019, 20], [2018, 41], [2017, 29], [2016, 54], [2015, 76], [2014, 79], [2013, 117], [2012, 96]]}, {'id': 'W3010131837', 'doi': 'https://doi.org/10.1126/science.aba9757', 'title': 'The effect of travel restrictions on the spread of the 2019 novel coronavirus (COVID-19) outbreak', 'type': 'journal-article', 'publication_date': '2020-03-06', 'host_venue': 'V3880285', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A3038305246', ['I12912129']], ['A3003286660', ['I12912129']], ['A2070742457', ['I2277624104']], ['A1420843873', ['I134940468']], ['A3005846314', ['I134940468']], ['A148108666', ['I2277624104']], ['A2209047775', ['I12912129']], ['A3215825655', ['I12912129']], ['A3170181773', ['I134940468']], ['A2530261676', ['I874236823']], ['A2080844218', ['I874236823']], ['A3120926544', ['I12912129']], ['A2745881564', ['I24943067']], ['A2780016032', ['I201448701', 'I4210089486']], ['A2131578109', ['I132690573']], ['A2707826896', ['I134940468', 'I12912129']]], 'cited_by_count': 2225, 'concepts': [['C2781402358', '0.93033385'], ['C116675565', '0.8066642'], ['C89623803', '0.7504633'], ['C3008058167', '0.65962785'], ['C2775941552', '0.6068168']], 'referenced_works': ['W2012506695', 'W2016674662', 'W2032616735', 'W2077281606', 'W2096145431', 'W2147166346', 'W2158956373', 'W2608435517', 'W3001118548', 'W3002764620', 'W3003573988', 'W3003668884', 'W3004026249', 'W3004397688', 'W3004912618', 'W3005380425', 'W3006189429', 'W3006320625', 'W3008582897', 'W3010395627', 'W3010719493', 'W3101335933'], 'abstract': 'Outbreak to pandemic In response to global dispersion of severe acute respiratory syndrome–coronavirus 2 (SARS-CoV-2), quarantine measures have been implemented around the world. To understand how travel and quarantine influence the dynamics of the spread of this novel human virus, Chinazzi et al. applied a global metapopulation disease transmission model to epidemiological data from China. They concluded that the travel quarantine introduced in Wuhan on 23 January 2020 only delayed epidemic progression by 3 to 5 days within China, but international travel restrictions did help to slow spread elsewhere in the world until mid-February. Their results suggest that early detection, hand washing, self-isolation, and household quarantine will likely be more effective than travel restrictions at mitigating this pandemic. Science , this issue p. 395', 'counts_by_year': [[2022, 408], [2021, 931], [2020, 877], [2019, 1]]}, {'id': 'W2950504429', 'doi': 'https://doi.org/10.1080/2159676x.2019.1628806', 'title': 'Reflecting on reflexive thematic analysis', 'type': 'journal-article', 'publication_date': '2019-06-13', 'host_venue': 'V2764513707', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2096614597', ['I154130895']], ['A2145196104', ['I178535277']]], 'cited_by_count': 2209, 'concepts': [['C13200473', '0.83696336'], ['C93692415', '0.65851206'], ['C74196892', '0.4578352'], ['C41008148', '0.3480996'], ['C144024400', '0.26849708']], 'referenced_works': ['W81685006', 'W792039022', 'W1966931250', 'W1979290264', 'W2000693941', 'W2108220391', 'W2117984940', 'W2129485653', 'W2145688328', 'W2149285103', 'W2155427662', 'W2320257247', 'W2332267897', 'W2426090710', 'W2535682359', 'W2578740911', 'W2590250792', 'W4232488826', 'W4232977219', 'W4244932993', 'W4248854933', 'W4249811741'], 'abstract': 'Since initially writing on thematic analysis in 2006, the popularity of the method we outlined has exploded, the variety of TA approaches have expanded, and, not least, our thinking has developed a...', 'counts_by_year': [[2022, 1059], [2021, 846], [2020, 281], [2019, 21]]}, {'id': 'W2739439285', 'doi': 'https://doi.org/10.1002/wcms.1327', 'title': 'Software update: the ORCA program system, version 4.0', 'type': 'journal-article', 'publication_date': '2018-01-01', 'host_venue': 'V136120998', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2183648210', ['I4210110444']]], 'cited_by_count': 2206, 'concepts': [['C41008148', '0.6108362'], ['C2777904410', '0.49471518'], ['C115903868', '0.44697535'], ['C199360897', '0.385754'], ['C111919701', '0.32561186']], 'referenced_works': ['W24312131', 'W1554563779', 'W1584911069', 'W1970623278', 'W1970926733', 'W1976774792', 'W1991358468', 'W1999440372', 'W2004575088', 'W2011841937', 'W2018674598', 'W2033134911', 'W2039293281', 'W2039862840', 'W2040445705', 'W2045236580', 'W2057902656', 'W2064168338', 'W2067718414', 'W2078511427', 'W2083228438', 'W2087864238', 'W2226304710', 'W2233212601', 'W2274081676', 'W2321084666', 'W2326395847', 'W2329781407', 'W2330314235', 'W2336270984', 'W2336340955', 'W2411505598', 'W2474256331', 'W2476483011', 'W2506613235', 'W2516351507', 'W2529420251', 'W2548579624', 'W2567342479', 'W2588164354', 'W2591086733', 'W2607884604', 'W2611394338', 'W2625002383', 'W2626135170'], 'abstract': 'This short update provides an overview of the capabilities that have been added to the ORCA electronic structure package (version 4.0) since publication of the first article in 2012. WIREs Comput Mol Sci 2018, 8:e1327. doi: 10.1002/wcms.1327\r\n\r\nThis article is categorized under: \r\n\r\nElectronic Structure Theory > Ab Initio Electronic Structure Methods\r\nElectronic Structure Theory > Density Functional Theory\r\nSoftware > Quantum Chemistry', 'counts_by_year': [[2022, 577], [2021, 705], [2020, 482], [2019, 295], [2018, 130], [2017, 5]]}, {'id': 'W2606722458', 'doi': 'https://doi.org/10.1145/3079856.3080246', 'title': 'In-Datacenter Performance Analysis of a Tensor Processing Unit', 'type': 'proceedings-article', 'publication_date': '2017-06-24', 'host_venue': 'V4306420111', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2472600557', ['I1291425158']], ['A2123023106', ['I1291425158']], ['A2612619264', ['I1291425158']], ['A2098984663', ['I1291425158']], ['A2329212395', ['I1291425158']], ['A2607019965', ['I1291425158']], ['A2600383731', ['I1291425158']], ['A2718476072', ['I1291425158']], ['A2606016759', ['I1291425158']], ['A2003441194', ['I1291425158']], ['A2606572492', ['I1291425158']], ['A2606200086', ['I1291425158']], ['A2702710080', ['I1291425158']], ['A2557827654', ['I1291425158']], ['A2605654851', ['I1291425158']], ['A2150488665', ['I1291425158']], ['A2605796463', ['I1291425158']], ['A2429370538', ['I1291425158']], ['A2605987460', ['I1291425158']], ['A2606515600', ['I1291425158']], ['A2605550313', ['I1291425158']], ['A2728759069', ['I1291425158']], ['A2125988291', ['I1291425158']], ['A2682050853', ['I1291425158']], ['A2605400395', ['I1291425158']], ['A2635411945', ['I1291425158']], ['A2948129283', ['I1291425158']], ['A2659859423', ['I1291425158']], ['A2564637752', ['I1291425158']], ['A2653001703', ['I1291425158']], ['A2605690127', ['I1291425158']], ['A2605788962', ['I1291425158']], ['A2928009436', ['I1291425158']], ['A2735991384', ['I1291425158']], ['A2734745916', ['I1291425158']], ['A2179558064', ['I1291425158']], ['A2736069521', ['I1291425158']], ['A225512766', ['I1291425158']], ['A2764656696', ['I1291425158']], ['A2734489838', ['I1291425158']], ['A2561749897', ['I1291425158']], ['A2734577406', ['I1291425158']], ['A2735456473', ['I1291425158']], ['A2735234499', ['I1291425158']], ['A2735280446', ['I1291425158']], ['A2734757900', ['I1291425158']], ['A2734326025', ['I1291425158']], ['A2735559755', ['I1291425158']], ['A2745849704', ['I1291425158']], ['A2737917234', ['I1291425158']], ['A2735567678', ['I1291425158']], ['A2736238406', ['I1291425158']], ['A2010970115', ['I1291425158']], ['A2735213229', ['I1291425158']], ['A2736173331', ['I1291425158']], ['A2223580880', ['I1291425158']], ['A2562131157', ['I1291425158']], ['A2735929718', ['I1291425158']], ['A2735018223', ['I1291425158']], ['A2581188111', ['I1291425158']], ['A2734967460', ['I1291425158']], ['A2617556799', ['I1291425158']], ['A2734420502', ['I1291425158']], ['A2736278843', ['I1291425158']], ['A3100138820', ['I1291425158']], ['A2735837224', ['I1291425158']], ['A2735733225', ['I1291425158']], ['A2558171474', ['I1291425158']], ['A2735001011', ['I1291425158']], ['A2735755426', ['I1291425158']], ['A2735684164', ['I1291425158']], ['A2089062156', ['I1291425158']], ['A2734507409', ['I1291425158']], ['A2735498832', ['I1291425158']], ['A2734432318', ['I1291425158']], ['A2157729263', ['I1291425158']]], 'cited_by_count': 2202, 'concepts': [['C41008148', '0.8079537'], ['C49154492', '0.5744966'], ['C77390884', '0.5059137'], ['C173608175', '0.49851084'], ['C157764524', '0.4570733']], 'referenced_works': ['W199085213', 'W1967767043', 'W1976575875', 'W1981071268', 'W1982063824', 'W2002555321', 'W2017509362', 'W2070167224', 'W2094756095', 'W2102543317', 'W2117539524', 'W2134807578', 'W2257979135', 'W2294528575', 'W2302255633', 'W2515080096', 'W2540279855', 'W2542250562', 'W2563299394', 'W2906043559', 'W2950656546', 'W4230989867', 'W4239722617', 'W4243519499', 'W4245199738', 'W4251575795', 'W4251607986', 'W4254672563'], 'abstract': "Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.", 'counts_by_year': [[2022, 197], [2021, 598], [2020, 546], [2019, 492], [2018, 312], [2017, 54], [2012, 2]]}, {'id': 'W2601564443', 'doi': 'https://doi.org/10.1109/iccv.2017.89', 'title': 'Deformable Convolutional Networks', 'type': 'proceedings-article', 'publication_date': '2017-03-17', 'host_venue': 'V4306419272', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2108977331', ['I1290206253']], ['A2946527827', ['I1290206253']], ['A2674760608', ['I1290206253']], ['A2908434237', ['I1290206253']], ['A2773119742', ['I76130692']], ['A2263964151', ['I99065089']], ['A2630841711', ['I1290206253']]], 'cited_by_count': 2193, 'concepts': [['C41008148', '0.83977675'], ['C70437156', '0.7813629'], ['C154945302', '0.7635248'], ['C81363708', '0.7574128'], ['C45347329', '0.6507614']], 'referenced_works': ['W1536680647', 'W1610060839', 'W1903029394', 'W1912570122', 'W1913341271', 'W1932624639', 'W2031489346', 'W2072072671', 'W2102605133', 'W2117228865', 'W2123099218', 'W2124386111', 'W2144794286', 'W2162915993', 'W2168356304', 'W2194775991', 'W2340427832', 'W2340897893', 'W2565639579', 'W2605344185', 'W2962850830', 'W2963037989'], 'abstract': 'Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the first time, we show that learning dense spatial transformation in deep CNNs is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/msracver/Deformable-ConvNets.', 'counts_by_year': [[2022, 357], [2021, 710], [2020, 582], [2019, 393], [2018, 137], [2017, 10]]}, {'id': 'W2232317135', 'doi': 'https://doi.org/10.1016/j.knosys.2015.12.022', 'title': 'SCA: A Sine Cosine Algorithm for solving optimization problems', 'type': 'journal-article', 'publication_date': '2016-03-15', 'host_venue': 'V10169007', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A3006208000', ['I2188195594', 'I11701301']]], 'cited_by_count': 2188, 'concepts': [['C41008148', '0.8561225'], ['C186661526', '0.77056277'], ['C178009071', '0.62472516'], ['C11413529', '0.58045167'], ['C126255220', '0.32489124']], 'referenced_works': ['W883434633', 'W1538045029', 'W1595159159', 'W1968075052', 'W1968635316', 'W1974336701', 'W1975436849', 'W1980750007', 'W1980843902', 'W1981482533', 'W1997600725', 'W1999284878', 'W2003961265', 'W2009207502', 'W2014601121', 'W2020320008', 'W2020634673', 'W2021699183', 'W2028017445', 'W2029631366', 'W2039847237', 'W2043003550', 'W2043117900', 'W2052367557', 'W2053498776', 'W2056811412', 'W2062638825', 'W2065401134', 'W2072955302', 'W2085837260', 'W2101474703', 'W2109469182', 'W2117250519', 'W2118044993', 'W2121365620', 'W2127864991', 'W2139026594', 'W2143560894', 'W2146439308', 'W2147271386', 'W2151554678', 'W2154943049', 'W2166513470', 'W2167580870', 'W2168081761', 'W2261079877', 'W2326804461'], 'abstract': "Abstract   This paper proposes a novel population-based optimization algorithm called Sine Cosine Algorithm (SCA) for solving optimization problems. The SCA creates multiple initial random candidate solutions and requires them to fluctuate outwards or towards the best solution using a mathematical model based on sine and cosine functions. Several random and adaptive variables also are integrated to this algorithm to emphasize exploration and exploitation of the search space in different milestones of optimization. The performance of SCA is benchmarked in three test phases. Firstly, a set of well-known test cases including unimodal, multi-modal, and composite functions are employed to test exploration, exploitation, local optima avoidance, and convergence of SCA. Secondly, several performance metrics (search history, trajectory, average fitness of solutions, and the best solution during optimization) are used to qualitatively observe and confirm the performance of SCA on shifted two-dimensional test functions. Finally, the cross-section of an aircraft's wing is optimized by SCA as a real challenging case study to verify and demonstrate the performance of this algorithm in practice. The results of test functions and performance metrics prove that the algorithm proposed is able to explore different regions of a search space, avoid local optima, converge towards the global optimum, and exploit promising regions of a search space during optimization effectively. The SCA algorithm obtains a smooth shape for the airfoil with a very low drag, which demonstrates that this algorithm can highly be effective in solving real problems with constrained and unknown search spaces. Note that the source codes of the SCA algorithm are publicly available at  http://www.alimirjalili.com/SCA.html .", 'counts_by_year': [[2022, 667], [2021, 611], [2020, 424], [2019, 263], [2018, 148], [2017, 60], [2016, 6]]}, {'id': 'W2781738013', 'doi': 'https://doi.org/10.22331/q-2018-08-06-79', 'title': 'Quantum Computing in the NISQ era and beyond', 'type': 'journal-article', 'publication_date': '2018-01-02', 'host_venue': 'V4210226432', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2038389481', []]], 'cited_by_count': 2179, 'concepts': [['C58053490', '0.8025773'], ['C41008148', '0.6788682'], ['C203087015', '0.5931716'], ['C58849907', '0.535288'], ['C113775141', '0.51355124']], 'referenced_works': ['W1492999010', 'W1571552325', 'W1968850365', 'W2001527002', 'W2002372750', 'W2006290558', 'W2028918948', 'W2030867075', 'W2034053794', 'W2041432245', 'W2041506125', 'W2042127289', 'W2137147061', 'W2161685427', 'W2162676140', 'W2167110831', 'W2174510006', 'W2179731956', 'W2208142350', 'W2214536953', 'W2268604949', 'W2349487082', 'W2468792555', 'W2482126025', 'W2512312062', 'W2559394418', 'W2564229214', 'W2604445392', 'W2735457220', 'W2744305830', 'W2754797405', 'W2768206303', 'W2790700887', 'W2919115771', 'W3037447387', 'W3098876835', 'W3100417094', 'W4297991408'], 'abstract': "Noisy Intermediate-Scale Quantum (NISQ) technology will be available in the near future. Quantum computers with 50-100 qubits may be able to perform tasks which surpass the capabilities of today's classical digital computers, but noise in quantum gates will limit the size of quantum circuits that can be executed reliably. NISQ devices will be useful tools for exploring many-body quantum physics, and may have other useful applications, but the 100-qubit quantum computer will not change the world right away - we should regard it as a significant step toward the more powerful quantum technologies of the future. Quantum technologists should continue to strive for more accurate quantum gates and, eventually, fully fault-tolerant quantum computing.", 'counts_by_year': [[2022, 654], [2021, 771], [2020, 493], [2019, 226], [2018, 32]]}, {'id': 'W2223470559', 'doi': 'https://doi.org/10.1108/imds-09-2015-0382', 'title': 'Using PLS path modeling in new technology research: updated guidelines', 'type': 'journal-article', 'publication_date': '2016-01-07', 'host_venue': 'V37320504', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A1996907579', ['I94624287']], ['A258918929', ['I4210136822']], ['A2154113125', ['I88874673']]], 'cited_by_count': 2177, 'concepts': [['C71104824', '0.86579204'], ['C22354355', '0.7569844'], ['C40722632', '0.6094463'], ['C82793941', '0.6058239'], ['C196083921', '0.59420544']], 'referenced_works': ['W88556870', 'W96624510', 'W144264889', 'W169042837', 'W1530597771', 'W1534477976', 'W1550181012', 'W1557976512', 'W1925416780', 'W1963554663', 'W1966744380', 'W1968206427', 'W1970794614', 'W1975077471', 'W1981513336', 'W1984947418', 'W1986092722', 'W1988050849', 'W1999371986', 'W2001768043', 'W2005092827', 'W2005723452', 'W2030360178', 'W2030630447', 'W2036736987', 'W2037327173', 'W2039861700', 'W2042301765', 'W2052299306', 'W2070818909', 'W2071666535', 'W2072964722', 'W2090552208', 'W2092718724', 'W2093058549', 'W2096205569', 'W2098755711', 'W2100739170', 'W2100872401', 'W2102963621', 'W2105846236', 'W2108401663', 'W2112560104', 'W2118904325', 'W2120419626', 'W2129375743', 'W2130892515', 'W2131292268', 'W2131568786', 'W2137195939', 'W2138071797', 'W2140561695', 'W2149608872', 'W2150785224', 'W2158123328', 'W2169281074', 'W2232992278', 'W2326958296', 'W2400194500', 'W3080706485', 'W3121501521', 'W3140593190', 'W3147343449', 'W3150796314', 'W4235678817'], 'abstract': 'Purpose – Partial least squares (PLS) path modeling is a variance-based structural equation modeling (SEM) technique that is widely applied in business and social sciences. Its ability to model composites and factors makes it a formidable statistical tool for new technology research. Recent reviews, discussions, and developments have led to substantial changes in the understanding and use of PLS. The paper aims to discuss these issues. Design/methodology/approach – This paper aggregates new insights and offers a fresh look at PLS path modeling. It presents new developments, such as consistent PLS, confirmatory composite analysis, and the heterotrait-monotrait ratio of correlations. Findings – PLS path modeling is the method of choice if a SEM contains both factors and composites. Novel tests of exact fit make a confirmatory use of PLS path modeling possible. Originality/value – This paper provides updated guidelines of how to use PLS and how to report and interpret its results.', 'counts_by_year': [[2022, 515], [2021, 576], [2020, 410], [2019, 324], [2018, 219], [2017, 87], [2016, 44], [2015, 1]]}, {'id': 'W2307770531', 'doi': 'https://doi.org/10.1007/978-3-319-46484-8_29', 'title': 'Stacked Hourglass Networks for Human Pose Estimation', 'type': 'book-chapter', 'publication_date': '2016-10-08', 'host_venue': 'V106296714', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2551234027', ['I27837315']], ['A2951192307', ['I27837315']], ['A2608828817', ['I27837315']]], 'cited_by_count': 2177, 'concepts': [['C127532173', '0.9781761'], ['C41008148', '0.85840046'], ['C52102323', '0.6287211'], ['C154945302', '0.5951392'], ['C31972630', '0.58875245']], 'referenced_works': ['W602397586', 'W845365781', 'W1745334888', 'W1903029394', 'W1905829557', 'W1930528368', 'W1932979330', 'W1936750108', 'W1948751323', 'W2013640163', 'W2022508996', 'W2022699039', 'W2043217799', 'W2060280062', 'W2080873731', 'W2097117768', 'W2103015390', 'W2112796928', 'W2113325037', 'W2120419212', 'W2121969814', 'W2128271252', 'W2135533529', 'W2143487029', 'W2174722029', 'W2175012183', 'W2194775991', 'W2272752931', 'W2398640840', 'W2963474899', 'W2963484342', 'W2964304707'], 'abstract': 'This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a "stacked hourglass" network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.', 'counts_by_year': [[2022, 248], [2021, 628], [2020, 575], [2019, 472], [2018, 190], [2017, 50], [2016, 12]]}, {'id': 'W4211152862', 'doi': 'https://doi.org/10.1002/adma.201601694', 'title': 'Heterojunction Photocatalysts', 'type': 'journal-article', 'publication_date': '2017-02-21', 'host_venue': 'V99352657', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2154417694', ['I196699116']], ['A2307123779', ['I185163786', 'I196699116']], ['A1004125181', ['I149910238']], ['A2461385205', ['I185163786']], ['A2145077868', ['I185163786']]], 'cited_by_count': 2166, 'concepts': [['C79794668', '0.87735426'], ['C65165184', '0.84218204'], ['C192562407', '0.80176497'], ['C108225325', '0.5259895'], ['C2779679103', '0.51690197']], 'referenced_works': ['W771914031', 'W789650493', 'W972635201', 'W1189638278', 'W1227131133', 'W1705344437', 'W1780831949', 'W1964946620', 'W1966771620', 'W1967344963', 'W1973373124', 'W1973459402', 'W1975109963', 'W1976492109', 'W1976598897', 'W1977844233', 'W1979112470', 'W1982086087', 'W1984473516', 'W1986252415', 'W1989893726', 'W1990879618', 'W1994672555', 'W1999695878', 'W2002140666', 'W2007773236', 'W2007779886', 'W2010870725', 'W2011087065', 'W2013200536', 'W2015249810', 'W2019873312', 'W2019882583', 'W2020107131', 'W2022621714', 'W2024276736', 'W2025806519', 'W2027563442', 'W2028213311', 'W2028787238', 'W2029443292', 'W2029770600', 'W2030313025', 'W2032257182', 'W2032993878', 'W2033475797', 'W2034290864', 'W2036400710', 'W2037735856', 'W2038731743', 'W2039475059', 'W2042492216', 'W2043008781', 'W2044684548', 'W2046619060', 'W2052632138', 'W2058108555', 'W2058122340', 'W2059298554', 'W2059579378', 'W2061588150', 'W2067644124', 'W2070093383', 'W2071050580', 'W2072096075', 'W2079151014', 'W2079188978', 'W2080333568', 'W2084403853', 'W2085698864', 'W2085786322', 'W2086150366', 'W2086483321', 'W2087009980', 'W2089142154', 'W2089596785', 'W2090870015', 'W2093214315', 'W2094758572', 'W2095547289', 'W2096363471', 'W2097551166', 'W2101925199', 'W2102558003', 'W2103942044', 'W2105708469', 'W2106687461', 'W2109963515', 'W2111062994', 'W2119040809', 'W2119913861', 'W2121958253', 'W2124516466', 'W2124882298', 'W2130751135', 'W2133467767', 'W2138007064', 'W2138869469', 'W2140239697', 'W2142014478', 'W2145274462', 'W2145650166', 'W2146028353', 'W2146926579', 'W2149256400', 'W2151083851', 'W2154420421', 'W2155036604', 'W2159789432', 'W2161936697', 'W2162060649', 'W2164027200', 'W2165553965', 'W2168671702', 'W2169937648', 'W2172355211', 'W2198868487', 'W2210057184', 'W2251445938', 'W2259995196', 'W2297446247', 'W2313547874', 'W2313640174', 'W2316921421', 'W2331265280', 'W2333789365', 'W2334732038', 'W2427477006', 'W2481581164', 'W2556631836', 'W4211024534', 'W4255304946'], 'abstract': 'Semiconductor-based photocatalysis attracts wide attention because of its ability to directly utilize solar energy for production of solar fuels, such as hydrogen and hydrocarbon fuels and for degradation of various pollutants. However, the efficiency of photocatalytic reactions remains low due to the fast electron-hole recombination and low light utilization. Therefore, enormous efforts have been undertaken to solve these problems. Particularly, properly engineered heterojunction photocatalysts are shown to be able to possess higher photocatalytic activity because of spatial separation of photogenerated electron-hole pairs. Here, the basic principles of various heterojunction photocatalysts are systematically discussed. Recent efforts toward the development of heterojunction photocatalysts for various photocatalytic applications are also presented and appraised. Finally, a brief summary and perspectives on the challenges and future directions in the area of heterojunction photocatalysts are also provided.', 'counts_by_year': [[2022, 571], [2021, 504], [2020, 411], [2019, 338], [2018, 268], [2017, 52]]}, {'id': 'W2533800772', 'doi': 'https://doi.org/10.1146/annurev-bioeng-071516-044442', 'title': 'Deep Learning in Medical Image Analysis', 'type': 'journal-article', 'publication_date': '2017-06-20', 'host_venue': 'V171268996', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2150708589', ['I114027177']], ['A2106769687', ['I114027177']], ['A2574530655', ['I197347611']]], 'cited_by_count': 2155, 'concepts': [['C108583219', '0.80012894'], ['C154945302', '0.72664976'], ['C41008148', '0.72640795'], ['C165696696', '0.6692418'], ['C89600930', '0.54278564']], 'referenced_works': ['W22040386', 'W66427752', 'W66531091', 'W281036081', 'W377632744', 'W581674602', 'W948663339', 'W954267072', 'W1457602677', 'W1498436455', 'W1504543387', 'W1802125594', 'W1865800230', 'W1871050032', 'W1884191083', 'W1894871967', 'W1901129140', 'W1903029394', 'W1905882502', 'W1908634864', 'W1910131649', 'W1964155876', 'W1972498024', 'W1974874858', 'W1980114101', 'W1988115241', 'W1992537691', 'W1992998173', 'W1993845689', 'W1999240723', 'W2009561775', 'W2010727394', 'W2017257315', 'W2036109700', 'W2040870580', 'W2059911466', 'W2072128103', 'W2075304274', 'W2076063813', 'W2082526668', 'W2097117768', 'W2100495367', 'W2103857226', 'W2112467442', 'W2112796928', 'W2115167851', 'W2116064496', 'W2116895317', 'W2117130368', 'W2117539524', 'W2124386111', 'W2126598020', 'W2127956423', 'W2136922672', 'W2138716135', 'W2140351287', 'W2145287260', 'W2160815625', 'W2164700406', 'W2168031320', 'W2168175751', 'W2211483859', 'W2217077692', 'W2238108400', 'W2248620004', 'W2253429366', 'W2257979135', 'W2260678261', 'W2275865840', 'W2284198383', 'W2295309472', 'W2310992461', 'W2312404985', 'W2313289912', 'W2318872361', 'W2322371438', 'W2323200062', 'W2334763311', 'W2341106171', 'W2342591535', 'W2343172899', 'W2343973580', 'W2344270078', 'W2344858100', 'W2398921440', 'W2403358744', 'W2406638278', 'W2441649867', 'W2467126091', 'W2560920277', 'W2919115771', 'W3104258355', 'W4238388420', 'W4254300646'], 'abstract': 'This review covers computer-assisted analysis of images in the field of medical imaging. Recent advances in machine learning, especially with regard to deep learning, are helping to identify, classify, and quantify patterns in medical images. At the core of these advances is the ability to exploit hierarchical feature representations learned solely from data, instead of features designed by hand according to domain-specific knowledge. Deep learning is rapidly becoming the state of the art, leading to enhanced performance in various medical applications. We introduce the fundamentals of deep learning methods and review their successes in image registration, detection of anatomical and cellular structures, tissue segmentation, computer-aided disease diagnosis and prognosis, and so on. We conclude by discussing research issues and suggesting future directions for further improvement.', 'counts_by_year': [[2022, 404], [2021, 606], [2020, 533], [2019, 373], [2018, 194], [2017, 35], [2016, 1]]}, {'id': 'W2973457155', 'doi': 'https://doi.org/10.1107/s2059798319011471', 'title': 'Macromolecular structure determination using X-rays, neutrons and electrons: recent developments in <i>Phenix</i>', 'type': 'journal-article', 'publication_date': '2019-10-01', 'host_venue': 'V4210191086', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A1175209802', ['I148283060']], ['A229716128', ['I148283060']], ['A2152546334', ['I181547552']], ['A2677895748', ['I241749']], ['A2150607053', ['I170897317']], ['A1971500302', ['I241749']], ['A2203551669', ['I170897317']], ['A2131352049', ['I1343871089']], ['A2487294372', ['I170897317']], ['A2140040299', ['I241749']], ['A1986061537', ['I148283060']], ['A1619997443', ['I241749']], ['A2307555430', ['I148283060']], ['A2157529955', ['I170897317']], ['A2137213460', ['I241749']], ['A2102065301', ['I170897317']], ['A2113126598', ['I170897317']], ['A3001503475', ['I241749']], ['A1289394121', ['I148283060']], ['A2973623500', ['I241749']], ['A237809531', ['I4210110743']], ['A1403136718', ['I154526488', 'I90183372']], ['A2597844890', ['I170897317']], ['A2303347423', ['I170897317']], ['A2118241700', ['I95457486', 'I148283060']]], 'cited_by_count': 2151, 'concepts': [['C2777904410', '0.6281252'], ['C177212765', '0.6241327'], ['C41008148', '0.6126569'], ['C152568617', '0.5759807'], ['C37789001', '0.45217445']], 'referenced_works': ['W1503688477', 'W1586792701', 'W1710565104', 'W1755025530', 'W1838872057', 'W1948300520', 'W1965277349', 'W1966041739', 'W1971944966', 'W1982891627', 'W1985854865', 'W1987759270', 'W1989160710', 'W1995017064', 'W1996607909', 'W1997972388', 'W2000843938', 'W2001265759', 'W2006958997', 'W2007697641', 'W2008700425', 'W2009110345', 'W2011301426', 'W2013083986', 'W2016466447', 'W2017393660', 'W2019701891', 'W2023788554', 'W2027249579', 'W2027797186', 'W2027918314', 'W2030424572', 'W2034144274', 'W2034484189', 'W2035838126', 'W2039105249', 'W2040940915', 'W2045481255', 'W2051925784', 'W2052056634', 'W2054263998', 'W2060914577', 'W2072404529', 'W2074986801', 'W2077980341', 'W2080191389', 'W2082039335', 'W2084393469', 'W2086812659', 'W2088437224', 'W2092220359', 'W2092450662', 'W2092769931', 'W2094646744', 'W2096184392', 'W2098167941', 'W2103714813', 'W2105221293', 'W2106290432', 'W2107527796', 'W2109184569', 'W2110518308', 'W2111270835', 'W2114155688', 'W2115339329', 'W2115813898', 'W2116138854', 'W2116275717', 'W2116389515', 'W2117338104', 'W2119342149', 'W2119489910', 'W2120611061', 'W2120702985', 'W2124026197', 'W2124266141', 'W2124464974', 'W2124546624', 'W2125254321', 'W2126807072', 'W2127130852', 'W2131350133', 'W2132493466', 'W2135083090', 'W2136553340', 'W2139042077', 'W2139544554', 'W2139654449', 'W2143694829', 'W2144795375', 'W2149026172', 'W2149360962', 'W2152562710', 'W2154122002', 'W2154197653', 'W2154714625', 'W2155667921', 'W2157052500', 'W2163341755', 'W2170163394', 'W2180229411', 'W2288190125', 'W2293860015', 'W2314496521', 'W2325521056', 'W2421613153', 'W2481449681', 'W2506310782', 'W2516983008', 'W2529135110', 'W2566907429', 'W2583422618', 'W2585093856', 'W2591431842', 'W2620656507', 'W2621369359', 'W2734356283', 'W2753200340', 'W2754450298', 'W2765322245', 'W2775368449', 'W2785617381', 'W2786390877', 'W2788833468', 'W2791492223', 'W2791670278', 'W2795861088', 'W2801376357', 'W2807241155', 'W2882999099', 'W2887049384', 'W2897504342', 'W2897736099', 'W2898364362', 'W2899058127', 'W2908089432', 'W2938491141', 'W2951455412', 'W2952267119', 'W4256485542', 'W4300881021'], 'abstract': 'Diffraction (X-ray, neutron and electron) and electron cryo-microscopy are powerful methods to determine three-dimensional macromolecular structures, which are required to understand biological processes and to develop new therapeutics against diseases. The overall structure-solution workflow is similar for these techniques, but nuances exist because the properties of the reduced experimental data are different. Software tools for structure determination should therefore be tailored for each method. Phenix is a comprehensive software package for macromolecular structure determination that handles data from any of these techniques. Tasks performed with Phenix include data-quality assessment, map improvement, model building, the validation/rebuilding/refinement cycle and deposition. Each tool caters to the type of experimental data. The design of Phenix emphasizes the automation of procedures, where possible, to minimize repetitive and time-consuming manual tasks, while default parameters are chosen to encourage best practice. A graphical user interface provides access to many command-line features of Phenix and streamlines the transition between programs, project tracking and re-running of previous tasks.', 'counts_by_year': [[2022, 769], [2021, 858], [2020, 507], [2019, 14]]}, {'id': 'W2767683865', 'doi': 'https://doi.org/10.1093/nar/gkx1089', 'title': 'HMDB 4.0: the human metabolome database for 2018', 'type': 'journal-article', 'publication_date': '2018-01-04', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2118994065', ['I185491429']], ['A2546637811', ['I154425047']], ['A2096010429', ['I154425047']], ['A2107894144', ['I154425047']], ['A2752316631', ['I154425047']], ['A1963268390', ['I154425047']], ['A290514958', ['I154425047']], ['A2801683349', ['I154425047']], ['A2767389256', ['I154425047']], ['A2026752087', ['I154425047']], ['A2767509121', ['I154425047']], ['A2767746375', ['I154425047']], ['A2767963169', ['I154425047']], ['A115983743', ['I154425047']], ['A2103475127', ['I154425047']], ['A2162292392', ['I154425047']], ['A2136264431', ['I154425047']], ['A2767411751', ['I154425047']], ['A2103281349', ['I154425047']], ['A2767268171', ['I154425047']], ['A2250777276', ['I154425047']], ['A2767463920', ['I154425047']], ['A1936857475', ['I42237331']], ['A2166265495', ['I154425047']], ['A2105842450', ['I154425047']], ['A2473197116', ['I154425047']], ['A2166632530', ['I198244214']], ['A1948239873', ['I42237331']]], 'cited_by_count': 2150, 'concepts': [['C135870905', '0.88671887'], ['C21565614', '0.8024169'], ['C2777477808', '0.7158352'], ['C86803240', '0.55790526'], ['C116834253', '0.47152576']], 'referenced_works': ['W1162960685', 'W1934481644', 'W1935434993', 'W1960974681', 'W1963702345', 'W1964880353', 'W1970709685', 'W1982390234', 'W1983624870', 'W1986657538', 'W1989277387', 'W2018518196', 'W2053385401', 'W2059327215', 'W2063028213', 'W2084375915', 'W2096769282', 'W2098499808', 'W2101394823', 'W2107869090', 'W2117300383', 'W2121249519', 'W2123572062', 'W2125069590', 'W2125318012', 'W2128601422', 'W2165681080', 'W2177317049', 'W2340933449', 'W2462785116', 'W2504691963', 'W2533446135', 'W2548357532', 'W2554354547', 'W2559588208', 'W2562812629', 'W2604808360', 'W4210702584', 'W4229743141', 'W4246954632'], 'abstract': "The Human Metabolome Database or HMDB (www.hmdb.ca) is a web-enabled metabolomic database containing comprehensive information about human metabolites along with their biological roles, physiological concentrations, disease associations, chemical reactions, metabolic pathways, and reference spectra. First described in 2007, the HMDB is now considered the standard metabolomic resource for human metabolic studies. Over the past decade the HMDB has continued to grow and evolve in response to emerging needs for metabolomics researchers and continuing changes in web standards. This year's update, HMDB 4.0, represents the most significant upgrade to the database in its history. For instance, the number of fully annotated metabolites has increased by nearly threefold, the number of experimental spectra has grown by almost fourfold and the number of illustrated metabolic pathways has grown by a factor of almost 60. Significant improvements have also been made to the HMDB's chemical taxonomy, chemical ontology, spectral viewing, and spectral/text searching tools. A great deal of brand new data has also been added to HMDB 4.0. This includes large quantities of predicted MS/MS and GC-MS reference spectral data as well as predicted (physiologically feasible) metabolite structures to facilitate novel metabolite identification. Additional information on metabolite-SNP interactions and the influence of drugs on metabolite levels (pharmacometabolomics) has also been added. Many other important improvements in the content, the interface, and the performance of the HMDB website have been made and these should greatly enhance its ease of use and its potential applications in nutrition, biochemistry, clinical chemistry, clinical genetics, medicine, and metabolomics science.", 'counts_by_year': [[2022, 448], [2021, 646], [2020, 541], [2019, 399], [2018, 113], [2014, 1]]}, {'id': 'W2901303766', 'doi': 'https://doi.org/10.1093/nar/gky1120', 'title': 'The NHGRI-EBI GWAS Catalog of published genome-wide association studies, targeted arrays and summary statistics 2019', 'type': 'journal-article', 'publication_date': '2019-01-08', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2609156039', ['I1303153112']], ['A1971827112', ['I1303153112']], ['A2801005943', ['I1303153112']], ['A2785706624', ['I1303153112']], ['A2769214899', ['I1303153112']], ['A2785496552', ['I1303153112']], ['A2509215578', ['I1303153112']], ['A2271944062', ['I1303153112']], ['A2241453124', ['I1336263701', 'I2802476451']], ['A2901648081', ['I1303153112']], ['A2653547720', ['I1303153112']], ['A2289709902', ['I1303153112']], ['A1517510072', ['I1303153112']], ['A2574272158', ['I1303153112']], ['A2901472636', ['I1303153112']], ['A2340891374', ['I1303153112']], ['A742642064', ['I1303153112']], ['A2240922891', ['I4210090236']], ['A1997756573', ['I4210090236']], ['A2121717666', ['I1303153112']], ['A1968424439', ['I1303153112']], ['A3037708016', ['I4210090236']], ['A2210664129', ['I1303153112']], ['A2046771686', ['I1303153112']]], 'cited_by_count': 2125, 'concepts': [['C106208931', '0.7173165'], ['C32792767', '0.57509893'], ['C113843644', '0.5022371'], ['C86803240', '0.47488946'], ['C41008148', '0.41294345']], 'referenced_works': ['W1533942137', 'W1975705143', 'W2002715948', 'W2058401000', 'W2087779040', 'W2101983393', 'W2104549677', 'W2118822739', 'W2143282831', 'W2147018385', 'W2158005146', 'W2161633633', 'W2163013660', 'W2302501749', 'W2336130565', 'W2544485223', 'W2550031966', 'W2557530941', 'W2558411436', 'W2559028527', 'W2601822003', 'W2609876203', 'W2609955914', 'W2617316722', 'W2724938313', 'W2734534935', 'W2735404270', 'W2736356540', 'W2755166087', 'W2765253743', 'W2793718487', 'W2951286101', 'W2951751247', 'W4252232449'], 'abstract': "The GWAS Catalog delivers a high-quality curated collection of all published genome-wide association studies enabling investigations to identify causal variants, understand disease mechanisms, and establish targets for novel therapies. The scope of the Catalog has also expanded to targeted and exome arrays with 1000 new associations added for these technologies. As of September 2018, the Catalog contains 5687 GWAS comprising 71673 variant-trait associations from 3567 publications. New content includes 284 full P-value summary statistics datasets for genome-wide and new targeted array studies, representing 6 × 109 individual variant-trait statistics. In the last 12 months, the Catalog's user interface was accessed by ∼90000 unique users who viewed >1 million pages. We have improved data access with the release of a new RESTful API to support high-throughput programmatic access, an improved web interface and a new summary statistics database. Summary statistics provision is supported by a new format proposed as a community standard for summary statistics data representation. This format was derived from our experience in standardizing heterogeneous submissions, mapping formats and in harmonizing content. Availability: https://www.ebi.ac.uk/gwas/.", 'counts_by_year': [[2022, 588], [2021, 791], [2020, 546], [2019, 193], [2018, 4], [2016, 1]]}, {'id': 'W2180612164', 'doi': 'https://doi.org/10.1109/eurosp.2016.36', 'title': 'The Limitations of Deep Learning in Adversarial Settings', 'type': 'proceedings-article', 'publication_date': '2016-03-21', 'host_venue': 'V4306418620', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A248975517', ['I130769515']], ['A2056207806', ['I130769515']], ['A2193269139', ['I135310074']], ['A2076493382', ['I135310074']], ['A2250297608', ['I130769515']], ['A2059211748', ['I166416128']]], 'cited_by_count': 2120, 'concepts': [['C37736160', '0.9166546'], ['C41008148', '0.80152726'], ['C154945302', '0.7510411'], ['C108583219', '0.69444454'], ['C2984842247', '0.68430936']], 'referenced_works': ['W9657784', 'W1966948031', 'W2018061979', 'W2032247543', 'W2095577883', 'W2105037940', 'W2108069432', 'W2112796928', 'W2117130368', 'W2125908420', 'W2136922672', 'W2137983211', 'W2144906988', 'W2145287260', 'W2147768505', 'W2151298633', 'W4231109964'], 'abstract': 'Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.', 'counts_by_year': [[2022, 186], [2021, 492], [2020, 556], [2019, 470], [2018, 296], [2017, 92], [2016, 24], [2015, 1]]}, {'id': 'W2890167612', 'doi': 'https://doi.org/10.1186/s12951-018-0392-8', 'title': 'Nano based drug delivery systems: recent developments and future prospects', 'type': 'journal-article', 'publication_date': '2018-09-19', 'host_venue': 'V41231013', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2604069452', ['I205490536']], ['A2529346604', ['I205490536']], ['A2097870276', ['I181391015', 'I879563668']], ['A2172850728', ['I181391015', 'I879563668']], ['A1972622317', ['I8961855']], ['A2008722428', ['I8961855']], ['A2193964727', ['I2799391333']], ['A2042319754', ['I879563668']], ['A2113264034', ['I130343225']], ['A2943019880', ['I152869788']], ['A2209967112', ['I55060895']], ['A2148875667', ['I205490536']]], 'cited_by_count': 2118, 'concepts': [['C15083742', '0.8486812'], ['C2779820397', '0.6422473'], ['C171250308', '0.58866954'], ['C71924100', '0.37232554'], ['C41008148', '0.33862454']], 'referenced_works': ['W164136508', 'W178364032', 'W837950325', 'W948795147', 'W1088146625', 'W1238203966', 'W1487878063', 'W1498725963', 'W1558022610', 'W1559815414', 'W1564553363', 'W1567847444', 'W1652344970', 'W1757165467', 'W1768583667', 'W1770710039', 'W1781591963', 'W1855860163', 'W1866674918', 'W1901682227', 'W1923755602', 'W1925375217', 'W1949277921', 'W1964429118', 'W1964722139', 'W1964957951', 'W1965063429', 'W1966517756', 'W1967080357', 'W1969287626', 'W1971500643', 'W1974338066', 'W1974854083', 'W1974889520', 'W1976418823', 'W1976827948', 'W1978120234', 'W1978609085', 'W1980427890', 'W1980879798', 'W1981930609', 'W1982228392', 'W1984996101', 'W1987021228', 'W1989230986', 'W1991393505', 'W1991494036', 'W1992535564', 'W1993331221', 'W1994306170', 'W1994936258', 'W1998934929', 'W2000303365', 'W2001854851', 'W2001921621', 'W2002896114', 'W2003147476', 'W2005326953', 'W2007307087', 'W2007755698', 'W2010268411', 'W2010288088', 'W2010312818', 'W2012453176', 'W2013765163', 'W2015175257', 'W2016925709', 'W2017255665', 'W2019985255', 'W2020373947', 'W2020379763', 'W2022396726', 'W2023585844', 'W2023834592', 'W2023959068', 'W2028593763', 'W2030224115', 'W2031570851', 'W2033069724', 'W2033468145', 'W2033588393', 'W2035931888', 'W2037834896', 'W2046070477', 'W2050473438', 'W2051925512', 'W2053890812', 'W2056962182', 'W2057941758', 'W2058104225', 'W2059061770', 'W2061706654', 'W2061955697', 'W2069390639', 'W2070517405', 'W2071133343', 'W2075559965', 'W2077075154', 'W2078761335', 'W2079555182', 'W2079875646', 'W2085277747', 'W2085479511', 'W2085911180', 'W2086169249', 'W2087020662', 'W2087413148', 'W2088172653', 'W2088467380', 'W2089155556', 'W2091021629', 'W2094298976', 'W2094456976', 'W2097603366', 'W2097975939', 'W2105502591', 'W2106399070', 'W2113865463', 'W2115524611', 'W2118165386', 'W2119186031', 'W2125156265', 'W2128168622', 'W2132925790', 'W2136548272', 'W2140571082', 'W2140813917', 'W2143463421', 'W2150391637', 'W2155824390', 'W2155908573', 'W2159411213', 'W2159603410', 'W2163975451', 'W2177205490', 'W2183835768', 'W2186379769', 'W2187976253', 'W2207005572', 'W2222132335', 'W2242638709', 'W2253324924', 'W2289045877', 'W2289886805', 'W2293444034', 'W2295224762', 'W2296389641', 'W2312194630', 'W2315207544', 'W2315784420', 'W2319771224', 'W2320866708', 'W2321825468', 'W2322234018', 'W2323133287', 'W2328936102', 'W2334202065', 'W2337514604', 'W2341429116', 'W2342805349', 'W2344598639', 'W2344949123', 'W2345198528', 'W2346448021', 'W2377171213', 'W2388009875', 'W2390012465', 'W2397341940', 'W2402319960', 'W2410643285', 'W2412554453', 'W2415083690', 'W2421275313', 'W2425706463', 'W2440054719', 'W2473477171', 'W2473736632', 'W2476407594', 'W2490849378', 'W2496476352', 'W2507169934', 'W2508883898', 'W2514502452', 'W2515764167', 'W2517840899', 'W2519461793', 'W2519996058', 'W2531834066', 'W2550400378', 'W2553395507', 'W2553831350', 'W2557200452', 'W2558584623', 'W2561004881', 'W2567109577', 'W2568141085', 'W2570883506', 'W2571688447', 'W2575367849', 'W2584275599', 'W2586322963', 'W2587005936', 'W2588691843', 'W2590228427', 'W2590551804', 'W2592683305', 'W2594437595', 'W2596775702', 'W2602684234', 'W2605626462', 'W2606017739', 'W2606662964', 'W2610608736', 'W2611661147', 'W2614773512', 'W2721932952', 'W2726037479', 'W2736591160', 'W2737417837', 'W2739685450', 'W2746831896', 'W2749151718', 'W2751235455', 'W2751837387', 'W2751868510', 'W2752133697', 'W2752958725', 'W2753159843', 'W2756173910', 'W2760251644', 'W2765945609', 'W2766487611', 'W2766870420', 'W2768835484', 'W2771155547', 'W2771842669', 'W2772877544', 'W2774357999', 'W2780877489', 'W2783111760', 'W2783196030', 'W2789244501', 'W2792311144', 'W2981835105', 'W4211093154', 'W4245272488', 'W4245622808'], 'abstract': 'Nanomedicine and nano delivery systems are a relatively new but rapidly developing science where materials in the nanoscale range are employed to serve as means of diagnostic tools or to deliver therapeutic agents to specific targeted sites in a controlled manner. Nanotechnology offers multiple benefits in treating chronic human diseases by site-specific, and target-oriented delivery of precise medicines. Recently, there are a number of outstanding applications of the nanomedicine (chemotherapeutic agents, biological agents, immunotherapeutic agents etc.) in the treatment of various diseases. The current review, presents an updated summary of recent advances in the field of nanomedicines and nano based drug delivery systems through comprehensive scrutiny of the discovery and application of nanomaterials in improving both the efficacy of novel and old drugs (e.g., natural products) and selective diagnosis through disease marker molecules. The opportunities and challenges of nanomedicines in drug delivery from synthetic/natural sources to their clinical applications are also discussed. In addition, we have included information regarding the trends and perspectives in nanomedicine area.', 'counts_by_year': [[2022, 777], [2021, 807], [2020, 431], [2019, 94], [2018, 1]]}, {'id': 'W2912990441', 'doi': 'https://doi.org/10.1038/s41587-019-0036-z', 'title': 'SignalP 5.0 improves signal peptide predictions using deep neural networks', 'type': 'journal-article', 'publication_date': '2019-02-18', 'host_venue': 'V106963461', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2732128904', ['I96673099']], ['A2572777647', ['I96673099']], ['A2052823787', ['I124055696']], ['A2093869587', ['I96673099']], ['A2065379623', ['I124055696', 'I96673099']], ['A620960263', ['I96673099', 'I2801134892']], ['A727971404', ['I161593684', 'I2800139495']], ['A2494357043', ['I96673099']]], 'cited_by_count': 2117, 'concepts': [['C50644808', '0.68725747'], ['C10858879', '0.6359109'], ['C2779843651', '0.5010228'], ['C2779281246', '0.49859643'], ['C41008148', '0.43616045']], 'referenced_works': ['W60686164', 'W950853366', 'W1602952910', 'W1935158889', 'W1938173378', 'W1971147414', 'W1973904153', 'W2006201402', 'W2020969907', 'W2041652393', 'W2042620330', 'W2047298552', 'W2047663485', 'W2064675550', 'W2094201044', 'W2104294109', 'W2105537627', 'W2107158607', 'W2109553965', 'W2122550753', 'W2123186393', 'W2126981692', 'W2130393314', 'W2131778683', 'W2133245867', 'W2134759004', 'W2143210482', 'W2149241518', 'W2149407349', 'W2149649966', 'W2158623906', 'W2158714788', 'W2161746138', 'W2164025376', 'W2164186284', 'W2164751980', 'W2165698076', 'W2170747616', 'W2171091522', 'W2600277188', 'W2730472814', 'W2776525063', 'W2809371850', 'W2919115771', 'W4233120011'], 'abstract': 'Signal peptides (SPs) are short amino acid sequences in the amino terminus of many newly synthesized proteins that target proteins into, or across, membranes. Bioinformatic tools can predict SPs from amino acid sequences, but most cannot distinguish between various types of signal peptides. We present a deep neural network-based approach that improves SP prediction across all domains of life and distinguishes between three types of prokaryotic SPs.', 'counts_by_year': [[2022, 610], [2021, 831], [2020, 531], [2019, 138], [2018, 3]]}, {'id': 'W2800392236', 'doi': 'https://doi.org/10.1186/s13059-017-1382-0', 'title': 'SCANPY: large-scale single-cell gene expression data analysis', 'type': 'journal-article', 'publication_date': '2018-02-06', 'host_venue': 'V81160022', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2337662871', ['I3018134672']], ['A2638161309', ['I3018134672']], ['A2229870300', ['I62916508']]], 'cited_by_count': 2111, 'concepts': [['C519991488', '0.67944336'], ['C34736171', '0.6277952'], ['C2776214188', '0.61760294'], ['C36464697', '0.6095979'], ['C86803240', '0.58056545']], 'referenced_works': ['W1631320694', 'W1967327758', 'W1973094248', 'W1982729887', 'W1984883254', 'W2006554089', 'W2010653277', 'W2011301426', 'W2047594345', 'W2069089843', 'W2102212449', 'W2109890799', 'W2125886217', 'W2128120736', 'W2131681506', 'W2146292423', 'W2155992983', 'W2167482691', 'W2181523240', 'W2190545194', 'W2342016684', 'W2342249984', 'W2344887288', 'W2509957998', 'W2516455020', 'W2540833380', 'W2555892463', 'W2561754210', 'W2591733518', 'W2735067671', 'W2739367945', 'W2747877289', 'W2750796620', 'W2767792097', 'W2919115771', 'W2951158909', 'W2951506174', 'W2951764674', 'W2952650776', 'W2963899128', 'W4213108508'], 'abstract': 'SCANPY is a scalable toolkit for analyzing single-cell gene expression data. It includes methods for preprocessing, visualization, clustering, pseudotime and trajectory inference, differential expression testing, and simulation of gene regulatory networks. Its Python-based implementation efficiently deals with data sets of more than one million cells ( https://github.com/theislab/Scanpy ). Along with SCANPY, we present ANNDATA, a generic class for handling annotated data matrices ( https://github.com/theislab/anndata ).', 'counts_by_year': [[2022, 647], [2021, 719], [2020, 485], [2019, 205], [2018, 49], [2017, 3]]}, {'id': 'W4250359879', 'doi': 'https://doi.org/10.1093/nar/gkx1098', 'title': 'Ensembl 2018', 'type': 'journal-article', 'publication_date': '2017-11-16', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A1980315147', []], ['A4258004911', []], ['A4212599557', []], ['A4258004912', []], ['A4258004913', []], ['A4258004914', []], ['A2769423853', []], ['A4258004915', []], ['A4258004916', []], ['A2800604722', []], ['A4258004917', []], ['A4258004918', []], ['A4258004919', []], ['A4258004920', []], ['A106429203', []], ['A4258004921', []], ['A4258004922', []], ['A265768378', []], ['A4258004923', []], ['A4258004924', []], ['A4258004925', []], ['A4258004926', []], ['A2071231283', []], ['A4258004927', []], ['A4258004928', []], ['A4258004929', []], ['A4258004930', []], ['A2475562344', []], ['A4258004931', []], ['A2048990951', []], ['A4258004932', []], ['A4258004933', []], ['A4258004934', []], ['A4258004935', []], ['A4258004936', []], ['A4258004937', []], ['A4258004938', []], ['A4258004939', []], ['A4258004940', []], ['A4258004941', []], ['A4258004942', []], ['A4258004943', []], ['A2007620206', []], ['A4258004944', []], ['A4258004945', []], ['A4258004946', []], ['A4258004947', []], ['A2342696407', []], ['A4258004948', []], ['A2431463432', []], ['A1513018408', []], ['A4258004949', []], ['A4258004950', []], ['A4258004951', []], ['A2210664129', []], ['A4258004952', []], ['A2121717666', []]], 'cited_by_count': 2106, 'concepts': [['C141674004', '0.99165595'], ['C189206191', '0.7192635'], ['C141231307', '0.57332665'], ['C86803240', '0.5630336'], ['C2781148417', '0.53277695']], 'referenced_works': ['W836250636', 'W1907868928', 'W2015885891', 'W2049302531', 'W2059145105', 'W2063322714', 'W2096465161', 'W2100203037', 'W2101735289', 'W2104549677', 'W2107916366', 'W2115950580', 'W2120141270', 'W2121984048', 'W2122732537', 'W2131581981', 'W2137886330', 'W2156564520', 'W2174602966', 'W2256016639', 'W2259938310', 'W2278840695', 'W2281531600', 'W2302501749', 'W2309021002', 'W2320983896', 'W2417483443', 'W2464717012', 'W2559722174', 'W2601787998', 'W2951090538', 'W4210659358', 'W4210702584', 'W4252553099'], 'abstract': 'The Ensembl project has been aggregating, processing, integrating and redistributing genomic datasets since the initial releases of the draft human genome, with the aim of accelerating genomics research through rapid open distribution of public data. Large amounts of raw data are thus transformed into knowledge, which is made available via a multitude of channels, in particular our browser (http://www.ensembl.org). Over time, we have expanded in multiple directions. First, our resources describe multiple fields of genomics, in particular gene annotation, comparative genomics, genetics and epigenomics. Second, we cover a growing number of genome assemblies; Ensembl Release 90 contains exactly 100. Third, our databases feed simultaneously into an array of services designed around different use cases, ranging from quick browsing to genome-wide bioinformatic analysis. We present here the latest developments of the Ensembl project, with a focus on managing an increasing number of assemblies, supporting efforts in genome interpretation and improving our browser.', 'counts_by_year': [[2022, 170], [2021, 350], [2020, 614], [2019, 703], [2018, 261], [2017, 5], [2016, 2], [2012, 1]]}, {'id': 'W2734356283', 'doi': 'https://doi.org/10.1002/pro.3235', 'title': 'UCSF ChimeraX: Meeting modern challenges in visualization and analysis', 'type': 'journal-article', 'publication_date': '2018-01-01', 'host_venue': 'V156919612', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2006193790', ['I180670191']], ['A2119315065', ['I180670191']], ['A2139850466', ['I180670191']], ['A1455141185', ['I180670191']], ['A2114256413', ['I180670191']], ['A2612353087', ['I180670191']], ['A36403351', ['I180670191']]], 'cited_by_count': 2094, 'concepts': [['C36464697', '0.7810427'], ['C170130773', '0.7095479'], ['C41008148', '0.70714843'], ['C205711294', '0.6234008'], ['C2777904410', '0.5653179']], 'referenced_works': ['W1533371356', 'W1819141267', 'W1965112674', 'W1994344551', 'W1995808589', 'W2004360551', 'W2004730453', 'W2013315897', 'W2029428843', 'W2029582401', 'W2029667189', 'W2030947664', 'W2032592607', 'W2036348412', 'W2053270078', 'W2053448121', 'W2054889627', 'W2056236390', 'W2064140868', 'W2065283382', 'W2085277871', 'W2096525273', 'W2097508475', 'W2113403117', 'W2127322768', 'W2129138058', 'W2131736388', 'W2132629607', 'W2134967712', 'W2137531873', 'W2138511932', 'W2147231162', 'W2150852650', 'W2153544371', 'W2154069299', 'W2159675211', 'W2160943260', 'W2170441710', 'W2224056471', 'W2422129218', 'W2508241396', 'W2529135110', 'W2537623931', 'W2558272290', 'W2759448220', 'W4210702584', 'W4233379990', 'W4240401512', 'W4251386234', 'W4293003482'], 'abstract': 'UCSF ChimeraX is next-generation software for the visualization and analysis of molecular structures, density maps, 3D microscopy, and associated data. It addresses challenges in the size, scope, and disparate types of data attendant with cutting-edge experimental methods, while providing advanced options for high-quality rendering (interactive ambient occlusion, reliable molecular surface calculations, etc.) and professional approaches to software design and distribution. This article highlights some specific advances in the areas of visualization and usability, performance, and extensibility. ChimeraX is free for noncommercial use and is available from http://www.rbvi.ucsf.edu/chimerax/ for Windows, Mac, and Linux.', 'counts_by_year': [[2022, 538], [2021, 719], [2020, 552], [2019, 210], [2018, 65], [2017, 8]]}, {'id': 'W2963800363', 'doi': 'https://doi.org/10.1109/cvpr.2018.00917', 'title': 'High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs', 'type': 'proceedings-article', 'publication_date': '2018-06-18', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2167546878', ['I1304085615']], ['A2112232458', ['I95457486']], ['A2295189809', ['I1304085615']], ['A1987057275', ['I1304085615']], ['A2688031072', ['I1304085615']]], 'cited_by_count': 2093, 'concepts': [['C41008148', '0.8731631'], ['C2779803651', '0.78796035'], ['C2780992000', '0.73912555'], ['C154945302', '0.65403575'], ['C2781238097', '0.5827997']], 'referenced_works': ['W1903029394', 'W2026019603', 'W2103504761', 'W2103559027', 'W2134921974', 'W2138632986', 'W2163879248', 'W2194775991', 'W2340897893', 'W2475287302', 'W2556872594', 'W2560023338', 'W2566832195', 'W2584009249', 'W2592232824', 'W2593414223', 'W2612063021', 'W2625219738', 'W2737258237', 'W2738588019', 'W2744091666', 'W2962793481', 'W2963073614', 'W2963420272', 'W2963444790', 'W2963470893', 'W2963522749', 'W2963590054', 'W2963709863', 'W2964024144', 'W2964313012', 'W4240805545', 'W4243684731'], 'abstract': 'We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048 A— 1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.', 'counts_by_year': [[2022, 229], [2021, 712], [2020, 614], [2019, 429], [2018, 104], [2017, 2]]}, {'id': 'W2544360569', 'doi': 'https://doi.org/10.1038/nprot.2016.136', 'title': 'The MaxQuant computational platform for mass spectrometry-based shotgun proteomics', 'type': 'journal-article', 'publication_date': '2016-12-01', 'host_venue': 'V109387254', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A1207563747', ['I4210150093']], ['A1259385706', ['I4210150093']], ['A2223602229', ['I4210150093']]], 'cited_by_count': 2091, 'concepts': [['C41008148', '0.75115645'], ['C68289359', '0.70168793'], ['C177212765', '0.67739105'], ['C2780385302', '0.5943069'], ['C2777904410', '0.54649895']], 'referenced_works': ['W25499092', 'W35024136', 'W784374387', 'W1480858518', 'W1851565937', 'W1974667981', 'W1974809941', 'W1978640976', 'W1979240652', 'W1981153918', 'W1983285663', 'W1984450835', 'W1988146323', 'W1996310767', 'W2015556811', 'W2036321220', 'W2041399392', 'W2042510382', 'W2048280438', 'W2051145352', 'W2073371472', 'W2078598194', 'W2080752012', 'W2087474953', 'W2096057003', 'W2104391291', 'W2106074866', 'W2118001361', 'W2120016245', 'W2123442465', 'W2124091237', 'W2125574061', 'W2127100016', 'W2128551987', 'W2129897014', 'W2140638448', 'W2141091417', 'W2143340505', 'W2146122183', 'W2151619201', 'W2152983548', 'W2156244206', 'W2170776626', 'W2172295047', 'W2438740772', 'W2463195069', 'W2952600548', 'W4242256187'], 'abstract': 'MaxQuant is one of the most frequently used platforms for mass-spectrometry (MS)-based proteomics data analysis. Since its first release in 2008, it has grown substantially in functionality and can be used in conjunction with more MS platforms. Here we present an updated protocol covering the most important basic computational workflows, including those designed for quantitative label-free proteomics, MS1-level labeling and isobaric labeling techniques. This protocol presents a complete description of the parameters used in MaxQuant, as well as of the configuration options of its integrated search engine, Andromeda. This protocol update describes an adaptation of an existing protocol that substantially modifies the technique. Important concepts of shotgun proteomics and their implementation in MaxQuant are briefly reviewed, including different quantification strategies and the control of false-discovery rates (FDRs), as well as the analysis of post-translational modifications (PTMs). The MaxQuant output tables, which contain information about quantification of proteins and PTMs, are explained in detail. Furthermore, we provide a short version of the workflow that is applicable to data sets with simple and standard experimental designs. The MaxQuant algorithms are efficiently parallelized on multiple processors and scale well from desktop computers to servers with many cores. The software is written in C# and is freely available at http://www.maxquant.org.', 'counts_by_year': [[2022, 465], [2021, 573], [2020, 487], [2019, 304], [2018, 189], [2017, 66], [2016, 2], [2015, 1]]}, {'id': 'W2738900493', 'doi': 'https://doi.org/10.1016/j.advengsoft.2017.07.002', 'title': 'Salp Swarm Algorithm: A bio-inspired optimizer for engineering design problems', 'type': 'journal-article', 'publication_date': '2017-12-01', 'host_venue': 'V16540516', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A3006208000', ['I11701301']], ['A1871099923', ['I87216513']], ['A3006208000', ['I78757542']], ['A2114835760', ['I11701301']], ['A2035841121', ['I114972647']], ['A3006208000', ['I60158472']]], 'cited_by_count': 2090, 'concepts': [['C181335050', '0.5854185'], ['C41008148', '0.47843865'], ['C126255220', '0.4430457'], ['C11413529', '0.4281491'], ['C127413603', '0.36129987']], 'referenced_works': ['W175452270', 'W415907358', 'W883434633', 'W1423218505', 'W1541288193', 'W1595159159', 'W1968370297', 'W1973412675', 'W1975661701', 'W1976093834', 'W1980048226', 'W1980432290', 'W1983362686', 'W1985334587', 'W1985460844', 'W1990966828', 'W1992656046', 'W1993885071', 'W1994239177', 'W1997600725', 'W1999284878', 'W2001979953', 'W2002713790', 'W2003890325', 'W2003961265', 'W2006694777', 'W2007351764', 'W2007435376', 'W2010882086', 'W2013533212', 'W2020320008', 'W2022912942', 'W2024008934', 'W2024060531', 'W2034988449', 'W2036973403', 'W2038984515', 'W2047156683', 'W2053900989', 'W2056416254', 'W2060500705', 'W2061438946', 'W2065401134', 'W2072245049', 'W2072955302', 'W2076220666', 'W2076608183', 'W2083000360', 'W2091638274', 'W2093195672', 'W2096524645', 'W2108179244', 'W2111592791', 'W2118044993', 'W2125899728', 'W2126105956', 'W2131355461', 'W2135879356', 'W2140796089', 'W2143055330', 'W2143381319', 'W2143560894', 'W2145479420', 'W2147271386', 'W2149815769', 'W2150031358', 'W2151554678', 'W2165171393', 'W2167580870', 'W2168081761', 'W2172242248', 'W2232317135', 'W2290883490', 'W2521957736', 'W2560490998', 'W2561648699', 'W2912808862', 'W4246598646'], 'abstract': 'A novel optimization algorithm called Salp Swarm Optimizer (SSA) is proposed.Multi-objective Salp Swarm Algorithm (MSSA) is proposed to solve multi-objective problems.Both algorithms are tested on several mathematical optimization functions.Two challenging engineering design problems are solved: airfoil design and marine propeller design.The qualitative and quantitative results prove the efficiency of SSA and MSSA. This work proposes two novel optimization algorithms called Salp Swarm Algorithm (SSA) and Multi-objective Salp Swarm Algorithm (MSSA) for solving optimization problems with single and multiple objectives. The main inspiration of SSA and MSSA is the swarming behaviour of salps when navigating and foraging in oceans. These two algorithms are tested on several mathematical optimization functions to observe and confirm their effective behaviours in finding the optimal solutions for optimization problems. The results on the mathematical functions show that the SSA algorithm is able to improve the initial random solutions effectively and converge towards the optimum. The results of MSSA show that this algorithm can approximate Pareto optimal solutions with high convergence and coverage. The paper also considers solving several challenging and computationally expensive engineering design problems (e.g. airfoil design and marine propeller design) using SSA and MSSA. The results of the real case studies demonstrate the merits of the algorithms proposed in solving real-world problems with difficult and unknown search spaces.', 'counts_by_year': [[2022, 632], [2021, 652], [2020, 449], [2019, 244], [2018, 99], [2017, 2]]}, {'id': 'W2473418344', 'doi': 'https://doi.org/10.1145/2976749.2978318', 'title': 'Deep Learning with Differential Privacy', 'type': 'proceedings-article', 'publication_date': '2016-10-24', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2162106674', ['I1291425158']], ['A2562464834', ['I1291425158']], ['A1822555116', ['I2747134083']], ['A2388919843', ['I1291425158']], ['A2139677914', ['I1291425158']], ['A2125003972', ['I1291425158']], ['A2605633800', ['I1291425158']]], 'cited_by_count': 2053, 'concepts': [['C23130292', '0.8995676'], ['C41008148', '0.8393949'], ['C136197465', '0.70719737'], ['C119857082', '0.61683935'], ['C108583219', '0.5706634']], 'referenced_works': ['W1498436455', 'W1557833142', 'W1677182931', 'W1873763122', 'W1981635503', 'W1982183556', 'W1985511977', 'W1992926795', 'W1997690112', 'W2022097286', 'W2051267297', 'W2053637704', 'W2077217970', 'W2096870293', 'W2097117768', 'W2112796928', 'W2138865266', 'W2142947774', 'W2154711640', 'W2158213899', 'W2159024459', 'W2167372639', 'W2225981128', 'W2245160765', 'W2250539671', 'W2257979135', 'W2473418344', 'W2546302380', 'W2998508934', 'W4205228770', 'W4248358572'], 'abstract': 'Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.', 'counts_by_year': [[2022, 298], [2021, 641], [2020, 616], [2019, 319], [2018, 121], [2017, 50], [2016, 4], [2015, 1]]}, {'id': 'W2917207851', 'doi': 'https://doi.org/10.1093/nar/gky379', 'title': 'The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2018 update', 'type': 'journal-article', 'publication_date': '2018-07-02', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A837107254', ['I145311948']], ['A2165697970', ['I145311948']], ['A2625989380', ['I161046081']], ['A2520192813', ['I2746051580']], ['A2518815875', ['I130769515']], ['A2336641661', ['I130769515']], ['A2109325469', ['I130769515']], ['A2212237980', ['I145311948']], ['A2008526058', ['I130769515']], ['A2327525373', ['I161046081']], ['A256748975', ['I145311948']], ['A2011731670', ['I130769515']], ['A2066812455', ['I2801952686']], ['A2334479550', ['I165690674']], ['A2176000030', ['I161046081']], ['A2763753394', ['I2799300731']], ['A2002910627', ['I165690674']], ['A2895040106', ['I145311948']], ['A99770489', ['I130769515']], ['A2229110458', ['I64467955']]], 'cited_by_count': 2037, 'concepts': [['C98444146', '0.5635077'], ['C113843644', '0.5264657'], ['C192209626', '0.46178797'], ['C41008148', '0.42604896'], ['C136764020', '0.35981438']], 'referenced_works': ['W1596936080', 'W1969739015', 'W1989703391', 'W2001818158', 'W2013434113', 'W2025660327', 'W2043701535', 'W2063495480', 'W2069152576', 'W2072970694', 'W2097464861', 'W2108718991', 'W2113534011', 'W2113565103', 'W2115646700', 'W2121015328', 'W2141040548', 'W2142678478', 'W2146545525', 'W2148317584', 'W2158789637', 'W2159675211', 'W2161645441', 'W2169456326', 'W2179438025', 'W2254034232', 'W2323326409', 'W2341539131', 'W2346143774', 'W2432815617', 'W2508061336', 'W2525496611', 'W2567309032', 'W2592811885', 'W2604622795', 'W2613409207', 'W2619713135', 'W2621270629', 'W2802842982', 'W2951322997'], 'abstract': "Galaxy (homepage: https://galaxyproject.org, main public server: https://usegalaxy.org) is a web-based scientific analysis platform used by tens of thousands of scientists across the world to analyze large biomedical datasets such as those found in genomics, proteomics, metabolomics and imaging. Started in 2005, Galaxy continues to focus on three key challenges of data-driven biomedical science: making analyses accessible to all researchers, ensuring analyses are completely reproducible, and making it simple to communicate analyses so that they can be reused and extended. During the last two years, the Galaxy team and the open-source community around Galaxy have made substantial improvements to Galaxy's core framework, user interface, tools, and training materials. Framework and user interface improvements now enable Galaxy to be used for analyzing tens of thousands of datasets, and >5500 tools are now available from the Galaxy ToolShed. The Galaxy community has led an effort to create numerous high-quality tutorials focused on common types of genomic analyses. The Galaxy developer and user communities continue to grow and be integral to Galaxy's development. The number of Galaxy public servers, developers contributing to the Galaxy framework and its tools, and users of the main Galaxy server have all increased substantially.", 'counts_by_year': [[2022, 498], [2021, 623], [2020, 524], [2019, 337], [2018, 54]]}, {'id': 'W2345791363', 'doi': 'https://doi.org/10.1093/nar/gkw387', 'title': 'PHASTER: a better, faster version of the PHAST phage search tool', 'type': 'journal-article', 'publication_date': '2016-07-08', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2162292392', ['I154425047']], ['A2103281349', ['I154425047']], ['A2096010429', ['I154425047']], ['A290514958', ['I154425047']], ['A2166265495', ['I154425047']], ['A2136264431', ['I154425047']], ['A2118994065', ['I154425047', 'I40752897']]], 'cited_by_count': 1987, 'concepts': [['C141231307', '0.64208317'], ['C37789001', '0.6244635'], ['C86803240', '0.62337685'], ['C2777904410', '0.59648913'], ['C2776321320', '0.5627449']], 'referenced_works': ['W171943169', 'W204992930', 'W1921607560', 'W1984225141', 'W2005129098', 'W2037015298', 'W2055043387', 'W2069689025', 'W2100363512', 'W2119859604', 'W2122210493', 'W2130124027', 'W2136101247', 'W2141394464', 'W2142678478', 'W2158406706', 'W2170747616', 'W4236236547'], 'abstract': "PHASTER (PHAge Search Tool - Enhanced Release) is a significant upgrade to the popular PHAST web server for the rapid identification and annotation of prophage sequences within bacterial genomes and plasmids. Although the steps in the phage identification pipeline in PHASTER remain largely the same as in the original PHAST, numerous software improvements and significant hardware enhancements have now made PHASTER faster, more efficient, more visually appealing and much more user friendly. In particular, PHASTER is now 4.3× faster than PHAST when analyzing a typical bacterial genome. More specifically, software optimizations have made the backend of PHASTER 2.7X faster than PHAST, while the addition of 80 CPUs to the PHASTER compute cluster are responsible for the remaining speed-up. PHASTER can now process a typical bacterial genome in 3 min from the raw sequence alone, or in 1.5 min when given a pre-annotated GenBank file. A number of other optimizations have also been implemented, including automated algorithms to reduce the size and redundancy of PHASTER's databases, improvements in handling multiple (metagenomic) queries and higher user traffic, along with the ability to perform automated look-ups against 14 000 previously PHAST/PHASTER annotated bacterial genomes (which can lead to complete phage annotations in seconds as opposed to minutes). PHASTER's web interface has also been entirely rewritten. A new graphical genome browser has been added, gene/genome visualization tools have been improved, and the graphical interface is now more modern, robust and user-friendly. PHASTER is available online at www.phaster.ca.", 'counts_by_year': [[2022, 398], [2021, 499], [2020, 443], [2019, 296], [2018, 221], [2017, 121], [2016, 9]]}, {'id': 'W2395579298', 'doi': 'https://doi.org/10.1186/s40537-016-0043-6', 'title': 'A survey of transfer learning', 'type': 'journal-article', 'publication_date': '2016-05-28', 'host_venue': 'V2737955091', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2475751630', ['I63772739']], ['A3186327443', ['I63772739']], ['A2679287578', ['I63772739']]], 'cited_by_count': 1985, 'concepts': [['C41008148', '0.7517893'], ['C150899416', '0.6047441'], ['C68597687', '0.5626236'], ['C2522767166', '0.3461147'], ['C154945302', '0.30399972']], 'referenced_works': ['W32692049', 'W1510073064', 'W1820403671', 'W1978920452', 'W1981658663', 'W1982696459', 'W1984363873', 'W1991207565', 'W2000987725', 'W2002424773', 'W2008695654', 'W2014703436', 'W2025768430', 'W2034368206', 'W2034852725', 'W2037265949', 'W2038569950', 'W2041496338', 'W2050496630', 'W2050752817', 'W2051390658', 'W2055103902', 'W2062152791', 'W2062179223', 'W2062518264', 'W2075728230', 'W2080521990', 'W2081621443', 'W2090923791', 'W2092012321', 'W2096943734', 'W2098027503', 'W2098355853', 'W2100664256', 'W2106097867', 'W2106261996', 'W2113831957', 'W2115403315', 'W2118338035', 'W2119605622', 'W2120149881', 'W2120559187', 'W2121341364', 'W2122838776', 'W2122922389', 'W2124961556', 'W2125263373', 'W2126574503', 'W2128053425', 'W2132755184', 'W2134982367', 'W2143104527', 'W2145868540', 'W2147152072', 'W2149466042', 'W2151103935', 'W2151375682', 'W2152556536', 'W2153353890', 'W2153963799', 'W2158108973', 'W2159570078', 'W2160660844', 'W2161047120', 'W2161381512', 'W2162854380', 'W2163345210', 'W2164943005', 'W2165698076', 'W2169495281', 'W2170607218', 'W2280985394', 'W2398280087', 'W2399386490', 'W2538008885', 'W2735735104', 'W2963578416'], 'abstract': 'Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments.', 'counts_by_year': [[2022, 409], [2021, 624], [2020, 446], [2019, 284], [2018, 139], [2017, 63], [2016, 11]]}, {'id': 'W2897748473', 'doi': 'https://doi.org/10.1016/j.ijsu.2018.10.028', 'title': 'The SCARE 2018 statement: Updating consensus Surgical CAse REport (SCARE) guidelines', 'type': 'journal-article', 'publication_date': '2018-12-01', 'host_venue': 'V67965910', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A1940434747', ['I2799869770']], ['A2748201011', ['I97018004']], ['A2705334638', ['I79619799']], ['A2545773349', ['I2801663869']], ['A1937107987', ['I2801259928']], ['A719935200', ['I1283280774']]], 'cited_by_count': 1982, 'concepts': [['C71924100', '0.75083935'], ['C2779356329', '0.74734074'], ['C2779495148', '0.6833949'], ['C60641444', '0.68227136'], ['C2780233690', '0.5674921']], 'referenced_works': ['W1968468300', 'W2061378336', 'W2144992253', 'W2252491773', 'W2513329101', 'W2741989018'], 'abstract': 'The SCARE Guidelines were published in 2016 to provide a structure for reporting surgical case reports. Since their publication, SCARE guidelines have been widely endorsed by authors, journal editors, and reviewers, and have helped to improve reporting transparency of case reports across a range of surgical specialties. In order to encourage further progress in reporting quality, the SCARE guidelines must themselves be kept up to date. We completed a Delphi consensus exercise to update the SCARE guidelines. A Delphi consensus exercise was undertaken. All members of the previous Delphi group were invited to participate, in addition to researchers who have previously studied case reports, and editors from the International Journal of Surgery Case Reports. The expert group was sent an online questionnaire where they were asked to rate their agreement with proposed changes to each of the 24 items. 56 people agreed to participate and 45 (80%) invitees completed the survey which put forward modifications to the original guideline. The collated responses resulted in modifications. There was high agreement amongst the expert group. A modified and improved SCARE checklist is presented, after a Delphi consensus exercise was completed. The SCARE 2018 Statement: Updating Consensus S urgical CA se RE port (SCARE) Guidelines. • A Delphi consensus exercise to update the SCARE guidelines. • 45 (80%) invitees completed the survey and there was high agreement amongst the expert group. • The collated responses resulted in modifications, and a improved SCARE checklist is presented.', 'counts_by_year': [[2022, 47], [2021, 193], [2020, 1237], [2019, 505]]}, {'id': 'W2343448572', 'doi': 'https://doi.org/10.1109/comst.2016.2532458', 'title': 'Next Generation 5G Wireless Networks: A Comprehensive Survey', 'type': 'journal-article', 'publication_date': '2016-02-19', 'host_venue': 'V23688054', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2342636011', ['I848706']], ['A2256804153', ['I2250650973']], ['A2189781981', ['I848706']]], 'cited_by_count': 1967, 'concepts': [['C41008148', '0.79191864'], ['C31258907', '0.5639819'], ['C108037233', '0.5043868'], ['C555944384', '0.50358886'], ['C76155785', '0.3711943']], 'referenced_works': ['W565267543', 'W612133031', 'W970685087', 'W1496659747', 'W1508202077', 'W1543436456', 'W1584244501', 'W1650643421', 'W1793086066', 'W1935253045', 'W1963489908', 'W1963917504', 'W1967040861', 'W1969281101', 'W1970883840', 'W1975507068', 'W1976020855', 'W1981661761', 'W1982985050', 'W1983816900', 'W1986226687', 'W1986570368', 'W1987378516', 'W1989746362', 'W1990955244', 'W1991056455', 'W1991210791', 'W1991379467', 'W1991489047', 'W1991824403', 'W1991898587', 'W2002794223', 'W2003330667', 'W2004596331', 'W2006313153', 'W2006599533', 'W2008592974', 'W2008668719', 'W2009447832', 'W2014770015', 'W2016374480', 'W2017735472', 'W2018552671', 'W2020077811', 'W2022479666', 'W2022598164', 'W2023182510', 'W2023456652', 'W2023535408', 'W2023835067', 'W2025252243', 'W2026477847', 'W2026566616', 'W2026892459', 'W2027414550', 'W2027934846', 'W2027941036', 'W2028210397', 'W2028277982', 'W2029468290', 'W2034278951', 'W2034870457', 'W2037409218', 'W2038628431', 'W2040010103', 'W2040921991', 'W2042308583', 'W2042768148', 'W2046010885', 'W2046526308', 'W2047020478', 'W2047942585', 'W2049429509', 'W2052190282', 'W2053456811', 'W2054692642', 'W2055376915', 'W2058641838', 'W2059162326', 'W2060601190', 'W2062086877', 'W2062840023', 'W2064032030', 'W2064969374', 'W2066757314', 'W2067175483', 'W2068849277', 'W2069131139', 'W2069701731', 'W2076769624', 'W2077103973', 'W2077145024', 'W2077793016', 'W2077925014', 'W2077949195', 'W2081383438', 'W2082291889', 'W2084099189', 'W2084316162', 'W2086229459', 'W2087377901', 'W2088717366', 'W2090495398', 'W2092256136', 'W2094623974', 'W2094715492', 'W2095843437', 'W2099682235', 'W2103723095', 'W2103874569', 'W2103970559', 'W2103972037', 'W2104074482', 'W2104185379', 'W2104523419', 'W2104621580', 'W2104705987', 'W2106400463', 'W2108090232', 'W2108672684', 'W2109268311', 'W2109478041', 'W2110188845', 'W2111095459', 'W2111892549', 'W2112913030', 'W2113257905', 'W2116334496', 'W2119137614', 'W2120144288', 'W2120255052', 'W2121318675', 'W2122999882', 'W2129079462', 'W2129258733', 'W2132647318', 'W2132712787', 'W2134733390', 'W2136870707', 'W2137365129', 'W2137693329', 'W2138170080', 'W2138785527', 'W2140050405', 'W2141682101', 'W2142642123', 'W2143669931', 'W2144239375', 'W2147584524', 'W2150081756', 'W2150351031', 'W2151854870', 'W2152026166', 'W2152082728', 'W2153794195', 'W2156077514', 'W2157646602', 'W2157977389', 'W2160160578', 'W2161044775', 'W2163770563', 'W2166733284', 'W2168780914', 'W2169046853', 'W2170469173', 'W2320326971', 'W2322069560', 'W2330658783', 'W2332732515', 'W2418334384', 'W2490984131', 'W2532716006', 'W2535737789', 'W2537381390', 'W2552690635', 'W2963712721', 'W3007940350', 'W3098915991', 'W3101483526', 'W3101609953'], 'abstract': 'The vision of next generation 5G wireless communications lies in providing very high data rates (typically of Gbps order), extremely low latency, manifold increase in base station capacity, and significant improvement in users’ perceived quality of service (QoS), compared to current 4G LTE networks. Ever increasing proliferation of smart devices, introduction of new emerging multimedia applications, together with an exponential rise in wireless data (multimedia) demand and usage is already creating a significant burden on existing cellular networks. 5G wireless systems, with improved data rates, capacity, latency, and QoS are expected to be the panacea of most of the current cellular networks’ problems. In this survey, we make an exhaustive review of wireless evolution toward 5G networks. We first discuss the new architectural changes associated with the radio access network (RAN) design, including air interfaces, smart antennas, cloud and heterogeneous RAN. Subsequently, we make an in-depth survey of underlying novel mm-wave physical layer technologies, encompassing new channel model estimation, directional antenna design, beamforming algorithms, and massive MIMO technologies. Next, the details of MAC layer protocols and multiplexing schemes needed to efficiently support this new physical layer are discussed. We also look into the killer applications, considered as the major driving force behind 5G. In order to understand the improved user experience, we provide highlights of new QoS, QoE, and SON features associated with the 5G evolution. For alleviating the increased network energy consumption and operating expenditure, we make a detail review on energy awareness and cost efficiency. As understanding the current status of 5G implementation is important for its eventual commercialization, we also discuss relevant field trials, drive tests, and simulation experiments. Finally, we point out major existing research issues and identify possible future research directions.', 'counts_by_year': [[2022, 209], [2021, 387], [2020, 479], [2019, 369], [2018, 310], [2017, 185], [2016, 27]]}, {'id': 'W2759190050', 'doi': 'https://doi.org/10.3389/fpubh.2017.00258', 'title': 'An Overview of Heart Rate Variability Metrics and Norms', 'type': 'journal-article', 'publication_date': '2017-09-28', 'host_venue': 'V2595931848', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2136407005', ['I199639920']], ['A2123555743', ['I2801500240']]], 'cited_by_count': 1960, 'concepts': [['C71635504', '0.7828073'], ['C2779343474', '0.6160899'], ['C44725695', '0.580212'], ['C100533687', '0.5160153'], ['C41008148', '0.4588514']], 'referenced_works': ['W8485088', 'W159391136', 'W202726670', 'W209441309', 'W1204490507', 'W1517218240', 'W1531145115', 'W1539278380', 'W1570752686', 'W1834554108', 'W1854129652', 'W1863864373', 'W1867719656', 'W1913253923', 'W1939541397', 'W1965873360', 'W1967900132', 'W1968036378', 'W1968321652', 'W1969770478', 'W1969980989', 'W1973581302', 'W1979885502', 'W1981384766', 'W1981456562', 'W1984161508', 'W1989514365', 'W1995421025', 'W1997293964', 'W1999369240', 'W2000659208', 'W2004096525', 'W2005516971', 'W2007479430', 'W2010409828', 'W2010977132', 'W2011565562', 'W2015711125', 'W2017346656', 'W2020898763', 'W2024997766', 'W2025257580', 'W2028960923', 'W2031718697', 'W2034758365', 'W2036056901', 'W2039850135', 'W2047748360', 'W2048509252', 'W2048511709', 'W2054340269', 'W2057418424', 'W2058825945', 'W2062618262', 'W2065781997', 'W2070395437', 'W2075652159', 'W2076262152', 'W2076386730', 'W2079480282', 'W2081856525', 'W2082227074', 'W2092031893', 'W2095233676', 'W2097431774', 'W2099733421', 'W2102423296', 'W2107161633', 'W2108591604', 'W2108804492', 'W2112350979', 'W2112400936', 'W2113640125', 'W2117359150', 'W2117814310', 'W2118573445', 'W2119887007', 'W2122428830', 'W2122464624', 'W2123814189', 'W2126518215', 'W2128107556', 'W2128242332', 'W2130213780', 'W2133797783', 'W2135938811', 'W2136373709', 'W2136414988', 'W2136429434', 'W2148713726', 'W2150364995', 'W2150789152', 'W2157846167', 'W2159759971', 'W2162033213', 'W2166686247', 'W2170785660', 'W2172343406', 'W2179962966', 'W2202628133', 'W2277469691', 'W2285072859', 'W2295416986', 'W2296659055', 'W2298185395', 'W2398204186', 'W2417551327', 'W2418136892', 'W2441429072', 'W2510676745', 'W2517521203', 'W2551490154', 'W2570770964', 'W2579638499', 'W2589100236', 'W2756299859', 'W2992796884', 'W4250638070', 'W4298441315'], 'abstract': 'Healthy biological systems exhibit complex patterns of variability that can be described by mathematical chaos. Heart rate variability (HRV) consists of changes in the time intervals between consecutive heartbeats called interbeat intervals (IBIs). A healthy heart is not a metronome. The oscillations of a healthy heart are complex and constantly changing, which allow the cardiovascular system to rapidly adjust to sudden physical and psychological challenges to homeostasis. This article briefly reviews current perspectives on the mechanisms that generate 24 h, short-term (~5 min), and ultra-short-term (<5 min) HRV, the importance of HRV, and its implications for health and performance. The authors provide an overview of widely-used HRV time-domain, frequency-domain, and non-linear metrics. Time-domain indices quantify the amount of HRV observed during monitoring periods that may range from ~2 min to 24 h. Frequency-domain values calculate the absolute or relative amount of signal energy within component bands. Non-linear measurements quantify the unpredictability and complexity of a series of IBIs. The authors survey published normative values for clinical, healthy, and optimal performance populations. They stress the importance of measurement context, including recording period length, subject age, and sex, on baseline HRV values. They caution that 24 h, short-term, and ultra-short-term normative values are not interchangeable. They encourage professionals to supplement published norms with findings from their own specialized populations. Finally, the authors provide an overview of HRV assessment strategies for clinical and optimal performance interventions.', 'counts_by_year': [[2022, 552], [2021, 616], [2020, 492], [2019, 241], [2018, 56], [2017, 1]]}, {'id': 'W2763355946', 'doi': 'https://doi.org/10.1038/nrclinonc.2017.141', 'title': 'Radiomics: the bridge between medical imaging and personalized medicine', 'type': 'journal-article', 'publication_date': '2017-10-04', 'host_venue': 'V41981144', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A1739850404', ['I2800191616']], ['A2081715637', ['I2800191616']], ['A2051276896', ['I2800191616']], ['A2261498208', ['I2800191616']], ['A2329148285', ['I2800191616']], ['A3007023543', ['I2800191616']], ['A2763307131', ['I2800191616']], ['A2526364874', ['I2800191616']], ['A2061973555', ['I2800191616']], ['A2025817101', ['I2800191616']], ['A2570436874', ['I2800191616']], ['A2104245452', ['I2800191616']], ['A1910810897', ['I2800191616']], ['A2114274010', ['I2800191616']], ['A2021986263', ['I2800191616']], ['A2083198532', ['I2800191616']], ['A2007656458', ['I887968799', 'I2800191616']], ['A2254237125', ['I2800191616']], ['A2616991658', ['I2800191616']]], 'cited_by_count': 1958, 'concepts': [['C2778559731', '0.92079854'], ['C71924100', '0.671504'], ['C31601959', '0.611018'], ['C32220436', '0.5094815'], ['C19527891', '0.5077115']], 'referenced_works': ['W124239641', 'W828015039', 'W1408981388', 'W1512206713', 'W1778604558', 'W1804185641', 'W1824790222', 'W1943338723', 'W1955548161', 'W1964254408', 'W1967358484', 'W1972111840', 'W1979141644', 'W1981991897', 'W1982488143', 'W1985905712', 'W1986129954', 'W1987054640', 'W1989340328', 'W1997192661', 'W2007432187', 'W2008985174', 'W2010745216', 'W2011458181', 'W2012012103', 'W2014482764', 'W2021247307', 'W2022175380', 'W2023467037', 'W2028433016', 'W2032074177', 'W2040400995', 'W2042183844', 'W2046282597', 'W2046757928', 'W2049553585', 'W2054580590', 'W2055575746', 'W2061077502', 'W2062398277', 'W2062744135', 'W2065608903', 'W2067428647', 'W2074228092', 'W2077763531', 'W2078271269', 'W2078607774', 'W2081566809', 'W2084782766', 'W2087676836', 'W2095868657', 'W2095989416', 'W2097475056', 'W2100065311', 'W2103004421', 'W2105882193', 'W2105912300', 'W2106304305', 'W2106787323', 'W2111137187', 'W2114281628', 'W2119910794', 'W2121274101', 'W2121331582', 'W2124735791', 'W2124932867', 'W2127326807', 'W2128625986', 'W2128739912', 'W2131687087', 'W2135285836', 'W2136829301', 'W2138490951', 'W2138831289', 'W2140828776', 'W2140867038', 'W2141059777', 'W2146787063', 'W2154009746', 'W2156711378', 'W2160046825', 'W2160377567', 'W2163269048', 'W2165927297', 'W2166063692', 'W2166401924', 'W2169512394', 'W2172058373', 'W2174661749', 'W2192798466', 'W2236433007', 'W2259390591', 'W2266995761', 'W2289073724', 'W2315074612', 'W2327203407', 'W2336302003', 'W2341840252', 'W2346265746', 'W2346343836', 'W2346789575', 'W2398056625', 'W2404901863', 'W2409456704', 'W2409649574', 'W2409717125', 'W2413354483', 'W2414064355', 'W2417742286', 'W2461805626', 'W2470491115', 'W2507699577', 'W2509099314', 'W2517065464', 'W2526584490', 'W2560367415', 'W2566749675', 'W2572174216', 'W2593227218', 'W2599895745', 'W4239309742'], 'abstract': 'Radiomics, the high-throughput mining of quantitative image features from standard-of-care medical imaging that enables data to be extracted and applied within clinical-decision support systems to improve diagnostic, prognostic, and predictive accuracy, is gaining importance in cancer research. Radiomic analysis exploits sophisticated image analysis tools and the rapid development and validation of medical imaging data that uses image-based signatures for precision diagnosis and treatment, providing a powerful tool in modern medicine. Herein, we describe the process of radiomics, its pitfalls, challenges, opportunities, and its capacity to improve clinical decision making, emphasizing the utility for patients with cancer. Currently, the field of radiomics lacks standardized evaluation of both the scientific integrity and the clinical relevance of the numerous published radiomics investigations resulting from the rapid growth of this area. Rigorous evaluation criteria and reporting guidelines need to be established in order for radiomics to mature as a discipline. Herein, we provide guidance for investigations to meet this urgent need in the field of radiomics.', 'counts_by_year': [[2022, 591], [2021, 640], [2020, 399], [2019, 225], [2018, 93], [2017, 8]]}, {'id': 'W2945976633', 'doi': 'https://doi.org/10.1038/s42256-019-0048-x', 'title': 'Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead', 'type': 'journal-article', 'publication_date': '2019-05-01', 'host_venue': 'V2912241403', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2141705163', ['I170897317']]], 'cited_by_count': 1955, 'concepts': [['C94966114', '0.9097743'], ['C2777363581', '0.80756176'], ['C41008148', '0.56972665'], ['C26517878', '0.5256547'], ['C102587632', '0.50466835']], 'referenced_works': ['W1492170170', 'W1905782493', 'W1973682096', 'W1994407253', 'W2013587512', 'W2026905436', 'W2046945713', 'W2072136246', 'W2082262280', 'W2132166479', 'W2134598164', 'W2141097525', 'W2149033360', 'W2239135493', 'W2493343568', 'W2579555219', 'W2604231045', 'W2743731382', 'W2765813195', 'W2811374795', 'W2895051075', 'W2896176822', 'W2898694742', 'W2910705748', 'W2963125461', 'W2963816926', 'W3098636317', 'W3102834905'], 'abstract': 'Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward - it is to design models that are inherently interpretable. This manuscript clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision.', 'counts_by_year': [[2022, 607], [2021, 825], [2020, 458], [2019, 57], [2018, 5], [2012, 1]]}, {'id': 'W2952481429', 'doi': 'https://doi.org/10.1038/s41598-017-17204-5', 'title': 'QuPath: Open source software for digital pathology image analysis', 'type': 'journal-article', 'publication_date': '2017-12-04', 'host_venue': 'V196734849', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A1712819804', ['I126231945']], ['A2050071110', ['I126231945', 'I1289110261']], ['A2125124304', ['I126231945']], ['A2095587550', ['I126231945']], ['A1866664419', ['I126231945']], ['A1998999888', ['I126231945']], ['A2059895183', ['I126231945', 'I1289110261']], ['A2125520447', ['I126231945']], ['A2132663495', ['I126231945']], ['A2117645251', ['I126231945']], ['A2162802305', ['I126231945', 'I1289110261']], ['A293617728', ['I126231945', 'I1289110261']], ['A2155408071', ['I126231945']]], 'cited_by_count': 1950, 'concepts': [['C41008148', '0.8372922'], ['C61423126', '0.7408392'], ['C2777904410', '0.65197206'], ['C2777522853', '0.6504996'], ['C32833848', '0.60439867']], 'referenced_works': ['W1481239115', 'W1549827563', 'W1597336200', 'W1970221755', 'W1974195684', 'W1977653087', 'W1982488143', 'W2002882918', 'W2006302594', 'W2019605179', 'W2031502025', 'W2037586865', 'W2037947871', 'W2044465660', 'W2045778975', 'W2071036685', 'W2073618702', 'W2076617429', 'W2078171000', 'W2099540110', 'W2115320363', 'W2118246710', 'W2133554539', 'W2142300779', 'W2144535237', 'W2147604272', 'W2167279371', 'W2234666690', 'W2356803257', 'W2495896890', 'W2496265832', 'W2560367415', 'W2572174216', 'W2911964244', 'W4246337125'], 'abstract': "QuPath is new bioimage analysis software designed to meet the growing need for a user-friendly, extensible, open-source solution for digital pathology and whole slide image analysis. In addition to offering a comprehensive panel of tumor identification and high-throughput biomarker evaluation tools, QuPath provides researchers with powerful batch-processing and scripting functionality, and an extensible platform with which to develop and share new algorithms to analyze complex tissue images. Furthermore, QuPath's flexible design makes it suitable for a wide range of additional image analysis applications across biomedical research.", 'counts_by_year': [[2022, 688], [2021, 684], [2020, 365], [2019, 164], [2018, 46]]}, {'id': 'W3032971139', 'doi': 'https://doi.org/10.1038/s41586-020-2405-7', 'title': 'Estimating the effects of non-pharmaceutical interventions on COVID-19 in Europe', 'type': 'journal-article', 'publication_date': '2020-06-08', 'host_venue': 'V137773608', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2050534514', ['I47508984']], ['A2615983930', ['I47508984']], ['A2087782728', ['I47508984']], ['A3016995958', ['I47508984']], ['A2059842926', ['I47508984']], ['A2969807079', ['I47508984']], ['A2773111252', ['I47508984']], ['A3021428579', ['I47508984']], ['A3019908733', ['I47508984']], ['A2149773940', ['I47508984']], ['A3023348346', ['I47508984']], ['A2142290638', ['I47508984']], ['A2013963948', ['I40120149', 'I47508984']], ['A2139755909', ['I47508984']], ['A3018640697', ['I47508984']], ['A2133255215', ['I47508984']], ['A1964950153', ['I47508984']], ['A2116982717', ['I47508984']]], 'cited_by_count': 1944, 'concepts': [['C27415008', '0.76356196'], ['C70437156', '0.7432172'], ['C89623803', '0.68203235'], ['C3008058167', '0.5290519'], ['C149923435', '0.47475985']], 'referenced_works': ['W1981457167', 'W2035886603', 'W2097446414', 'W2163930085', 'W2982204749', 'W3011243131', 'W3012284084', 'W3013967887', 'W3013985547', 'W3018970359', 'W3020184843', 'W3025090814', 'W3025684696', 'W3026382261', 'W3037177566', 'W3040552450'], 'abstract': 'Following the detection of the new coronavirus1 severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and its spread outside of China, Europe has experienced large epidemics of coronavirus disease 2019 (COVID-19). In response, many European countries have implemented non-pharmaceutical interventions, such as the closure of schools and national lockdowns. Here we study the effect of major interventions across 11 European countries for the period from the start of the COVID-19 epidemics in February 2020 until 4 May 2020, when lockdowns started to be lifted. Our model calculates backwards from observed deaths to estimate transmission that occurred several weeks previously, allowing for the time lag between infection and death. We use partial pooling of information between countries, with both individual and shared effects on the time-varying reproduction number (Rt). Pooling allows for more information to be used, helps to overcome idiosyncrasies in the data and enables more-timely estimates. Our model relies on fixed estimates of some epidemiological parameters (such as the infection fatality rate), does not include importation or subnational variation and assumes that changes in Rt are an immediate response to interventions rather than gradual changes in behaviour. Amidst the ongoing pandemic, we rely on death data that are incomplete, show systematic biases in reporting and are subject to future consolidation. We estimate that—for all of the countries we consider here—current interventions have been sufficient to drive Rt below 1 (probability Rt < 1.0 is greater than 99%) and achieve control of the epidemic. We estimate that across all 11 countries combined, between 12 and 15 million individuals were infected with SARS-CoV-2 up to 4 May 2020, representing between 3.2% and 4.0% of the population. Our results show that major non-pharmaceutical interventions—and lockdowns in particular—have had a large effect on reducing transmission. Continued intervention should be considered to keep transmission of SARS-CoV-2 under control.', 'counts_by_year': [[2022, 513], [2021, 942], [2020, 483], [2019, 1]]}, {'id': 'W2300242332', 'doi': 'https://doi.org/10.1007/978-3-319-46493-0_32', 'title': 'XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks', 'type': 'book-chapter', 'publication_date': '2016-10-08', 'host_venue': 'V4306463941', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2066712224', ['I2945602774']], ['A2152316929', ['I2945602774']], ['A2392241600', ['I201448701']], ['A1988090614', ['I2945602774', 'I201448701']]], 'cited_by_count': 1937, 'concepts': [['C41008148', '0.8599665'], ['C81363708', '0.6720051'], ['C57684291', '0.58773124'], ['C154945302', '0.5828804'], ['C50644808', '0.5213272']], 'referenced_works': ['W1536680647', 'W1605005685', 'W1903029394', 'W1996901117', 'W2013305145', 'W2097117768', 'W2102605133', 'W2103496339', 'W2194775991', 'W2211979669', 'W2394932179', 'W2397287600', 'W2964350391'], 'abstract': 'We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32\\(\\times \\) memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58\\(\\times \\) faster convolutional operations (in terms of number of the high precision operations) and 32\\(\\times \\) memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than \\(16\\,\\%\\) in top-1 accuracy. Our code is available at: http://allenai.org/plato/xnornet.', 'counts_by_year': [[2022, 155], [2021, 444], [2020, 546], [2019, 477], [2018, 228], [2017, 76], [2016, 10], [2015, 1]]}, {'id': 'W2928467655', 'doi': 'https://doi.org/10.1016/s0140-6736(19)30041-8', 'title': 'Health effects of dietary risks in 195 countries, 1990–2017: a systematic analysis for the Global Burden of Disease Study 2017', 'type': 'journal-article', 'publication_date': '2019-05-11', 'host_venue': 'V49861241', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2337146604', []], ['A2527981122', []], ['A3010334873', []], ['A2528576217', []], ['A2899962804', []], ['A2099944687', []], ['A1460372564', []], ['A2526738623', []], ['A1261223370', []], ['A2804173468', []], ['A1828904284', []], ['A2165518812', []], ['A2209012582', []], ['A1865820898', []], ['A1968471205', []], ['A2053519999', []], ['A1773288047', []], ['A1205674922', []], ['A2016239449', []], ['A2082872497', []], ['A2483563741', []], ['A2762951047', []], ['A2054394482', []], ['A2150892176', []], ['A2105920767', []], ['A2366254680', []], ['A3019388298', []], ['A344498437', []], ['A2793367655', []], ['A2899701500', []], ['A2112779362', []], ['A1991156927', []], ['A1897000002', []], ['A1916184962', []], ['A2525638264', []], ['A2052207220', []], ['A2170374040', []], ['A130753112', []], ['A2719099999', []], ['A2102600385', []], ['A1989487947', []], ['A1985992664', []], ['A2098038954', []], ['A2177286373', []], ['A2661338970', []], ['A2136108649', []], ['A2153277603', []], ['A2077617762', []], ['A2170943676', []], ['A2087097567', []], ['A1235058181', []], ['A2614752668', []], ['A2159017802', []], ['A2904117306', []], ['A2498133086', []], ['A2018198572', []], ['A1984201670', []], ['A2142404985', []], ['A2438219672', []], ['A1900504705', []], ['A2132501245', []], ['A2066511934', []], ['A3157605970', []], ['A1963193313', []], ['A3120043664', []], ['A3211412615', []], ['A2137545149', []], ['A188612772', []], ['A2128434403', []], ['A2562465928', []], ['A2040328808', []], ['A2792833490', []], ['A1631637737', []], ['A2614287902', []], ['A2664379405', []], ['A1969880502', []], ['A2102508135', []], ['A1935550320', []], ['A2430086965', []], ['A2109432494', []], ['A2644442497', []], ['A2151970082', []], ['A2287785407', []], ['A20044890', []], ['A2169326384', []], ['A2113265520', []], ['A2133215214', []], ['A2900026058', []], ['A2899784136', []], ['A2157298618', []], ['A2020141480', []], ['A1587262111', []], ['A2572175195', []], ['A2528735446', []], ['A2152681086', []], ['A2132455678', []], ['A2308680639', []], ['A2133495838', []], ['A2169176658', []], ['A2251827248', []], ['A2205227065', []], ['A2089080248', []], ['A3037742823', []], ['A1972757265', []], ['A1920971052', []], ['A2221776659', []], ['A1978294612', []], ['A1964282592', []], ['A2717273037', []], ['A1625818276', []], ['A1980880360', []], ['A1575105442', []], ['A2171619173', []], ['A2952451616', []], ['A2607599903', []], ['A2803757361', []], ['A284438857', []], ['A2155047149', []], ['A3165398205', []], ['A2888942052', []], ['A2894260413', []], ['A2635102198', []], ['A2620247946', []], ['A2025574871', []], ['A772519725', []], ['A2019219434', []], ['A1793908362', []], ['A2581861377', []], ['A2112328861', []], ['A2560293163', []], ['A3088485169', []], ['A2316804223', []]], 'cited_by_count': 1932, 'concepts': [['C71924100', '0.8324896'], ['C99454951', '0.78562015'], ['C3020448403', '0.5621814'], ['C2908647359', '0.54966307'], ['C2780664029', '0.5494378']], 'referenced_works': ['W1617145133', 'W1749485363', 'W1778213696', 'W1947392633', 'W1965747337', 'W1973527091', 'W1989460149', 'W2018103227', 'W2036545856', 'W2037561382', 'W2047726787', 'W2064103401', 'W2090841926', 'W2096036657', 'W2105833062', 'W2106795926', 'W2110052313', 'W2120202585', 'W2122520859', 'W2125436925', 'W2131745222', 'W2131964552', 'W2138955883', 'W2146620513', 'W2146706361', 'W2153267681', 'W2156848121', 'W2164910714', 'W2272263214', 'W2277886283', 'W2404706218', 'W2608221406', 'W2753051611', 'W2886197746', 'W3025238321', 'W3143437408'], 'abstract': 'Suboptimal diet is an important preventable risk factor for non-communicable diseases (NCDs); however, its impact on the burden of NCDs has not been systematically evaluated. This study aimed to evaluate the consumption of major foods and nutrients across 195 countries and to quantify the impact of their suboptimal intake on NCD mortality and morbidity.By use of a comparative risk assessment approach, we estimated the proportion of disease-specific burden attributable to each dietary risk factor (also referred to as population attributable fraction) among adults aged 25 years or older. The main inputs to this analysis included the intake of each dietary factor, the effect size of the dietary factor on disease endpoint, and the level of intake associated with the lowest risk of mortality. Then, by use of disease-specific population attributable fractions, mortality, and disability-adjusted life-years (DALYs), we calculated the number of deaths and DALYs attributable to diet for each disease outcome.In 2017, 11 million (95% uncertainty interval [UI] 10-12) deaths and 255 million (234-274) DALYs were attributable to dietary risk factors. High intake of sodium (3 million [1-5] deaths and 70 million [34-118] DALYs), low intake of whole grains (3 million [2-4] deaths and 82 million [59-109] DALYs), and low intake of fruits (2 million [1-4] deaths and 65 million [41-92] DALYs) were the leading dietary risk factors for deaths and DALYs globally and in many countries. Dietary data were from mixed sources and were not available for all countries, increasing the statistical uncertainty of our estimates.This study provides a comprehensive picture of the potential impact of suboptimal diet on NCD mortality and morbidity, highlighting the need for improving diet across nations. Our findings will inform implementation of evidence-based dietary interventions and provide a platform for evaluation of their impact on human health annually.Bill & Melinda Gates Foundation.', 'counts_by_year': [[2022, 599], [2021, 687], [2020, 493], [2019, 152]]}, {'id': 'W2963767194', 'doi': 'https://doi.org/10.1109/cvpr.2018.00916', 'title': 'StarGAN: Unified Generative Adversarial Networks for Multi-domain Image-to-Image Translation', 'type': 'proceedings-article', 'publication_date': '2018-06-18', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2992070603', ['I4210161052']], ['A2739888481', ['I4210161052']], ['A2768617158', ['I64281891']], ['A2224083272', ['I60922564']], ['A2164738181', ['I200769079']], ['A2148380128', ['I4210161052']]], 'cited_by_count': 1929, 'concepts': [['C2779757391', '0.83768296'], ['C41008148', '0.8153161'], ['C63479239', '0.75052756'], ['C115961682', '0.66360253'], ['C48044578', '0.65879726']], 'referenced_works': ['W1834627138', 'W2150283722', 'W2194775991', 'W2566832195', 'W2579578355', 'W2592232824', 'W2607170299', 'W2962793481', 'W2963073614', 'W2963470893'], 'abstract': "Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.", 'counts_by_year': [[2022, 243], [2021, 631], [2020, 565], [2019, 391], [2018, 96]]}, {'id': 'W2464708700', 'doi': 'https://doi.org/10.1007/978-3-319-46723-8_49', 'title': '3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation', 'type': 'book-chapter', 'publication_date': '2016-10-17', 'host_venue': 'V106296714', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2566686716', ['I161046081']], ['A2339955140', ['I2801240073', 'I161046081']], ['A1841575195', ['I161046081']], ['A2014530249', ['I161046081']], ['A2044097715', ['I1291425158', 'I161046081']]], 'cited_by_count': 1922, 'concepts': [['C41008148', '0.84576917'], ['C2776321320', '0.6249299'], ['C89600930', '0.6125643'], ['C154945302', '0.58259493'], ['C124504099', '0.4388554']], 'referenced_works': ['W1901129140', 'W1903029394', 'W1948751323', 'W1981481405', 'W2026616100', 'W2101195175', 'W2106146968', 'W2155893237', 'W2183341477', 'W2284198383', 'W2292862470', 'W2963391479'], 'abstract': 'This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases.', 'counts_by_year': [[2022, 306], [2021, 566], [2020, 470], [2019, 367], [2018, 158], [2017, 46], [2016, 4], [2012, 1]]}, {'id': 'W2539107597', 'doi': 'https://doi.org/10.1038/natrevmats.2016.71', 'title': 'Designing hydrogels for controlled drug delivery', 'type': 'journal-article', 'publication_date': '2016-10-18', 'host_venue': 'V2764917516', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2114833564', ['I44142251']], ['A1921312983', ['I44142251']]], 'cited_by_count': 1922, 'concepts': [['C108586683', '0.8627083'], ['C2779820397', '0.80695087'], ['C2780035454', '0.5576745'], ['C153083717', '0.4879741'], ['C41008148', '0.47393304']], 'referenced_works': ['W1087259', 'W1485721124', 'W1502809559', 'W1508810383', 'W1539945441', 'W1592120214', 'W1609710757', 'W1615617154', 'W1655997880', 'W1669332385', 'W1728059589', 'W1821415384', 'W1854274831', 'W1875996132', 'W1925829020', 'W1929445570', 'W1947713323', 'W1963745369', 'W1963970451', 'W1966104800', 'W1966793261', 'W1967052064', 'W1968602531', 'W1969757222', 'W1970127412', 'W1970252617', 'W1970682035', 'W1971500643', 'W1972653036', 'W1972675654', 'W1974266608', 'W1974285372', 'W1977373895', 'W1977467418', 'W1977922405', 'W1978753416', 'W1979565474', 'W1980746046', 'W1982808845', 'W1984240603', 'W1986096614', 'W1987898868', 'W1988250258', 'W1989480064', 'W1989783023', 'W1990201411', 'W1991508955', 'W1994329884', 'W1996720815', 'W1996747774', 'W1997388726', 'W1997978522', 'W1998020561', 'W1998531886', 'W1998677002', 'W1999505080', 'W1999930817', 'W2000352722', 'W2000540654', 'W2000935683', 'W2000997628', 'W2001198522', 'W2001902689', 'W2004175796', 'W2004616899', 'W2006560164', 'W2007736465', 'W2008033906', 'W2008039399', 'W2009841111', 'W2009884345', 'W2010723976', 'W2013011979', 'W2014771513', 'W2015417771', 'W2016299303', 'W2017948987', 'W2018818236', 'W2021217882', 'W2022282748', 'W2023954079', 'W2026038729', 'W2026780757', 'W2027116665', 'W2028582107', 'W2029375419', 'W2032306620', 'W2032654453', 'W2032760352', 'W2033945189', 'W2034947704', 'W2035479043', 'W2038922964', 'W2039074967', 'W2039667504', 'W2040784316', 'W2043669923', 'W2047757400', 'W2047866615', 'W2047980414', 'W2048017747', 'W2050163391', 'W2050647239', 'W2052948667', 'W2054745605', 'W2055037957', 'W2055580243', 'W2056304970', 'W2056725095', 'W2058191567', 'W2059669211', 'W2060449729', 'W2060835907', 'W2061089961', 'W2062614509', 'W2062951494', 'W2064230565', 'W2064672899', 'W2065025587', 'W2065415874', 'W2065804894', 'W2068406406', 'W2070166910', 'W2073442004', 'W2073895184', 'W2074407243', 'W2074501348', 'W2074630757', 'W2074971761', 'W2077003064', 'W2077440292', 'W2077466603', 'W2077644034', 'W2077844244', 'W2078615529', 'W2079273552', 'W2079407952', 'W2080044875', 'W2081212964', 'W2081306620', 'W2082218425', 'W2082516633', 'W2083094711', 'W2083280522', 'W2083359872', 'W2084401861', 'W2085038604', 'W2085929270', 'W2086394168', 'W2087612635', 'W2088737354', 'W2089023014', 'W2089125503', 'W2090409777', 'W2091007535', 'W2091857676', 'W2092029860', 'W2092289962', 'W2092528383', 'W2092858957', 'W2094600652', 'W2095016306', 'W2095170053', 'W2095318835', 'W2096827743', 'W2097025940', 'W2098094440', 'W2102082330', 'W2103031461', 'W2107660675', 'W2109464227', 'W2111814792', 'W2114343111', 'W2115504193', 'W2116351792', 'W2117630026', 'W2119476922', 'W2119748365', 'W2122226217', 'W2124140010', 'W2124518459', 'W2124926420', 'W2127049662', 'W2129549504', 'W2130606072', 'W2131412965', 'W2133834863', 'W2133897272', 'W2134208433', 'W2134835131', 'W2135880302', 'W2136592960', 'W2144633240', 'W2144777184', 'W2145187299', 'W2145496621', 'W2147153041', 'W2147182021', 'W2148510145', 'W2151186872', 'W2151776485', 'W2152676524', 'W2155291988', 'W2157934624', 'W2158300218', 'W2158643446', 'W2162864804', 'W2164143250', 'W2165162226', 'W2165642945', 'W2166548167', 'W2167947388', 'W2168448281', 'W2168565232', 'W2168968689', 'W2169859308', 'W2172280914', 'W2190923751', 'W2206958686', 'W2224910067', 'W2231813729', 'W2259823352', 'W2262119483', 'W2270498673', 'W2282962776', 'W2288119981', 'W2289734660', 'W2290333155', 'W2296660964', 'W2312306775', 'W2314370819', 'W2319679416', 'W2319837694', 'W2323069486', 'W2328727699', 'W2329054428', 'W2511036621', 'W4231516524', 'W4256389940'], 'abstract': 'Hydrogel delivery systems can leverage therapeutically beneficial outcomes of drug delivery and have found clinical use. Hydrogels can provide spatial and temporal control over the release of various therapeutic agents, including small-molecule drugs, macromolecular drugs and cells. Owing to their tunable physical properties, controllable degradability and capability to protect labile drugs from degradation, hydrogels serve as a platform in which various physiochemical interactions with the encapsulated drugs control their release. In this Review, we cover multiscale mechanisms underlying the design of hydrogel drug delivery systems, focusing on physical and chemical properties of the hydrogel network and the hydrogel-drug interactions across the network, mesh, and molecular (or atomistic) scales. We discuss how different mechanisms interact and can be integrated to exert fine control in time and space over the drug presentation. We also collect experimental release data from the literature, review clinical translation to date of these systems, and present quantitative comparisons between different systems to provide guidelines for the rational design of hydrogel delivery systems.', 'counts_by_year': [[2022, 414], [2021, 531], [2020, 488], [2019, 303], [2018, 138], [2017, 37]]}, {'id': 'W2510973425', 'doi': 'https://doi.org/10.1038/ng.3656', 'title': 'Next-generation genotype imputation service and methods', 'type': 'journal-article', 'publication_date': '2016-10-01', 'host_venue': 'V137905309', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2186633199', ['I27837315']], ['A2066390143', ['I143397708']], ['A2118993252', ['I143397708']], ['A49746010', ['I27837315', 'I159650629']], ['A2493617706', ['I27837315']], ['A2132816084', ['I27837315']], ['A2307445584', ['I188538660']], ['A2113571590', ['I4210139686']], ['A2945141954', ['I4210145894']], ['A2021626791', ['I130238516']], ['A1189913679', ['I4210136897']], ['A2127993160', ['I79576946']], ['A2164552791', ['I136199984']], ['A723323371', ['I130238516']], ['A1844463566', ['I4210139686']], ['A2139922620', ['I27837315']], ['A1597337812', ['I159650629']], ['A2472932565', ['I143397708']], ['A2137325747', ['I27837315']], ['A2305243664', ['I27837315']], ['A225127237', ['I1319360392', 'I27837315', 'I143397708']]], 'cited_by_count': 1916, 'concepts': [['C58041806', '0.92666435'], ['C106208931', '0.57339615'], ['C135763542', '0.51724344'], ['C86803240', '0.5108599'], ['C41008148', '0.48773775']], 'referenced_works': ['W1920311388', 'W1978116054', 'W1982918926', 'W1990139627', 'W1992436001', 'W2005935256', 'W2033913430', 'W2058401000', 'W2061539393', 'W2063652569', 'W2067539811', 'W2078889486', 'W2080613667', 'W2086699924', 'W2087036932', 'W2096791516', 'W2102714321', 'W2104549677', 'W2105306382', 'W2111063423', 'W2111307685', 'W2132731072', 'W2142386149', 'W2143602768', 'W2149791927', 'W2163104142', 'W2163705275', 'W2166501262', 'W2170093293', 'W2171777347', 'W2173213060', 'W2230276206', 'W2233221674', 'W2478496979', 'W2511515754', 'W4240204556'], 'abstract': 'Genotype imputation is a key component of genetic association studies, where it increases power, facilitates meta-analysis, and aids interpretation of signals. Genotype imputation is computationally demanding and, with current tools, typically requires access to a high-performance computing cluster and to a reference panel of sequenced genomes. Here we describe improvements to imputation machinery that reduce computational requirements by more than an order of magnitude with no loss of accuracy in comparison to standard imputation tools. We also describe a new web-based service for imputation that facilitates access to new reference panels and greatly improves user experience and productivity.', 'counts_by_year': [[2022, 433], [2021, 527], [2020, 381], [2019, 308], [2018, 189], [2017, 71], [2016, 5], [2015, 2]]}, {'id': 'W3004804052', 'doi': 'https://doi.org/10.1016/s2215-0366(20)30046-8', 'title': 'Timely mental health care for the 2019 novel coronavirus outbreak is urgently needed', 'type': 'journal-article', 'publication_date': '2020-02-04', 'host_venue': 'V2531556786', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2107352720', ['I204512498']], ['A2909311342', ['I58200834']], ['A2987009890', ['I204512498']], ['A2614676490', ['I4210112448']], ['A2166948836', ['I4210112448']], ['A2138714441', ['I14243506']], ['A2171254308', ['I4210150069']]], 'cited_by_count': 1916, 'concepts': [['C191935318', '0.773932'], ['C116675565', '0.7739295'], ['C3008058167', '0.757151'], ['C3007834351', '0.64227974'], ['C138816342', '0.63632506']], 'referenced_works': ['W78078439', 'W1586661029', 'W1995288620', 'W2169337111', 'W3001465255', 'W3002539152'], 'abstract': 'The 2019 novel coronavirus (2019-nCoV) pneumonia, believed to have originated in a wet market in Wuhan, Hubei province, China at the end of 2019, has gained intense attention nationwide and globally. To lower the risk of further disease transmission, the authority in Wuhan suspended public transport indefinitely from Jan 23, 2020; similar measures were adopted soon in many other cities in China. As of Jan 25, 2020, 30 Chinese provinces, municipalities, and autonomous regions covering over 1·3 billion people have initiated first-level responses to major public health emergencies.', 'counts_by_year': [[2022, 383], [2021, 827], [2020, 705]]}, {'id': 'W2603766943', 'doi': 'https://doi.org/10.1145/3052973.3053009', 'title': 'Practical Black-Box Attacks against Machine Learning', 'type': 'proceedings-article', 'publication_date': '2017-04-02', 'host_venue': 'V4306417956', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A248975517', ['I130769515']], ['A2056207806', ['I130769515']], ['A1822555116', ['I2747134083']], ['A2193269139', ['I135310074']], ['A2250297608', ['I130769515']], ['A2059211748', ['I166416128']]], 'cited_by_count': 1915, 'concepts': [['C37736160', '0.8686962'], ['C41008148', '0.7867296'], ['C41065033', '0.76273143'], ['C541664917', '0.7082563'], ['C2984842247', '0.6751891']], 'referenced_works': ['W2038296020', 'W2067713319', 'W2095577883', 'W2119885577', 'W2151298633', 'W2180612164', 'W2535873859'], 'abstract': 'Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.', 'counts_by_year': [[2022, 194], [2021, 456], [2020, 524], [2019, 450], [2018, 238], [2017, 49], [2016, 1]]}, {'id': 'W2884561390', 'doi': 'https://doi.org/10.1109/tpami.2018.2858826', 'title': 'Focal Loss for Dense Object Detection', 'type': 'journal-article', 'publication_date': '2020-02-01', 'host_venue': 'V199944782', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2618037599', ['I2252078561']], ['A2642611022', ['I2252078561']], ['A2473549963', ['I2252078561']], ['A2164292938', ['I2252078561']], ['A1944499404', ['I2252078561']]], 'cited_by_count': 1902, 'concepts': [['C94915269', '0.8198314'], ['C41008148', '0.77845764'], ['C154945302', '0.60585576'], ['C2776151529', '0.5706982'], ['C95623464', '0.52733624']], 'referenced_works': ['W1536680647', 'W1903029394', 'W1958328135', 'W2031489346', 'W2036989445', 'W2056695679', 'W2068730032', 'W2088049833', 'W2102605133', 'W2147800946', 'W2159386181', 'W2161969291', 'W2164598857', 'W2194775991', 'W2288122362', 'W2549139847', 'W2557728737', 'W2565639579', 'W2570343428', 'W2963037989', 'W2963150697', 'W2963516811', 'W2964269771'], 'abstract': 'The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.', 'counts_by_year': [[2022, 691], [2021, 625], [2020, 317], [2019, 221], [2018, 37], [2017, 2]]}, {'id': 'W2908201961', 'doi': 'https://doi.org/10.1038/s41591-018-0300-7', 'title': 'High-performance medicine: the convergence of human and artificial intelligence', 'type': 'journal-article', 'publication_date': '2019-01-01', 'host_venue': 'V203256638', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A1943535906', ['I1311914864']]], 'cited_by_count': 1886, 'concepts': [['C177212765', '0.8579831'], ['C79974875', '0.6490259'], ['C2780233690', '0.5906037'], ['C41008148', '0.54965407'], ['C75684735', '0.5480261']], 'referenced_works': ['W1600506198', 'W1943063538', 'W1966716734', 'W1968670396', 'W1995341919', 'W2001771035', 'W2017086716', 'W2074099390', 'W2126160338', 'W2154651644', 'W2175465469', 'W2198606573', 'W2221443338', 'W2238160795', 'W2257438637', 'W2358754356', 'W2399240576', 'W2404901863', 'W2408386558', 'W2513928994', 'W2514628397', 'W2528491735', 'W2550447077', 'W2554980225', 'W2557738935', 'W2558381168', 'W2559582770', 'W2563411328', 'W2569817387', 'W2580456502', 'W2581082771', 'W2582821003', 'W2585915213', 'W2588601551', 'W2592719727', 'W2604972438', 'W2606437410', 'W2607075141', 'W2608231518', 'W2608902687', 'W2611467245', 'W2613441532', 'W2621835653', 'W2623779865', 'W2734784508', 'W2734838926', 'W2735045133', 'W2738724892', 'W2741564801', 'W2744939564', 'W2748551393', 'W2752747624', 'W2758333670', 'W2760160110', 'W2760861884', 'W2761366790', 'W2765203617', 'W2765571304', 'W2766099988', 'W2767142522', 'W2767411797', 'W2770203518', 'W2772723798', 'W2772874515', 'W2774292910', 'W2775714759', 'W2782551527', 'W2782976400', 'W2783299048', 'W2783616919', 'W2786147899', 'W2786426577', 'W2787816121', 'W2787885772', 'W2788426287', 'W2788633781', 'W2790066236', 'W2790216347', 'W2790421204', 'W2793781442', 'W2794419196', 'W2794700984', 'W2795052379', 'W2795106634', 'W2796559433', 'W2797749376', 'W2798772226', 'W2799723178', 'W2799760772', 'W2799950076', 'W2800012666', 'W2800142021', 'W2801132216', 'W2805310212', 'W2805619986', 'W2805701040', 'W2807121762', 'W2810166631', 'W2810442486', 'W2883161521', 'W2883478711', 'W2883945062', 'W2883972171', 'W2884558319', 'W2884775595', 'W2885408863', 'W2886555678', 'W2886785829', 'W2887091285', 'W2887114371', 'W2887355428', 'W2888094747', 'W2888175467', 'W2888971284', 'W2889381121', 'W2889611173', 'W2889871190', 'W2892741787', 'W2892773404', 'W2894010682', 'W2894319790', 'W2894917609', 'W2895598278', 'W2895763047', 'W2896287590', 'W2896757914', 'W2896817483', 'W2897228760', 'W2897434820', 'W2897490102', 'W2897583329', 'W2900958875', 'W2952935243', 'W2953130175', 'W2964291773', 'W3101156210', 'W3103323619', 'W4213078347', 'W4242302408', 'W4251489771'], 'abstract': 'The use of artificial intelligence, and the deep-learning subtype in particular, has been enabled by the use of labeled big data, along with markedly enhanced computing power and cloud storage, across all sectors. In medicine, this is beginning to have an impact at three levels: for clinicians, predominantly via rapid, accurate image interpretation; for health systems, by improving workflow and the potential for reducing medical errors; and for patients, by enabling them to process their own data to promote health. The current limitations, including bias, privacy and security, and lack of transparency, along with the future directions of these applications will be discussed in this article. Over time, marked improvements in accuracy, productivity, and workflow will likely be actualized, but whether that will be used to improve the patient-doctor relationship or facilitate its erosion remains to be seen.', 'counts_by_year': [[2022, 458], [2021, 633], [2020, 536], [2019, 255], [2018, 1], [2014, 1]]}, {'id': 'W2962949934', 'doi': 'https://doi.org/10.1016/j.patcog.2017.10.013', 'title': 'Recent advances in convolutional neural networks', 'type': 'journal-article', 'publication_date': '2018-05-01', 'host_venue': 'V414566', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2634692963', ['I172675005']], ['A2659472065', ['I172675005']], ['A2338033149', ['I172675005']], ['A2919725078', ['I172675005']], ['A2208763518', ['I172675005']], ['A2090963083', ['I172675005']], ['A2638651394', ['I172675005']], ['A2124653075', ['I172675005']], ['A2607606047', ['I172675005']], ['A2093782331', ['I172675005']], ['A2232213365', ['I172675005']]], 'cited_by_count': 1880, 'concepts': [['C81363708', '0.7521541'], ['C41008148', '0.65147805'], ['C154945302', '0.50029445'], ['C153180895', '0.34735072']], 'referenced_works': ['W639708223', 'W823218635', 'W1512976292', 'W1833143043', 'W1885185971', 'W1922126009', 'W1968752118', 'W1971955426', 'W1978491093', 'W1980287119', 'W1983364832', 'W1984309565', 'W1993482030', 'W1993991024', 'W1996777517', 'W2007339694', 'W2009243364', 'W2011900468', 'W2022508996', 'W2027883219', 'W2033849769', 'W2034978228', 'W2035784046', 'W2046084401', 'W2056695679', 'W2064675550', 'W2071469153', 'W2074788634', 'W2075158265', 'W2089356193', 'W2094614786', 'W2109255472', 'W2111244981', 'W2112796928', 'W2117671523', 'W2117731089', 'W2119913432', 'W2121609805', 'W2136848157', 'W2137664016', 'W2145827727', 'W2145889472', 'W2147207799', 'W2148141637', 'W2151912295', 'W2156875677', 'W2160815625', 'W2161663128', 'W2162395775', 'W2168117308', 'W2168356304', 'W2171590421', 'W2184544926', 'W2222317246', 'W2253590344', 'W2264784471', 'W2271432203', 'W2278889731', 'W2289708887', 'W2294438834', 'W2308665943', 'W2395611524', 'W2470052106', 'W2506506742', 'W2508429489', 'W2510249351', 'W2524635875', 'W2533973874', 'W2560354825', 'W2604690669', 'W2620673467', 'W2620694480', 'W2621061298', 'W2743367112', 'W2962984063', 'W3098357269'], 'abstract': 'In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on different aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.', 'counts_by_year': [[2022, 503], [2021, 547], [2020, 471], [2019, 250], [2018, 94], [2017, 7], [2016, 1]]}, {'id': 'W2558748708', 'doi': 'https://doi.org/10.1109/msp.2017.2693418', 'title': 'Geometric Deep Learning: Going beyond Euclidean data', 'type': 'journal-article', 'publication_date': '2017-07-11', 'host_venue': 'V120977877', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2076464609', ['I57201433']], ['A2148877438', ['I9617848']], ['A2053214915', ['I36672615']], ['A1748740921', ['I4210114444']], ['A338287261', ['I5124864']]], 'cited_by_count': 1875, 'concepts': [['C108583219', '0.72307086'], ['C41008148', '0.6894232'], ['C154945302', '0.6047217'], ['C129782007', '0.5464786'], ['C50644808', '0.5336023']], 'referenced_works': ['W271358502', 'W764651262', 'W1501856433', 'W1644641054', 'W1763606621', 'W1951806617', 'W1965154800', 'W1967479046', 'W1974956622', 'W1976483474', 'W1977213563', 'W1988037271', 'W1994906459', 'W2001141328', 'W2007206727', 'W2013442102', 'W2015861736', 'W2016423476', 'W2022508996', 'W2036163530', 'W2038943263', 'W2044834685', 'W2047161559', 'W2053186076', 'W2054141820', 'W2072072671', 'W2082929484', 'W2090891622', 'W2097308346', 'W2098578926', 'W2100657858', 'W2101491865', 'W2104812688', 'W2110953678', 'W2116341502', 'W2124455902', 'W2124608575', 'W2127048411', 'W2132914434', 'W2138621090', 'W2144487656', 'W2145287260', 'W2147800946', 'W2151035455', 'W2153624566', 'W2153959628', 'W2154567221', 'W2157523980', 'W2159397589', 'W2160815625', 'W2168356304', 'W2203450678', 'W2229484904', 'W2230873857', 'W2261689926', 'W2273818272', 'W2320271346', 'W2398467116', 'W2591711955', 'W2618530766', 'W2919115771', 'W2962731536', 'W2962865163', 'W2963021451', 'W3104097132', 'W3105705953', 'W4205947740', 'W4210770595', 'W4213367101', 'W4239890145', 'W4241422529', 'W4242771523'], 'abstract': 'Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.', 'counts_by_year': [[2022, 267], [2021, 545], [2020, 492], [2019, 378], [2018, 173], [2017, 17], [2015, 1]]}, {'id': 'W2128672031', 'doi': 'https://doi.org/10.1023/a:1010090405266', 'title': None, 'type': 'journal-article', 'publication_date': '2019-05-08', 'host_venue': 'V5405189', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2740947004', ['I12912129']], ['A246870902', ['I98358874']], ['A1998032601', ['I12912129']]], 'cited_by_count': 1865, 'concepts': [['C41008148', '0.5914578'], ['C9652623', '0.56237084'], ['C2778464652', '0.4967087'], ['C2779343474', '0.48067054'], ['C26517878', '0.4798012']], 'referenced_works': ['W10714736', 'W44112122', 'W48957714', 'W64276040', 'W67079408', 'W96081297', 'W122319305', 'W126859721', 'W157562232', 'W159191692', 'W171807808', 'W173647602', 'W188850785', 'W280128266', 'W332028463', 'W573232330', 'W1487224832', 'W1487408605', 'W1488375996', 'W1491168590', 'W1495077382', 'W1498170831', 'W1500989946', 'W1501218016', 'W1502662603', 'W1503398984', 'W1504850170', 'W1505564915', 'W1508502558', 'W1509452512', 'W1513772907', 'W1514272540', 'W1514499253', 'W1516544374', 'W1522951504', 'W1527971709', 'W1532654511', 'W1533638164', 'W1539447265', 'W1544517305', 'W1549061150', 'W1552169927', 'W1556274146', 'W1559918484', 'W1562981304', 'W1572099486', 'W1574410118', 'W1575020259', 'W1579756385', 'W1583380718', 'W1593604466', 'W1596347158', 'W1598960279', 'W1615595151', 'W1652032257', 'W1657704689', 'W1674363625', 'W1754883368', 'W1757796397', 'W1762500617', 'W1765105432', 'W1814069098', 'W1923807516', 'W1927369651', 'W1968431527', 'W1969258302', 'W1969731555', 'W1975230313', 'W1980990187', 'W1985093013', 'W1985994912', 'W1993584577', 'W1996025150', 'W2004550196', 'W2007162415', 'W2008555470', 'W2015250214', 'W2020405260', 'W2026080185', 'W2028510317', 'W2031929578', 'W2036190613', 'W2038577305', 'W2043226657', 'W2052259295', 'W2054750126', 'W2054771737', 'W2055143352', 'W2061504687', 'W2061698657', 'W2062188790', 'W2066635329', 'W2067018993', 'W2068394020', 'W2069503920', 'W2070661660', 'W2072794470', 'W2073421029', 'W2073555374', 'W2075677013', 'W2076064414', 'W2076891662', 'W2078632526', 'W2079511459', 'W2083137466', 'W2086762211', 'W2088563966', 'W2088745669', 'W2090046750', 'W2093103468', 'W2095995891', 'W2096178388', 'W2097713162', 'W2097856935', 'W2098241131', 'W2098613108', 'W2101218365', 'W2101892490', 'W2102651584', 'W2105594594', 'W2105656578', 'W2106172458', 'W2106811180', 'W2110293626', 'W2111590691', 'W2111833414', 'W2113609601', 'W2113889826', 'W2116459397', 'W2117272332', 'W2119471769', 'W2120811645', 'W2121353282', 'W2121903938', 'W2122410182', 'W2124241059', 'W2124344619', 'W2124347194', 'W2126316555', 'W2128477394', 'W2129264276', 'W2129514287', 'W2129536637', 'W2132622103', 'W2132854830', 'W2133020360', 'W2133077903', 'W2133367045', 'W2135194391', 'W2135941076', 'W2136796925', 'W2137079713', 'W2137943964', 'W2138309709', 'W2140980106', 'W2145622131', 'W2146544759', 'W2150650036', 'W2155909075', 'W2155968351', 'W2156750468', 'W2160749806', 'W2162077280', 'W2165619126', 'W2166075343', 'W2168318917', 'W2168359464', 'W2168405694', 'W2168839459', 'W2170112109', 'W2170756108', 'W2171084228', 'W2171344460', 'W2171798962', 'W2178771565', 'W2179417754', 'W2183865782', 'W2257979135', 'W2340966270', 'W2398560691', 'W2402164836', 'W2482450324', 'W2489287324', 'W2607713949', 'W2803743229', 'W2808915486', 'W2913117594', 'W2949600864', 'W2962938178', 'W2963067607', 'W2963797557', 'W2988569019', 'W3022778360', 'W3144878813'], 'abstract': 'This paper provides an overview of research and development activities in the field of autonomous agents and multi-agent systems. It aims to identify key concepts and applications, and to indicate how they relate to one-another. Some historical context to the field of agent-based computing is given, and contemporary research directions are presented. Finally, a range of open issues and future challenges are highlighted.', 'counts_by_year': [[2022, 13], [2021, 26], [2020, 24], [2019, 38], [2018, 39], [2017, 41], [2016, 48], [2015, 61], [2014, 54], [2013, 65], [2012, 72]]}, {'id': 'W2504691963', 'doi': 'https://doi.org/10.1038/nbt.3597', 'title': 'Sharing and community curation of mass spectrometry data with Global Natural Products Social Molecular Networking', 'type': 'journal-article', 'publication_date': '2016-08-01', 'host_venue': 'V106963461', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2121048590', ['I36258959']], ['A2535345621', ['I36258959']], ['A1977813814', ['I6750721']], ['A2514802937', ['I6750721']], ['A2498296796', ['I6750721']], ['A2517534198', ['I36258959']], ['A2118818890', ['I36258959']], ['A1998697858', ['I6750721']], ['A754687162', ['I36258959']], ['A2344022115', ['I6750721']], ['A2310869093', ['I6750721']], ['A2597044816', ['I6750721']], ['A2464102483', ['I6750721']], ['A2098150814', ['I6750721']], ['A2117303831', ['I97018004']], ['A2044650595', ['I150209017']], ['A2104492156', ['I150209017']], ['A2039582032', []], ['A2267567819', ['I8961855']], ['A2035677575', ['I1307098950']], ['A2164498557', ['I6750721']], ['A2125878823', ['I26538001']], ['A2182985958', ['I88179191']], ['A2425153738', ['I36258959']], ['A2514005076', ['I36258959']], ['A2168955592', []], ['A3037380009', ['I150209017']], ['A600246365', ['I148283060']], ['A2136445761', ['I136199984']], ['A2318703221', ['I56067802']], ['A2106866167', ['I130238516']], ['A1989930027', ['I90183372']], ['A2283082009', ['I96673099']], ['A2239620056', ['I96673099']], ['A1812605448', ['I8961855']], ['A2016289770', ['I36258959']], ['A2483774928', ['I161318765']], ['A2107168266', ['I161318765']], ['A2149213986', ['I35440088']], ['A2168820440', ['I39422238']], ['A1830095943', ['I150209017']], ['A2107823109', ['I142974352']], ['A2534179757', ['I84653119']], ['A1802889048', ['I22465464']], ['A2402595654', ['I96673099']], ['A2014986815', ['I41156924']], ['A2101875677', ['I114027177']], ['A3037049078', ['I592451']], ['A2013898710', ['I592451']], ['A2585852787', ['I128212801']], ['A2074905611', ['I96673099']], ['A2596724736', ['I6750721']], ['A2507683772', []], ['A210566123', []], ['A3087866093', ['I6750721']], ['A2099767764', ['I17974374']], ['A2497589885', ['I17974374']], ['A2197256436', ['I17974374']], ['A2686166032', []], ['A2124796799', ['I150209017']], ['A2198593962', ['I56067802']], ['A2101686076', ['I35440088']], ['A1128820198', ['I107639228']], ['A2509150051', ['I150209017']], ['A2494938595', ['I35440088']], ['A3174769690', []], ['A2186340883', ['I36258959']], ['A340400158', ['I150209017']], ['A2429293785', ['I36258959']], ['A152261909', ['I35440088']], ['A3037198557', ['I185103710']], ['A868280607', ['I36258959']], ['A2078301530', ['I131249849']], ['A2626078551', ['I96673099']], ['A2254504077', []], ['A2308104942', ['I39422238']], ['A2778050236', ['I95457486']], ['A2308665010', ['I19700959']], ['A2029530826', ['I6750721']], ['A2067239923', ['I131249849']], ['A2052726048', ['I114027177']], ['A2153023592', ['I17974374']], ['A1971264826', ['I150209017']], ['A2015088023', ['I56067802']], ['A2108991382', ['I148283060']], ['A2145944804', []], ['A2618824624', []], ['A2056297746', ['I150209017']], ['A2113563849', ['I117965899']], ['A2597029946', ['I117965899']], ['A2969422064', ['I117965899']], ['A2647191221', ['I117965899']], ['A2176181809', ['I17974374']], ['A2018559166', ['I36258959']], ['A2236867554', ['I36258959']], ['A2507505808', ['I6750721']], ['A2144668024', ['I6750721']], ['A2408128977', ['I6750721']], ['A2117458855', ['I114457229']], ['A2559315588', []], ['A2325193365', ['I1298838906']], ['A2035102806', []], ['A1959637751', ['I1298838906']], ['A2173314442', ['I114457229']], ['A2197849173', ['I142606810']], ['A1981858215', ['I142606810']], ['A2410527159', ['I1299303238']], ['A2168868964', ['I1299303238']], ['A2630036233', ['I1299303238']], ['A2042150171', ['I1299303238']], ['A2097689153', ['I1299303238']], ['A2124235950', []], ['A2021261575', ['I142606810']], ['A2182357491', ['I161318765']], ['A2112480543', ['I19820366']], ['A2151648700', ['I19820366']], ['A2159013505', ['I36258959']], ['A2132402394', ['I150209017']], ['A2068736320', ['I36258959']], ['A567710207', ['I36258959']], ['A2072858308', ['I185103710']], ['A2118848851', []], ['A2095816424', ['I17974374']], ['A136024802', ['I6750721']], ['A2169019283', ['I6750721']], ['A1478187186', ['I6750721']], ['A2024858011', ['I6750721']]], 'cited_by_count': 1865, 'concepts': [['C162356407', '0.5963512'], ['C91632574', '0.5907587'], ['C2776608160', '0.5373714'], ['C185592680', '0.45549995'], ['C70721500', '0.4518069']], 'referenced_works': ['W1000261187', 'W1601644125', 'W1948611833', 'W1960974681', 'W1965387310', 'W1966156878', 'W1979663509', 'W1980074832', 'W1982215868', 'W1986517195', 'W1993935949', 'W1995509429', 'W1997386061', 'W2003545535', 'W2007404630', 'W2014060571', 'W2018375934', 'W2021904169', 'W2026216948', 'W2035164474', 'W2042160423', 'W2043472907', 'W2043810674', 'W2052877141', 'W2056190528', 'W2057761395', 'W2059327215', 'W2069928158', 'W2076069550', 'W2084375915', 'W2099983582', 'W2130253098', 'W2130479394', 'W2135707678', 'W2136216414', 'W2144803530', 'W2154289641', 'W2159675211', 'W2161062388', 'W2165681080', 'W2179948434', 'W2222545236', 'W2223985870', 'W2294110806', 'W2322764356', 'W2326842020', 'W2403360525', 'W2417825531', 'W4213149192', 'W4242729757'], 'abstract': "The potential of the diverse chemistries present in natural products (NP) for biotechnology and medicine remains untapped because NP databases are not searchable with raw data and the NP community has no way to share data other than in published papers. Although mass spectrometry (MS) techniques are well-suited to high-throughput characterization of NP, there is a pressing need for an infrastructure to enable sharing and curation of data. We present Global Natural Products Social Molecular Networking (GNPS; http://gnps.ucsd.edu), an open-access knowledge base for community-wide organization and sharing of raw, processed or identified tandem mass (MS/MS) spectrometry data. In GNPS, crowdsourced curation of freely available community-wide reference MS libraries will underpin improved annotations. Data-driven social-networking should facilitate identification of spectra and foster collaborations. We also introduce the concept of 'living data' through continuous reanalysis of deposited data.", 'counts_by_year': [[2022, 394], [2021, 524], [2020, 369], [2019, 273], [2018, 162], [2017, 122], [2016, 19], [2015, 1]]}, {'id': 'W1731081199', 'doi': 'https://doi.org/10.1007/978-3-319-58347-1_10', 'title': 'Domain-Adversarial Training of Neural Networks', 'type': 'book-chapter', 'publication_date': '2016-01-01', 'host_venue': 'V4306463941', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2496948775', ['I125989756']], ['A2271586418', ['I125989756']], ['A2507508090', ['I43406934']], ['A2130645691', ['I43406934']], ['A1963576484', ['I135117807']], ['A2076579854', ['I43406934']], ['A2128721840', ['I43406934']], ['A2241827754', ['I125989756']]], 'cited_by_count': 1855, 'concepts': [['C37736160', '0.872118'], ['C2777211547', '0.75697005'], ['C36503486', '0.6534429'], ['C50644808', '0.6031843'], ['C41008148', '0.6028126']], 'referenced_works': ['W6908809', 'W22861983', 'W41482161', 'W204268067', 'W1009458120', 'W1544165511', 'W1565327149', 'W1577269164', 'W1596233070', 'W1608944489', 'W1617650991', 'W1722318740', 'W1978380814', 'W2025768430', 'W2030558520', 'W2033547469', 'W2047632871', 'W2064447488', 'W2083544878', 'W2095705004', 'W2104068492', 'W2104094955', 'W2108069432', 'W2110158442', 'W2112483442', 'W2112796928', 'W2117155597', 'W2118585731', 'W2120587290', 'W2125889200', 'W2126443908', 'W2128053425', 'W2131953535', 'W2140844045', 'W2141067830', 'W2147093486', 'W2147629985', 'W2149466042', 'W2155541015', 'W2158108973', 'W2158815628', 'W2159570078', 'W2161381512', 'W2163605009', 'W2165741220', 'W2186639548', 'W2335728318', 'W2395859096', 'W2405601855', 'W2949821452', 'W2950094539', 'W2951670162', 'W2951700157', 'W2952186574', 'W2963826681'], 'abstract': 'We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains.\r\n\r\nThe approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages.\r\n\r\nWe demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.', 'counts_by_year': [[2022, 52], [2021, 512], [2020, 559], [2019, 392], [2018, 209], [2017, 103], [2016, 26], [2015, 1], [2014, 1], [2012, 1]]}, {'id': 'W2964241181', 'doi': 'https://doi.org/10.1109/cvpr.2018.00644', 'title': 'Cascade R-CNN: Delving Into High Quality Object Detection', 'type': 'proceedings-article', 'publication_date': '2018-06-18', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2116880480', ['I36258959']], ['A2133983782', ['I36258959']]], 'cited_by_count': 1853, 'concepts': [['C22019652', '0.8705524'], ['C94915269', '0.84390354'], ['C41008148', '0.71644783'], ['C34146451', '0.68625504'], ['C154945302', '0.6207682']], 'referenced_works': ['W1536680647', 'W1689909837', 'W1932624639', 'W1934410531', 'W2031489346', 'W2088049833', 'W2090530238', 'W2102605133', 'W2120396754', 'W2136000821', 'W2155893237', 'W2168356304', 'W2194775991', 'W2216125271', 'W2565639579', 'W2601564443', 'W2963037989', 'W2963150697', 'W2963296245', 'W2963351448', 'W2963418361', 'W2963516811', 'W2963873508', 'W3097096317'], 'abstract': 'In object detection, an intersection over union (IoU) threshold is required to define positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overfitting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overfitting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code will be made available at https://github.com/zhaoweicai/cascade-rcnn.', 'counts_by_year': [[2022, 417], [2021, 698], [2020, 458], [2019, 257], [2018, 22]]}, {'id': 'W2127309075', 'doi': 'https://doi.org/10.1016/j.neuroimage.2015.10.019', 'title': 'An integrated approach to correction for off-resonance effects and subject movement in diffusion MR imaging', 'type': 'journal-article', 'publication_date': '2016-01-15', 'host_venue': 'V103225281', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2946638410', ['I4210101881']], ['A1838132751', ['I4210101881']]], 'cited_by_count': 1849, 'concepts': [['C100053769', '0.66005296'], ['C41008148', '0.5460626'], ['C69357855', '0.53980494'], ['C2779751349', '0.5366755'], ['C97820695', '0.49790716']], 'referenced_works': ['W1024451551', 'W1890720342', 'W1968913378', 'W1969339432', 'W1970099691', 'W1974508089', 'W1974578052', 'W1976723820', 'W1977508444', 'W1981203052', 'W1984322424', 'W1984453610', 'W1985629949', 'W1987263024', 'W1988529488', 'W1992107615', 'W1993053013', 'W2006096283', 'W2013312794', 'W2013546384', 'W2014315179', 'W2016114300', 'W2017077053', 'W2018720899', 'W2020541664', 'W2024729467', 'W2027864333', 'W2042958451', 'W2050921210', 'W2051515940', 'W2053610521', 'W2053759320', 'W2053838094', 'W2058919776', 'W2059189228', 'W2063072252', 'W2066170692', 'W2067214598', 'W2067560632', 'W2069522536', 'W2071472234', 'W2073431334', 'W2078669193', 'W2078904820', 'W2094491633', 'W2122662954', 'W2126255160', 'W2128207744', 'W2138764991', 'W2147133578', 'W2148726987', 'W2149249019', 'W2150696106', 'W2165810607', 'W2166023226', 'W2168844688', 'W2408062231', 'W4210583703'], 'abstract': 'In this paper we describe a method for retrospective estimation and correction of eddy current (EC)-induced distortions and subject movement in diffusion imaging. In addition a susceptibility-induced field can be supplied and will be incorporated into the calculations in a way that accurately reflects that the two fields (susceptibility- and EC-induced) behave differently in the presence of subject movement. The method is based on registering the individual volumes to a model free prediction of what each volume should look like, thereby enabling its use on high b-value data where the contrast is vastly different in different volumes. In addition we show that the linear EC-model commonly used is insufficient for the data used in the present paper (high spatial and angular resolution data acquired with Stejskal-Tanner gradients on a 3T Siemens Verio, a 3T Siemens Connectome Skyra or a 7T Siemens Magnetome scanner) and that a higher order model performs significantly better. The method is already in extensive practical use and is used by four major projects (the WU-UMinn HCP, the MGH HCP, the UK Biobank and the Whitehall studies) to correct for distortions and subject movement.', 'counts_by_year': [[2022, 351], [2021, 470], [2020, 399], [2019, 307], [2018, 192], [2017, 96], [2016, 31], [2015, 2]]}, {'id': 'W2511515754', 'doi': 'https://doi.org/10.1038/ng.3643', 'title': 'A reference panel of 64,976 haplotypes for genotype imputation', 'type': 'journal-article', 'publication_date': '2016-08-22', 'host_venue': 'V137905309', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2115374308', ['I2802476451']], ['A2186633199', ['I27837315']], ['A2100036854', ['I40120149']], ['A98905150', ['I114457229']], ['A2152418708', ['I23923803']], ['A1945221331', ['I2799318839']], ['A2123139493', ['I27837315']], ['A225127237', ['I27837315']], ['A1242522392', ['I2802476451']], ['A2133268072', ['I40120149']], ['A2710283555', ['I2802476451']], ['A3024029107', ['I197604219']], ['A2132816084', ['I27837315']], ['A1943150464', ['I36234482']], ['A2050342510', []], ['A2307445584', ['I188538660']], ['A2139922620', ['I27837315']], ['A2526263312', ['I27837315']], ['A2104698110', ['I40120149']], ['A1988141520', []], ['A2129527657', ['I201448701']], ['A2958769496', ['I181697535']], ['A1901246291', ['I913958620']], ['A2302657313', ['I27837315']], ['A1982856781', ['I142444530']], ['A2037369389', []], ['A2028532243', ['I2802476451']], ['A2005085472', ['I142444530']], ['A2336592905', []], ['A2050418226', ['I197604219']], ['A2100525881', ['I2802476451']], ['A1959077594', ['I865915315']], ['A2002274367', ['I27837315']], ['A1971092244', ['I183935753']], ['A1964058294', ['I27837315']], ['A337586379', ['I197604219']], ['A2112768246', ['I98677209']], ['A2577504377', ['I136199984']], ['A2500047411', ['I27837315']], ['A2113571590', ['I1299303238']], ['A2100172157', ['I1299303238']], ['A2149864179', ['I36234482']], ['A2168326394', ['I36234482']], ['A1998994756', ['I32762134']], ['A2099847585', ['I2799318839']], ['A2093918228', ['I32762134']], ['A2079907023', ['I1299303238']], ['A2066390143', ['I143397708']], ['A2570663853', ['I27837315']], ['A2331008296', ['I107606265']], ['A2945141954', []], ['A2319840580', ['I133731052']], ['A2111355138', ['I1326427960']], ['A2780785969', ['I23923803']], ['A2035630911', ['I204778367']], ['A2082446753', ['I204778367']], ['A119041783', ['I27837315']], ['A2191696813', ['I241749']], ['A2021626791', ['I130238516']], ['A2005615643', ['I62916508']], ['A2128348509', ['I23923803']], ['A2638119709', ['I36234482']], ['A2605662202', ['I114027177']], ['A2591400394', ['I185261750']], ['A2440741749', ['I2799318839']], ['A3174673283', ['I201448701']], ['A711766389', ['I107606265']], ['A2163743836', ['I181697535']], ['A2066771690', ['I142444530']], ['A1904083210', ['I27837315']], ['A2128678756', ['I5023651']], ['A2146787534', []], ['A2172308787', []], ['A1189913679', ['I1299303238']], ['A2346993964', ['I143397708']], ['A352779862', ['I121797337']], ['A2118092543', ['I183935753']], ['A62300903', ['I183935753']], ['A2127993160', ['I79576946']], ['A2094092244', ['I23923803']], ['A705222334', []], ['A2096345614', []], ['A235802380', []], ['A177686970', ['I2799318839']], ['A2343358114', ['I169381384']], ['A2094254802', []], ['A166685849', ['I2802476451']], ['A2940902506', ['I142444530']], ['A2286951327', ['I27837315']], ['A2167833623', ['I98677209']], ['A2311530707', ['I23923803']], ['A1837395989', ['I193662353']], ['A1416178009', ['I169381384']], ['A2062062575', ['I136199984']], ['A727411131', ['I1326427960']], ['A2291488562', []], ['A2472878971', []], ['A296207759', ['I27837315']], ['A723323371', ['I130238516']], ['A1920547575', ['I133731052']], ['A2066802861', ['I241749']], ['A2267905284', ['I2802476451']], ['A1844463566', ['I1299303238']], ['A1597337812', ['I197604219']], ['A2106186865', ['I2802476451']], ['A2935873304', []], ['A2137325747', ['I27837315']], ['A2472664475', ['I40120149']], ['A1445484688', ['I2802476451']], ['A2305243664', ['I27837315']], ['A2056155260', ['I40120149']]], 'cited_by_count': 1848, 'concepts': [['C58041806', '0.91115093'], ['C86803240', '0.7869035'], ['C197754878', '0.7717304'], ['C153209595', '0.7020008'], ['C157410074', '0.6933764']], 'referenced_works': ['W1627352250', 'W1978116054', 'W1985305462', 'W1992436001', 'W2017753262', 'W2051317473', 'W2061733455', 'W2087036932', 'W2104238967', 'W2104549677', 'W2111307685', 'W2119279196', 'W2131014997', 'W2132731072', 'W2138253007', 'W2141042406', 'W2142563181', 'W2149791927', 'W2163104142', 'W2166501262', 'W2170093293', 'W2216458865', 'W2309446455', 'W2415473920'], 'abstract': 'We describe a reference panel of 64,976 human haplotypes at 39,235,157 SNPs constructed using whole-genome sequence data from 20 studies of predominantly European ancestry. Using this resource leads to accurate genotype imputation at minor allele frequencies as low as 0.1% and a large increase in the number of SNPs tested in association studies, and it can help to discover and refine causal loci. We describe remote server resources that allow researchers to carry out imputation and phasing consistently and efficiently.', 'counts_by_year': [[2022, 275], [2021, 441], [2020, 364], [2019, 323], [2018, 299], [2017, 129], [2016, 14], [2015, 2]]}, {'id': 'W2755950973', 'doi': 'https://doi.org/10.1016/j.joi.2017.08.007', 'title': 'bibliometrix : An R-tool for comprehensive science mapping analysis', 'type': 'journal-article', 'publication_date': '2017-11-01', 'host_venue': 'V205292342', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A923435168', ['I71267560']], ['A2504414573', ['I197809005']]], 'cited_by_count': 1846, 'concepts': [['C41008148', '0.6361582'], ['C2522767166', '0.3629617']], 'referenced_works': ['W767067438', 'W1799889357', 'W1963753783', 'W1966104031', 'W1966538856', 'W1968283201', 'W1975107573', 'W1976620775', 'W1977714176', 'W1981340255', 'W1981886524', 'W1989669128', 'W1996229960', 'W2002117998', 'W2017680781', 'W2027998681', 'W2034800299', 'W2042852414', 'W2045108252', 'W2052710775', 'W2056147580', 'W2066888021', 'W2069134922', 'W2075220720', 'W2094864959', 'W2097148950', 'W2098162425', 'W2108680868', 'W2128438887', 'W2135455887', 'W2146695321', 'W2150220236', 'W2171458572', 'W2225280195', 'W2285085441', 'W2408216567', 'W2472756135', 'W2963453445', 'W3103443220', 'W3125707221', 'W4241271818', 'W4243208282', 'W4248252387'], 'abstract': 'Abstract   The use of bibliometrics is gradually extending to all disciplines. It is particularly suitable for science mapping at a time when the emphasis on empirical contributions is producing voluminous, fragmented, and controversial research streams. Science mapping is complex and unwieldly because it is multi-step and frequently requires numerous and diverse software tools, which are not all necessarily freeware. Although automated workflows that integrate these software tools into an organized data flow are emerging, in this paper we propose a unique open-source tool, designed by the authors, called bibliometrix, for performing comprehensive science mapping analysis. bibliometrix supports a recommended workflow to perform bibliometric analyses. As it is programmed in R, the proposed tool is flexible and can be rapidly upgraded and integrated with other statistical R-packages. It is therefore useful in a constantly changing science such as bibliometrics.', 'counts_by_year': [[2022, 787], [2021, 674], [2020, 292], [2019, 70], [2018, 20], [2017, 1]]}, {'id': 'W2621266390', 'doi': 'https://doi.org/10.1103/physrevlett.118.221101', 'title': 'GW170104: Observation of a 50-Solar-Mass Binary Black Hole Coalescence at Redshift 0.2', 'type': 'journal-article', 'publication_date': '2017-06-01', 'host_venue': 'V24807848', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A3178754429', ['I122411786']], ['A2931258442', ['I122411786']], ['A2898869343', ['I121820613']], ['A1989600986', ['I131729948']], ['A2102662472', ['I33213144']], ['A3130519413', ['I122411786']], ['A2781536124', ['I70900168']], ['A2420842909', ['I16337185']], ['A2120816080', ['I122411786']], ['A2643273335', ['I1285433949']], ['A3169239100', ['I1285433949']], ['A3100404251', ['I368840534']], ['A2606179050', ['I157725225']], ['A2063771739', ['I241749']], ['A2196045220', []], ['A2149021947', ['I63966007']], ['A2603756616', ['I80849659']], ['A2013867193', []], ['A3188632010', ['I59781447']], ['A2590089022', ['I11947397']], ['A3131221433', ['I43579087']], ['A2116332243', ['I157725225']], ['A2832504483', ['I108290504']], ['A1977554423', ['I118347636']], ['A2097044971', ['I1294671590']], ['A3192986197', ['I122411786']], ['A3124361617', ['I122411786']], ['A3212470564', ['I43579087']], ['A2860335995', ['I277688954']], ['A3193127485', ['I122411786']], ['A2730442849', ['I122411786']], ['A3192504475', ['I122411786']], ['A3187769929', ['I142934699']], ['A2872020220', ['I277688954']], ['A2259678854', ['I19149307']], ['A2947746710', ['I116067653']], ['A2113463528', ['I1285433949']], ['A1989490620', ['I159176309']], ['A2031868467', ['I122411786']], ['A2928136190', []], ['A1986532585', ['I114112103']], ['A354908839', ['I1285433949']], ['A3212970892', ['I84475105']], ['A2984619410', ['I142934699']], ['A2624902964', ['I1285433949']], ['A2776483237', ['I169173203']], ['A2980672605', []], ['A3212616752', ['I878022262']], ['A3037946954', ['I12097938']], ['A3212949844', ['I27483092']], ['A2079318338', []], ['A2431706484', ['I70983195']], ['A3177099217', ['I130238516']], ['A3192154922', ['I122411786']], ['A2596778389', ['I7882870']], ['A2951079984', ['I122411786']], ['A2161639735', []], ['A2136366644', ['I131729948']], ['A2580889767', ['I7882870']], ['A2041338530', ['I63966007']], ['A2862977381', ['I169173203']], ['A3190637119', []], ['A2106758119', []], ['A3211977126', ['I78577930']], ['A2030123867', ['I97018004']], ['A2440901623', ['I108290504']], ['A2137004321', []], ['A3125242436', ['I149899117']], ['A3208170591', ['I112859197']], ['A2463697456', ['I138689650']], ['A3120418608', ['I106118109']], ['A2761866993', ['I149899117']], ['A3202648083', ['I99542240']], ['A3213399745', ['I277688954']], ['A2561527376', ['I7882870']], ['A3149347399', ['I122411786']], ['A3188724226', ['I149899117']], ['A2167365184', ['I79619799']], ['A416185457', ['I83816512']], ['A2902171395', []], ['A2083399793', []], ['A2294285147', ['I70983195']], ['A3211899950', []], ['A2305878762', ['I19880235']], ['A2592016208', ['I122411786']], ['A2259017461', ['I33213144']], ['A3206513768', []], ['A3213116417', ['I868834043']], ['A252456976', ['I149899117']], ['A1685694851', ['I63966007']], ['A2242844166', ['I114112103']], ['A1977286476', []], ['A2784445771', ['I70983195']], ['A2191991431', ['I277688954']], ['A3037119030', ['I122411786']], ['A2137817755', ['I122411786']], ['A2573281761', ['I177877127']], ['A2110062665', ['I177877127']], ['A3183534842', []], ['A3036941851', ['I145872427']], ['A2878600144', ['I149899117']], ['A2734424565', ['I149899117']], ['A2510367362', ['I114983960']], ['A3187225073', ['I114983960']], ['A2429888154', ['I149899117']], ['A2656910661', ['I56067802']], ['A401219682', ['I2799516425']], ['A3176056021', []], ['A3176036958', ['I122411786']], ['A2950710615', ['I108290504']], ['A2752961157', ['I59781447']], ['A3188004614', ['I169173203']], ['A2161572305', []], ['A3187368144', []], ['A2133056318', ['I43579087']], ['A2579247217', ['I19880235']], ['A3207909444', ['I190397597']], ['A2590013777', ['I181233156']], ['A2762264107', ['I2746051580']], ['A3213443375', ['I114983960']], ['A3022976532', ['I149899117']], ['A3174630990', ['I277688954']], ['A2964759560', ['I43579087']], ['A3177457120', ['I188497080']], ['A2916988494', ['I122411786']], ['A3174410473', ['I70983195']], ['A2828582320', ['I79619799']], ['A3204194466', ['I63966007']], ['A3214576486', ['I122411786']], ['A3186585704', ['I121820613']], ['A1920643890', ['I63966007']], ['A2049455156', ['I4654613']], ['A3192455471', ['I865915315']], ['A1949674659', ['I149899117']], ['A2955044138', ['I2799516425']], ['A3160394671', ['I169173203']], ['A2047849238', ['I97018004']], ['A2816657102', ['I149899117']], ['A1985904051', ['I130701444']], ['A2595927603', ['I100532134']], ['A3211859875', ['I122411786']], ['A3212580957', ['I130701444']], ['A3214030669', ['I122411786']], ['A2284239319', []], ['A3020614285', ['I1306266525']], ['A2252054064', ['I83816512']], ['A3033306342', ['I145872427']], ['A2946402245', ['I74801974']], ['A2552037243', ['I5681781']], ['A2119394733', ['I99065089']], ['A3208452685', ['I149899117']], ['A2633311535', ['I169173203']], ['A2942460859', []], ['A3168193628', ['I12315562']], ['A2743089085', ['I166972335']], ['A2253635596', ['I277688954']], ['A3183896882', ['I116067653']], ['A2853512116', ['I43579087']], ['A2428212893', ['I368840534']], ['A2215987423', ['I277688954']], ['A3137638649', []], ['A3207900275', []], ['A2593215993', ['I122411786']], ['A3012343862', ['I190397597']], ['A3213777938', ['I108290504']], ['A2993381879', ['I116067653']], ['A3171906794', ['I130769515']], ['A3081522446', ['I7882870']], ['A2293492264', ['I25846049']], ['A3213317439', ['I153230381']], ['A2355490837', ['I169173203']], ['A2812767936', ['I43579087']], ['A2949330054', ['I185261750']], ['A3212324755', ['I12097938']], ['A3187385361', ['I40347166']], ['A3175291849', ['I122411786']], ['A2107266570', ['I33213144']], ['A1966111217', []], ['A2026320856', []], ['A2595735354', ['I166972335']], ['A3135857111', ['I4921948']], ['A3211378190', ['I66946132']], ['A1996499331', ['I118347636']], ['A2107960519', ['I114983960']], ['A2302273048', ['I177877127']], ['A2570425278', ['I241749']], ['A3022631697', ['I2746051580']], ['A3131885961', ['I177725633']], ['A2876288685', ['I177877127']], ['A2886637637', ['I33213144']], ['A3191252368', ['I875825670']], ['A2742982685', ['I97018004']], ['A3211866706', ['I83816512']], ['A3089244475', []], ['A2806972367', ['I130701444']], ['A3192959517', ['I114983960']], ['A2561889523', ['I368840534']], ['A2793250799', []], ['A2629010976', ['I2746051580']], ['A2798575997', ['I861853513']], ['A1993090682', ['I132053463']], ['A2946504086', ['I158011677']], ['A2949908670', ['I80849659']], ['A2345183096', []], ['A2980616383', ['I79619799']], ['A2743835024', []], ['A3088498399', ['I121820613']], ['A3196881120', ['I78577930']], ['A2343198265', ['I23732399']], ['A2104489708', ['I12315562']], ['A1997433525', []], ['A2309834725', ['I80849659']], ['A2794293094', ['I188497080']], ['A3088771138', ['I79510175']], ['A3182150140', ['I114983960']], ['A2973073773', ['I78577930']], ['A2607649874', ['I122411786']], ['A2879856988', ['I905677539']], ['A2252855434', ['I130701444']], ['A2008373628', ['I177877127']], ['A3087786845', []], ['A3190232124', ['I122411786']], ['A3212834775', ['I12315562']], ['A2277296262', ['I43579087']], ['A3129173276', ['I2802326326']], ['A2324162412', ['I121820613']], ['A2829642999', ['I174135032']], ['A2851429363', ['I142934699']], ['A2144671710', ['I7882870']], ['A3216622180', ['I7882870']], ['A3186922099', []], ['A2044934989', ['I1306266525']], ['A3104715842', ['I149899117']], ['A2042635623', []], ['A1979152311', ['I149899117']], ['A2135574779', []], ['A3180139287', ['I33213144']], ['A2146738652', []], ['A3211521442', []], ['A2168664669', ['I277688954']], ['A3214511527', ['I70983195']], ['A2015763309', ['I91136226']], ['A2612479978', ['I130701444']], ['A2828709917', ['I70983195']], ['A3214707733', ['I97018004']], ['A220773886', ['I1174212']], ['A3010691662', ['I1294671590']], ['A2304156027', []], ['A1418452008', ['I2746051580']], ['A2116577483', ['I79619799']], ['A437422485', ['I149899117']], ['A2569824804', ['I149899117']], ['A2789344678', ['I149899117']], ['A3088584594', []], ['A2950226714', []], ['A2880828452', ['I27825529']], ['A2616433085', ['I868834043']], ['A2249106444', ['I12097938']], ['A3213061463', ['I59781447']], ['A2977241735', ['I2802326326']], ['A3212049868', []], ['A3147297631', ['I193223587']], ['A2917561175', ['I78577930']], ['A3011099301', ['I108290504']], ['A3214657051', ['I861853513']], ['A3212506750', ['I861853513']], ['A3144773985', ['I108290504']], ['A2274420929', ['I40347166']], ['A2643732197', ['I1294671590']], ['A2147618017', ['I63966007']], ['A2598058840', ['I368840534']], ['A1744413271', ['I149899117']], ['A2981499915', ['I79510175']], ['A2966439833', ['I7882870']], ['A2560970344', ['I79619799']], ['A2305700984', ['I43579087']], ['A2241179355', ['I149899117']], ['A3106418577', ['I122411786']], ['A3206711596', []], ['A2230265955', ['I99065089']], ['A2761346687', ['I2799516425']], ['A2302301766', ['I111979921']], ['A3197027745', []], ['A3160246366', ['I91136226']], ['A3091867744', ['I188497080']], ['A3197649087', []], ['A2778232883', ['I149899117']], ['A3214447848', ['I122411786']], ['A3148340371', ['I122411786']], ['A3016124112', ['I33213144']], ['A2791266189', ['I63966007']], ['A2467688294', ['I63966007']], ['A2028554557', ['I12097938']], ['A3190793682', ['I122411786']], ['A2917506697', ['I63966007']], ['A2948178927', []], ['A2821390857', ['I78577930']], ['A2914472670', ['I116067653']], ['A2592053980', ['I70983195']], ['A2307450509', ['I79510175']], ['A2820447793', ['I99065089']], ['A3207155494', []], ['A3102738309', ['I40347166']], ['A2641626098', ['I79619799']], ['A3213456684', ['I79510175']], ['A3211611348', ['I166088655']], ['A1999419386', ['I79510175']], ['A2773897234', ['I149899117']], ['A3211840338', ['I122411786']], ['A2149221475', ['I97018004']], ['A3108351319', ['I63966007']], ['A1833794549', ['I108290504']], ['A3175940303', ['I80849659']], ['A1998320771', []], ['A1965779292', ['I108290504']], ['A2984447949', []], ['A3103283984', ['I169173203']], ['A3196836202', ['I70983195']], ['A2792463995', ['I1294671590']], ['A3212026953', ['I7882870']], ['A3191197435', ['I185261750']], ['A3213800390', ['I118347636']], ['A2697261290', ['I130701444']], ['A2591496726', ['I114983960']], ['A2139140847', ['I861853513']], ['A2653205707', []], ['A3213489065', ['I106118109']], ['A2026851826', ['I79619799']], ['A2167050613', ['I181233156']], ['A2580419259', ['I277688954']], ['A2946183167', ['I122411786']], ['A2112263148', ['I63966007']], ['A2782723294', []], ['A3173498059', ['I33213144']], ['A3191281857', []], ['A3000397202', ['I149899117']], ['A2742943521', ['I138943879']], ['A3200044225', ['I59781447']], ['A2884489641', ['I79619799']], ['A2340803116', ['I98677209']], ['A2608772347', ['I27483092']], ['A687187703', ['I5681781']], ['A3213055845', ['I59781447']], ['A2848021311', []], ['A3197661198', ['I84475105']], ['A3214056200', []], ['A2607714946', ['I122996671']], ['A2616884440', ['I1306266525']], ['A3211664842', []], ['A3164183435', []], ['A3184262436', []], ['A2944315956', ['I157725225']], ['A2282535663', []], ['A2436983654', ['I227486990']], ['A2472445730', ['I2799516425']], ['A3213672858', ['I130701444']], ['A2224979328', ['I11947397']], ['A3213670258', ['I11947397']], ['A3210093540', ['I145872427']], ['A2977042228', ['I121820613']], ['A2974774671', []], ['A1931051767', []], ['A2896549439', ['I84475105']], ['A2970255773', ['I27825529']], ['A2588028427', ['I149899117']], ['A2120371615', ['I33213144']], ['A2743085654', ['I79510175']], ['A3213909439', ['I121820613']], ['A2267511333', ['I108290504']], ['A2021276475', ['I11947397']], ['A1968023185', ['I19880235']], ['A2873961809', ['I122411786']], ['A3187762700', []], ['A3193155114', ['I2799516425']], ['A2982931411', ['I875825670']], ['A3211802894', ['I7882870']], ['A1859510812', ['I1294671590']], ['A2137278808', ['I7882870']], ['A2951105902', ['I63966007']], ['A2837878796', []], ['A3164983791', ['I190397597']], ['A2470660414', ['I79619799']], ['A2134513389', ['I145872427']], ['A3018296462', ['I149899117']], ['A2895613899', ['I149899117']], ['A2743903848', ['I277688954']], ['A3192147838', ['I190397597']], ['A3140717951', ['I99065089']], ['A2580259573', ['I130769515']], ['A3037040773', []], ['A2966636198', ['I122411786']], ['A3122707594', ['I122411786']], ['A3212331716', ['I27837315']], ['A3213215445', ['I72951846']], ['A2946415084', ['I122411786']], ['A2420107187', ['I7882870']], ['A3164587974', ['I11947397']], ['A3196871450', ['I149899117']], ['A2959089861', []], ['A2776268936', ['I130769515']], ['A2332457619', ['I79510175']], ['A1990656394', ['I177725633']], ['A2141423093', []], ['A2973814101', ['I121820613']], ['A2157477235', ['I190397597']], ['A2503627946', ['I201448701']], ['A2042136083', ['I149899117']], ['A2190268344', ['I7882870']], ['A2973622029', ['I185261750']], ['A2974203684', ['I7882870']], ['A2135192745', ['I155173764']], ['A2236585722', ['I2746051580']], ['A2985328053', []], ['A2344571799', ['I114983960']], ['A3116516404', ['I277688954']], ['A2151255333', []], ['A2303437377', ['I7882870']], ['A2083397653', ['I7882870']], ['A2287281526', ['I7882870']], ['A2935802216', ['I155173764']], ['A2805556525', ['I122411786']], ['A2042955919', ['I149899117']], ['A2046589782', ['I7882870']], ['A2894018816', []], ['A2013597997', ['I1294671590']], ['A3214534791', []], ['A2067057196', ['I40347166']], ['A2149005230', ['I79510175']], ['A2744851237', ['I43579087']], ['A1920053454', ['I7882870']], ['A2138217739', ['I7882870']], ['A2120138609', ['I177877127']], ['A2966114616', ['I149899117']], ['A2771330512', ['I157725225']], ['A2761304767', ['I277688954']], ['A3214494604', ['I84475105']], ['A2053798579', ['I905677539']], ['A2580008531', ['I7882870']], ['A2915743262', []], ['A2781178463', ['I149899117']], ['A2974730225', []], ['A3212112723', ['I12315562']], ['A1898921624', ['I861853513']], ['A2334165722', ['I7882870']], ['A3179832972', ['I2746051580']], ['A2763511771', ['I122411786']], ['A3122260426', ['I11947397']], ['A2651934103', []], ['A2260744060', ['I2746051580']], ['A2112891699', ['I130701444']], ['A2075323785', ['I245364917']], ['A3014921206', ['I181647926']], ['A371636951', ['I905677539']], ['A3095792077', ['I121820613']], ['A470399586', ['I11947397']], ['A3188168968', ['I43439940']], ['A2468505319', ['I7882870']], ['A3149395330', []], ['A2137027242', ['I177877127']], ['A3214701367', ['I149899117']], ['A2966236035', ['I79510175']], ['A2998692115', ['I111979921']], ['A3188988166', []], ['A2812052584', ['I878022262']], ['A2974510088', ['I122411786']], ['A3131816118', ['I181233156']], ['A2324032376', ['I149899117']], ['A2763762051', ['I121820613']], ['A2745139675', ['I157725225']], ['A2951704019', ['I63966007']], ['A3014997762', []], ['A3106411652', ['I114112103']], ['A2946076134', []], ['A2144846099', ['I114983960']], ['A2375511350', ['I7882870']], ['A2076592991', ['I157725225']], ['A2332964699', ['I91136226']], ['A2743957451', ['I79510175']], ['A2893661853', ['I201448701']], ['A3214404249', ['I19880235']], ['A3193063450', []], ['A2797091175', ['I149899117']], ['A2105246234', []], ['A2973371795', []], ['A3144260476', []], ['A3205387863', ['I139264467']], ['A2992126523', ['I104338594']], ['A3169792634', ['I5681781']], ['A2834197742', []], ['A3037258224', ['I4921948']], ['A1978017669', ['I130701444']], ['A3214571791', ['I5681781']], ['A3213936075', []], ['A2703584364', ['I149899117']], ['A3137194014', []], ['A2974393256', ['I159176309']], ['A2923035099', ['I33213144']], ['A2108037976', ['I149899117']], ['A3198669105', ['I149899117']], ['A3172560174', []], ['A2973973483', ['I122411786']], ['A2431304288', ['I63966007']], ['A2833732568', ['I159176309']], ['A3021014529', ['I122411786']], ['A1685051018', ['I4654613']], ['A3094131044', ['I122411786']], ['A2202420371', ['I149899117']], ['A2838408147', ['I149899117']], ['A2198090973', ['I149899117']], ['A3183401008', ['I99542240']], ['A3198109926', ['I149899117']], ['A2234325469', ['I185261750']], ['A2809203402', []], ['A2581106287', ['I11947397']], ['A2131558627', ['I25846049']], ['A3175777026', []], ['A2090376259', ['I43579087']], ['A2834120332', ['I149899117']], ['A3126151952', ['I177725633']], ['A2955755552', []], ['A2660334908', ['I43579087']], ['A2791156957', ['I155173764']], ['A2306815261', ['I97018004']], ['A2561810233', ['I63966007']], ['A3212669462', ['I277688954']], ['A1973573521', ['I56590836']], ['A3147653104', []], ['A3113890189', ['I122411786']], ['A3207540947', []], ['A2021249184', ['I861853513']], ['A2549372245', ['I7882870']], ['A2809015571', ['I4921948']], ['A3212667814', ['I4575257']], ['A2095771935', ['I139264467']], ['A2103134365', ['I104338594']], ['A1169343919', ['I7882870']], ['A2142495155', ['I149899117']], ['A3121789463', ['I12097938']], ['A3193633694', ['I193223587']], ['A3129373350', ['I277688954']], ['A3131819562', ['I2799516425']], ['A2795157789', ['I56590836']], ['A2884995774', ['I177725633']], ['A3204770835', ['I63966007']], ['A2682071947', ['I1294504835']], ['A3198756242', ['I177877127']], ['A3010430677', ['I177725633']], ['A425630834', []], ['A2027578991', ['I79510175']], ['A2626309292', ['I70983195']], ['A3181190867', []], ['A2953771372', ['I1294671590']], ['A3190696561', []], ['A2031087335', []], ['A3101521172', ['I149899117']], ['A2547743495', ['I142934699']], ['A3189629458', ['I149899117']], ['A2745238556', ['I116067653']], ['A2826849062', ['I149899117']], ['A2984096928', ['I63966007']], ['A2131331663', ['I122411786']], ['A3213828037', ['I868834043']], ['A357350526', ['I149899117']], ['A2063177140', ['I63966007']], ['A2442311525', ['I121820613']], ['A2744105030', ['I177725633']], ['A2953110040', ['I70983195']], ['A2425517206', ['I70983195']], ['A3201457288', ['I130769515']], ['A1954864400', []], ['A3190282054', ['I1294671590']], ['A2764146561', ['I114983960']], ['A2530083674', ['I130238516']], ['A3170207227', ['I7882870']], ['A3099062606', ['I118347636']], ['A3213923457', ['I43579087']], ['A2209475002', []], ['A2072248821', ['I112859197']], ['A2788508077', ['I2799516425']], ['A2432693565', ['I78577930']], ['A3211662862', ['I78577930']], ['A2016056937', ['I157725225']], ['A3199007853', ['I97018004']], ['A3185243625', ['I122411786']], ['A3189577759', ['I190397597']], ['A201038113', ['I114983960']], ['A2102079205', ['I7882870']], ['A2163983367', ['I63966007']], ['A2576349919', ['I122411786']], ['A2986016436', ['I63966007']], ['A3188738125', ['I2799516425']], ['A3172795602', ['I122411786']], ['A3214530577', ['I7882870']], ['A3187908163', ['I861853513']], ['A2105490290', ['I130238516']], ['A1663768879', ['I63966007']], ['A3213275891', ['I78577930']], ['A2469800559', ['I63966007']], ['A2002271410', ['I1174212']], ['A3183609872', ['I72951846']], ['A2465526937', []], ['A1973105240', ['I118347636']], ['A3212080398', []], ['A2294070359', ['I63966007']], ['A2949277257', ['I145610796']], ['A2580450533', ['I122411786']], ['A2121893416', ['I122411786']], ['A3213209345', ['I118347636']], ['A2993123622', ['I118347636']], ['A2650871170', ['I12097938']], ['A1994734163', ['I130769515']], ['A3207708254', ['I149899117']], ['A2902246656', []], ['A2742568143', ['I16337185']], ['A2001351290', ['I165779595']], ['A3196574167', []], ['A2533606300', ['I43579087']], ['A2943856697', []], ['A3170098099', ['I114983960']], ['A3190502927', ['I122411786']], ['A2468204009', ['I7882870']], ['A2428639699', ['I130769515']], ['A3018553469', ['I2746051580']], ['A2304030444', ['I130238516']], ['A356943668', ['I861853513']], ['A2798531098', ['I79619799']], ['A2608264944', ['I1294671590']], ['A2910186728', ['I79619799']], ['A2070676417', ['I16285277']], ['A3088851510', []], ['A2596795233', ['I33213144']], ['A2107287278', ['I861853513']], ['A2766726314', ['I111979921']], ['A2634403777', ['I63966007']], ['A2780469136', ['I23732399']], ['A2148062765', ['I114983960']], ['A3129467551', []], ['A2842074435', ['I149899117']], ['A2963708514', ['I24676775']], ['A2118856850', ['I59781447']], ['A2136639717', ['I19880235']], ['A2899302788', ['I33213144']], ['A2123327683', ['I63966007']], ['A2608603388', []], ['A2099520814', []], ['A3211526149', ['I63966007']], ['A2576892988', ['I190397597']], ['A2481071036', ['I166088655']], ['A3014210212', ['I241749']], ['A3213760675', []], ['A2970106919', []], ['A2082008669', ['I2802326326']], ['A2952436053', ['I2799516425']], ['A2893928364', ['I79619799']], ['A2108725442', ['I33213144']], ['A2763198395', ['I79510175']], ['A3185592983', ['I149899117']], ['A2595464485', ['I43579087']], ['A3135606165', ['I2802326326']], ['A2266688588', ['I59781447']], ['A3067585087', []], ['A1982961642', ['I5681781']], ['A3212714246', ['I70983195']], ['A2834318602', ['I7882870']], ['A2936050007', ['I130701444']], ['A2957965052', ['I116067653']], ['A2005208586', ['I861853513']], ['A2599763636', ['I127439422']], ['A2439563356', ['I145872427']], ['A3206407848', []], ['A2598550388', ['I83816512']], ['A2863447672', ['I149899117']], ['A3212653464', ['I27837315']], ['A3003721860', ['I201448701']], ['A3124076646', ['I7882870']], ['A2972243581', ['I177725633']], ['A2171044953', ['I118347636']], ['A1969889460', ['I145872427']], ['A2781815556', ['I149899117']], ['A2272353480', ['I145872427']], ['A2023299128', ['I149899117']], ['A2591738817', ['I149899117']], ['A2963376855', []], ['A2097564241', []], ['A2566916996', ['I2802326326']], ['A2871160851', ['I70983195']], ['A2988703613', []], ['A2848307458', ['I43579087']], ['A2469545255', ['I63966007']], ['A2975435233', ['I138943879']], ['A3103100839', []], ['A3121867565', []], ['A2949236356', ['I149899117']], ['A2975609283', ['I905677539']], ['A3172241110', ['I149899117']], ['A3214157093', []], ['A2780453633', []], ['A3208341317', ['I130238516']], ['A2745157023', ['I33213144']], ['A2108716544', ['I155173764']], ['A2579844043', ['I5681781']], ['A3191279662', []], ['A2104041286', ['I12315562']], ['A3022666979', ['I130769515']], ['A2619620630', ['I1294504835']], ['A2743962002', ['I177877127']], ['A2976108310', ['I122996671']], ['A3212223573', []], ['A3197931354', ['I181233156']], ['A3066450176', []], ['A3165668364', []], ['A2840184033', ['I159176309']], ['A2953715031', ['I25846049']], ['A3149136633', ['I122411786']], ['A2743103778', ['I177725633']], ['A2521298114', ['I111979921']], ['A1967288566', ['I79510175']], ['A3213312486', []], ['A2794180081', []], ['A3095169362', []], ['A2766819460', ['I149899117']], ['A2777411521', ['I97018004']], ['A3039043906', []], ['A2253053690', ['I7882870']], ['A3208048655', []], ['A3189327450', ['I108290504']], ['A3191224944', []], ['A3177866358', []], ['A3212639572', ['I7882870']], ['A2974034227', ['I122411786']], ['A1975865521', ['I100532134']], ['A2084820644', ['I70983195']], ['A2142834634', []], ['A3214407301', ['I143104139']], ['A3125804629', []], ['A2954703215', ['I122411786']], ['A2574613773', ['I111979921']], ['A2155309926', ['I185261750']], ['A2833209380', ['I7882870']], ['A3207651378', ['I861853513']], ['A3164910520', ['I114983960']], ['A3211695890', ['I190397597']], ['A2151351881', ['I16337185']], ['A3206472845', []], ['A3189702922', ['I1294671590']], ['A2091317358', ['I16337185']], ['A2147448641', ['I7882870']], ['A395586697', ['I108290504']], ['A1654852912', []], ['A2970243390', ['I169173203']], ['A2790661800', ['I149899117']], ['A2140290691', ['I7882870']], ['A2974524012', ['I59781447']], ['A2022981214', ['I84475105']], ['A1915066629', ['I79510175']], ['A3198640294', ['I43579087']], ['A2811913941', ['I149899117']], ['A2575409521', ['I16337185']], ['A3103146582', ['I149899117']], ['A3206970968', ['I193223587']], ['A3198074476', ['I19880235']], ['A3100080145', ['I149899117']], ['A3188415666', []], ['A2469000994', []], ['A2976081679', ['I149899117']], ['A3157012711', ['I43579087']], ['A2920643673', ['I177877127']], ['A2510288784', ['I56590836']], ['A3197230867', ['I2802326326']], ['A2118448810', ['I122411786']], ['A1963716262', ['I181233156']], ['A2972396686', []], ['A2614875211', ['I118347636']], ['A3088945647', []], ['A3172586782', ['I106118109']], ['A3212947902', []], ['A3212689698', []], ['A3197632095', ['I2802326326']], ['A3196868043', ['I2802326326']], ['A362084126', ['I861853513']], ['A2918218855', ['I149899117']], ['A1884058422', ['I108290504']], ['A3173173182', ['I142934699']], ['A3207885158', ['I114983960']], ['A3183286780', []], ['A3181212731', ['I868834043']], ['A2089513116', ['I33213144']], ['A2077388583', ['I16285277']], ['A2223535313', ['I70983195']], ['A2461163460', ['I861853513']], ['A1782561727', ['I157725225']], ['A2743627872', ['I149899117']], ['A2941682954', ['I27837315']], ['A3205324612', ['I155173764']], ['A2805807800', ['I7882870']], ['A3160767430', ['I7882870']], ['A3183674337', ['I277688954']], ['A2971684032', []], ['A2878446963', ['I2799516425']], ['A2812330481', ['I122411786']], ['A3214401601', ['I181233156']], ['A2791886272', ['I2802326326']], ['A2100129471', ['I131729948']], ['A3214716399', []], ['A3088823900', []], ['A2946809466', ['I46305939']], ['A2744100075', ['I201448701']], ['A2469241872', ['I7882870']], ['A3206323586', ['I149899117']], ['A2962726707', []], ['A2147981533', []], ['A975051260', ['I1174212']], ['A2773400098', ['I122411786']], ['A2959485217', []], ['A2965214009', ['I43579087']], ['A2222685358', ['I183935753']], ['A2271697176', []], ['A2957078916', ['I122996671']], ['A3134223752', ['I149899117']], ['A3135876474', ['I127439422']], ['A3125232773', ['I56590836']], ['A2143863161', ['I111979921']], ['A3214693145', ['I122411786']], ['A2973649355', []], ['A2268325361', ['I111979921']], ['A2976615018', ['I70983195']], ['A3005587794', ['I1294671590']], ['A2917981810', ['I79510175']], ['A1994600236', ['I70983195']], ['A3187848715', ['I27837315']], ['A2528509270', []], ['A2942673923', ['I114112103']], ['A3198562296', ['I181233156']], ['A2091781350', ['I111979921']], ['A2552146066', ['I84475105']], ['A2170197708', ['I149899117']], ['A2263080307', ['I122411786']], ['A3213085247', ['I159176309']], ['A2949097735', ['I181233156']], ['A2895669897', ['I159176309']], ['A2780652161', ['I149899117']], ['A2777934897', ['I149899117']], ['A2743706024', ['I149899117']], ['A2170966916', ['I149899117']], ['A2226082458', ['I84475105']], ['A3021341707', ['I7882870']], ['A3002678630', ['I118347636']], ['A2099037435', ['I157725225']], ['A3088163438', []], ['A3126429971', ['I27674431']], ['A3115898744', []], ['A1908375455', ['I116067653']], ['A2972136177', []], ['A2311189936', ['I118347636']], ['A3214605660', []], ['A3122312680', ['I1294504835']], ['A2169968594', ['I111979921']], ['A2885759830', ['I149899117']], ['A2980611740', ['I97018004']], ['A3212511150', ['I66946132']], ['A2143397848', ['I43579087']], ['A2791879238', ['I63966007']], ['A3205204008', ['I130701444']], ['A3147198915', ['I130701444']], ['A2590111591', ['I43579087']], ['A3207028838', ['I99542240']], ['A3180273036', []], ['A2575681443', ['I80849659']], ['A2287315714', ['I122411786']], ['A3042780887', ['I1306266525']], ['A3013619970', ['I149899117']], ['A3205612424', ['I121820613']], ['A3206976328', []], ['A3165812064', ['I905677539']], ['A2059989788', ['I118347636']], ['A2761446687', []], ['A3159832477', ['I142934699']], ['A2973327313', ['I122411786']], ['A3196301643', []], ['A2742862949', ['I43579087']], ['A2038388809', ['I7882870']], ['A2924118733', []], ['A2029544047', ['I59781447']], ['A2500174149', ['I7882870']], ['A2157639568', []], ['A2989114630', ['I78577930']], ['A2976094966', ['I149899117']], ['A1541186076', ['I7882870']], ['A2976816135', ['I159176309']], ['A3198399133', ['I149899117']], ['A2903551516', ['I43579087']], ['A2591107197', ['I79619799']], ['A3083537935', ['I2802326326']], ['A2580845160', ['I7882870']], ['A400490453', ['I190397597']], ['A2065534013', ['I19880235']], ['A2975410111', ['I35046152']], ['A3176384363', []], ['A2851724123', ['I102298084']], ['A2789987291', ['I165779595']], ['A3213214962', []], ['A2461023037', ['I79510175']], ['A2042879894', []], ['A2762642778', ['I84475105']], ['A3191806085', ['I169173203']], ['A3211900768', ['I181233156']], ['A2040393559', ['I33213144']], ['A3212099725', ['I227486990']], ['A2995618979', ['I149899117']], ['A2113359325', ['I1294504835']], ['A2134195473', ['I122411786']], ['A2777362391', ['I149899117']], ['A3213845937', ['I79619799']], ['A2162845826', []], ['A3212450814', []], ['A3141516437', []], ['A1417201204', ['I122411786']], ['A2109240833', ['I56590836']], ['A3107409158', []], ['A2953014576', ['I79510175']], ['A443913643', []], ['A3212614577', ['I7882870']], ['A3083331595', ['I108290504']], ['A3007345822', ['I7882870']], ['A3139912005', ['I122411786']], ['A3133310043', ['I79619799']], ['A2478539643', []], ['A3088769739', []], ['A2906434032', ['I368840534']], ['A2768379193', ['I33213144']], ['A3095261960', ['I193223587']], ['A2271925699', ['I102064193']], ['A3212807160', []], ['A3128012180', ['I63966007']], ['A3213155827', ['I122411786']], ['A2976055498', ['I2802326326']], ['A3207720097', ['I43579087']], ['A3168011387', ['I60060512']], ['A2680674295', ['I11947397']], ['A3145841316', ['I122411786']], ['A3118211531', ['I79510175']], ['A1968817782', ['I1174212']], ['A3113623432', ['I114112103']], ['A420791329', ['I122411786']], ['A3101698037', ['I2802326326']], ['A2072883853', ['I122411786']], ['A3114985514', []], ['A3089223803', []], ['A2139670350', ['I865915315']], ['A2955877840', []], ['A3214210169', ['I70983195']], ['A2258435089', []], ['A2122667816', []], ['A3213982056', ['I7882870']], ['A2963217010', ['I138689650']], ['A3211531267', ['I122411786']], ['A2976862466', ['I122411786']], ['A2841294023', []], ['A2232612787', ['I79619799']], ['A2874945874', []], ['A2617753731', ['I79619799']], ['A2422158808', ['I5681781']], ['A3087785070', ['I201448701']], ['A3212186086', ['I122411786']], ['A3190535392', ['I2799516425']], ['A3187337232', ['I190397597']], ['A2076617041', ['I190397597']], ['A3135550114', ['I43579087']], ['A3206956357', ['I79619799']], ['A3214699647', ['I868834043']], ['A3198816670', ['I114983960']], ['A2794769002', ['I63966007']], ['A3213082654', ['I70983195']], ['A1990234647', ['I27483092']], ['A3088317034', []], ['A2596783131', ['I33213144']], ['A2974838029', ['I79619799']], ['A2806245680', ['I19880235']], ['A3211750527', ['I122411786']], ['A2869745026', ['I166972335']], ['A2163347761', ['I166972335']], ['A1838071985', ['I40347166']], ['A2743303635', []], ['A3198547128', ['I121820613']], ['A2974763369', ['I122411786']], ['A2997733971', ['I43579087']], ['A2763702278', []], ['A2097660264', ['I79619799']], ['A3215552898', ['I130769515']], ['A3160097155', ['I79619799']], ['A2980424894', ['I177725633']], ['A3180260855', ['I177877127']], ['A2195821694', ['I118347636']], ['A2951717091', []], ['A2806399133', ['I2799516425']], ['A2561660945', ['I132053463']], ['A3211483640', []], ['A3131437222', ['I114112103']], ['A3212444959', ['I149899117']], ['A3197332458', ['I122411786']], ['A2405613006', ['I63966007']], ['A2993470130', ['I177877127']], ['A2919554698', ['I157725225']], ['A3133253250', ['I149899117']], ['A3102057580', ['I149899117']], ['A2014299767', ['I149899117']], ['A2097961538', ['I155173764']], ['A2469540581', ['I33213144']], ['A3123187546', ['I56590836']], ['A2996670930', ['I7882870']], ['A2175238087', ['I122411786']], ['A3145747630', ['I155173764']], ['A2518612237', ['I60205797']], ['A2893230900', ['I149899117']], ['A2999412613', ['I149899117']], ['A2157982768', ['I149899117']], ['A3113580033', ['I122411786']], ['A2090992818', ['I149899117']], ['A1914572088', ['I7882870']], ['A2988580849', ['I149899117']], ['A3214192475', ['I155173764']], ['A2744444273', ['I177725633']], ['A2793555429', []], ['A2588464670', ['I7882870']], ['A2609393006', ['I149899117']], ['A2328863242', []], ['A2250629068', ['I63966007']], ['A3124858029', ['I122411786']], ['A3088353587', ['I66946132']], ['A2605415677', ['I118347636']], ['A2563109565', ['I63966007']], ['A1434570200', ['I2799516425']], ['A2881695771', []], ['A2973270439', ['I84475105']], ['A2745106814', []], ['A2869236588', []], ['A3211822923', ['I111979921']], ['A3125218237', ['I122411786']], ['A2776586902', ['I16285277']], ['A2761741246', ['I7882870']], ['A3094532032', ['I155173764']], ['A2635620662', ['I177877127']], ['A3028549774', ['I111979921']], ['A2419807468', ['I111979921']], ['A2852550122', ['I177877127']], ['A3186584125', ['I185261750']], ['A2622566261', ['I63966007']], ['A2078501612', ['I122411786']]], 'cited_by_count': 1823, 'concepts': [['C121332964', '0.94355994'], ['C190330329', '0.5978895'], ['C44870925', '0.59422064'], ['C130044071', '0.51650923'], ['C50341732', '0.49004877']], 'referenced_works': ['W275843794', 'W1134494461', 'W1559490316', 'W1640843629', 'W1783545336', 'W1812366509', 'W1965995299', 'W1968985209', 'W1972393270', 'W1972474135', 'W1974046066', 'W1982649138', 'W1983439542', 'W1984734622', 'W1989374967', 'W1992161867', 'W1993208475', 'W1995114597', 'W1996004845', 'W2003018730', 'W2003219437', 'W2006229977', 'W2014804681', 'W2015295900', 'W2022385453', 'W2028399298', 'W2033106348', 'W2036334887', 'W2037468948', 'W2037560748', 'W2037615822', 'W2037939815', 'W2043389609', 'W2043645849', 'W2044542193', 'W2048695882', 'W2050579405', 'W2051952627', 'W2052321554', 'W2052438420', 'W2054109161', 'W2057980978', 'W2060014682', 'W2061930234', 'W2065103443', 'W2070717869', 'W2074119073', 'W2075152653', 'W2082428228', 'W2088656436', 'W2090568279', 'W2095105526', 'W2096077283', 'W2100622876', 'W2103931654', 'W2104863352', 'W2106117339', 'W2106723786', 'W2110067346', 'W2115462982', 'W2115797426', 'W2126380692', 'W2131373099', 'W2134866927', 'W2139823471', 'W2144797524', 'W2145297661', 'W2147286576', 'W2147686177', 'W2152377128', 'W2153135464', 'W2154984826', 'W2155802808', 'W2158117921', 'W2166436681', 'W2166453963', 'W2169650135', 'W2198581161', 'W2207139482', 'W2230262155', 'W2237829933', 'W2252795400', 'W2254226187', 'W2254470438', 'W2255303387', 'W2256216577', 'W2256597892', 'W2260244587', 'W2262566890', 'W2266202083', 'W2266746071', 'W2268129260', 'W2268511610', 'W2268530831', 'W2268619227', 'W2273488465', 'W2273772273', 'W2273892807', 'W2277474668', 'W2277521171', 'W2277666539', 'W2277737850', 'W2289010674', 'W2289338257', 'W2298546845', 'W2308616882', 'W2330523759', 'W2338704010', 'W2340200134', 'W2342129039', 'W2347511317', 'W2412899893', 'W2415259147', 'W2437571239', 'W2442816495', 'W2461588289', 'W2462805925', 'W2479210508', 'W2482412227', 'W2506531441', 'W2513431819', 'W2513472193', 'W2515363224', 'W2518300041', 'W2519962468', 'W2523032431', 'W2545747680', 'W2546409652', 'W2547616757', 'W2550934118', 'W2555668341', 'W2558219853', 'W2558824482', 'W2562346410', 'W2570794662', 'W2604642821', 'W2606613431', 'W2953142974', 'W3049399914', 'W3098245943', 'W3098569287', 'W3098778082', 'W3099071754', 'W3099145134', 'W3099518669', 'W3099770000', 'W3099898204', 'W3101477146', 'W3102004940', 'W3102905749', 'W3105158216', 'W3105665039', 'W3106379026', 'W4234856688', 'W4291109148', 'W4292875581', 'W4300498648'], 'abstract': 'We describe the observation of GW170104, a gravitational-wave signal produced by the coalescence of a pair of stellar-mass black holes. The signal was measured on January 4, 2017 at 10:11:58.6 UTC by the twin advanced detectors of the Laser Interferometer Gravitational-Wave Observatory during their second observing run, with a network signal-to-noise ratio of 13 and a false alarm rate less than 1 in 70,000 years. The inferred component black hole masses are $31.2^{+8.4}_{-6.0}\\,M_\\odot$ and $19.4^{+5.3}_{-5.9}\\,M_\\odot$ (at the 90% credible level). The black hole spins are best constrained through measurement of the effective inspiral spin parameter, a mass-weighted combination of the spin components perpendicular to the orbital plane, $\\chi_\\mathrm{eff} = -0.12^{+0.21}_{-0.30}.$ This result implies that spin configurations with both component spins positively aligned with the orbital angular momentum are disfavored. The source luminosity distance is $880^{+450}_{-390}~\\mathrm{Mpc}$ corresponding to a redshift of $z = 0.18^{+0.08}_{-0.07}$. We constrain the magnitude of modifications to the gravitational-wave dispersion relation and perform null tests of general relativity. Assuming that gravitons are dispersed in vacuum like massive particles, we bound the graviton mass to $m_g \\le 7.7 \\times 10^{-23}~\\mathrm{eV}/c^2$. In all cases, we find that GW170104 is consistent with general relativity.', 'counts_by_year': [[2022, 91], [2021, 209], [2020, 271], [2019, 426], [2018, 613], [2017, 211], [2016, 1], [2013, 1]]}, {'id': 'W2803427589', 'doi': 'https://doi.org/10.1038/s41570-018-0010-1', 'title': 'Heterogeneous single-atom catalysis', 'type': 'journal-article', 'publication_date': '2018-06-01', 'host_venue': 'V4210174246', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2481982138', ['I180585399']], ['A2248141713', ['I99065089']], ['A2199372937', ['I180585399', 'I4210165038']]], 'cited_by_count': 1819, 'concepts': [['C161790260', '0.5259186'], ['C58312451', '0.4619999'], ['C185592680', '0.38509214'], ['C41008148', '0.25770962'], ['C173608175', '0.09650907']], 'referenced_works': ['W1162924051', 'W1168565912', 'W1209331024', 'W1609025494', 'W1853374276', 'W1870760280', 'W1965324127', 'W1965457783', 'W1969252570', 'W1976819694', 'W1979371521', 'W1979719326', 'W1983389814', 'W1983711689', 'W1984372824', 'W1985059073', 'W1990469294', 'W1992277893', 'W1993998340', 'W2004548388', 'W2005060157', 'W2005355202', 'W2005887771', 'W2006792934', 'W2010016085', 'W2012647111', 'W2014824706', 'W2028977658', 'W2033270806', 'W2042997794', 'W2061367556', 'W2066443367', 'W2070642118', 'W2073753357', 'W2076376028', 'W2079439077', 'W2080475618', 'W2083566388', 'W2087570091', 'W2092903091', 'W2093299928', 'W2094430532', 'W2097848778', 'W2099703449', 'W2109694766', 'W2110535401', 'W2111631030', 'W2116657207', 'W2117508593', 'W2125863915', 'W2134716155', 'W2139628787', 'W2154949549', 'W2157871632', 'W2174170240', 'W2197330948', 'W2197652168', 'W2231840775', 'W2232828257', 'W2234065533', 'W2242811882', 'W2267714992', 'W2269800614', 'W2274056276', 'W2278253851', 'W2281185144', 'W2289744516', 'W2297872481', 'W2304696404', 'W2312602819', 'W2314274336', 'W2322451794', 'W2323239271', 'W2325095228', 'W2326772021', 'W2330897836', 'W2332000202', 'W2333004875', 'W2333714774', 'W2337021912', 'W2339650397', 'W2340387000', 'W2340423559', 'W2342049399', 'W2343203850', 'W2343654742', 'W2353573779', 'W2356997838', 'W2409322059', 'W2409420614', 'W2415897117', 'W2474398724', 'W2495999341', 'W2502288548', 'W2510282394', 'W2512721314', 'W2517538953', 'W2521736035', 'W2522872739', 'W2523476675', 'W2523873427', 'W2531551506', 'W2554715677', 'W2556309208', 'W2556488538', 'W2557321703', 'W2557861838', 'W2558739653', 'W2562690717', 'W2567452826', 'W2579601412', 'W2583267334', 'W2592253840', 'W2601189489', 'W2602237473', 'W2604746963', 'W2606012476', 'W2606452069', 'W2607231338', 'W2613910893', 'W2616710946', 'W2616799570', 'W2617194355', 'W2624508105', 'W2735002352', 'W2736999751', 'W2738062132', 'W2740657411', 'W2754345762', 'W2754741824', 'W2755172668', 'W2755830156', 'W2756103432', 'W2756146708', 'W2762985090', 'W2765469889', 'W2768069631', 'W2769795026', 'W2773190775', 'W2773935583', 'W2783694712', 'W2790507915', 'W2792126723', 'W2793353452', 'W2794821307', 'W2799648054', 'W2963248393'], 'abstract': 'Single-atom catalysis has arguably become the most active new frontier in heterogeneous catalysis. Aided by recent advances in practical synthetic methodologies, characterization techniques and computational modelling, we now have a large number of single-atom catalysts (SACs) that exhibit distinctive performances for a wide variety of chemical reactions. This Perspective summarizes recent experimental and computational efforts aimed at understanding the bonding in SACs and how this relates to catalytic performance. The examples described here illustrate the utility of SACs in a broad scope of industrially important reactions and highlight the advantages these catalysts have over those presently used. SACs have well-defined active centres, such that unique opportunities exist for the rational design of new catalysts with high activities, selectivities and stabilities. Indeed, given a certain practical application, we can often design a suitable SAC; thus, the field has developed very rapidly and afforded promising catalyst leads. Moreover, the control we have over certain SAC structures paves the way for designing base metal catalysts with the activities of noble metal catalysts. It appears that we are entering a new era of heterogeneous catalysis in which we have control over well-dispersed single-atom active sites whose properties we can readily tune. Single-atom catalysts are heterogeneous materials featuring active metals sites atomically dispersed on a surface. This Review describes methods by which we prepare and characterize these materials, as well as how we can tune their catalytic performance in a variety of important reactions.', 'counts_by_year': [[2022, 510], [2021, 523], [2020, 479], [2019, 258], [2018, 40]]}, {'id': 'W2529052661', 'doi': 'https://doi.org/10.1016/j.ymeth.2016.09.016', 'title': 'TrackMate: An open and extensible platform for single-particle tracking', 'type': 'journal-article', 'publication_date': '2017-02-15', 'host_venue': 'V73539393', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2185995468', ['I157536573']], ['A2546420995', ['I157536573']], ['A2113479336', ['I135310074']], ['A2773831018', ['I135310074']], ['A2225139563', ['I135310074']], ['A2012474867', ['I157536573']], ['A2050223303', ['I135310074']], ['A89165356', ['I157536573']], ['A2126056381', ['I29680605']]], 'cited_by_count': 1818, 'concepts': [['C41008148', '0.781174'], ['C36464697', '0.7716374'], ['C101468663', '0.7528963'], ['C4924752', '0.6108473'], ['C2779343474', '0.5689786']], 'referenced_works': ['W48566380', 'W65966364', 'W1524992328', 'W1533361117', 'W1758083191', 'W1911220294', 'W1969697456', 'W1973421894', 'W1979037300', 'W1982170719', 'W1982185125', 'W1985589554', 'W1986006829', 'W1986141440', 'W1986490461', 'W1987765355', 'W1992668542', 'W2000396646', 'W2004435513', 'W2006302594', 'W2010519082', 'W2019602502', 'W2031502025', 'W2035993805', 'W2037947871', 'W2042629182', 'W2048420864', 'W2060050992', 'W2079262572', 'W2086503139', 'W2100427942', 'W2103706175', 'W2105916176', 'W2105934661', 'W2110213173', 'W2112385640', 'W2117601757', 'W2118313094', 'W2129140439', 'W2132355119', 'W2138793801', 'W2138934663', 'W2143623375', 'W2144678847', 'W2150499718', 'W2155653216', 'W2156755609', 'W2165312739', 'W2167279371', 'W2169373177', 'W2170358380', 'W2193222177', 'W2194208451', 'W2196096209', 'W2255466733', 'W2257375213', 'W2265800186', 'W2274916336', 'W2468870810'], 'abstract': 'We present TrackMate, an open source Fiji plugin for the automated, semi-automated, and manual tracking of single-particles. It offers a versatile and modular solution that works out of the box for end users, through a simple and intuitive user interface. It is also easily scriptable and adaptable, operating equally well on 1D over time, 2D over time, 3D over time, or other single and multi-channel image variants. TrackMate provides several visualization and analysis tools that aid in assessing the relevance of results. The utility of TrackMate is further enhanced through its ability to be readily customized to meet specific tracking problems. TrackMate is an extensible platform where developers can easily write their own detection, particle linking, visualization or analysis algorithms within the TrackMate environment. This evolving framework provides researchers with the opportunity to quickly develop and optimize new algorithms based on existing TrackMate modules without the need of having to write de novo user interfaces, including visualization, analysis and exporting tools. The current capabilities of TrackMate are presented in the context of three different biological problems. First, we perform Caenorhabditis-elegans lineage analysis to assess how light-induced damage during imaging impairs its early development. Our TrackMate-based lineage analysis indicates the lack of a cell-specific light-sensitive mechanism. Second, we investigate the recruitment of NEMO (NF-κB essential modulator) clusters in fibroblasts after stimulation by the cytokine IL-1 and show that photodamage can generate artifacts in the shape of TrackMate characterized movements that confuse motility analysis. Finally, we validate the use of TrackMate for quantitative lifetime analysis of clathrin-mediated endocytosis in plant cells.', 'counts_by_year': [[2022, 406], [2021, 466], [2020, 436], [2019, 273], [2018, 166], [2017, 64], [2016, 5]]}, {'id': 'W2963026768', 'doi': 'https://doi.org/10.18653/v1/p18-1031', 'title': 'Universal Language Model Fine-tuning for Text Classification', 'type': 'proceedings-article', 'publication_date': '2018-01-18', 'host_venue': 'V4306420508', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2123203741', ['I137902535']], ['A2140581490', ['I188760350']]], 'cited_by_count': 1813, 'concepts': [['C2781235140', '0.87452793'], ['C41008148', '0.85847396'], ['C2780451532', '0.70831263'], ['C137293760', '0.68166196'], ['C154945302', '0.6733234']], 'referenced_works': ['W189596042', 'W1614862348', 'W1836465849', 'W1903029394', 'W1948751323', 'W2062118960', 'W2072715695', 'W2085766370', 'W2113459411', 'W2138857742', 'W2144415203', 'W2153579005', 'W2155541015', 'W2159291411', 'W2163302275', 'W2165698076', 'W2168681026', 'W2194775991', 'W2510153535', 'W2523246573', 'W2549835527', 'W2587528408', 'W2595304170', 'W2606347107', 'W2740721704', 'W2743945814', 'W2767434619', 'W2799269579', 'W2962676330', 'W2962739339', 'W2963012544', 'W2963216553', 'W2963263347', 'W2963446712', 'W2963494889', 'W2963563735', 'W2963571341', 'W2963706742', 'W2963751529', 'W2963756346', 'W2963872035', 'W2963899155', 'W2963908579', 'W2963918774', 'W2964054038', 'W2989499211', 'W3093419064', 'W3104240813', 'W3106003309'], 'abstract': 'Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.', 'counts_by_year': [[2022, 191], [2021, 532], [2020, 594], [2019, 459], [2018, 35], [2017, 1], [2012, 1]]}, {'id': 'W2513727910', 'doi': 'https://doi.org/10.1111/2041-210x.12628', 'title': '<scp>ggtree</scp> : an <scp>r</scp> package for visualization and annotation of phylogenetic trees with their covariates and other associated data', 'type': 'journal-article', 'publication_date': '2017-01-01', 'host_venue': 'V1131227', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2125523940', ['I889458895']], ['A2509205188', ['I889458895']], ['A3156857686', ['I889458895']], ['A2109064762', ['I889458895']], ['A2123201679', ['I889458895']]], 'cited_by_count': 1811, 'concepts': [['C2984074130', '0.7925634'], ['C119043178', '0.7123508'], ['C2776321320', '0.69791037'], ['C193252679', '0.6912863'], ['C36464697', '0.5843733']], 'referenced_works': ['W1521650410', 'W1605984840', 'W1972157500', 'W2000684107', 'W2011163009', 'W2020350008', 'W2026062398', 'W2056279562', 'W2068187483', 'W2081379565', 'W2104244040', 'W2110335151', 'W2125230147', 'W2138178197', 'W2141052558', 'W2143391112', 'W2149841552', 'W2151409320', 'W2151602856', 'W2158859599', 'W2159032237', 'W2160697532', 'W2161036972', 'W2162371815', 'W4230096730'], 'abstract': 'Summary\r\nWe present an r package, ggtree, which provides programmable visualization and annotation of phylogenetic trees.\r\nggtree can read more tree file formats than other softwares, including newick, nexus, NHX, phylip and jplace formats, and support visualization of phylo, multiphylo, phylo4, phylo4d, obkdata and phyloseq tree objects defined in other r packages. It can also extract the tree/branch/node-specific and other data from the analysis outputs of beast, epa, hyphy, paml, phylodog, pplacer, r8s, raxml and revbayes software, and allows using these data to annotate the tree.\r\nThe package allows colouring and annotation of a tree by numerical/categorical node attributes, manipulating a tree by rotating, collapsing and zooming out clades, highlighting user selected clades or operational taxonomic units and exploration of a large tree by zooming into a selected portion.\r\nA two-dimensional tree can be drawn by scaling the tree width based on an attribute of the nodes. A tree can be annotated with an associated numerical matrix (as a heat map), multiple sequence alignment, subplots or silhouette images.\r\nThe package ggtree is released under the artistic-2.0 license. The source code and documents are freely available through bioconductor (http://www.bioconductor.org/packages/ggtree).', 'counts_by_year': [[2022, 430], [2021, 547], [2020, 386], [2019, 241], [2018, 137], [2017, 63], [2016, 4]]}, {'id': 'W2963846996', 'doi': 'https://doi.org/10.18653/v1/n18-1101', 'title': 'A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference', 'type': 'proceedings-article', 'publication_date': '2018-06-01', 'host_venue': 'V4306420633', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2609376944', ['I57206974']], ['A2609518292', ['I57206974']], ['A1967404238', ['I57206974']]], 'cited_by_count': 1807, 'concepts': [['C41008148', '0.86483437'], ['C204321447', '0.79670715'], ['C154945302', '0.7093753'], ['C2776214188', '0.6796093'], ['C2777530160', '0.65291274']], 'referenced_works': ['W327273178', 'W1632114991', 'W1711163617', 'W1840435438', 'W1849277567', 'W1897507002', 'W2045254372', 'W2064675550', 'W2087451659', 'W2095705004', 'W2097606805', 'W2120708938', 'W2130359236', 'W2131953535', 'W2155541015', 'W2158108973', 'W2163605009', 'W2185175083', 'W2219598741', 'W2250539671', 'W2308720496', 'W2525127255', 'W2599247984', 'W2608787653', 'W2963542836', 'W2963917673', 'W2963918774', 'W2964121744', 'W3151206145'], 'abstract': 'This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.', 'counts_by_year': [[2022, 108], [2021, 742], [2020, 551], [2019, 286], [2018, 103], [2017, 16]]}, {'id': 'W2963626623', 'doi': 'https://doi.org/10.18653/v1/e17-2068', 'title': 'Bag of Tricks for Efficient Text Classification', 'type': 'proceedings-article', 'publication_date': '2017-04-01', 'host_venue': 'V4306418011', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2512114774', ['I2252078561']], ['A2114720862', ['I78577930']], ['A1882694979', ['I2252078561']], ['A292626543', ['I2252078561']]], 'cited_by_count': 1805, 'concepts': [['C41008148', '0.7577012'], ['C95623464', '0.5953609'], ['C154945302', '0.57420635'], ['C12725497', '0.46854722'], ['C204321447', '0.3801725']], 'referenced_works': ['W21006490', 'W1550206324', 'W1615991656', 'W1775434803', 'W1832693441', 'W1965154800', 'W2070996757', 'W2097726431', 'W2100714283', 'W2113552117', 'W2118585731', 'W2147152072', 'W2149671658', 'W2149684865', 'W2154359981', 'W2157462866', 'W2250384498', 'W2250966211', 'W2252335727', 'W2413904250', 'W2950577311', 'W2963012544'], 'abstract': 'This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.', 'counts_by_year': [[2022, 175], [2021, 431], [2020, 516], [2019, 429], [2018, 196], [2017, 52], [2016, 4]]}, {'id': 'W3015988827', 'doi': 'https://doi.org/10.1126/science.abb5793', 'title': 'Projecting the transmission dynamics of SARS-CoV-2 through the postpandemic period', 'type': 'journal-article', 'publication_date': '2020-04-14', 'host_venue': 'V3880285', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2226646271', ['I136199984']], ['A2785185276', ['I136199984']], ['A2158741097', ['I136199984']], ['A1969298456', ['I136199984']], ['A1247097111', ['I136199984']]], 'cited_by_count': 1805, 'concepts': [['C761482', '0.70017356'], ['C3007834351', '0.63055336'], ['C3008058167', '0.586922'], ['C116675565', '0.5711888'], ['C2777648638', '0.5474818']], 'referenced_works': ['W1933155703', 'W1979065938', 'W2025350760', 'W2037053732', 'W2093258104', 'W2100989087', 'W2101660588', 'W2102187991', 'W2104474203', 'W2117002055', 'W2136829296', 'W2147166346', 'W2153749443', 'W2156349196', 'W2164680958', 'W2166837306', 'W2169258494', 'W2306794997', 'W2334023814', 'W2739568112', 'W2792545913', 'W2908688678', 'W3003668884', 'W3006065484', 'W3006390878', 'W3007929363', 'W3008028633', 'W3009155494', 'W3009265184', 'W3009459491', 'W3010233963', 'W3010383561', 'W3010647567', 'W3010977491', 'W3012225289', 'W3012284084', 'W3013215798', 'W3013542256', 'W3013594674', 'W3104691176', 'W3105018088', 'W4205225613'], 'abstract': 'What happens next? Four months into the severe acute respiratory syndrome–coronavirus 2 (SARS-CoV-2) outbreak, we still do not know enough about postrecovery immune protection and environmental and seasonal influences on transmission to predict transmission dynamics accurately. However, we do know that humans are seasonally afflicted by other, less severe coronaviruses. Kissler et al. used existing data to build a deterministic model of multiyear interactions between existing coronaviruses, with a focus on the United States, and used this to project the potential epidemic dynamics and pressures on critical care capacity over the next 5 years. The long-term dynamics of SARS-CoV-2 strongly depends on immune responses and immune cross-reactions between the coronaviruses, as well as the timing of introduction of the new virus into a population. One scenario is that a resurgence in SARS-CoV-2 could occur as far into the future as 2025. Science , this issue p. 860', 'counts_by_year': [[2022, 244], [2021, 733], [2020, 827], [2019, 1]]}, {'id': 'W2942829220', 'doi': 'https://doi.org/10.1093/nar/gkz310', 'title': 'antiSMASH 5.0: updates to the secondary metabolite genome mining pipeline', 'type': 'journal-article', 'publication_date': '2019-07-02', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2110019574', ['I4210149061']], ['A2907554097', ['I4210149061']], ['A2939302475', ['I8087733']], ['A2908477362', ['I4210149061']], ['A1959219177', ['I8087733']], ['A2420484326', ['I157485424', 'I4210149061']], ['A2630647180', ['I913481162']], ['A2211543134', ['I4210149061']]], 'cited_by_count': 1804, 'concepts': [['C2780416260', '0.77892137'], ['C86803240', '0.6562963'], ['C141231307', '0.64618355'], ['C70721500', '0.56812286'], ['C2777763344', '0.55919087']], 'referenced_works': ['W1908709110', 'W1964640602', 'W1975439502', 'W1985653548', 'W1990935073', 'W2009257824', 'W2010569979', 'W2056996708', 'W2094402554', 'W2135639274', 'W2142190505', 'W2145974700', 'W2154069557', 'W2156912993', 'W2168882468', 'W2171908015', 'W2187341651', 'W2201339123', 'W2224056471', 'W2256742438', 'W2283794411', 'W2302372433', 'W2410975458', 'W2535017472', 'W2603886489', 'W2604808360', 'W2608736608', 'W2609551972', 'W2610680882', 'W2614151742', 'W2761688050', 'W2765371419', 'W2791113281', 'W2890778905', 'W2895788262', 'W2899779000', 'W2899986496', 'W2902404827', 'W2902906807', 'W2906874775', 'W2951775080', 'W4243780888'], 'abstract': 'Abstract Secondary metabolites produced by bacteria and fungi are an important source of antimicrobials and other bioactive compounds. In recent years, genome mining has seen broad applications in identifying and characterizing new compounds as well as in metabolic engineering. Since 2011, the ‘antibiotics and secondary metabolite analysis shell—antiSMASH’ (https://antismash.secondarymetabolites.org) has assisted researchers in this, both as a web server and a standalone tool. It has established itself as the most widely used tool for identifying and analysing biosynthetic gene clusters (BGCs) in bacterial and fungal genome sequences. Here, we present an entirely redesigned and extended version 5 of antiSMASH. antiSMASH 5 adds detection rules for clusters encoding the biosynthesis of acyl-amino acids, β-lactones, fungal RiPPs, RaS-RiPPs, polybrominated diphenyl ethers, C-nucleosides, PPY-like ketones and lipolanthines. For type II polyketide synthase-encoding gene clusters, antiSMASH 5 now offers more detailed predictions. The HTML output visualization has been redesigned to improve the navigation and visual representation of annotations. We have again improved the runtime of analysis steps, making it possible to deliver comprehensive annotations for bacterial genomes within a few minutes. A new output file in the standard JavaScript object notation (JSON) format is aimed at downstream tools that process antiSMASH results programmatically.', 'counts_by_year': [[2022, 391], [2021, 803], [2020, 518], [2019, 91]]}, {'id': 'W2944545583', 'doi': 'https://doi.org/10.1093/nar/gkz369', 'title': 'g:Profiler: a web server for functional enrichment analysis and conversions of gene lists (2019 update)', 'type': 'journal-article', 'publication_date': '2019-07-02', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2944808539', ['I56085075']], ['A2322945262', ['I56085075']], ['A1993044060', ['I56085075']], ['A2437153855', ['I56085075']], ['A2143407497', ['I56085075']], ['A2156813903', ['I56085075']], ['A2152127321', ['I56085075']]], 'cited_by_count': 1804, 'concepts': [['C141674004', '0.82227737'], ['C154504017', '0.8144258'], ['C71901391', '0.7633992'], ['C86803240', '0.67189854'], ['C77088390', '0.47062707']], 'referenced_works': ['W1617495083', 'W1935348821', 'W1969353942', 'W1989277387', 'W2010457001', 'W2012034410', 'W2027847170', 'W2061912939', 'W2063280109', 'W2100980426', 'W2103017472', 'W2124954099', 'W2130410032', 'W2135030836', 'W2148043260', 'W2156564520', 'W2159675211', 'W2163265140', 'W2191641478', 'W2345356016', 'W2517051411', 'W2537679995', 'W2559577985', 'W2611604995', 'W2611696724', 'W2767546566', 'W2767914218', 'W2793921999', 'W2897805491', 'W2901226905', 'W2901332105', 'W2910111050', 'W2911561283', 'W2917207851', 'W2918086501', 'W2949366051', 'W3136918052', 'W4233698560', 'W4243979784'], 'abstract': 'Abstract Biological data analysis often deals with lists of genes arising from various studies. The g:Profiler toolset is widely used for finding biological categories enriched in gene lists, conversions between gene identifiers and mappings to their orthologs. The mission of g:Profiler is to provide a reliable service based on up-to-date high quality data in a convenient manner across many evidence types, identifier spaces and organisms. g:Profiler relies on Ensembl as a primary data source and follows their quarterly release cycle while updating the other data sources simultaneously. The current update provides a better user experience due to a modern responsive web interface, standardised API and libraries. The results are delivered through an interactive and configurable web design. Results can be downloaded as publication ready visualisations or delimited text files. In the current update we have extended the support to 467 species and strains, including vertebrates, plants, fungi, insects and parasites. By supporting user uploaded custom GMT files, g:Profiler is now capable of analysing data from any organism. All past releases are maintained for reproducibility and transparency. The 2019 update introduces an extensive technical rewrite making the services faster and more flexible. g:Profiler is freely available at https://biit.cs.ut.ee/gprofiler.', 'counts_by_year': [[2022, 656], [2021, 686], [2020, 397], [2019, 60], [2018, 1]]}, {'id': 'W2604319603', 'doi': 'https://doi.org/10.1109/jproc.2017.2761740', 'title': 'Efficient Processing of Deep Neural Networks: A Tutorial and Survey', 'type': 'journal-article', 'publication_date': '2017-11-20', 'host_venue': 'V68686220', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2093716610', ['I63966007']], ['A2212861991', ['I63966007']], ['A2630402700', ['I63966007']], ['A2103115969', ['I63966007']]], 'cited_by_count': 1792, 'concepts': [['C50644808', '0.58801633'], ['C41008148', '0.5729916'], ['C154945302', '0.48141572'], ['C2522767166', '0.3331493']], 'referenced_works': ['W1019830208', 'W1535810436', 'W1542981317', 'W1574719918', 'W1677182931', 'W1903029394', 'W1923344279', 'W1972907356', 'W1975253511', 'W1981220134', 'W1984541135', 'W1992781178', 'W2004455575', 'W2009832130', 'W2026369565', 'W2044535169', 'W2048266589', 'W2048508570', 'W2064675550', 'W2067523571', 'W2069743665', 'W2070167224', 'W2088338354', 'W2094756095', 'W2096645269', 'W2097117768', 'W2099912917', 'W2102605133', 'W2107726111', 'W2112739286', 'W2112796928', 'W2117539524', 'W2117696986', 'W2119112357', 'W2138913040', 'W2141504882', 'W2145607950', 'W2152839228', 'W2155893237', 'W2160815625', 'W2162651880', 'W2172654076', 'W2183341477', 'W2194775991', 'W2198606573', 'W2233116163', 'W2257979135', 'W2276892413', 'W2285660444', 'W2289252105', 'W2314470091', 'W2400507664', 'W2431931973', 'W2433743436', 'W2442974303', 'W2508602506', 'W2509192851', 'W2511743527', 'W2513554817', 'W2516141709', 'W2518281301', 'W2518511512', 'W2524241275', 'W2531409750', 'W2554302513', 'W2581082771', 'W2586654419', 'W2593841437', 'W2605347906', 'W2625457103', 'W2626712922', 'W2694935213', 'W2745228312', 'W2919115771', 'W2963403664', 'W2964195221', 'W3024621361', 'W4244330903', 'W4247198796', 'W4249932213'], 'abstract': 'Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.', 'counts_by_year': [[2022, 268], [2021, 491], [2020, 493], [2019, 354], [2018, 166], [2017, 14], [2012, 1]]}, {'id': 'W3008294222', 'doi': 'https://doi.org/10.1016/s2214-109x(20)30074-7', 'title': 'Feasibility of controlling COVID-19 outbreaks by isolation of cases and contacts', 'type': 'journal-article', 'publication_date': '2020-04-01', 'host_venue': 'V4210176958', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2765635069', ['I4210089966']], ['A2835261579', ['I4210089966']], ['A3006242220', ['I4210089966']], ['A3006236200', ['I4210089966']], ['A2644929985', ['I4210089966']], ['A3005524366', ['I4210089966']], ['A2889493969', ['I4210089966']], ['A2094073093', ['I4210089966']], ['A2239392000', ['I4210089966']], ['A3009512101', []], ['A2231526214', ['I4210089966']], ['A3004279427', ['I4210089966']], ['A2640060883', []], ['A2776297621', []], ['A1966109781', []], ['A2777073030', []], ['A2231526214', []], ['A3005493534', []], ['A3009321980', []], ['A2889721592', []], ['A3001207621', ['I4210089966']], ['A1784943301', ['I4210089966']]], 'cited_by_count': 1791, 'concepts': [['C113162765', '0.94341636'], ['C116675565', '0.8585921'], ['C2775941552', '0.7762205'], ['C761482', '0.70003235'], ['C113280763', '0.64504147']], 'referenced_works': ['W1990049863', 'W2069251911', 'W2078691743', 'W2140763962', 'W2154594890', 'W2156744480', 'W2405927069', 'W2472721343', 'W2480265190', 'W2601351842', 'W2610490986', 'W2889977852', 'W2901291256', 'W2927333233', 'W3001118548', 'W3003668884', 'W3004026249', 'W3004313240', 'W3004397688', 'W3005995896', 'W3008590414'], 'abstract': '<h2>Summary</h2><h3>Background</h3> Isolation of cases and contact tracing is used to control outbreaks of infectious diseases, and has been used for coronavirus disease 2019 (COVID-19). Whether this strategy will achieve control depends on characteristics of both the pathogen and the response. Here we use a mathematical model to assess if isolation and contact tracing are able to control onwards transmission from imported cases of COVID-19. <h3>Methods</h3> We developed a stochastic transmission model, parameterised to the COVID-19 outbreak. We used the model to quantify the potential effectiveness of contact tracing and isolation of cases at controlling a severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)-like pathogen. We considered scenarios that varied in the number of initial cases, the basic reproduction number (<i>R</i><sub>0</sub>), the delay from symptom onset to isolation, the probability that contacts were traced, the proportion of transmission that occurred before symptom onset, and the proportion of subclinical infections. We assumed isolation prevented all further transmission in the model. Outbreaks were deemed controlled if transmission ended within 12 weeks or before 5000 cases in total. We measured the success of controlling outbreaks using isolation and contact tracing, and quantified the weekly maximum number of cases traced to measure feasibility of public health effort. <h3>Findings</h3> Simulated outbreaks starting with five initial cases, an <i>R</i><sub>0</sub> of 1·5, and 0% transmission before symptom onset could be controlled even with low contact tracing probability; however, the probability of controlling an outbreak decreased with the number of initial cases, when <i>R</i><sub>0</sub> was 2·5 or 3·5 and with more transmission before symptom onset. Across different initial numbers of cases, the majority of scenarios with an <i>R</i><sub>0</sub> of 1·5 were controllable with less than 50% of contacts successfully traced. To control the majority of outbreaks, for <i>R</i><sub>0</sub> of 2·5 more than 70% of contacts had to be traced, and for an <i>R</i><sub>0</sub> of 3·5 more than 90% of contacts had to be traced. The delay between symptom onset and isolation had the largest role in determining whether an outbreak was controllable when <i>R</i><sub>0</sub> was 1·5. For <i>R</i><sub>0</sub> values of 2·5 or 3·5, if there were 40 initial cases, contact tracing and isolation were only potentially feasible when less than 1% of transmission occurred before symptom onset. <h3>Interpretation</h3> In most scenarios, highly effective contact tracing and case isolation is enough to control a new outbreak of COVID-19 within 3 months. The probability of control decreases with long delays from symptom onset to isolation, fewer cases ascertained by contact tracing, and increasing transmission before symptoms. This model can be modified to reflect updated transmission characteristics and more specific definitions of outbreak control to assess the potential success of local response efforts. <h3>Funding</h3> Wellcome Trust, Global Challenges Research Fund, and Health Data Research UK.', 'counts_by_year': [[2022, 211], [2021, 703], [2020, 873], [2019, 1]]}, {'id': 'W2285847257', 'doi': 'https://doi.org/10.1002/adfm.201504755', 'title': 'Stretchable, Skin-Mountable, and Wearable Strain Sensors and Their Potential Applications: A Review', 'type': 'journal-article', 'publication_date': '2016-03-01', 'host_venue': 'V135204980', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2512461119', ['I4210135521']], ['A2193800920', ['I4210100816']], ['A2158977848', ['I157485424']], ['A2166047619', ['I4210135521']]], 'cited_by_count': 1789, 'concepts': [['C192562407', '0.89903754'], ['C150594956', '0.7822807'], ['C2778022156', '0.58378994'], ['C54290928', '0.54688346'], ['C171250308', '0.4763351']], 'referenced_works': ['W1139244977', 'W1159358749', 'W1482619460', 'W1792065635', 'W1826399803', 'W1843167350', 'W1964191741', 'W1966664568', 'W1969031129', 'W1970509721', 'W1973618974', 'W1974865362', 'W1976780894', 'W1977366816', 'W1977995219', 'W1979369614', 'W1982085124', 'W1987592799', 'W1989172022', 'W1989577367', 'W1991444627', 'W1991575272', 'W1992580017', 'W1992684665', 'W1993110089', 'W1995042675', 'W1995296652', 'W1997355510', 'W1998544072', 'W1999292148', 'W1999653735', 'W1999959880', 'W2000871568', 'W2001113652', 'W2001650137', 'W2002205722', 'W2004072722', 'W2004636727', 'W2004835256', 'W2005592897', 'W2006346031', 'W2008434880', 'W2012610087', 'W2013186224', 'W2016461817', 'W2016464593', 'W2017377064', 'W2018260525', 'W2021206182', 'W2022758665', 'W2024154422', 'W2024209309', 'W2025411414', 'W2025487006', 'W2026285757', 'W2027460388', 'W2028364272', 'W2029953460', 'W2030146254', 'W2032300755', 'W2035620276', 'W2037488682', 'W2037858115', 'W2039191438', 'W2041931450', 'W2041978394', 'W2047240866', 'W2048735459', 'W2057878036', 'W2059468085', 'W2059544274', 'W2059772445', 'W2060962654', 'W2061325945', 'W2064546290', 'W2065916006', 'W2070464492', 'W2074575205', 'W2074766298', 'W2075253490', 'W2078274548', 'W2078344108', 'W2078835847', 'W2078967567', 'W2079762480', 'W2079872393', 'W2087797223', 'W2089426165', 'W2090915280', 'W2092156403', 'W2092547185', 'W2093779999', 'W2094158847', 'W2095095383', 'W2096050373', 'W2097880175', 'W2099353039', 'W2103282677', 'W2106996050', 'W2107184659', 'W2111931721', 'W2112889810', 'W2114680254', 'W2114692928', 'W2115489808', 'W2115537846', 'W2122720938', 'W2124823640', 'W2129547735', 'W2129678820', 'W2131368080', 'W2131496831', 'W2132717567', 'W2133025798', 'W2133147905', 'W2133838039', 'W2135840203', 'W2136868307', 'W2141068900', 'W2142900746', 'W2146191673', 'W2148594799', 'W2149032140', 'W2150357048', 'W2151386142', 'W2153136150', 'W2155605920', 'W2155678615', 'W2156471291', 'W2158417770', 'W2160524585', 'W2164025253', 'W2167582037', 'W2168743033', 'W2171632855', 'W2171929340', 'W2257832580', 'W2313024814', 'W2315563763', 'W2323521842', 'W2325333929', 'W2432872509', 'W3098022872', 'W4234009147'], 'abstract': 'There is a growing demand for flexible and soft electronic devices. In particular, stretchable, skin-mountable, and wearable strain sensors are needed for several potential applications including personalized health-monitoring, human motion detection, human-machine interfaces, soft robotics, and so forth. This Feature Article presents recent advancements in the development of flexible and stretchable strain sensors. The article shows that highly stretchable strain sensors are successfully being developed by new mechanisms such as disconnection between overlapped nanomaterials, crack propagation in thin films, and tunneling effect, different from traditional strain sensing mechanisms. Strain sensing performances of recently reported strain sensors are comprehensively studied and discussed, showing that appropriate choice of composite structures as well as suitable interaction between functional nanomaterials and polymers are essential for the high performance strain sensing. Next, simulation results of piezoresistivity of stretchable strain sensors by computational models are reported. Finally, potential applications of flexible strain sensors are described. This survey reveals that flexible, skin-mountable, and wearable strain sensors have potential in diverse applications while several grand challenges have to be still overcome.', 'counts_by_year': [[2022, 253], [2021, 393], [2020, 333], [2019, 374], [2018, 259], [2017, 139], [2016, 32]]}, {'id': 'W2919979744', 'doi': 'https://doi.org/10.1016/j.future.2019.02.028', 'title': 'Harris hawks optimization: Algorithm and applications', 'type': 'journal-article', 'publication_date': '2019-08-01', 'host_venue': 'V186357190', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2476050101', ['I165932596', 'I23946033']], ['A3006208000', ['I11701301']], ['A2035841121', ['I114972647']], ['A2060383311', ['I114972647']], ['A2734702566', ['I94800806']], ['A2160886829', ['I146620803']]], 'cited_by_count': 1788, 'concepts': [['C2780343955', '0.6732632'], ['C109718341', '0.6379883'], ['C41008148', '0.63118434'], ['C185798385', '0.5745449'], ['C11413529', '0.5126645']], 'referenced_works': ['W883434633', 'W1595159159', 'W1777985119', 'W1859193303', 'W1929236209', 'W1972279253', 'W1983774268', 'W1984296381', 'W1984762043', 'W1990952381', 'W1992656046', 'W1997600725', 'W1999284878', 'W2001422417', 'W2002713790', 'W2003049821', 'W2003890325', 'W2004381207', 'W2006694777', 'W2007351764', 'W2012115555', 'W2015512846', 'W2019813799', 'W2023370127', 'W2024060531', 'W2024833075', 'W2026258334', 'W2027382431', 'W2027393638', 'W2029846254', 'W2031183907', 'W2034988449', 'W2040675064', 'W2041629655', 'W2052745152', 'W2061438946', 'W2062358112', 'W2063136261', 'W2065401134', 'W2072955302', 'W2074194260', 'W2076858147', 'W2083000360', 'W2096166399', 'W2096673585', 'W2107941094', 'W2111393363', 'W2117640392', 'W2119000420', 'W2134061862', 'W2135879356', 'W2140796089', 'W2143055330', 'W2146439308', 'W2147271386', 'W2149815769', 'W2150382849', 'W2151554678', 'W2154943049', 'W2156773695', 'W2168081761', 'W2221148967', 'W2232748179', 'W2261079877', 'W2290883490', 'W2465078833', 'W2520618604', 'W2585392941', 'W2606276573', 'W2613771876', 'W2738900493', 'W2747672495', 'W2768434535', 'W2776226778', 'W2801536506', 'W2883013658', 'W2885770227', 'W2887171350', 'W2899250423', 'W2912733464', 'W2914128779', 'W3105346980', 'W4246598646', 'W4250503569'], 'abstract': 'In this paper, a novel population-based, nature-inspired optimization paradigm is proposed, which is called Harris Hawks Optimizer (HHO). The main inspiration of HHO is the cooperative behavior and chasing style of Harris’ hawks in nature called surprise pounce. In this intelligent strategy, several hawks cooperatively pounce a prey from different directions in an attempt to surprise it. Harris hawks can reveal a variety of chasing patterns based on the dynamic nature of scenarios and escaping patterns of the prey. This work mathematically mimics such dynamic patterns and behaviors to develop an optimization algorithm. The effectiveness of the proposed HHO optimizer is checked, through a comparison with other nature-inspired techniques, on 29 benchmark problems and several real-world engineering problems. The statistical results and comparisons show that the HHO algorithm provides very promising and occasionally competitive results compared to well-established metaheuristic techniques. Source codes of HHO are publicly available at http://www.alimirjalili.com/HHO.html and http://www.evo-ml.com/2019/03/02/hho . • A mathematical model is proposed to simulate the hunting behavior of Harris’ Hawks. • An optimization algorithm is proposed using the mathematical model. • The proposed HHO algorithm is tested on several benchmarks. • The performance of HHO is also examined on several engineering design problems. • The results show the merits of the HHO algorithm as compared to the existing algorithms.', 'counts_by_year': [[2022, 778], [2021, 643], [2020, 296], [2019, 55]]}, {'id': 'W2520774990', 'doi': 'https://doi.org/10.1007/978-3-319-46478-7_31', 'title': 'A Discriminative Feature Learning Approach for Deep Face Recognition', 'type': 'book-chapter', 'publication_date': '2016-10-08', 'host_venue': 'V106296714', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2152036132', ['I4210145761']], ['A2481619292', ['I4210145761']], ['A2107648850', ['I4210145761']], ['A2411627636', ['I177725633']]], 'cited_by_count': 1782, 'concepts': [['C97931131', '0.8695518'], ['C41008148', '0.8594384'], ['C154945302', '0.72823143'], ['C31510193', '0.66954273'], ['C153180895', '0.6153373']], 'referenced_works': ['W7811848', 'W28988658', 'W1677182931', 'W1834627138', 'W1944615693', 'W1950843348', 'W1983364832', 'W1998808035', 'W2019464758', 'W2024922353', 'W2067425370', 'W2076434944', 'W2088171019', 'W2097117768', 'W2108598243', 'W2112796928', 'W2117539524', 'W2122111042', 'W2133444763', 'W2145287260', 'W2155893237', 'W2163808566', 'W2325939864', 'W2431335693', 'W3099206234'], 'abstract': 'Convolutional neural networks (CNNs) have been widely used in computer vision community, significantly improving the state-of-the-art. In most of the available CNNs, the softmax loss function is used as the supervision signal to train the deep model. In order to enhance the discriminative power of the deeply learned features, this paper proposes a new supervision signal, called center loss, for face recognition task. Specifically, the center loss simultaneously learns a center for deep features of each class and penalizes the distances between the deep features and their corresponding class centers. More importantly, we prove that the proposed center loss function is trainable and easy to optimize in the CNNs. With the joint supervision of softmax loss and center loss, we can train a robust CNNs to obtain the deep features with the two key learning objectives, inter-class dispension and intra-class compactness as much as possible, which are very essential to face recognition. It is encouraging to see that our CNNs (with such joint supervision) achieve the state-of-the-art accuracy on several important face recognition benchmarks, Labeled Faces in the Wild (LFW), YouTube Faces (YTF), and MegaFace Challenge. Especially, our new approach achieves the best results on MegaFace (the largest public domain face benchmark) under the protocol of small training set (contains under 500000 images and under 20000 persons), significantly improving the previous results and setting new state-of-the-art for both face recognition and face verification tasks.', 'counts_by_year': [[2022, 174], [2021, 384], [2020, 446], [2019, 379], [2018, 279], [2017, 113], [2016, 6], [2015, 1]]}, {'id': 'W2522738478', 'doi': 'https://doi.org/10.1227/neu.0000000000001432', 'title': 'Guidelines for the Management of Severe Traumatic Brain Injury, Fourth Edition', 'type': 'journal-article', 'publication_date': '2017-01-01', 'host_venue': 'V111727011', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2306460898', ['I165690674']], ['A2403888742', ['I165690674']], ['A3013840316', ['I165690674']], ['A2120854446', ['I139290212']], ['A1131278822', ['I223532165']], ['A2198012606', ['I170201317']], ['A2047662435', ['I223532165']], ['A2108512884', ['I201448701']], ['A2144516244', ['I97018004']], ['A2064570782', ['I141945490']], ['A2601427431', ['I189460059']], ['A2251628427', ['I170201317']], ['A2052217775', ['I1288882113']], ['A339567616', ['I201448701']], ['A2790333988', ['I72816309']], ['A2130456883', ['I150468666']], ['A2019101267', ['I97018004']]], 'cited_by_count': 1781, 'concepts': [['C71924100', '0.83769935'], ['C2778012447', '0.6710189'], ['C2780182762', '0.604998'], ['C19648533', '0.59788215'], ['C141330323', '0.57929164']], 'referenced_works': ['W21842017', 'W113238761', 'W1482545969', 'W1876563630', 'W1966395864', 'W1999160411', 'W2008611744', 'W2015378461', 'W2018777522', 'W2018867075', 'W2022036219', 'W2022153416', 'W2030782694', 'W2035127479', 'W2037268291', 'W2038516679', 'W2045663952', 'W2051353898', 'W2058778905', 'W2075256030', 'W2079029826', 'W2080570503', 'W2082410527', 'W2086088193', 'W2098710076', 'W2111466276', 'W2120654493', 'W2121275412', 'W2130372761', 'W2132968358', 'W2134203382', 'W2137347848', 'W2137706047', 'W2152611882', 'W2159196441', 'W2164313843', 'W2417990517'], 'abstract': 'The scope and purpose of this work is 2-fold: to synthesize the available evidence and to translate it into recommendations. This document provides recommendations only when there is evidence to support them. As such, they do not constitute a complete protocol for clinical use. Our intention is that these recommendations be used by others to develop treatment protocols, which necessarily need to incorporate consensus and clinical judgment in areas where current evidence is lacking or insufficient. We think it is important to have evidence-based recommendations to clarify what aspects of practice currently can and cannot be supported by evidence, to encourage use of evidence-based treatments that exist, and to encourage creativity in treatment and research in areas where evidence does not exist. The communities of neurosurgery and neuro-intensive care have been early pioneers and supporters of evidence-based medicine and plan to continue in this endeavor. The complete guideline document, which summarizes and evaluates the literature for each topic, and supplemental appendices (A-I) are available online at https://www.braintrauma.org/coma/guidelines.', 'counts_by_year': [[2022, 304], [2021, 369], [2020, 390], [2019, 305], [2018, 217], [2017, 131], [2016, 24], [2015, 38]]}, {'id': 'W2963495494', 'doi': 'https://doi.org/10.1109/cvpr.2017.683', 'title': 'Residual Attention Network for Image Classification', 'type': 'proceedings-article', 'publication_date': '2017-07-01', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2609206393', ['I2945522305']], ['A2610035800', ['I99065089']], ['A2893763385', ['I2945522305']], ['A2714686765', ['I177725633']], ['A2972766841', ['I2945522305']], ['A2139493915', ['I139759216']], ['A2227253382', ['I177725633']], ['A2166284823', ['I177725633']]], 'cited_by_count': 1760, 'concepts': [['C41008148', '0.8197185'], ['C155512373', '0.7954861'], ['C38858127', '0.6318909'], ['C185798385', '0.60611445'], ['C154945302', '0.602121']], 'referenced_works': ['W1677182931', 'W1745334888', 'W1770420845', 'W1903029394', 'W1923115158', 'W2064675550', 'W2068730032', 'W2097117768', 'W2144764737', 'W2194775991', 'W2209882149', 'W2221625691', 'W2520951797'], 'abstract': 'In this work, we propose "Residual Attention Network", a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion. Our Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers. Extensive analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the effectiveness of every module mentioned above. Our Residual Attention Network achieves state-of-the-art object recognition performance on three benchmark datasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) and ImageNet (4.8% single model and single crop, top-5 error). Note that, our method achieves 0.6% top-1 accuracy improvement with 46% trunk depth and 69% forward FLOPs comparing to ResNet-200. The experiment also demonstrates that our network is robust against noisy labels.', 'counts_by_year': [[2022, 328], [2021, 548], [2020, 487], [2019, 319], [2018, 72], [2017, 4], [2012, 1]]}, {'id': 'W3124420883', 'doi': 'https://doi.org/10.1109/tro.2016.2624754', 'title': 'Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age', 'type': 'journal-article', 'publication_date': '2016-12-01', 'host_venue': 'V144620930', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2117284162', ['I3133089283']], ['A95560969', ['I4210143601']], ['A2119339680', ['I262776074']], ['A2636414215', ['I5681781']], ['A2299514913', ['I202697423']], ['A2552994551', ['I255234318']], ['A2120202076', ['I5681781']], ['A2166738359', ['I63966007']]], 'cited_by_count': 1750, 'concepts': [['C86369673', '0.8408963'], ['C63479239', '0.7322588'], ['C34413123', '0.707837'], ['C48044578', '0.6653882'], ['C90509273', '0.6399002']], 'referenced_works': ['W49679257', 'W198101923', 'W645436802', 'W801273237', 'W900890939', 'W956540729', 'W1024421181', 'W1176413013', 'W1491601961', 'W1514716220', 'W1536078990', 'W1536617987', 'W1539726733', 'W1547347985', 'W1553966841', 'W1554704324', 'W1567646882', 'W1574773379', 'W1579142030', 'W1592619852', 'W1593286969', 'W1595410283', 'W1595654559', 'W1603426217', 'W1606576718', 'W1643815198', 'W1656165940', 'W1684737195', 'W1826354755', 'W1827716646', 'W1891947214', 'W1893935112', 'W1905829557', 'W1938204631', 'W1966173730', 'W1967329161', 'W1967653388', 'W1968315983', 'W1969080261', 'W1970652409', 'W1972624884', 'W1976905874', 'W1977758817', 'W1979177351', 'W1983905625', 'W1986879390', 'W1987648924', 'W1988757690', 'W1989838848', 'W1990884907', 'W1991172434', 'W1994292819', 'W1997452076', 'W2000321478', 'W2000668356', 'W2001164479', 'W2001496915', 'W2003919215', 'W2009422376', 'W2009655761', 'W2010331587', 'W2010806162', 'W2011767579', 'W2012421030', 'W2015144449', 'W2015996585', 'W2016574277', 'W2020950478', 'W2024162894', 'W2024908906', 'W2028184164', 'W2029780423', 'W2031446601', 'W2031489346', 'W2033923716', 'W2038328483', 'W2040134986', 'W2044739225', 'W2044914660', 'W2047049913', 'W2047328415', 'W2048500680', 'W2052014837', 'W2054442768', 'W2054804188', 'W2054969198', 'W2056298239', 'W2056458921', 'W2056998420', 'W2061176333', 'W2064169939', 'W2065353791', 'W2065860562', 'W2069635875', 'W2071571922', 'W2071634722', 'W2071906076', 'W2072951627', 'W2073700113', 'W2077069816', 'W2080823437', 'W2080861014', 'W2081250498', 'W2083347703', 'W2087076983', 'W2093659073', 'W2096020743', 'W2096052462', 'W2096649264', 'W2097012286', 'W2097696373', 'W2100185791', 'W2100481345', 'W2105018789', 'W2105198794', 'W2105413810', 'W2107156416', 'W2107402720', 'W2107667896', 'W2107905629', 'W2108134361', 'W2110405746', 'W2110872054', 'W2112673789', 'W2112788346', 'W2113778448', 'W2114084853', 'W2116134448', 'W2117539524', 'W2118223742', 'W2119203353', 'W2119493293', 'W2119538218', 'W2122455045', 'W2124313187', 'W2125341325', 'W2125416623', 'W2125873991', 'W2127824950', 'W2128017662', 'W2129091674', 'W2129744044', 'W2131846894', 'W2131946505', 'W2134396142', 'W2137052305', 'W2137376508', 'W2137813581', 'W2142423373', 'W2142924305', 'W2143703915', 'W2144824356', 'W2145429668', 'W2146702612', 'W2146881125', 'W2149211411', 'W2151290401', 'W2152671441', 'W2154280780', 'W2155147723', 'W2155620850', 'W2158057652', 'W2161123599', 'W2162731263', 'W2163394682', 'W2164429173', 'W2164549458', 'W2166132830', 'W2167687475', 'W2168359464', 'W2169781427', 'W2171244244', 'W2172011507', 'W2172103629', 'W2180077832', 'W2200124539', 'W2211420311', 'W2220063164', 'W2235617807', 'W2235755591', 'W2250172176', 'W2252678175', 'W2278591674', 'W2284029970', 'W2284939270', 'W2288477559', 'W2290655987', 'W2294894645', 'W2295583866', 'W2300779272', 'W2323286530', 'W2334590713', 'W2408654640', 'W2410664773', 'W2412691628', 'W2416442089', 'W2419212966', 'W2419416064', 'W2482726005', 'W2489822048', 'W2519559998', 'W2537007072', 'W2963288928', 'W2963537367', 'W3103648783', 'W3146325346', 'W4211222846', 'W4231002530', 'W4241431727', 'W4246744050', 'W4248598408', 'W4254357733', 'W4301141993'], 'abstract': "Simultaneous Localization and Mapping (SLAM)consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications, and witnessing a steady transition of this technology to industry. We survey the current state of SLAM. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?", 'counts_by_year': [[2022, 250], [2021, 426], [2020, 418], [2019, 356], [2018, 210], [2017, 84], [2016, 4]]}, {'id': 'W2174602966', 'doi': 'https://doi.org/10.1093/nar/gkv1222', 'title': 'ClinVar: public archive of interpretations of clinically relevant variants', 'type': 'journal-article', 'publication_date': '2016-01-04', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2085829377', ['I4210109390']], ['A2577548852', ['I4210109390']], ['A2331041094', ['I4210109390']], ['A2130117684', ['I4210109390']], ['A2531169227', ['I4210109390']], ['A2530906326', ['I4210109390']], ['A2329829249', ['I4210109390']], ['A2104614090', ['I4210109390']], ['A2101223612', ['I4210109390']], ['A2574699896', ['I4210109390']], ['A2654673092', ['I4210109390']], ['A2134053873', ['I4210109390']], ['A313403037', ['I4210109390']], ['A2064526041', ['I4210109390']], ['A2531225140', ['I4210109390']], ['A2307242731', ['I4210109390']], ['A2530336643', ['I4210109390']], ['A2325505185', ['I4210109390']], ['A370127644', ['I4210109390']]], 'cited_by_count': 1745, 'concepts': [['C71901391', '0.602484'], ['C202264299', '0.5544184'], ['C86803240', '0.49878407'], ['C119839945', '0.46484795'], ['C169485995', '0.45934424']], 'referenced_works': ['W1505770165', 'W1971192989', 'W1973234126', 'W1995801428', 'W2013919595', 'W2015160508', 'W2037458235', 'W2051978340', 'W2096525273', 'W2112589398', 'W2126209963', 'W2154866190', 'W2160344363', 'W2162151166', 'W2187581821', 'W4237629094'], 'abstract': "ClinVar (https://www.ncbi.nlm.nih.gov/clinvar/) at the National Center for Biotechnology Information (NCBI) is a freely available archive for interpretations of clinical significance of variants for reported conditions. The database includes germline and somatic variants of any size, type or genomic location. Interpretations are submitted by clinical testing laboratories, research laboratories, locus-specific databases, OMIM®, GeneReviews™, UniProt, expert panels and practice guidelines. In NCBI's Variation submission portal, submitters upload batch submissions or use the Submission Wizard for single submissions. Each submitted interpretation is assigned an accession number prefixed with SCV. ClinVar staff review validation reports with data types such as HGVS (Human Genome Variation Society) expressions; however, clinical significance is reported directly from submitters. Interpretations are aggregated by variant-condition combination and assigned an accession number prefixed with RCV. Clinical significance is calculated for the aggregate record, indicating consensus or conflict in the submitted interpretations. ClinVar uses data standards, such as HGVS nomenclature for variants and MedGen identifiers for conditions. The data are available on the web as variant-specific views; the entire data set can be downloaded via ftp. Programmatic access for ClinVar records is available through NCBI's E-utilities. Future development includes providing a variant-centric XML archive and a web page for details of SCV submissions.", 'counts_by_year': [[2022, 180], [2021, 250], [2020, 276], [2019, 329], [2018, 346], [2017, 270], [2016, 94]]}, {'id': 'W2222589778', 'doi': 'https://doi.org/10.1021/acs.jctc.5b00864', 'title': 'OPLS3: A Force Field Providing Broad Coverage of Drug-like Small Molecules and Proteins', 'type': 'journal-article', 'publication_date': '2016-01-12', 'host_venue': 'V189701308', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2306670502', ['I4210161498']], ['A2099036939', ['I4210161498']], ['A2171542886', ['I4210161498']], ['A2648376037', ['I4210161498']], ['A2550849621', ['I4210161498']], ['A2312234212', ['I4210161498']], ['A2342647531', ['I4210161498']], ['A1975799931', ['I4210161498']], ['A2574475982', ['I4210161498']], ['A2115248651', ['I4210161498']], ['A2304116807', ['I4210161498']], ['A2560255253', ['I4210161498']], ['A147653530', ['I4210161498']], ['A1978362656', ['I32971472']], ['A2950179100', ['I4210161498']], ['A246194277', ['I78577930']]], 'cited_by_count': 1743, 'concepts': [['C10803110', '0.8461782'], ['C161624437', '0.6793632'], ['C185592680', '0.5563002'], ['C32909587', '0.5234534'], ['C89025888', '0.4852731']], 'referenced_works': ['W1911338432', 'W1952317074', 'W1966534221', 'W1968707111', 'W1972923939', 'W1973209866', 'W1975588280', 'W1984052581', 'W1987275960', 'W1989814214', 'W1994055814', 'W1996156386', 'W1996551126', 'W2000815209', 'W2003985145', 'W2008505552', 'W2008758209', 'W2011207690', 'W2015679656', 'W2021220203', 'W2021832765', 'W2026590485', 'W2026598007', 'W2027408247', 'W2032044839', 'W2036131411', 'W2036250557', 'W2037159954', 'W2041189078', 'W2042490093', 'W2043749332', 'W2045418085', 'W2049475242', 'W2050333027', 'W2051381895', 'W2054881399', 'W2055585899', 'W2060003952', 'W2060757799', 'W2060872117', 'W2063779505', 'W2067236515', 'W2070915333', 'W2071827681', 'W2073745636', 'W2078641649', 'W2081396007', 'W2083038332', 'W2087925988', 'W2091286894', 'W2091562724', 'W2091845985', 'W2095327124', 'W2101247857', 'W2110183365', 'W2120987099', 'W2122199548', 'W2125938217', 'W2127522551', 'W2128494768', 'W2132776121', 'W2137322489', 'W2144288821', 'W2154670681', 'W2161605421', 'W2171047927', 'W2175111709', 'W2317057295', 'W2321111305', 'W2323942455', 'W2404280981', 'W2420659973', 'W4238663480', 'W4240523659', 'W4245700002'], 'abstract': 'The parametrization and validation of the OPLS3 force field for small molecules and proteins are reported. Enhancements with respect to the previous version (OPLS2.1) include the addition of off-atom charge sites to represent halogen bonding and aryl nitrogen lone pairs as well as a complete refit of peptide dihedral parameters to better model the native structure of proteins. To adequately cover medicinal chemical space, OPLS3 employs over an order of magnitude more reference data and associated parameter types relative to other commonly used small molecule force fields (e.g., MMFF and OPLS_2005). As a consequence, OPLS3 achieves a high level of accuracy across performance benchmarks that assess small molecule conformational propensities and solvation. The newly fitted peptide dihedrals lead to significant improvements in the representation of secondary structure elements in simulated peptides and native structure stability over a number of proteins. Together, the improvements made to both the small molecule and protein force field lead to a high level of accuracy in predicting protein-ligand binding measured over a wide range of targets and ligands (less than 1 kcal/mol RMS error) representing a 30% improvement over earlier variants of the OPLS force field.', 'counts_by_year': [[2022, 275], [2021, 426], [2020, 354], [2019, 272], [2018, 223], [2017, 144], [2016, 46]]}, {'id': 'W2560474170', 'doi': 'https://doi.org/10.1109/cvpr.2017.179', 'title': 'FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2075599143', ['I161046081']], ['A2694753790', ['I161046081']], ['A2723350364', ['I161046081']], ['A1966434052', ['I161046081']], ['A1799886526', ['I161046081']], ['A2014530249', ['I161046081']]], 'cited_by_count': 1739, 'concepts': [['C155542232', '0.79693174'], ['C157202957', '0.7885148'], ['C41008148', '0.7615627'], ['C192209626', '0.58607686'], ['C108583219', '0.5647233']], 'referenced_works': ['W764651262', 'W1496571393', 'W1904063580', 'W1951289974', 'W2076756823', 'W2113221323', 'W2131747574', 'W2151834591', 'W2197046994', 'W2294238219', 'W2295628903', 'W2296073425', 'W2307770531', 'W2507953016', 'W2963317244', 'W2963474899', 'W2963659353', 'W2964002255', 'W3100388886'], 'abstract': 'The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a subnetwork specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.', 'counts_by_year': [[2022, 159], [2021, 466], [2020, 489], [2019, 398], [2018, 208], [2017, 18]]}, {'id': 'W2964082701', 'doi': 'https://doi.org/10.1109/sp.2016.41', 'title': 'Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks', 'type': 'proceedings-article', 'publication_date': '2016-05-22', 'host_venue': 'V4306418833', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A248975517', ['I130769515']], ['A2056207806', ['I130769515']], ['A2953866209', ['I135310074']], ['A2193269139', ['I135310074']], ['A2059211748', ['I166416128']]], 'cited_by_count': 1739, 'concepts': [['C37736160', '0.8960898'], ['C27158222', '0.8343115'], ['C41008148', '0.74505925'], ['C204030448', '0.74323916'], ['C63479239', '0.7251884']], 'referenced_works': ['W9657784', 'W1932198206', 'W1966948031', 'W1979339361', 'W2018061979', 'W2032247543', 'W2095577883', 'W2105037940', 'W2112594540', 'W2112739286', 'W2124537004', 'W2125908420', 'W2136655611', 'W2144906988', 'W2151298633', 'W2180612164', 'W2574797807', 'W4242989628', 'W4256462051'], 'abstract': 'Deep learning algorithms have been shown to perform extremely well on manyclassical machine learning problems. However, recent studies have shown thatdeep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force adeep neural network (DNN) to provide adversary-selected outputs. Such attackscan seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles canbe crashed, illicit or illegal content can bypass content filters, or biometricauthentication systems can be manipulated to allow improper access. In thiswork, we introduce a defensive mechanism called defensive distillationto reduce the effectiveness of adversarial samples on DNNs. We analyticallyinvestigate the generalizability and robustness properties granted by the useof defensive distillation when training DNNs. We also empirically study theeffectiveness of our defense mechanisms on two DNNs placed in adversarialsettings. The study shows that defensive distillation can reduce effectivenessof sample creation from 95% to less than 0.5% on a studied DNN. Such dramaticgains can be explained by the fact that distillation leads gradients used inadversarial sample creation to be reduced by a factor of 1030. We alsofind that distillation increases the average minimum number of features thatneed to be modified to create adversarial samples by about 800% on one of theDNNs we tested.', 'counts_by_year': [[2022, 166], [2021, 389], [2020, 446], [2019, 392], [2018, 240], [2017, 86], [2016, 16], [2015, 1], [2014, 1]]}, {'id': 'W3013594674', 'doi': 'https://doi.org/10.1126/science.abb4218', 'title': 'The effect of human mobility and control measures on the COVID-19 epidemic in China', 'type': 'journal-article', 'publication_date': '2020-05-01', 'host_venue': 'V3880285', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2128727216', ['I1288882113', 'I40120149', 'I136199984']], ['A3010170495', ['I12912129']], ['A2494969294', ['I40120149', 'I69094615']], ['A2523041657', ['I43439940']], ['A2750308943', ['I12912129']], ['A2113647411', ['I3143470655']], ['A2138901498', ['I40120149']], ['A1999695442', ['I40120149']], ['A2113665552', ['I136199984']], ['A2071868534', ['I136199984']], ['A1915684033', ['I1288882113', 'I136199984']], ['A3014189247', ['I157536573', 'I184646667']], ['A2707826896', ['I134940468', 'I12912129']], ['A2268875888', ['I4210166112']], ['A2125714363', ['I40120149']], ['A2232962782', ['I133836048', 'I40120149']], ['A2277136547', ['I12912129']]], 'cited_by_count': 1739, 'concepts': [['C191935318', '0.8208609'], ['C761482', '0.66598654'], ['C27415008', '0.6624511'], ['C3008058167', '0.6578699'], ['C116675565', '0.591981']], 'referenced_works': ['W1863440143', 'W1951724000', 'W1968393246', 'W1998725525', 'W2076873241', 'W2097360283', 'W2140763962', 'W2151570219', 'W2158196600', 'W2161335593', 'W2515101565', 'W2524589976', 'W2797474345', 'W2920135474', 'W2995313608', 'W3001897055', 'W3003573988', 'W3003668884', 'W3004026249', 'W3004479334', 'W3004516828', 'W3004912618', 'W3006163015', 'W3006320625', 'W3007104434', 'W3008028633', 'W3008806964', 'W3008818676', 'W3009003996', 'W3010270046', 'W3012798837', 'W3013188135', 'W3013594674', 'W3151795292', 'W4294541781'], 'abstract': 'The ongoing coronavirus disease 2019 (COVID-19) outbreak expanded rapidly throughout China. Major behavioral, clinical, and state interventions were undertaken to mitigate the epidemic and prevent the persistence of the virus in human populations in China and worldwide. It remains unclear how these unprecedented interventions, including travel restrictions, affected COVID-19 spread in China. We used real-time mobility data from Wuhan and detailed case data including travel history to elucidate the role of case importation in transmission in cities across China and to ascertain the impact of control measures. Early on, the spatial distribution of COVID-19 cases in China was explained well by human mobility data. After the implementation of control measures, this correlation dropped and growth rates became negative in most locations, although shifts in the demographics of reported cases were still indicative of local chains of transmission outside of Wuhan. This study shows that the drastic control measures implemented in China substantially mitigated the spread of COVID-19.', 'counts_by_year': [[2022, 353], [2021, 783], [2020, 594]]}, {'id': 'W3041188046', 'doi': 'https://doi.org/10.1002/asi.23552', 'title': 'The sharing economy: Why people participate in collaborative consumption', 'type': 'journal-article', 'publication_date': '2016-09-01', 'host_venue': 'V4210197613', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A1228781357', ['I9927081', 'I3130035786']], ['A174999221', ['I180519160']], ['A1957862955', ['I2801770344']]], 'cited_by_count': 1737, 'concepts': [['C33199155', '0.82616055'], ['C30772137', '0.6313932'], ['C144133560', '0.43820217'], ['C41008148', '0.36466178'], ['C56739046', '0.3526212']], 'referenced_works': ['W1508509982', 'W1540565333', 'W1597565573', 'W1598145799', 'W1965515469', 'W1985419904', 'W1986892266', 'W1991180302', 'W1996724467', 'W1997321313', 'W2001841448', 'W2003496454', 'W2007323286', 'W2009546847', 'W2011309176', 'W2021067430', 'W2026318397', 'W2040332982', 'W2042539525', 'W2047103421', 'W2047655663', 'W2052729098', 'W2055135015', 'W2055703005', 'W2057148748', 'W2070950781', 'W2076924522', 'W2081168357', 'W2081768880', 'W2085548520', 'W2095867983', 'W2096547436', 'W2099697766', 'W2099907913', 'W2104554137', 'W2119681300', 'W2121001699', 'W2121812884', 'W2125326971', 'W2130950828', 'W2134049161', 'W2140457586', 'W2146125869', 'W2159018222', 'W2160383953', 'W2162049359', 'W2163498454', 'W2164854792', 'W2167036891', 'W2170488209', 'W2184849112', 'W2562939000', 'W3022009422', 'W3022734214', 'W3121995858', 'W3123281112', 'W3147795728', 'W4233654598', 'W4237079254', 'W4242883546', 'W4242911310'], 'abstract': "Information and communications technologies ICTs have enabled the rise of so-called Collaborative Consumption CC: the peer-to-peer-based activity of obtaining, giving, or sharing the access to goods and services, coordinated through community-based online services. CC has been expected to alleviate societal problems such as hyper-consumption, pollution, and poverty by lowering the cost of economic coordination within communities. However, beyond anecdotal evidence, there is a dearth of understanding why people participate in CC. Therefore, in this article we investigate people's motivations to participate in CC. The study employs survey data N=168 gathered from people registered onto a CC site. The results show that participation in CC is motivated by many factors such as its sustainability, enjoyment of the activity as well as economic gains. An interesting detail in the result is that sustainability is not directly associated with participation unless it is at the same time also associated with positive attitudes towards CC. This suggests that sustainability might only be an important factor for those people for whom ecological consumption is important. Furthermore, the results suggest that in CC an attitude-behavior gap might exist; people perceive the activity positively and say good things about it, but this good attitude does not necessary translate into action.", 'counts_by_year': [[2022, 187], [2021, 335], [2020, 401], [2019, 345], [2018, 226], [2017, 158], [2016, 62], [2015, 18], [2014, 3]]}, {'id': 'W2565516711', 'doi': 'https://doi.org/10.1016/j.neucom.2016.12.038', 'title': 'A survey of deep neural network architectures and their applications', 'type': 'journal-article', 'publication_date': '2017-04-19', 'host_venue': 'V45693802', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2511876261', ['I59433898']], ['A2104171312', ['I59433898']], ['A2101154754', ['I59433898']], ['A2108293171', ['I191208505']], ['A3160455571', ['I185163786', 'I78978612']], ['A1988427257', ['I185163786']]], 'cited_by_count': 1722, 'concepts': [['C41008148', '0.67028695'], ['C50644808', '0.6324376'], ['C154945302', '0.58857995'], ['C108583219', '0.5569249'], ['C2984842247', '0.48074389']], 'referenced_works': ['W580138839', 'W614374662', 'W823218635', 'W856456657', 'W1051230991', 'W1159662880', 'W1457602677', 'W1571745681', 'W1695065985', 'W1705374184', 'W1801780804', 'W1801865513', 'W1852994950', 'W1864564002', 'W1885185971', 'W1925417509', 'W1932198206', 'W1964073652', 'W1964155876', 'W1967557378', 'W1969851134', 'W1973016470', 'W1978426462', 'W1979651826', 'W1984541135', 'W1991539813', 'W1993845689', 'W1993882792', 'W1994216910', 'W1996384036', 'W1998399571', 'W2003059629', 'W2004362043', 'W2006405885', 'W2008755574', 'W2011582941', 'W2013035813', 'W2014481529', 'W2015386604', 'W2017257315', 'W2017472655', 'W2021354639', 'W2022011789', 'W2023407680', 'W2025198378', 'W2025768430', 'W2028055618', 'W2029564360', 'W2032036568', 'W2033310064', 'W2035134151', 'W2039716246', 'W2042663758', 'W2043003570', 'W2048526313', 'W2048865854', 'W2060248624', 'W2076063813', 'W2076462394', 'W2077799289', 'W2078169166', 'W2078224158', 'W2084514013', 'W2093866254', 'W2096698681', 'W2098676252', 'W2099791118', 'W2100495367', 'W2101926813', 'W2106051978', 'W2106723645', 'W2107500316', 'W2108069432', 'W2108598243', 'W2112021726', 'W2112796928', 'W2116064496', 'W2116360511', 'W2117130368', 'W2118023920', 'W2123585936', 'W2125838338', 'W2130325614', 'W2132037657', 'W2134557905', 'W2136655611', 'W2136922672', 'W2139622435', 'W2141125852', 'W2141778357', 'W2142487393', 'W2147207799', 'W2147768505', 'W2156740722', 'W2160692033', 'W2160815625', 'W2163922914', 'W2168171912', 'W2168356304', 'W2190746225', 'W2192655208', 'W2210708668', 'W2212435496', 'W2213006595', 'W2216894533', 'W2221448138', 'W2238628508', 'W2242223225', 'W2250193862', 'W2254644702', 'W2257979135', 'W2284937233', 'W2293063104', 'W2293295816', 'W2294281604', 'W2294822684', 'W2295802084', 'W2299851147', 'W2310225923', 'W2315448583', 'W2339323489', 'W2344725271', 'W2418465508', 'W2433435592', 'W2513508218', 'W2546302380', 'W2547802736', 'W2557739469', 'W2919115771', 'W4231109964'], 'abstract': 'This work was supported in part the Royal Society of the UK, the National Natural Science Foundation of China under Grants 61329301, 61374010, and 61403319, and the Alexander von Humboldt Foundation of Germany.', 'counts_by_year': [[2022, 290], [2021, 456], [2020, 454], [2019, 295], [2018, 184], [2017, 39]]}, {'id': 'W4233698560', 'doi': 'https://doi.org/10.1093/nar/gkx1132', 'title': 'The Reactome Pathway Knowledgebase', 'type': 'journal-article', 'publication_date': '2017-11-14', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2099577479', []], ['A4273664072', []], ['A4273664073', []], ['A4273664074', []], ['A4273664075', []], ['A4273664076', []], ['A4273664077', []], ['A4273664078', []], ['A4273664079', []], ['A4273664080', []], ['A4273664081', []], ['A4273664082', []], ['A4273664083', []], ['A4273664084', []], ['A4273664085', []], ['A4273664086', []], ['A4273664087', []], ['A4273664088', []], ['A4273664089', []], ['A4273664090', []], ['A4273664091', []], ['A4273664092', []], ['A2468387294', []]], 'cited_by_count': 1722, 'concepts': [['C86803240', '0.66033494'], ['C70721500', '0.51343524'], ['C41008148', '0.47984216'], ['C60644358', '0.34924918'], ['C23123220', '0.3408562']], 'referenced_works': ['W1535641839', 'W1989277387', 'W2069157810', 'W2124045889', 'W2128601422', 'W2133465414', 'W2543238710', 'W2559466477', 'W2559588208', 'W2594844287', 'W2727134083'], 'abstract': "The Reactome Knowledgebase (https://reactome.org) provides molecular details of signal transduction, transport, DNA replication, metabolism, and other cellular processes as an ordered network of molecular transformations-an extended version of a classic metabolic map, in a single consistent data model. Reactome functions both as an archive of biological processes and as a tool for discovering unexpected functional relationships in data such as gene expression profiles or somatic mutation catalogues from tumor cells. To support the continued brisk growth in the size and complexity of Reactome, we have implemented a graph database, improved performance of data analysis tools, and designed new data structures and strategies to boost diagram viewer performance. To make our website more accessible to human users, we have improved pathway display and navigation by implementing interactive Enhanced High Level Diagrams (EHLDs) with an associated icon library, and subpathway highlighting and zooming, in a simplified and reorganized web site with adaptive design. To encourage re-use of our content, we have enabled export of pathway diagrams as 'PowerPoint' files.", 'counts_by_year': [[2022, 197], [2021, 417], [2020, 493], [2019, 461], [2018, 152], [2017, 1]]}, {'id': 'W2583396742', 'doi': 'https://doi.org/10.1126/science.aai9081', 'title': 'Efficient and stable solution-processed planar perovskite solar cells via contact passivation', 'type': 'journal-article', 'publication_date': '2017-02-17', 'host_venue': 'V3880285', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2942565022', ['I185261750']], ['A2918006896', ['I185261750']], ['A55969398', ['I185261750']], ['A2132139336', ['I185261750']], ['A2206091005', ['I185261750']], ['A2612659251', ['I185261750']], ['A2256476550', ['I185261750']], ['A1979593503', ['I185261750']], ['A3041127640', ['I185261750']], ['A2945469369', ['I185261750']], ['A2130380241', ['I185261750']], ['A2530925538', ['I185261750']], ['A2105502647', ['I185261750']], ['A2194980522', ['I185261750']], ['A2151579175', ['I185261750']], ['A2602548667', ['I185261750']], ['A1977309543', ['I185261750']], ['A2021786821', ['I185261750']]], 'cited_by_count': 1721, 'concepts': [['C33574316', '0.83682615'], ['C155011858', '0.8219972'], ['C192562407', '0.6277384'], ['C134786449', '0.6019281'], ['C175854130', '0.5798254']], 'referenced_works': ['W1553661931', 'W1629273100', 'W1802734970', 'W1916611353', 'W1927627425', 'W1981368803', 'W1998343415', 'W1999717631', 'W2000770615', 'W2004432407', 'W2011472471', 'W2041438187', 'W2054093403', 'W2059038908', 'W2065204918', 'W2067910821', 'W2095425644', 'W2095477500', 'W2100244982', 'W2158163016', 'W2195844396', 'W2196340795', 'W2206784480', 'W2216495470', 'W2229109093', 'W2237945597', 'W2260885620', 'W2264939720', 'W2268497559', 'W2270936141', 'W2271355543', 'W2281513767', 'W2301656337', 'W2314714121', 'W2320528970', 'W2331511205', 'W2343155643', 'W2390031586', 'W2398920238', 'W2403767637', 'W2405921647', 'W2407336804', 'W2410619939', 'W2461659796', 'W2465206327', 'W2473799868', 'W2474894284', 'W2475334816', 'W2488545181', 'W2507596404', 'W2518173943', 'W2519631333', 'W2521385727', 'W2522833408', 'W2527042386', 'W2536873010', 'W2549518562'], 'abstract': 'Passivating traps in perovskites Low-temperature processing of planar organic-inorganic perovskite solar cells made through solution processing would allow for simpler manufacturing and the use of flexible substrates. However, materials currently in use form interfaces with charge carrier trap states that limit performance. Tan et al. used chlorine-capped TiO 2 colloidal nanocrystal films as an electron-selective layer, which limited interface recombination in solution-processed solar cells. Such cells achieved certified efficiencies of 19.5% for active areas of 1.1 cm 2 . Science , this issue p. 722', 'counts_by_year': [[2022, 173], [2021, 289], [2020, 321], [2019, 395], [2018, 396], [2017, 144]]}, {'id': 'W2603777577', 'doi': 'https://doi.org/10.1109/iccv.2017.167', 'title': 'Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization', 'type': 'proceedings-article', 'publication_date': '2017-10-01', 'host_venue': 'V4306419272', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2720322089', ['I205783295']], ['A305618809', ['I205783295']]], 'cited_by_count': 1717, 'concepts': [['C136886441', '0.83047855'], ['C41008148', '0.72624683'], ['C50644808', '0.6848631'], ['C177264268', '0.47475985'], ['C137800194', '0.45981202']], 'referenced_works': ['W1999360130', 'W2109253138', 'W2275363859', 'W2471440592', 'W2475287302', 'W2583638424', 'W2962949994', 'W2963275094', 'W2963989815'], 'abstract': 'Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color & spatial controls, all using a single feed-forward neural network.', 'counts_by_year': [[2022, 223], [2021, 702], [2020, 492], [2019, 222], [2018, 68], [2017, 8]]}, {'id': 'W919187098', 'doi': 'https://doi.org/10.1007/s11747-015-0456-3', 'title': 'Institutions and axioms: an extension and update of service-dominant logic', 'type': 'journal-article', 'publication_date': '2016-01-01', 'host_venue': 'V92522684', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2043899690', ['I117965899']], ['A2087197139', ['I138006243']]], 'cited_by_count': 1711, 'concepts': [['C2778029271', '0.81259096'], ['C167729594', '0.71726626'], ['C144133560', '0.523074'], ['C2780378061', '0.49745634'], ['C41008148', '0.38658047']], 'referenced_works': ['W131951347', 'W271526086', 'W1511601860', 'W1965278510', 'W1970237540', 'W1972970784', 'W1973477318', 'W1976599136', 'W1976599252', 'W1980225821', 'W1986015197', 'W1987438072', 'W1989075208', 'W1990457230', 'W1992375703', 'W1993326409', 'W2002704355', 'W2002757881', 'W2003791428', 'W2006668375', 'W2009070185', 'W2013759460', 'W2014143923', 'W2015930340', 'W2016379417', 'W2022873147', 'W2028799738', 'W2037021691', 'W2041344184', 'W2044452299', 'W2049792501', 'W2053583812', 'W2054877429', 'W2056406089', 'W2056835755', 'W2059702009', 'W2066106849', 'W2075254626', 'W2075284967', 'W2077328169', 'W2078854905', 'W2084467099', 'W2085089829', 'W2089725322', 'W2096838238', 'W2098319730', 'W2098916257', 'W2099237005', 'W2105073835', 'W2106559068', 'W2109280325', 'W2110064109', 'W2120969145', 'W2126840412', 'W2135526934', 'W2136823593', 'W2139599822', 'W2143256503', 'W2143615203', 'W2154843660', 'W2158122897', 'W2159550836', 'W2163509832', 'W2167169191', 'W2254581393', 'W2313709257', 'W2492504580', 'W3022339544', 'W3121311682', 'W3121364520', 'W3123282572', 'W4232686304', 'W4233654598', 'W4238399289', 'W4238892533', 'W4241742455', 'W4241815540', 'W4245050769', 'W4246919849', 'W4247273961', 'W4248240512', 'W4252459496', 'W4296353554', 'W4297813569', 'W4298413470', 'W4301056702'], 'abstract': 'Service-dominant logic continues its evolution, facilitated by an active community of scholars throughout the world. Along its evolutionary path, there has been increased recognition of the need for a crisper and more precise delineation of the foundational premises and specification of the axioms of S-D logic. It also has become apparent that a limitation of the current foundational premises/axioms is the absence of a clearly articulated specification of the mechanisms of (often massive-scale) coordination and cooperation involved in the cocreation of value through markets and, more broadly, in society. This is especially important because markets are even more about cooperation than about the competition that is more frequently discussed. To alleviate this limitation and facilitate a better understanding of cooperation (and coordination), an eleventh foundational premise (fifth axiom) is introduced, focusing on the role of institutions and institutional arrangements in systems of value cocreation: service ecosystems. Literature on institutions across multiple social disciplines, including marketing, is briefly reviewed and offered as further support for this fifth axiom.', 'counts_by_year': [[2022, 262], [2021, 357], [2020, 337], [2019, 275], [2018, 232], [2017, 167], [2016, 71], [2015, 8]]}, {'id': 'W2900044275', 'doi': 'https://doi.org/10.1093/nar/gky1038', 'title': 'PANTHER version 14: more genomes, a new PANTHER GO-slim and improvements in enrichment analysis tools', 'type': 'journal-article', 'publication_date': '2019-01-08', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2104490322', ['I1174212']], ['A2963603248', ['I1174212']], ['A2900412192', ['I1174212']], ['A2171473469', ['I1174212', 'I37987034']]], 'cited_by_count': 1708, 'concepts': [['C86803240', '0.8921096'], ['C124388736', '0.72505814'], ['C193252679', '0.71456623'], ['C141231307', '0.70544904'], ['C90132467', '0.5434563']], 'referenced_works': ['W74841586', 'W1965917470', 'W1988023818', 'W1989277387', 'W2022472106', 'W2038464976', 'W2051058526', 'W2098425296', 'W2102020547', 'W2103017472', 'W2117602634', 'W2138122982', 'W2149049580', 'W2163500926', 'W2171815113', 'W2223549218', 'W2557496587', 'W2604808360', 'W2769449671', 'W3147254695', 'W4233120011', 'W4233698560'], 'abstract': 'PANTHER (Protein Analysis Through Evolutionary Relationships, http://pantherdb.org) is a resource for the evolutionary and functional classification of genes from organisms across the tree of life. We report the improvements we have made to the resource during the past two years. For evolutionary classifications, we have added more prokaryotic and plant genomes to the phylogenetic gene trees, expanding the representation of gene evolution in these lineages. We have refined many protein family boundaries, and have aligned PANTHER with the MEROPS resource for protease and protease inhibitor families. For functional classifications, we have developed an entirely new PANTHER GO-slim, containing over four times as many Gene Ontology terms as our previous GO-slim, as well as curated associations of genes to these terms. Lastly, we have made substantial improvements to the enrichment analysis tools available on the PANTHER website: users can now analyze over 900 different genomes, using updated statistical tests with false discovery rate corrections for multiple testing. The overrepresentation test is also available as a web service, for easy addition to third-party sites.', 'counts_by_year': [[2022, 370], [2021, 636], [2020, 569], [2019, 129], [2018, 3]]}, {'id': 'W2286771936', 'doi': 'https://doi.org/10.1038/nbt.3413', 'title': 'A 3D bioprinting system to produce human-scale tissue constructs with structural integrity', 'type': 'journal-article', 'publication_date': '2016-03-01', 'host_venue': 'V106963461', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2098318852', ['I12132619']], ['A2122281748', ['I12132619']], ['A2125650908', ['I12132619']], ['A1998479464', ['I12132619']], ['A2135814942', ['I12132619']], ['A2007418108', ['I12132619']]], 'cited_by_count': 1701, 'concepts': [['C108586683', '0.7318787'], ['C49892992', '0.67033035'], ['C2984185122', '0.6046515'], ['C136229726', '0.48494223'], ['C89429830', '0.43417755']], 'referenced_works': ['W1809073012', 'W1965511151', 'W1966594218', 'W1967696528', 'W1968013773', 'W1969936574', 'W1970771505', 'W1972540522', 'W1976785953', 'W1976851742', 'W1979169277', 'W1980846500', 'W1982397697', 'W1986735604', 'W1988398938', 'W1990629503', 'W1990793260', 'W1991073285', 'W1996715059', 'W1998642283', 'W1998734023', 'W1999007447', 'W2007055685', 'W2007310752', 'W2012593965', 'W2013632655', 'W2026952216', 'W2032032896', 'W2033870293', 'W2036145275', 'W2036520352', 'W2040984241', 'W2042782160', 'W2044586378', 'W2048644480', 'W2049067179', 'W2052353256', 'W2054992092', 'W2056830833', 'W2060640461', 'W2061796957', 'W2066046189', 'W2068553712', 'W2071033251', 'W2071425255', 'W2071949475', 'W2075814690', 'W2081851726', 'W2085269784', 'W2086518165', 'W2086762695', 'W2089779367', 'W2098900647', 'W2104480579', 'W2115728174', 'W2120316736', 'W2127296180', 'W2130171570', 'W2147410203', 'W2152056221', 'W2154289189', 'W2165438231', 'W2165694975', 'W2171561040', 'W4233887141'], 'abstract': 'A challenge for tissue engineering is producing three-dimensional (3D), vascularized cellular constructs of clinically relevant size, shape and structural integrity. We present an integrated tissue-organ printer (ITOP) that can fabricate stable, human-scale tissue constructs of any shape. Mechanical stability is achieved by printing cell-laden hydrogels together with biodegradable polymers in integrated patterns and anchored on sacrificial hydrogels. The correct shape of the tissue construct is achieved by representing clinical imaging data as a computer model of the anatomical defect and translating the model into a program that controls the motions of the printer nozzles, which dispense cells to discrete locations. The incorporation of microchannels into the tissue constructs facilitates diffusion of nutrients to printed cells, thereby overcoming the diffusion limit of 100-200 μm for cell survival in engineered tissues. We demonstrate capabilities of the ITOP by fabricating mandible and calvarial bone, cartilage and skeletal muscle. Future development of the ITOP is being directed to the production of tissues for human applications and to the building of more complex tissues and solid organs.', 'counts_by_year': [[2022, 234], [2021, 312], [2020, 335], [2019, 247], [2018, 255], [2017, 221], [2016, 93]]}, {'id': 'W2414781026', 'doi': 'https://doi.org/10.1016/j.jconrel.2016.06.017', 'title': 'DLS and zeta potential – What they are and what they are not?', 'type': 'journal-article', 'publication_date': '2016-08-10', 'host_venue': 'V161954266', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2238967613', ['I100930933']]], 'cited_by_count': 1701, 'concepts': [['C171250308', '0.65682447'], ['C86181022', '0.65372574'], ['C15083742', '0.5704956'], ['C14631669', '0.5701369'], ['C2780841128', '0.5569331']], 'referenced_works': ['W1475947892', 'W1527598299', 'W1564553363', 'W1578611973', 'W1860281480', 'W1879315968', 'W1925119982', 'W1963486780', 'W1969259546', 'W1971879956', 'W1976916591', 'W1980065701', 'W1981459441', 'W1984785286', 'W1986001908', 'W1986951817', 'W1987895902', 'W1988529113', 'W1988677712', 'W1993442975', 'W1996697333', 'W1997168496', 'W1999222950', 'W2005771030', 'W2005872642', 'W2006273975', 'W2007507786', 'W2007697148', 'W2009083397', 'W2010948469', 'W2011735501', 'W2012356826', 'W2014238842', 'W2015497119', 'W2017763625', 'W2019540869', 'W2027876233', 'W2028171126', 'W2028784203', 'W2029044993', 'W2030309966', 'W2030622678', 'W2036900525', 'W2038981640', 'W2039620984', 'W2043163044', 'W2046473425', 'W2047721234', 'W2047967468', 'W2048881497', 'W2049755149', 'W2050803054', 'W2054745605', 'W2056431016', 'W2058972969', 'W2059261154', 'W2059503044', 'W2064779937', 'W2065300520', 'W2066416694', 'W2067769271', 'W2070403068', 'W2070622052', 'W2072236397', 'W2072301209', 'W2075052396', 'W2075597980', 'W2076417585', 'W2079667908', 'W2081308470', 'W2083221204', 'W2085374195', 'W2086249925', 'W2088623562', 'W2089146874', 'W2089932008', 'W2090064426', 'W2094467684', 'W2099770700', 'W2103037903', 'W2106757326', 'W2107301970', 'W2107910518', 'W2109578509', 'W2111488320', 'W2112459741', 'W2118699123', 'W2120423203', 'W2123118992', 'W2123902150', 'W2131083445', 'W2131580401', 'W2132955511', 'W2135315262', 'W2140556727', 'W2143463421', 'W2145736500', 'W2147189124', 'W2147973073', 'W2149744058', 'W2156507886', 'W2157016784', 'W2158966777', 'W2159244822', 'W2161946327', 'W2167519680', 'W2170584001', 'W2185066948', 'W2186440110', 'W2187664814', 'W2276744583', 'W2286754526', 'W2288424886', 'W2296389641', 'W2313319811', 'W2316629158', 'W2333041227', 'W2335348086', 'W2438257166', 'W2512530700', 'W2516517430'], 'abstract': 'Adequate characterization of NPs (nanoparticles) is of paramount importance to develop well defined nanoformulations of therapeutic relevance. Determination of particle size and surface charge of NPs are indispensable for proper characterization of NPs. DLS (dynamic light scattering) and ZP (zeta potential) measurements have gained popularity as simple, easy and reproducible tools to ascertain particle size and surface charge. Unfortunately, on practical grounds plenty of challenges exist regarding these two techniques including inadequate understanding of the operating principles and dealing with critical issues like sample preparation and interpretation of the data. As both DLS and ZP have emerged from the realms of physical colloid chemistry - it is difficult for researchers engaged in nanomedicine research to master these two techniques. Additionally, there is little literature available in drug delivery research which offers a simple, concise account on these techniques. This review tries to address this issue while providing the fundamental principles of these techniques, summarizing the core mathematical principles and offering practical guidelines on tackling commonly encountered problems while running DLS and ZP measurements. Finally, the review tries to analyze the relevance of these two techniques from translatory perspective.', 'counts_by_year': [[2022, 335], [2021, 409], [2020, 403], [2019, 279], [2018, 185], [2017, 64], [2016, 15]]}, {'id': 'W2559466477', 'doi': 'https://doi.org/10.1093/nar/gkw1138', 'title': 'PANTHER version 11: expanded annotation data from Gene Ontology and Reactome pathways, and data analysis tool enhancements', 'type': 'journal-article', 'publication_date': '2017-01-04', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2104490322', ['I1174212']], ['A2171473469', ['I1174212']], ['A2963603248', ['I1174212']], ['A2558279641', ['I1174212']], ['A2581959369', ['I1174212']], ['A2979293847', ['I1174212']]], 'cited_by_count': 1697, 'concepts': [['C86803240', '0.58514005'], ['C2776321320', '0.5755162'], ['C71901391', '0.5268867'], ['C141231307', '0.43533248'], ['C2781148417', '0.4304819']], 'referenced_works': ['W74841586', 'W1965917470', 'W1988023818', 'W1989277387', 'W2000559685', 'W2098425296', 'W2102502076', 'W2117602634', 'W2128710769', 'W2137597742', 'W2151529283', 'W2164461702', 'W2171815113', 'W2223549218', 'W2401241322', 'W2521298622', 'W2604808360', 'W2739999456', 'W3145289798'], 'abstract': "The PANTHER database (Protein ANalysis THrough Evolutionary Relationships, http://pantherdb.org) contains comprehensive information on the evolution and function of protein-coding genes from 104 completely sequenced genomes. PANTHER software tools allow users to classify new protein sequences, and to analyze gene lists obtained from large-scale genomics experiments. In the past year, major improvements include a large expansion of classification information available in PANTHER, as well as significant enhancements to the analysis tools. Protein subfamily functional classifications have more than doubled due to progress of the Gene Ontology Phylogenetic Annotation Project. For human genes (as well as a few other organisms), PANTHER now also supports enrichment analysis using pathway classifications from the Reactome resource. The gene list enrichment tools include a new 'hierarchical view' of results, enabling users to leverage the structure of the classifications/ontologies; the tools also allow users to upload genetic variant data directly, rather than requiring prior conversion to a gene list. The updated coding single-nucleotide polymorphisms (SNP) scoring tool uses an improved algorithm. The hidden Markov model (HMM) search tools now use HMMER3, dramatically reducing search times and improving accuracy of E-value statistics. Finally, the PANTHER Tree-Attribute Viewer has been implemented in JavaScript, with new views for exploring protein sequence evolution.", 'counts_by_year': [[2022, 84], [2021, 171], [2020, 336], [2019, 518], [2018, 448], [2017, 138], [2016, 1], [2015, 1]]}, {'id': 'W2732026016', 'doi': 'https://doi.org/10.1109/tpami.2017.2723009', 'title': 'Places: A 10 Million Image Database for Scene Recognition', 'type': 'journal-article', 'publication_date': '2018-06-01', 'host_venue': 'V199944782', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2139274644', ['I63966007']], ['A1266997330', ['I138847295']], ['A2011491954', ['I63966007']], ['A2291917982', ['I63966007']], ['A2157095084', ['I63966007']]], 'cited_by_count': 1692, 'concepts': [['C81363708', '0.7917881'], ['C41008148', '0.7901664'], ['C154945302', '0.67608947'], ['C36464697', '0.6005778'], ['C64876066', '0.55756557']], 'referenced_works': ['W1566135517', 'W1677182931', 'W1861492603', 'W2006217757', 'W2007653981', 'W2017814585', 'W2031342017', 'W2031489346', 'W2038765747', 'W2062118960', 'W2064675550', 'W2070148066', 'W2081580037', 'W2087189381', 'W2097117768', 'W2108598243', 'W2112796928', 'W2117539524', 'W2145607950', 'W2147625498', 'W2152161678', 'W2162915993', 'W2166049352', 'W2194775991', 'W2257979135', 'W2277195237', 'W2340897893', 'W2911296969'], 'abstract': 'The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.', 'counts_by_year': [[2022, 229], [2021, 528], [2020, 454], [2019, 297], [2018, 152], [2017, 26], [2016, 1]]}, {'id': 'W2770026599', 'doi': 'https://doi.org/10.1093/nar/gkx1153', 'title': 'ClinVar: improving access to variant interpretations and supporting evidence', 'type': 'journal-article', 'publication_date': '2018-01-04', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2085829377', ['I4210109390']], ['A2577548852', ['I4210109390']], ['A2331041094', ['I4210109390']], ['A2130117684', ['I4210109390']], ['A2531169227', ['I4210109390']], ['A2530906326', ['I4210109390']], ['A2329829249', ['I4210109390']], ['A2104614090', ['I4210109390']], ['A2101223612', ['I4210109390']], ['A2654673092', ['I4210109390']], ['A2011796893', ['I4210109390']], ['A2134053873', ['I4210109390']], ['A2123797254', ['I4210109390']], ['A2768757396', ['I4210109390']], ['A3051233757', ['I4210109390']], ['A2768709592', ['I4210109390']], ['A313403037', ['I4210109390']], ['A2064526041', ['I4210109390']], ['A2802112202', ['I4210109390']], ['A2131562591', ['I4210109390']], ['A3053492186', ['I4210109390']], ['A370127644', ['I4210109390']]], 'cited_by_count': 1692, 'concepts': [['C177264268', '0.6410884'], ['C527412718', '0.5473157'], ['C23123220', '0.52780235'], ['C2780673598', '0.47315085'], ['C86803240', '0.4478041']], 'referenced_works': ['W2037458235', 'W2096525273', 'W2104549677', 'W2112589398', 'W2149992227', 'W2154866190', 'W2162151166', 'W2174602966', 'W2256016639', 'W2320983896', 'W2542446037', 'W2557385283', 'W4238718120', 'W4251386234'], 'abstract': "ClinVar (https://www.ncbi.nlm.nih.gov/clinvar/) is a freely available, public archive of human genetic variants and interpretations of their significance to disease, maintained at the National Institutes of Health. Interpretations of the clinical significance of variants are submitted by clinical testing laboratories, research laboratories, expert panels and other groups. ClinVar aggregates data by variant-disease pairs, and by variant (or set of variants). Data aggregated by variant are accessible on the website, in an improved set of variant call format files and as a new comprehensive XML report. ClinVar recently started accepting submissions that are focused primarily on providing phenotypic information for individuals who have had genetic testing. Submissions may come from clinical providers providing their own interpretation of the variant ('provider interpretation') or from groups such as patient registries that primarily provide phenotypic information from patients ('phenotyping only'). ClinVar continues to make improvements to its search and retrieval functions. Several new fields are now indexed for more precise searching, and filters allow the user to narrow down a large set of search results.", 'counts_by_year': [[2022, 448], [2021, 524], [2020, 382], [2019, 264], [2018, 71], [2017, 2]]}, {'id': 'W2979750740', 'doi': 'https://doi.org/10.1145/3326362', 'title': 'Dynamic Graph CNN for Learning on Point Clouds', 'type': 'journal-article', 'publication_date': '2019-10-10', 'host_venue': 'V185367456', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2784418645', ['I63966007']], ['A2785186749', ['I63966007']], ['A2149913060', ['I888304637']], ['A2170183435', ['I63966007']], ['A2076464609', ['I47508984']], ['A2129693131', ['I63966007']]], 'cited_by_count': 1683, 'concepts': [['C131979681', '0.8845817'], ['C41008148', '0.7752602'], ['C2776359362', '0.56110936'], ['C81363708', '0.5183931'], ['C41608201', '0.5015137']], 'referenced_works': ['W1644641054', 'W1992850481', 'W2007200979', 'W2047161559', 'W2072723786', 'W2091791686', 'W2098764590', 'W2099606917', 'W2101491865', 'W2102402541', 'W2116341502', 'W2139114878', 'W2143357187', 'W2147800946', 'W2160821342', 'W2165414070', 'W2173758409', 'W2203450678', 'W2211722331', 'W2460657278', 'W2518780089', 'W2553307952', 'W2558748708', 'W2603429625', 'W2606202972', 'W2728183739', 'W2737081152', 'W2766448241', 'W2768308213', 'W2788158258', 'W2796426482', 'W2962865163', 'W2962887844', 'W2963021451', 'W2963091558', 'W2964253930', 'W3103830808', 'W3117804044', 'W3122159272'], 'abstract': 'Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks including ModelNet40, ShapeNetPart, and S3DIS.', 'counts_by_year': [[2022, 343], [2021, 684], [2020, 514], [2019, 123], [2018, 17]]}, {'id': 'W2376573086', 'doi': 'https://doi.org/10.1093/nar/gkw408', 'title': 'ConSurf 2016: an improved methodology to estimate and visualize evolutionary conservation in macromolecules', 'type': 'journal-article', 'publication_date': '2016-07-08', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2017403194', ['I16391192']], ['A2126679614', ['I16391192']], ['A2152645382', ['I24603500']], ['A2116692207', ['I16391192']], ['A283707518', ['I16391192']], ['A2051963343', ['I16391192']], ['A1341781754', ['I16391192']]], 'cited_by_count': 1676, 'concepts': [['C86803240', '0.69279504'], ['C193252679', '0.6887089'], ['C199216141', '0.53871375'], ['C70721500', '0.53502584'], ['C24107716', '0.516659']], 'referenced_works': ['W1525734744', 'W1775447850', 'W1900110634', 'W1928030931', 'W1965582988', 'W1967293793', 'W1969042907', 'W1973035681', 'W1975316279', 'W1975563330', 'W1983379765', 'W1987407788', 'W2001650493', 'W2004945910', 'W2009596137', 'W2011703027', 'W2017519756', 'W2024341709', 'W2031053199', 'W2035503835', 'W2036149515', 'W2040481795', 'W2051210555', 'W2055043387', 'W2062018285', 'W2065014822', 'W2065283382', 'W2069663555', 'W2077539484', 'W2085946679', 'W2086561953', 'W2087918275', 'W2096702350', 'W2101220662', 'W2102461176', 'W2106116715', 'W2108230379', 'W2109228802', 'W2110062635', 'W2113024639', 'W2114886480', 'W2116099439', 'W2116171576', 'W2119498937', 'W2122057759', 'W2123956542', 'W2130479394', 'W2130520896', 'W2132632499', 'W2137991504', 'W2139134537', 'W2139881678', 'W2142635246', 'W2150444353', 'W2155469822', 'W2156125289', 'W2157122545', 'W2157237745', 'W2160378127', 'W2162574056', 'W2163352757', 'W2163627198', 'W2168621448', 'W2170747616', 'W2172057351', 'W2184115732', 'W2265709850', 'W2266439690', 'W2282925852', 'W4236236547', 'W4254881066'], 'abstract': 'The degree of evolutionary conservation of an amino acid in a protein or a nucleic acid in DNA/RNA reflects a balance between its natural tendency to mutate and the overall need to retain the structural integrity and function of the macromolecule. The ConSurf web server (http://consurf.tau.ac.il), established over 15 years ago, analyses the evolutionary pattern of the amino/nucleic acids of the macromolecule to reveal regions that are important for structure and/or function. Starting from a query sequence or structure, the server automatically collects homologues, infers their multiple sequence alignment and reconstructs a phylogenetic tree that reflects their evolutionary relations. These data are then used, within a probabilistic framework, to estimate the evolutionary rates of each sequence position. Here we introduce several new features into ConSurf, including automatic selection of the best evolutionary model used to infer the rates, the ability to homology-model query proteins, prediction of the secondary structure of query RNA molecules from sequence, the ability to view the biological assembly of a query (in addition to the single chain), mapping of the conservation grades onto 2D RNA models and an advanced view of the phylogenetic tree that enables interactively rerunning ConSurf with the taxa of a sub-tree.', 'counts_by_year': [[2022, 355], [2021, 405], [2020, 370], [2019, 256], [2018, 181], [2017, 95], [2016, 13]]}, {'id': 'W2206784480', 'doi': 'https://doi.org/10.1038/nnano.2015.230', 'title': 'Improved air stability of perovskite solar cells via solution-processed metal oxide transport layers', 'type': 'journal-article', 'publication_date': '2016-01-01', 'host_venue': 'V7822423', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2147368131', ['I161318765']], ['A2603505115', ['I161318765']], ['A2142349595', ['I161318765']], ['A2130812621', ['I91807558']], ['A2296726285', ['I161318765']], ['A2145553049', ['I161318765']], ['A2107526240', ['I161318765']], ['A2155600919', ['I161318765']], ['A2097989134', ['I161318765']], ['A2989789867', ['I161318765']], ['A2689561600', ['I161318765']], ['A2102854743', ['I161318765']]], 'cited_by_count': 1671, 'concepts': [['C155011858', '0.8277768'], ['C192562407', '0.7667198'], ['C74575197', '0.7089097'], ['C171560689', '0.62824124'], ['C32737372', '0.58652425']], 'referenced_works': ['W1553661931', 'W1629273100', 'W1965464747', 'W1970810735', 'W1977957201', 'W1986401348', 'W1994663208', 'W1998211376', 'W1998343415', 'W2001574526', 'W2011472471', 'W2030106935', 'W2035920177', 'W2038722395', 'W2041863854', 'W2042468043', 'W2045513766', 'W2054128950', 'W2055942760', 'W2056333199', 'W2057120802', 'W2057327293', 'W2067910821', 'W2070589101', 'W2078807245', 'W2081640113', 'W2095477500', 'W2100716359', 'W2102961544', 'W2102969048', 'W2104708743', 'W2106706072', 'W2112517274', 'W2114118829', 'W2116027828', 'W2117473879', 'W2119489545', 'W2125794096', 'W2131741190', 'W2133034794', 'W2141956602', 'W2144574847', 'W2147527687', 'W2154311504', 'W2159358478', 'W2169328609', 'W2317841632', 'W2325797939'], 'abstract': 'Lead halide perovskite solar cells have recently attracted tremendous attention because of their excellent photovoltaic efficiencies. However, the poor stability of both the perovskite material and the charge transport layers has so far prevented the fabrication of devices that can withstand sustained operation under normal conditions. Here, we report a solution-processed lead halide perovskite solar cell that has p-type NiO(x) and n-type ZnO nanoparticles as hole and electron transport layers, respectively, and shows improved stability against water and oxygen degradation when compared with devices with organic charge transport layers. Our cells have a p-i-n structure (glass/indium tin oxide/NiO(x)/perovskite/ZnO/Al), in which the ZnO layer isolates the perovskite and Al layers, thus preventing degradation. After 60 days storage in air at room temperature, our all-metal-oxide devices retain about 90% of their original efficiency, unlike control devices made with organic transport layers, which undergo a complete degradation after just 5 days. The initial power conversion efficiency of our devices is 14.6 ± 1.5%, with an uncertified maximum value of 16.1%.', 'counts_by_year': [[2022, 124], [2021, 199], [2020, 254], [2019, 268], [2018, 351], [2017, 293], [2016, 178], [2015, 3]]}, {'id': 'W2884367402', 'doi': 'https://doi.org/10.1109/tnnls.2018.2876865', 'title': 'Object Detection With Deep Learning: A Review', 'type': 'journal-article', 'publication_date': '2019-01-28', 'host_venue': 'V4210175523', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2194538157', ['I16365422']], ['A2397434390', ['I16365422']], ['A2883229311', ['I16365422']], ['A2123651450', ['I79516672']]], 'cited_by_count': 1671, 'concepts': [['C2776151529', '0.8328968'], ['C41008148', '0.8249961'], ['C154945302', '0.74238706'], ['C108583219', '0.73540115'], ['C81363708', '0.63475645']], 'referenced_works': ['W7746136', 'W118296737', 'W204268067', 'W204612701', 'W261873710', 'W318792885', 'W345900524', 'W639708223', 'W845365781', 'W1475617732', 'W1484953274', 'W1498436455', 'W1536680647', 'W1565402342', 'W1578066333', 'W1689909837', 'W1745334888', 'W1849007038', 'W1903029394', 'W1903127635', 'W1908020446', 'W1909234690', 'W1924762813', 'W1932624639', 'W1934410531', 'W1942214758', 'W1947031653', 'W1960182310', 'W1970456555', 'W1970850659', 'W1976818984', 'W1981902088', 'W1985436611', 'W1988790447', 'W1991367009', 'W1993164181', 'W1996326832', 'W2002781701', 'W2011900468', 'W2014854862', 'W2020236530', 'W2031454541', 'W2031489346', 'W2032256331', 'W2061883802', 'W2068730032', 'W2074777933', 'W2078903912', 'W2086791339', 'W2088049833', 'W2097117768', 'W2100495367', 'W2100807570', 'W2102605133', 'W2107634464', 'W2108598243', 'W2109255472', 'W2115579991', 'W2117287331', 'W2117606586', 'W2119112357', 'W2122768015', 'W2123099218', 'W2123229215', 'W2125556102', 'W2131774270', 'W2137110664', 'W2140090057', 'W2140262144', 'W2147347517', 'W2151103935', 'W2155893237', 'W2159686933', 'W2160815625', 'W2161106546', 'W2161185676', 'W2161381512', 'W2161565164', 'W2161969291', 'W2162741153', 'W2162915993', 'W2166623283', 'W2167998037', 'W2168013545', 'W2168356304', 'W2171378720', 'W2183341477', 'W2191835017', 'W2194775991', 'W2200528286', 'W2209882149', 'W2212216676', 'W2216125271', 'W2247274765', 'W2288122362', 'W2288514685', 'W2293078015', 'W2293332611', 'W2302502886', 'W2315287060', 'W2322112093', 'W2322480645', 'W2334805829', 'W2338972621', 'W2342242867', 'W2417750831', 'W2436544366', 'W2469885745', 'W2472480899', 'W2473640056', 'W2474389331', 'W2490270993', 'W2497039038', 'W2507697802', 'W2511589228', 'W2518599539', 'W2519205375', 'W2519810307', 'W2531915888', 'W2532752915', 'W2549139847', 'W2551403050', 'W2555182955', 'W2555618208', 'W2557728737', 'W2557827245', 'W2559085405', 'W2565639579', 'W2579152745', 'W2601564443', 'W2606884824', 'W2608567823', 'W2608858501', 'W2736384776', 'W2740569051', 'W2744613561', 'W2750432752', 'W2771283653', 'W2778652957', 'W2790852735', 'W2919115771', 'W2962699453', 'W2962917547', 'W2962992847', 'W2963037989', 'W2963150697', 'W2963188557', 'W2963418361', 'W2963516811', 'W2963635628', 'W2963690996', 'W2963770578', 'W2963813458', 'W2963828885', 'W2963873508', 'W2963960612', 'W2964050365', 'W2964095005', 'W2964297960', 'W2964327083', 'W3097096317', 'W3098090606', 'W3100591638', 'W3101276005', 'W3101998545', 'W3104979525', 'W4239059383', 'W4239147634', 'W4239510810'], 'abstract': "Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.", 'counts_by_year': [[2022, 493], [2021, 679], [2020, 387], [2019, 103], [2018, 2]]}, {'id': 'W2782708334', 'doi': 'https://doi.org/10.1038/nrd.2017.243', 'title': 'mRNA vaccines — a new era in vaccinology', 'type': 'journal-article', 'publication_date': '2018-01-12', 'host_venue': 'V186543748', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A1999490653', ['I79576946']], ['A2495726232', ['I79576946']], ['A2782805655', ['I170897317']], ['A2112772731', ['I79576946']]], 'cited_by_count': 1665, 'concepts': [['C70721500', '0.5060455'], ['C105580179', '0.46087128'], ['C71924100', '0.4384292'], ['C150903083', '0.38713652'], ['C86803240', '0.36102194']], 'referenced_works': ['W29899406', 'W80088001', 'W81209440', 'W144119440', 'W1215958058', 'W1420726270', 'W1522802573', 'W1538766821', 'W1594820532', 'W1596127779', 'W1784734551', 'W1813394114', 'W1833523230', 'W1858483258', 'W1895187084', 'W1915631065', 'W1956613997', 'W1963706689', 'W1964435104', 'W1965174032', 'W1966322307', 'W1966657425', 'W1971250275', 'W1972263472', 'W1974619894', 'W1977254423', 'W1979058462', 'W1979734780', 'W1981552958', 'W1981821156', 'W1984619541', 'W1986939156', 'W1988620094', 'W1992513864', 'W1994310269', 'W1994674548', 'W1995580450', 'W1995770815', 'W2003128034', 'W2004037261', 'W2004979714', 'W2006826131', 'W2006942625', 'W2009042469', 'W2013637325', 'W2013999805', 'W2014058696', 'W2014495927', 'W2014992495', 'W2016063463', 'W2016786790', 'W2018134130', 'W2024266527', 'W2025980739', 'W2026083914', 'W2026485598', 'W2038976543', 'W2039422212', 'W2043424458', 'W2051091654', 'W2051242197', 'W2051665875', 'W2052820133', 'W2054298563', 'W2055676760', 'W2057294180', 'W2058647478', 'W2060260809', 'W2060696356', 'W2062398277', 'W2063287711', 'W2065338955', 'W2066681478', 'W2066847748', 'W2067716683', 'W2067948957', 'W2069390230', 'W2071136305', 'W2072499238', 'W2073582306', 'W2076359982', 'W2078965027', 'W2079696222', 'W2082099294', 'W2082706052', 'W2082871926', 'W2083777428', 'W2083958143', 'W2085928743', 'W2086050574', 'W2087193158', 'W2087439897', 'W2087578607', 'W2087987876', 'W2091052259', 'W2093626188', 'W2094845525', 'W2095950646', 'W2096039130', 'W2097556922', 'W2098448632', 'W2100004531', 'W2101764878', 'W2102797150', 'W2104401785', 'W2106595509', 'W2109280271', 'W2110852884', 'W2110949543', 'W2122718880', 'W2124933711', 'W2125677437', 'W2125708644', 'W2126082800', 'W2126621258', 'W2127576136', 'W2133512921', 'W2136720677', 'W2139511604', 'W2140031668', 'W2142082096', 'W2143743183', 'W2146604395', 'W2147763501', 'W2149280513', 'W2150490625', 'W2151962109', 'W2153049488', 'W2156788486', 'W2161707204', 'W2161890613', 'W2162541845', 'W2162970812', 'W2163517233', 'W2163659675', 'W2165198071', 'W2167614869', 'W2168071482', 'W2169758620', 'W2176000775', 'W2186068577', 'W2186472985', 'W2190124789', 'W2190427975', 'W2194336158', 'W2208560199', 'W2217258572', 'W2217833765', 'W2221797292', 'W2246249321', 'W2257602426', 'W2267284177', 'W2290646886', 'W2291366206', 'W2291625544', 'W2296598106', 'W2327688901', 'W2327804995', 'W2335434828', 'W2339000572', 'W2340049980', 'W2340267638', 'W2346414942', 'W2397894493', 'W2408880518', 'W2412158473', 'W2417233462', 'W2419348795', 'W2434872344', 'W2461517514', 'W2463310825', 'W2465126304', 'W2469369476', 'W2513236841', 'W2515313544', 'W2521176812', 'W2527863850', 'W2530273804', 'W2536529113', 'W2547626368', 'W2550082644', 'W2554094503', 'W2560416222', 'W2564437942', 'W2564846737', 'W2567689582', 'W2572160893', 'W2573033399', 'W2581994937', 'W2582690078', 'W2584249987', 'W2587794762', 'W2587850379', 'W2591114082', 'W2593990758', 'W2594455756', 'W2600170844', 'W2605016096', 'W2606625852', 'W2606935378', 'W2609075891', 'W2610471486', 'W2621841059', 'W2625571305', 'W2626337750', 'W2680440451', 'W2714911268', 'W2724264513', 'W2728365508', 'W2734414012', 'W2738515079', 'W2742522266', 'W2907941435', 'W4206563421', 'W4210574840', 'W4241462697', 'W4248794611', 'W4251825292'], 'abstract': 'mRNA vaccines represent a promising alternative to conventional vaccine approaches because of their high potency, capacity for rapid development and potential for low-cost manufacture and safe administration. However, their application has until recently been restricted by the instability and inefficient in vivo delivery of mRNA. Recent technological advances have now largely overcome these issues, and multiple mRNA vaccine platforms against infectious diseases and several types of cancer have demonstrated encouraging results in both animal models and humans. This Review provides a detailed overview of mRNA vaccines and considers future directions and challenges in advancing this promising vaccine platform to widespread therapeutic use.', 'counts_by_year': [[2022, 575], [2021, 711], [2020, 246], [2019, 87], [2018, 43]]}, {'id': 'W3009468976', 'doi': 'https://doi.org/10.1016/s1473-3099(20)30144-4', 'title': 'Early dynamics of transmission and control of COVID-19: a mathematical modelling study', 'type': 'journal-article', 'publication_date': '2020-05-01', 'host_venue': 'V23772524', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2094073093', ['I4210089966']], ['A3005524366', ['I4210089966']], ['A3005493534', ['I4210089966']], ['A3007838758', ['I4210089966']], ['A2132272011', ['I4210089966']], ['A3001207621', ['I4210089966']], ['A1784943301', ['I4210089966']], ['A3009512101', []], ['A2231526214', ['I4210089966']], ['A2889493969', ['I4210089966']], ['A2767609261', ['I4210089966']], ['A3006242220', []], ['A2889721592', ['I4210089966']], ['A3009321980', ['I4210089966']], ['A2765635069', ['I4210089966']], ['A2644929985', ['I4210089966']], ['A1966109781', ['I4210089966']], ['A3004279427', ['I4210089966']], ['A3006236200', []], ['A2835261579', []], ['A2777073030', ['I4210089966']]], 'cited_by_count': 1665, 'concepts': [['C761482', '0.8500869'], ['C116675565', '0.76293516'], ['C3008058167', '0.64410836'], ['C3007834351', '0.58667505'], ['C71924100', '0.40032184']], 'referenced_works': ['W1042757214', 'W1990990266', 'W2069251911', 'W2096145431', 'W2102842925', 'W2110943126', 'W2145603505', 'W2146252062', 'W2607150872', 'W2747968860', 'W2963358957', 'W3003668884', 'W3003773899', 'W3004026249', 'W3004239190', 'W4210710163'], 'abstract': '<h2>Summary</h2><h3>Background</h3> An outbreak of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has led to 95 333 confirmed cases as of March 5, 2020. Understanding the early transmission dynamics of the infection and evaluating the effectiveness of control measures is crucial for assessing the potential for sustained transmission to occur in new areas. Combining a mathematical model of severe SARS-CoV-2 transmission with four datasets from within and outside Wuhan, we estimated how transmission in Wuhan varied between December, 2019, and February, 2020. We used these estimates to assess the potential for sustained human-to-human transmission to occur in locations outside Wuhan if cases were introduced. <h3>Methods</h3> We combined a stochastic transmission model with data on cases of coronavirus disease 2019 (COVID-19) in Wuhan and international cases that originated in Wuhan to estimate how transmission had varied over time during January, 2020, and February, 2020. Based on these estimates, we then calculated the probability that newly introduced cases might generate outbreaks in other areas. To estimate the early dynamics of transmission in Wuhan, we fitted a stochastic transmission dynamic model to multiple publicly available datasets on cases in Wuhan and internationally exported cases from Wuhan. The four datasets we fitted to were: daily number of new internationally exported cases (or lack thereof), by date of onset, as of Jan 26, 2020; daily number of new cases in Wuhan with no market exposure, by date of onset, between Dec 1, 2019, and Jan 1, 2020; daily number of new cases in China, by date of onset, between Dec 29, 2019, and Jan 23, 2020; and proportion of infected passengers on evacuation flights between Jan 29, 2020, and Feb 4, 2020. We used an additional two datasets for comparison with model outputs: daily number of new exported cases from Wuhan (or lack thereof) in countries with high connectivity to Wuhan (ie, top 20 most at-risk countries), by date of confirmation, as of Feb 10, 2020; and data on new confirmed cases reported in Wuhan between Jan 16, 2020, and Feb 11, 2020. <h3>Findings</h3> We estimated that the median daily reproduction number (<i>R</i><sub>t</sub>) in Wuhan declined from 2·35 (95% CI 1·15–4·77) 1 week before travel restrictions were introduced on Jan 23, 2020, to 1·05 (0·41–2·39) 1 week after. Based on our estimates of <i>R</i><sub>t</sub>, assuming SARS-like variation, we calculated that in locations with similar transmission potential to Wuhan in early January, once there are at least four independently introduced cases, there is a more than 50% chance the infection will establish within that population. <h3>Interpretation</h3> Our results show that COVID-19 transmission probably declined in Wuhan during late January, 2020, coinciding with the introduction of travel control measures. As more cases arrive in international locations with similar transmission potential to Wuhan before these control measures, it is likely many chains of transmission will fail to establish initially, but might lead to new outbreaks eventually. <h3>Funding</h3> Wellcome Trust, Health Data Research UK, Bill & Melinda Gates Foundation, and National Institute for Health Research.', 'counts_by_year': [[2022, 253], [2021, 585], [2020, 822]]}, {'id': 'W3092861045', 'doi': 'https://doi.org/10.1016/s0140-6736(20)30752-2', 'title': 'Global burden of 87 risk factors in 204 countries and territories, 1990–2019: a systematic analysis for the Global Burden of Disease Study 2019', 'type': 'journal-article', 'publication_date': '2020-10-17', 'host_venue': 'V49861241', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2316804223', ['I149251103']], ['A702408767', ['I149251103']], ['A2624032331', ['I149251103']], ['A1261223370', ['I149251103']], ['A2104043555', ['I124357947']], ['A2592738240', ['I149251103']], ['A2167087174', ['I145487455']], ['A3132293591', ['I145487455']], ['A2093499980', []], ['A317332464', ['I114982161']], ['A3034760013', []], ['A986168551', []], ['A275226674', []], ['A2342636326', ['I110200422']], ['A1964166387', ['I149251103']], ['A3045418908', ['I145487455']], ['A1950551245', ['I149251103']], ['A2280831594', []], ['A2263730145', ['I112312016']], ['A2248880431', ['I79510175']], ['A2128175525', ['I149251103']], ['A2755910248', ['I26092322']], ['A2347668881', ['I33103891']], ['A2131761341', []], ['A2163139810', ['I98251732']], ['A2251750973', ['I149251103']], ['A2151274743', []], ['A2422948822', ['I76569877']], ['A2440913922', ['I149251103']], ['A2243815348', ['I119939603']], ['A1244030808', ['I58048189']], ['A2132116411', []], ['A3140310613', ['I58048189']], ['A2546993401', []], ['A1865820898', ['I149251103']], ['A2569618614', ['I31746571']], ['A3001219971', []], ['A1968471205', ['I149251103']], ['A2343865831', []], ['A2279206299', ['I149251103']], ['A2186295676', ['I129902397']], ['A2754876412', ['I149251103']], ['A2999095571', ['I70640408']], ['A661629491', ['I149251103']], ['A3092169221', ['I42869670']], ['A3131107898', ['I28022161']], ['A3183122766', ['I12469534']], ['A3211988652', ['I47818738']], ['A3160122403', ['I149251103']], ['A2297801856', ['I17040458']], ['A2744791727', ['I149251103']], ['A1963515988', ['I149251103']], ['A2568822994', []], ['A3205550469', ['I15057530']], ['A2990686986', ['I28166907']], ['A3206004986', ['I133903919']], ['A2056562052', []], ['A2161907566', []], ['A2318996425', ['I28022161']], ['A2611880511', ['I114982161']], ['A2259421809', ['I149251103']], ['A2066760853', ['I149251103']], ['A2192027544', ['I149251103']], ['A2244188920', []], ['A2899698624', ['I149251103']], ['A2766346432', ['I149251103']], ['A1717678991', ['I88491126']], ['A2139329972', ['I91136226']], ['A2899762222', ['I86959772']], ['A2678734198', []], ['A1942997021', ['I149251103']], ['A2284490517', ['I149251103']], ['A2109138503', []], ['A3080688946', ['I149251103']], ['A174892660', ['I119896790']], ['A2790545250', []], ['A2560702751', ['I149251103']], ['A1607629799', []], ['A3037635098', ['I149251103']], ['A2519653036', ['I95990688']], ['A2496686921', ['I149251103']], ['A2234111569', []], ['A174217572', []], ['A2713334294', []], ['A2567127394', ['I149251103']], ['A1998606146', ['I149251103']], ['A3081295288', ['I149251103']], ['A3093198643', ['I149251103']], ['A1941987087', ['I39642139']], ['A3047004814', ['I149251103']], ['A2737325400', ['I149251103']], ['A3001610902', ['I149251103']], ['A2190077523', []], ['A2413366599', ['I149251103']], ['A2680723830', ['I149251103']], ['A2286493915', ['I149251103']], ['A2684321329', ['I149251103']], ['A2124052696', ['I149251103']], ['A3092969083', ['I8764889']], ['A2016239449', []], ['A2047305168', ['I133978751']], ['A2489420246', ['I149251103']], ['A2298751457', ['I112312016']], ['A2165921985', ['I149251103']], ['A228823166', ['I76414455']], ['A276270304', ['I149251103']], ['A2120748957', ['I149251103']], ['A2075268468', ['I145311948']], ['A3092762050', ['I149251103']], ['A1847310112', []], ['A2291403467', ['I80046288']], ['A2766816238', ['I42869670']], ['A3148601581', ['I58048189']], ['A3211591706', ['I149251103']], ['A2147005616', []], ['A2170988738', ['I149251103']], ['A2899595883', ['I149251103']], ['A2282049701', ['I149251103']], ['A2273669580', []], ['A3211961933', []], ['A2048019866', ['I149251103']], ['A3092671736', ['I114027177']], ['A2108298034', []], ['A1895530724', ['I149251103']], ['A2739217271', ['I149251103']], ['A3045100366', ['I32971472']], ['A3015806597', ['I149251103']], ['A2082872497', ['I17974374']], ['A3092790355', ['I74813324']], ['A2181334434', ['I25041050']], ['A1411622931', []], ['A3018790728', []], ['A2029878285', []], ['A3081015367', []], ['A2901592919', []], ['A2132358048', []], ['A1247365415', ['I149251103']], ['A1954828074', ['I19630809']], ['A2064131949', []], ['A2104707218', ['I39063666']], ['A3080962220', []], ['A2980382099', ['I149251103']], ['A2298100572', ['I31746571']], ['A3152826022', ['I74813324']], ['A2460701962', ['I110525433']], ['A3018141721', []], ['A3092787467', ['I149251103']], ['A2117569299', ['I149251103']], ['A2749537039', ['I39063666']], ['A3154531805', ['I149251103']], ['A2153935060', ['I149251103']], ['A2010446097', ['I31746571']], ['A2201475530', ['I149251103']], ['A678399540', ['I52357470']], ['A1977326363', []], ['A3150344558', ['I17937529']], ['A2797513518', ['I149251103']], ['A2980431408', ['I149251103']], ['A3145810491', ['I17877952']], ['A2940770083', ['I149251103']], ['A3198046493', ['I149251103']], ['A2755448035', ['I149251103']], ['A2148589911', ['I149251103']], ['A2144113992', ['I149251103']], ['A2232777415', ['I149251103']], ['A3176002406', []], ['A2153099415', ['I5124864']], ['A2150892176', ['I149251103']], ['A2091075195', ['I149251103']], ['A2050079315', ['I149251103']], ['A2345520266', ['I149251103']], ['A2161230630', ['I149251103']], ['A2105920767', ['I28166907']], ['A2132407021', ['I149251103']], ['A1919109350', ['I17974374']], ['A2306491945', []], ['A2767466220', []], ['A2766350609', ['I149251103']], ['A2899841467', ['I149251103']], ['A2634061616', []], ['A2366254680', ['I149251103']], ['A148669243', ['I86695891']], ['A2622991323', ['I79619799']], ['A2784364142', []], ['A2123807283', ['I149251103']], ['A319497597', ['I149251103']], ['A3142864610', ['I149251103']], ['A223049404', ['I118347636']], ['A3148948282', ['I149251103']], ['A3034072322', ['I149251103']], ['A2527551833', ['I139264467']], ['A3151877692', ['I124055696']], ['A2605620317', ['I135822937']], ['A3010670714', ['I149251103']], ['A2980420995', []], ['A353686740', ['I56590836']], ['A2266351856', []], ['A3171716521', ['I71267560']], ['A3155426172', ['I20121455']], ['A2762626255', ['I149251103']], ['A3148488435', ['I149251103']], ['A2130564229', []], ['A2143990383', ['I149251103']], ['A2804530546', ['I130442723']], ['A2615882244', ['I26999989']], ['A3092666502', ['I149251103']], ['A3093027985', ['I149251103']], ['A2980369479', ['I149251103']], ['A3037315331', ['I16904388']], ['A2239647081', []], ['A3019388298', []], ['A344498437', []], ['A3092754776', ['I149251103']], ['A2142720096', ['I28166907']], ['A2076714150', []], ['A2892452542', ['I149251103']], ['A2606395416', ['I88085062']], ['A2170375430', ['I118185606']], ['A2559927947', []], ['A2568267441', []], ['A908145174', ['I149251103']], ['A1661044825', []], ['A3142923456', ['I11701301']], ['A2061034448', []], ['A3092728353', ['I114027177']], ['A692807803', []], ['A2904903833', ['I149251103']], ['A3156481852', ['I78545622']], ['A3152298627', ['I149251103']], ['A2439543002', ['I149251103']], ['A3186749891', []], ['A2969258463', ['I149251103']], ['A3213819420', ['I149251103']], ['A2163797418', ['I149251103']], ['A2165901917', ['I8961855']], ['A2209288412', ['I70640408']], ['A3216364009', ['I149251103']], ['A2996630738', ['I149251103']], ['A3213580640', ['I147017597']], ['A3001282785', ['I149251103']], ['A1968062500', ['I149251103']], ['A2171790375', ['I129604602']], ['A2304245645', ['I130442723']], ['A2084326502', []], ['A2727222648', ['I121934306']], ['A9775719', []], ['A1987771207', ['I107720978']], ['A2519243818', ['I84524832']], ['A2714611908', ['I84524832']], ['A335245286', ['I117222138']], ['A3207537571', ['I149251103']], ['A2721765661', ['I145487455']], ['A2572132342', ['I149251103']], ['A3093138417', ['I149251103']], ['A3148828639', ['I149251103']], ['A1485388775', ['I70640408']], ['A3081178995', []], ['A1991156927', ['I149251103']], ['A2139978954', ['I149251103']], ['A2096652101', ['I149251103']], ['A3093197128', ['I149251103']], ['A1916184962', ['I145311948']], ['A2288262439', ['I149251103']], ['A2081665819', ['I149251103']], ['A1329768574', ['I88085062']], ['A1998511410', ['I9360294']], ['A2147768707', ['I149251103']], ['A2162545637', ['I149251103']], ['A2170374040', ['I70640408']], ['A2036794182', ['I39642139']], ['A2029153046', ['I33103891']], ['A130753112', []], ['A2899620150', ['I149251103']], ['A2599128148', []], ['A2160926455', ['I149251103']], ['A2899962804', ['I149251103']], ['A3156606865', []], ['A2133453768', ['I149251103']], ['A2517957986', []], ['A2239987130', ['I149251103']], ['A2171340180', ['I52357470']], ['A2102600385', ['I52325']], ['A2953371355', ['I149251103']], ['A2082083326', ['I45711476']], ['A1711691611', ['I149251103']], ['A2791323970', ['I149251103']], ['A2289990677', ['I149251103']], ['A2150511306', ['I86467917']], ['A3145015918', ['I149251103']], ['A3081464985', ['I149251103']], ['A2138052936', ['I65837984']], ['A2160364156', ['I17974374']], ['A2793752532', []], ['A2309765197', ['I149251103']], ['A2091590686', ['I149251103']], ['A419432919', []], ['A3118368217', ['I149251103']], ['A3017578368', ['I149251103']], ['A3093429808', ['I74813324']], ['A3093422416', ['I149251103']], ['A3093062147', ['I149251103']], ['A1971808073', []], ['A3026966797', ['I145311948']], ['A2135025260', []], ['A1860161057', ['I149251103']], ['A2436947065', ['I149251103']], ['A2054052856', ['I133903919']], ['A2788381763', ['I149251103']], ['A3167334697', ['I23923803']], ['A2294267312', ['I96673099']], ['A2965213727', ['I110525433']], ['A2900037740', ['I149251103']], ['A2174002162', ['I39555362']], ['A3147997541', []], ['A3028938137', ['I149251103']], ['A2700788008', ['I149251103']], ['A1975833861', ['I149251103']], ['A2409117092', []], ['A1822146201', ['I149251103']], ['A3149824608', ['I149251103']], ['A2136108649', ['I17974374']], ['A2088182740', []], ['A219780634', []], ['A2153277603', ['I39063666']], ['A2747377998', ['I64952554']], ['A2993367841', ['I149251103']], ['A2100946908', ['I68106152']], ['A3081450916', ['I149251103']], ['A3203234683', ['I149251103']], ['A2257103434', []], ['A2295826387', []], ['A2143261986', ['I94234084']], ['A3092773287', ['I149251103']], ['A1934246530', []], ['A2150717372', []], ['A3205739317', ['I145311948']], ['A3111784969', ['I149251103']], ['A2154481655', ['I149251103']], ['A2046123198', ['I149251103']], ['A2003644413', ['I53218197']], ['A2170943676', ['I102239671']], ['A3148803798', []], ['A2087097567', []], ['A2353328467', ['I149251103']], ['A2991944912', []], ['A3034544279', ['I149251103']], ['A3152638361', ['I149251103']], ['A3154981392', ['I143318147']], ['A3080859470', ['I149251103']], ['A2395864894', ['I149251103']], ['A2434796725', ['I149251103']], ['A1975482828', ['I149251103']], ['A2159017802', ['I149251103']], ['A2980471384', []], ['A3080981755', ['I149251103']], ['A2902222759', ['I149251103']], ['A3092718704', ['I149251103']], ['A3093026787', ['I149251103']], ['A2237597403', []], ['A2883371677', ['I39642139']], ['A2904371025', ['I51601045']], ['A2159703735', []], ['A2278135407', ['I76414455']], ['A2894101377', []], ['A1684365439', ['I19772626']], ['A2616075654', ['I149251103']], ['A3152613557', ['I149251103']], ['A2021199147', []], ['A2000709273', ['I149251103']], ['A110332668', []], ['A2085405621', ['I149251103']], ['A2949777497', ['I108714496']], ['A2305670555', ['I149251103']], ['A2113762876', ['I139660479']], ['A2887397395', ['I76130692']], ['A2161247198', []], ['A2637424415', ['I149251103']], ['A2425928874', ['I149251103']], ['A3147841791', ['I149251103']], ['A3176079125', ['I149251103']], ['A2969695656', ['I149251103']], ['A3186442776', ['I98285908']], ['A2980641198', ['I149251103']], ['A2118386970', ['I149251103']], ['A2797711522', ['I4068193']], ['A2150631237', ['I14245010']], ['A2479450839', ['I149251103']], ['A3093328934', ['I149251103']], ['A2111317264', []], ['A2804510925', ['I58048189']], ['A2756137649', ['I149251103']], ['A2099608108', ['I149251103']], ['A2144421627', ['I129604602']], ['A2061012529', ['I98285908']], ['A1693664124', ['I31746571']], ['A2983167631', ['I114499477']], ['A3152595819', ['I10947320']], ['A3092798150', ['I149251103']], ['A1897766453', ['I149251103']], ['A2165675880', ['I149251103']], ['A3092952890', ['I149251103']], ['A2073342092', ['I114982161']], ['A3146001028', ['I149251103']], ['A2090105886', []], ['A1945605689', ['I39642139']], ['A2142404985', ['I149251103']], ['A2653706514', ['I39642139']], ['A2053193323', ['I111088046']], ['A3205350661', ['I88085062']], ['A2230245500', ['I149251103']], ['A1865358906', ['I112312016']], ['A2803303234', []], ['A2123248410', ['I31746571']], ['A2170837810', ['I149251103']], ['A2438219672', ['I149251103']], ['A2114799707', ['I31746571']], ['A2013773010', ['I149251103']], ['A2145525382', ['I149251103']], ['A1900504705', []], ['A2061138801', ['I149251103']], ['A3208768073', ['I149251103']], ['A2048675361', ['I56085075']], ['A3088715257', ['I149251103']], ['A2133846596', ['I27577105']], ['A2329202034', ['I149251103']], ['A2056724151', ['I149251103']], ['A2747704814', ['I149251103']], ['A2076570182', ['I64532579']], ['A52183390', ['I63739035']], ['A2144883790', ['I133978751']], ['A1965717446', ['I39642139']], ['A2058870430', ['I22465464']], ['A2611126429', []], ['A2412726901', ['I149251103']], ['A2146891038', ['I7882870']], ['A1977521090', []], ['A1747172840', ['I39642139']], ['A3205448662', ['I12859529']], ['A3093086580', ['I149251103']], ['A2576156219', ['I149251103']], ['A3033239700', ['I149251103']], ['A2305436410', ['I149251103']], ['A1904862464', ['I149251103']], ['A2468155649', ['I43763821']], ['A3087624815', ['I149251103']], ['A1988637850', []], ['A2803581596', ['I145487455']], ['A2111570802', ['I149251103']], ['A2298094106', []], ['A2107703756', ['I149251103']], ['A2107678894', []], ['A3177409340', ['I149251103']], ['A3209956481', ['I149251103']], ['A706716812', ['I149251103']], ['A2210300720', ['I55464072']], ['A1969029766', ['I45129253']], ['A1169616326', ['I149251103']], ['A2192301710', ['I149251103']], ['A3145548121', ['I149251103']], ['A2309303682', ['I149251103']], ['A3152219568', ['I149251103']], ['A3017985799', ['I17877952']], ['A2591497774', ['I149251103']], ['A2087695571', []], ['A2128727216', ['I40120149']], ['A1901047912', ['I51452335']], ['A2980578169', ['I149251103']], ['A2131372770', ['I149251103']], ['A256209369', ['I70931966']], ['A2137545149', ['I149251103']], ['A2330134789', ['I136199984']], ['A1966636388', ['I98251732']], ['A2960357458', []], ['A3009577171', ['I149251103']], ['A2432171515', ['I34931013']], ['A2527007444', ['I149251103']], ['A2038807793', ['I149251103']], ['A2900504233', ['I149251103']], ['A2199730884', ['I135120706']], ['A2696553595', ['I149251103']], ['A2317931897', ['I149251103']], ['A1998151517', ['I124357947']], ['A2994390141', ['I123387679']], ['A2006036672', ['I17647740']], ['A2254161006', ['I149251103']], ['A2440267422', ['I149251103']], ['A2304074508', ['I14243506']], ['A2168895159', []], ['A3018424551', ['I149251103']], ['A2095606354', ['I149251103']], ['A3144255890', ['I149251103']], ['A3093292303', ['I149251103']], ['A2139270141', ['I149251103']], ['A2101549818', ['I149251103']], ['A3191976838', ['I56590836']], ['A2398601361', []], ['A2565902731', ['I91203450']], ['A2950852947', ['I149251103']], ['A3212054737', ['I27804330']], ['A2776297621', ['I149251103']], ['A3157514727', ['I149251103']], ['A2128434403', []], ['A2754494147', []], ['A2981397924', ['I17877952']], ['A2562465928', []], ['A2040328808', ['I17974374']], ['A3081056943', ['I149251103']], ['A2169496855', ['I149251103']], ['A2756296023', ['I149251103']], ['A2959174318', ['I149251103']], ['A2157698234', ['I87216513']], ['A2003908840', ['I87216513']], ['A2133394427', ['I47508984']], ['A2143396853', ['I149251103']], ['A3019876890', ['I149251103']], ['A1631637737', []], ['A1964422715', ['I110200422']], ['A2103078029', ['I149251103']], ['A2924877840', ['I149251103']], ['A2756291383', ['I149251103']], ['A2085943431', ['I149251103']], ['A2199401248', ['I39642139']], ['A2137998209', ['I149251103']], ['A2513397079', ['I149251103']], ['A3145709911', ['I149251103']], ['A2899831046', ['I149251103']], ['A2146427514', []], ['A2489473001', []], ['A1286028393', ['I149251103']], ['A2661072675', ['I149251103']], ['A2562518373', ['I149251103']], ['A3125562711', []], ['A2171983489', ['I145311948']], ['A2192121978', []], ['A3211925812', ['I149251103']], ['A2106301102', []], ['A2250989391', ['I124357947']], ['A3179492833', []], ['A3080089148', ['I149251103']], ['A2131922514', ['I149251103']], ['A202526925', []], ['A2102508135', ['I149251103']], ['A2256690567', ['I149251103']], ['A3018208011', ['I149251103']], ['A3092686090', []], ['A2320193999', ['I1443707']], ['A1311980873', []], ['A2598037195', ['I133731052']], ['A1919879098', []], ['A2900380420', []], ['A2419348531', ['I149251103']], ['A2890010402', ['I97565354']], ['A2109432494', ['I149251103']], ['A2120530206', ['I98251732']], ['A2594237171', []], ['A2297503242', ['I149251103']], ['A2805022591', ['I88491126']], ['A2023451452', []], ['A3205540517', ['I149251103']], ['A3206500226', ['I149251103']], ['A3205535711', []], ['A2019052684', ['I39642139']], ['A2217126065', ['I149251103']], ['A2039154347', ['I76414455']], ['A1987651379', ['I5237613']], ['A2110114893', []], ['A2097591507', ['I28022161']], ['A2964370475', ['I149251103']], ['A2003070898', ['I149251103']], ['A1981214840', ['I114982161']], ['A2891273749', ['I149251103']], ['A2980325716', ['I149251103']], ['A3093078139', ['I149251103']], ['A2980547539', ['I149251103']], ['A2126042519', ['I12789410']], ['A2287785407', ['I149251103']], ['A2292854733', ['I149251103']], ['A224987146', ['I149251103']], ['A2796883702', ['I149251103']], ['A2099977418', ['I149251103']], ['A2121066858', ['I39642139']], ['A2072898194', ['I149251103']], ['A2570985760', ['I133903919']], ['A2696008654', ['I51601045']], ['A2078422342', ['I149251103']], ['A2900096527', ['I149251103']], ['A2156641128', ['I149251103']], ['A2238764242', ['I149251103']], ['A2750470960', ['I149251103']], ['A2633131436', ['I149251103']], ['A2099881019', ['I149251103']], ['A2132822911', ['I149251103']], ['A2608759177', []], ['A2222871926', ['I149251103']], ['A1460372564', ['I149251103']], ['A2134948551', ['I139322472']], ['A2581219073', ['I74813324']], ['A1998687911', []], ['A3115008955', ['I149251103']], ['A2984963095', []], ['A2169326384', ['I149251103']], ['A2113265520', ['I149251103']], ['A2572786626', ['I112312016']], ['A3000826672', []], ['A3177463724', ['I149251103']], ['A2059048655', ['I149251103']], ['A2511179059', []], ['A2752679644', ['I149251103']], ['A2099916099', ['I76414455']], ['A1809145585', []], ['A2013758883', ['I149251103']], ['A2133215214', ['I149251103']], ['A2570005848', ['I149251103']], ['A3090495659', ['I149251103']], ['A2004487417', []], ['A2981616599', ['I149251103']], ['A2900026058', ['I149251103']], ['A2901189574', ['I10947320']], ['A2167229687', []], ['A3173968218', ['I149251103']], ['A2040971506', ['I5681781']], ['A2041938316', ['I28166907']], ['A8695191', ['I149251103']], ['A3092779702', ['I141595442']], ['A2011698556', ['I141595442']], ['A2899594322', ['I149251103']], ['A2529893633', ['I129604602']], ['A2598867768', ['I35928602']], ['A3080193408', ['I149251103']], ['A2230868571', ['I149251103']], ['A2201699675', []], ['A2045444862', ['I149251103']], ['A2527585475', ['I149251103']], ['A3080719767', ['I149251103']], ['A2899784136', ['I149251103']], ['A1235034958', ['I11892038']], ['A1074637623', []], ['A2288782178', []], ['A3034921230', ['I6406202']], ['A2164363926', []], ['A3093024011', ['I149251103']], ['A3004489217', ['I149251103']], ['A2900342250', ['I149251103']], ['A1993002079', []], ['A2130487955', ['I149251103']], ['A2894288472', ['I149251103']], ['A1896948145', ['I149251103']], ['A1986634445', []], ['A3211688282', ['I149251103']], ['A2530698201', ['I149251103']], ['A2098591421', []], ['A3093398046', ['I149251103']], ['A245889348', ['I149251103']], ['A2102343652', ['I149251103']], ['A2110511270', ['I67357951']], ['A2024441155', ['I149251103']], ['A3211998013', []], ['A3080891547', ['I149251103']], ['A2118258736', []], ['A2965065620', ['I149251103']], ['A2980577766', ['I19722']], ['A1783141849', ['I57206974']], ['A2157298618', ['I149251103']], ['A1991284286', ['I31746571']], ['A3145615489', ['I149251103']], ['A2780731423', []], ['A2980446841', ['I149251103']], ['A3080919366', ['I149251103']], ['A1714729726', ['I149251103']], ['A1890082838', ['I39642139']], ['A3147954267', ['I149251103']], ['A2306642447', ['I149251103']], ['A2051195365', ['I56590836']], ['A2170833900', ['I78757542']], ['A2145634610', ['I149251103']], ['A2693756408', []], ['A2673842419', ['I141945490']], ['A3093058792', ['I149251103']], ['A2137525664', []], ['A3178865510', ['I33459947']], ['A3007820631', ['I149251103']], ['A214528504', ['I149251103']], ['A2122261724', ['I149251103']], ['A2259493564', ['I149251103']], ['A3008447284', ['I133529467']], ['A2162568637', ['I106165777']], ['A2994426754', ['I149251103']], ['A3212110661', ['I149251103']], ['A2156989373', []], ['A2888029212', ['I95990688']], ['A2677238551', ['I39642139']], ['A1470162912', ['I149251103']], ['A2629524930', ['I149251103']], ['A2981924428', ['I149251103']], ['A2026017327', []], ['A2528034545', ['I149251103']], ['A3019289519', ['I149251103']], ['A2371503009', ['I149251103']], ['A2773706431', ['I76414455']], ['A2770400936', []], ['A395835951', []], ['A2191610393', ['I74899385']], ['A2208705000', ['I111088046']], ['A3093447780', ['I149251103']], ['A3081480917', ['I149251103']], ['A2952045355', ['I149251103']], ['A2126571783', ['I149251103']], ['A2117136343', ['I149251103']], ['A2183182747', ['I19630809']], ['A2322169031', ['I149251103']], ['A689632033', ['I129604602']], ['A2952209971', ['I31746571']], ['A3037233364', []], ['A2012517459', []], ['A3214254367', ['I149251103']], ['A3093338795', ['I149251103']], ['A2515956474', ['I95990688']], ['A3145811850', ['I80281795']], ['A3142622662', ['I149251103']], ['A3198472391', []], ['A2132455678', ['I149251103']], ['A2900329923', ['I149251103']], ['A2308680639', ['I149251103']], ['A2645102116', ['I80850581']], ['A601196031', ['I149251103']], ['A1972229181', []], ['A2133495838', ['I149251103']], ['A2803444128', ['I149251103']], ['A76907487', ['I149251103']], ['A2755582526', ['I149251103']], ['A2061814844', ['I149251103']], ['A691683588', ['I149251103']], ['A2122470004', []], ['A2014528437', ['I29891158']], ['A3144804828', ['I39642139']], ['A2115572493', ['I114982161']], ['A1816219995', ['I149251103']], ['A3153091951', ['I70640408']], ['A2442149409', ['I39642139']], ['A2266022754', ['I58048189']], ['A2694501703', ['I149251103']], ['A2165210302', ['I63739035']], ['A1976725761', []], ['A2598650706', []], ['A2127606781', ['I114724429']], ['A2465701595', ['I70640408']], ['A2158287216', ['I145487455']], ['A2713855850', ['I145487455']], ['A1985869573', ['I70640408']], ['A3091236465', []], ['A2104279826', ['I97018004']], ['A2122069637', []], ['A2524433498', ['I149251103']], ['A2795897520', ['I10947320']], ['A2151569403', ['I107720978']], ['A2251827248', []], ['A2205227065', ['I149251103']], ['A2483337482', ['I149251103']], ['A2153559775', []], ['A1996914635', ['I4068193']], ['A2980488666', []], ['A2217838924', []], ['A424594893', ['I114982161']], ['A2089080248', ['I124357947']], ['A3153185706', ['I149251103']], ['A106833059', []], ['A1905194158', ['I98251732']], ['A3081507571', ['I149251103']], ['A2126156898', ['I149251103']], ['A2966282774', ['I149251103']], ['A1976206691', ['I149251103']], ['A2086502566', ['I149251103']], ['A2103273789', ['I130442723']], ['A1972757265', []], ['A1890927917', ['I32389192']], ['A2013181484', ['I7877124']], ['A3001033543', ['I149251103']], ['A3212671684', ['I149251103']], ['A1920971052', ['I70640408']], ['A2257395298', []], ['A2167836184', ['I114027177']], ['A3092861566', []], ['A2894920819', ['I145311948']], ['A3145951632', ['I149251103']], ['A2148901667', ['I17533244']], ['A2560759403', ['I149251103']], ['A2049710663', ['I107720978']], ['A2804736168', ['I149251103']], ['A2263369703', ['I992397']], ['A1940959876', ['I39642139']], ['A2180025489', []], ['A1425510479', ['I149251103']], ['A2102328879', ['I149251103']], ['A2019136938', ['I149251103']], ['A2121553805', ['I149251103']], ['A2104989353', ['I149251103']], ['A3212222755', ['I149251103']], ['A2085533449', ['I149251103']], ['A2082798115', []], ['A2236749837', ['I91203450']], ['A2271697240', ['I114017466']], ['A3080838141', []], ['A329907494', []], ['A1995962597', ['I123185442']], ['A2758241083', ['I149251103']], ['A3019387392', ['I149251103']], ['A2167506362', []], ['A3152537613', ['I149251103']], ['A3140096266', ['I149251103']], ['A2009823816', []], ['A3212224332', ['I149251103']], ['A3154853457', ['I149251103']], ['A3142496762', ['I39642139']], ['A3183732493', ['I39642139']], ['A1978294612', ['I149251103']], ['A2165142774', []], ['A2744199795', ['I42869670']], ['A2073323141', ['I149251103']], ['A2768323355', ['I149251103']], ['A3092821710', ['I149251103']], ['A2437851007', ['I149251103']], ['A2308578969', ['I62370553']], ['A3212954370', ['I149251103']], ['A2012074336', ['I149251103']], ['A2311741314', ['I130442723']], ['A3207842373', ['I149251103']], ['A2187417062', ['I149251103']], ['A3156261715', ['I149251103']], ['A2112093150', ['I149251103']], ['A2052096873', ['I103531236']], ['A2585150966', ['I141945490']], ['A2617635383', ['I12789410']], ['A2198539236', ['I149251103']], ['A2755016573', []], ['A3207478929', ['I149251103']], ['A2159224674', []], ['A3080532982', []], ['A1259793373', []], ['A2093109083', ['I149251103']], ['A2061012529', ['I149251103']], ['A2888023641', ['I149251103']], ['A1976297322', ['I70640408']], ['A2235522935', ['I149251103']], ['A2650374411', []], ['A3164605908', ['I8764889']], ['A1985531105', []], ['A3193264458', ['I149251103']], ['A2804812000', ['I28022161']], ['A3093167151', ['I58048189']], ['A3129646985', ['I58048189']], ['A2579735154', ['I57206974']], ['A3080966122', ['I149251103']], ['A768366843', ['I149251103']], ['A2591217870', ['I149251103']], ['A2619345531', []], ['A3183707741', ['I21370196']], ['A2804786586', ['I149251103']], ['A1575105442', []], ['A646358098', []], ['A2171619173', ['I149251103']], ['A2250434770', ['I149251103']], ['A1303725643', ['I142617266']], ['A2093498034', ['I149251103']], ['A2952451616', ['I149251103']], ['A1964992651', ['I149251103']], ['A2793416557', ['I149251103']], ['A1966231656', ['I76414455']], ['A3018205737', ['I149251103']], ['A3140096431', ['I39063666']], ['A2804301087', []], ['A66691543', []], ['A3165398205', ['I149251103']], ['A2888927915', ['I39642139']], ['A33094978', ['I149251103']], ['A3049497679', []], ['A2040123927', ['I149251103']], ['A2712299672', ['I118501908']], ['A2888942052', ['I149251103']], ['A2894260413', ['I149251103']], ['A2100942514', []], ['A1837427614', ['I149251103']], ['A2123033560', []], ['A2962075517', ['I149251103']], ['A2103680419', ['I37461747']], ['A3092724448', ['I149251103']], ['A2767924615', ['I149251103']], ['A2106473094', ['I27837315']], ['A1967961136', []], ['A2617054560', ['I79576946']], ['A2990865657', ['I149251103']], ['A2125838075', ['I114027177']], ['A2299946479', ['I149251103']], ['A2893840996', ['I149251103']], ['A2182410343', []], ['A2885570561', ['I149251103']], ['A2105129513', ['I149251103']], ['A3115125153', ['I149251103']], ['A2718493480', ['I27781120']], ['A2955380462', ['I149251103']], ['A2145855513', ['I149251103']], ['A2112328861', ['I149251103']], ['A2900008388', ['I149251103']], ['A2796586280', ['I149251103']], ['A2899825995', ['I149251103']], ['A2233669889', []], ['A1970441424', ['I149251103']], ['A2108032137', ['I149251103']], ['A2559052947', []], ['A2762743234', ['I149251103']], ['A3092708820', ['I149251103']], ['A2679575603', ['I149251103']], ['A3034297231', ['I149251103']], ['A2139945721', ['I149251103']], ['A2560293163', []], ['A2439370755', []], ['A2941638972', ['I149251103']], ['A2030751991', ['I149251103']], ['A2322953211', ['I88085062']], ['A2801376805', ['I149251103']], ['A3092941760', ['I149251103']], ['A3088485169', ['I37461747']], ['A2609214619', ['I17040458']], ['A2671846033', ['I33103891']], ['A3157096520', ['I149251103']], ['A2108200049', ['I133903919']], ['A2253402509', ['I33103891']], ['A3020650419', []], ['A2715778683', ['I43922553']], ['A3088573658', ['I37461747']], ['A3015433196', ['I149251103']], ['A3044850530', []], ['A2941866114', ['I40120149']], ['A3213295296', ['I149251103']], ['A1946827375', ['I149251103']], ['A3126074672', ['I149251103']], ['A2158980598', ['I141945490']], ['A2337146604', ['I149251103']], ['A2106144155', ['I149251103']]], 'cited_by_count': 1663, 'concepts': [['C71924100', '0.60862565'], ['C99454951', '0.5476682'], ['C82789193', '0.5419896'], ['C12174686', '0.50830644'], ['C138816342', '0.5029678']], 'referenced_works': ['W521352', 'W112106683', 'W1617145133', 'W1948745084', 'W2019823288', 'W2048948439', 'W2063421373', 'W2109078197', 'W2110052313', 'W2125669390', 'W2143685991', 'W2158700710', 'W2160517070', 'W2165773881', 'W2341244791', 'W2462161029', 'W2550720962', 'W2735906728', 'W2804207405', 'W2807097728', 'W2810781955', 'W2889517514', 'W2891468622', 'W2897257410', 'W2899773405', 'W2903042202', 'W2909678677', 'W2912654919', 'W2912842935', 'W2928401928', 'W2950663303', 'W2963887614', 'W2966594091', 'W2972537166', 'W2977121540', 'W2995031679', 'W3005920146', 'W3025238321', 'W3043365850', 'W3080219561', 'W3092849554', 'W3101542255', 'W3124416644', 'W3143437408'], 'abstract': 'Rigorous analysis of levels and trends in exposure to leading risk factors and quantification of their effect on human health are important to identify where public health is making progress and in which cases current efforts are inadequate. The Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) 2019 provides a standardised and comprehensive assessment of the magnitude of risk factor exposure, relative risk, and attributable burden of disease.GBD 2019 estimated attributable mortality, years of life lost (YLLs), years of life lived with disability (YLDs), and disability-adjusted life-years (DALYs) for 87 risk factors and combinations of risk factors, at the global level, regionally, and for 204 countries and territories. GBD uses a hierarchical list of risk factors so that specific risk factors (eg, sodium intake), and related aggregates (eg, diet quality), are both evaluated. This method has six analytical steps. (1) We included 560 risk-outcome pairs that met criteria for convincing or probable evidence on the basis of research studies. 12 risk-outcome pairs included in GBD 2017 no longer met inclusion criteria and 47 risk-outcome pairs for risks already included in GBD 2017 were added based on new evidence. (2) Relative risks were estimated as a function of exposure based on published systematic reviews, 81 systematic reviews done for GBD 2019, and meta-regression. (3) Levels of exposure in each age-sex-location-year included in the study were estimated based on all available data sources using spatiotemporal Gaussian process regression, DisMod-MR 2.1, a Bayesian meta-regression method, or alternative methods. (4) We determined, from published trials or cohort studies, the level of exposure associated with minimum risk, called the theoretical minimum risk exposure level. (5) Attributable deaths, YLLs, YLDs, and DALYs were computed by multiplying population attributable fractions (PAFs) by the relevant outcome quantity for each age-sex-location-year. (6) PAFs and attributable burden for combinations of risk factors were estimated taking into account mediation of different risk factors through other risk factors. Across all six analytical steps, 30 652 distinct data sources were used in the analysis. Uncertainty in each step of the analysis was propagated into the final estimates of attributable burden. Exposure levels for dichotomous, polytomous, and continuous risk factors were summarised with use of the summary exposure value to facilitate comparisons over time, across location, and across risks. Because the entire time series from 1990 to 2019 has been re-estimated with use of consistent data and methods, these results supersede previously published GBD estimates of attributable burden.The largest declines in risk exposure from 2010 to 2019 were among a set of risks that are strongly linked to social and economic development, including household air pollution; unsafe water, sanitation, and handwashing; and child growth failure. Global declines also occurred for tobacco smoking and lead exposure. The largest increases in risk exposure were for ambient particulate matter pollution, drug use, high fasting plasma glucose, and high body-mass index. In 2019, the leading Level 2 risk factor globally for attributable deaths was high systolic blood pressure, which accounted for 10·8 million (95% uncertainty interval [UI] 9·51-12·1) deaths (19·2% [16·9-21·3] of all deaths in 2019), followed by tobacco (smoked, second-hand, and chewing), which accounted for 8·71 million (8·12-9·31) deaths (15·4% [14·6-16·2] of all deaths in 2019). The leading Level 2 risk factor for attributable DALYs globally in 2019 was child and maternal malnutrition, which largely affects health in the youngest age groups and accounted for 295 million (253-350) DALYs (11·6% [10·3-13·1] of all global DALYs that year). The risk factor burden varied considerably in 2019 between age groups and locations. Among children aged 0-9 years, the three leading detailed risk factors for attributable DALYs were all related to malnutrition. Iron deficiency was the leading risk factor for those aged 10-24 years, alcohol use for those aged 25-49 years, and high systolic blood pressure for those aged 50-74 years and 75 years and older.Overall, the record for reducing exposure to harmful risks over the past three decades is poor. Success with reducing smoking and lead exposure through regulatory policy might point the way for a stronger role for public policy on other risks in addition to continued efforts to provide information on risk factor harm to the general public.Bill & Melinda Gates Foundation.', 'counts_by_year': [[2022, 1053], [2021, 570], [2020, 23]]}, {'id': 'W2765322245', 'doi': 'https://doi.org/10.1002/pro.3330', 'title': 'MolProbity: More and better reference data for improved all-atom structure validation', 'type': 'journal-article', 'publication_date': '2018-01-01', 'host_venue': 'V156919612', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2106918335', ['I170897317']], ['A2070177508', ['I170897317']], ['A1986061537', ['I148283060']], ['A2157529955', ['I170897317']], ['A2597844890', ['I170897317']], ['A2030887064', ['I170897317']], ['A2250506735', ['I114027177']], ['A2032877750', ['I170897317']], ['A2203551669', ['I170897317']], ['A2150607053', ['I170897317']], ['A2487294372', ['I170897317']], ['A2122240977', ['I170897317']], ['A359224493', ['I170897317']], ['A2029765619', ['I114027177']], ['A2118241700', ['I148283060']], ['A2420158139', ['I28407311']], ['A2102065301', ['I170897317']], ['A2113126598', ['I170897317']]], 'cited_by_count': 1657, 'concepts': [['C519991488', '0.6630336'], ['C41008148', '0.66193503'], ['C2777904410', '0.48630533'], ['C43126263', '0.4358035'], ['C124101348', '0.37769333']], 'referenced_works': ['W1586792701', 'W1948300520', 'W1966403038', 'W1974228165', 'W1986191025', 'W1992279233', 'W1996607909', 'W2000850637', 'W2003625632', 'W2009110345', 'W2010101396', 'W2012913710', 'W2015882075', 'W2018127105', 'W2027918314', 'W2030841140', 'W2032842297', 'W2034144274', 'W2049850818', 'W2055711861', 'W2059059134', 'W2060419010', 'W2069152576', 'W2071085877', 'W2074986801', 'W2077786127', 'W2077980341', 'W2078163944', 'W2083176211', 'W2084040958', 'W2088933990', 'W2090225810', 'W2096184392', 'W2115339329', 'W2116389515', 'W2116712019', 'W2119489910', 'W2122339645', 'W2124026197', 'W2124464974', 'W2126807072', 'W2131350133', 'W2131639373', 'W2134216529', 'W2138473263', 'W2141538618', 'W2142497040', 'W2144528635', 'W2148847432', 'W2149026172', 'W2149393820', 'W2150155770', 'W2151896693', 'W2154714625', 'W2158945235', 'W2170214641', 'W2180229411', 'W2325521056', 'W2620998958', 'W2734356283', 'W2791492223', 'W2977035915', 'W4233998925'], 'abstract': 'This paper describes the current update on macromolecular model validation services that are provided at the MolProbity website, emphasizing changes and additions since the previous review in 2010. There have been many infrastructure improvements, including rewrite of previous Java utilities to now use existing or newly written Python utilities in the open-source CCTBX portion of the Phenix software system. This improves long-term maintainability and enhances the thorough integration of MolProbity-style validation within Phenix. There is now a complete MolProbity mirror site at http://molprobity.manchester.ac.uk. GitHub serves our open-source code, reference datasets, and the resulting multi-dimensional distributions that define most validation criteria. Coordinate output after Asn/Gln/His "flip" correction is now more idealized, since the post-refinement step has apparently often been skipped in the past. Two distinct sets of heavy-atom-to-hydrogen distances and accompanying van der Waals radii have been researched and improved in accuracy, one for the electron-cloud-center positions suitable for X-ray crystallography and one for nuclear positions. New validations include messages at input about problem-causing format irregularities, updates of Ramachandran and rotamer criteria from the million quality-filtered residues in a new reference dataset, the CaBLAM Cα-CO virtual-angle analysis of backbone and secondary structure for cryoEM or low-resolution X-ray, and flagging of the very rare cis-nonProline and twisted peptides which have recently been greatly overused. Due to wide application of MolProbity validation and corrections by the research community, in Phenix, and at the worldwide Protein Data Bank, newly deposited structures have continued to improve greatly as measured by MolProbity\'s unique all-atom clashscore.', 'counts_by_year': [[2022, 565], [2021, 529], [2020, 382], [2019, 146], [2018, 35]]}, {'id': 'W2195423816', 'doi': 'https://doi.org/10.1109/tnet.2015.2487344', 'title': 'Efficient Multi-User Computation Offloading for Mobile-Edge Cloud Computing', 'type': 'journal-article', 'publication_date': '2016-10-01', 'host_venue': 'V62238642', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2609056785', ['I74656192']], ['A2618503114', ['I1322087612']], ['A2150968102', ['I881766915']], ['A2162672489', ['I74656192']]], 'cited_by_count': 1655, 'concepts': [['C2781041963', '0.9180212'], ['C41008148', '0.8782793'], ['C79974875', '0.70900065'], ['C46814582', '0.68926436'], ['C2776061582', '0.62721676']], 'referenced_works': ['W1989480108', 'W2011683472', 'W2012423440', 'W2015729019', 'W2043794511', 'W2054692642', 'W2068300842', 'W2069211379', 'W2071875983', 'W2081130660', 'W2086574394', 'W2093843984', 'W2101788345', 'W2111395469', 'W2134026218', 'W2135099885', 'W2136877695', 'W2146570554', 'W2163596240', 'W2166846144', 'W2998218219', 'W3022321359', 'W3122087971', 'W4211091685', 'W4243888862'], 'abstract': 'Mobile-edge cloud computing is a new paradigm to provide cloud computing capabilities at the edge of pervasive radio access networks in close proximity to mobile users. In this paper, we first study the multi-user computation offloading problem for mobile-edge cloud computing in a multi-channel wireless interference environment. We show that it is NP-hard to compute a centralized optimal solution, and hence adopt a game theoretic approach for achieving efficient computation offloading in a distributed manner. We formulate the distributed computation offloading decision making problem among mobile device users as a multi-user computation offloading game. We analyze the structural property of the game and show that the game admits a Nash equilibrium and possesses the finite improvement property. We then design a distributed computation offloading algorithm that can achieve a Nash equilibrium, derive the upper bound of the convergence time, and quantify its efficiency ratio over the centralized optimal solutions in terms of two important performance metrics. We further extend our study to the scenario of multi-user computation offloading in the multi-channel wireless contention environment. Numerical results corroborate that the proposed algorithm can achieve superior computation offloading performance and scale well as the user size increases.', 'counts_by_year': [[2022, 228], [2021, 348], [2020, 405], [2019, 342], [2018, 227], [2017, 86], [2016, 15], [2014, 1]]}, {'id': 'W2588775853', 'doi': 'https://doi.org/10.1007/s10482-017-0844-4', 'title': 'A large-scale evaluation of algorithms to calculate average nucleotide identity', 'type': 'journal-article', 'publication_date': '2017-02-15', 'host_venue': 'V192712274', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2107565731', ['I4210133976']], ['A2124107405', ['I4210133976']], ['A2579629041', ['I139264467']], ['A3206194872', ['I139264467']], ['A2114569273', ['I4210133976']]], 'cited_by_count': 1646, 'concepts': [['C141231307', '0.5817549'], ['C11413529', '0.55048394'], ['C41008148', '0.5046073'], ['C2778355321', '0.4391664'], ['C124101348', '0.3567171']], 'referenced_works': ['W2055043387', 'W2076027587', 'W2079222081', 'W2107282968', 'W2119888547', 'W2121055398', 'W2124351063', 'W2127036970', 'W2206071891'], 'abstract': 'Average nucleotide identity (ANI) is a category of computational analysis that can be used to define species boundaries of Archaea and Bacteria. Calculating ANI usually involves the fragmentation of genome sequences, followed by nucleotide sequence search, alignment, and identity calculation. The original algorithm to calculate ANI used the BLAST program as its search engine. An improved ANI algorithm, called OrthoANI, was developed to accommodate the concept of orthology. Here, we compared four algorithms to compute ANI, namely ANIb (ANI algorithm using BLAST), ANIm (ANI using MUMmer), OrthoANIb (OrthoANI using BLAST) and OrthoANIu (OrthoANI using USEARCH) using >100,000 pairs of genomes with various genome sizes. By comparing values to the ANIb that is considered a standard, OrthoANIb and OrthoANIu exhibited good correlation in the whole range of ANI values. ANIm showed poor correlation for ANI of <90%. ANIm and OrthoANIu runs faster than ANIb by an order of magnitude. When genomes that are larger than 7 Mbp were analysed, the run-times of ANIm and OrthoANIu were shorter than that of ANIb by 53- and 22-fold, respectively. In conclusion, ANI calculation can be greatly sped up by the OrthoANIu method without losing accuracy. A web-service that can be used to calculate OrthoANIu between a pair of genome sequences is available at http://www.ezbiocloud.net/tools/ani . For large-scale calculation and integration in bioinformatics pipelines, a standalone JAVA program is available for download at http://www.ezbiocloud.net/tools/orthoaniu .', 'counts_by_year': [[2022, 355], [2021, 416], [2020, 470], [2019, 323], [2018, 76], [2017, 5]]}, {'id': 'W2520707372', 'doi': 'https://doi.org/10.1109/cvpr.2017.699', 'title': 'Unsupervised Monocular Depth Estimation with Left-Right Consistency', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2545931175', ['I45129253']], ['A2091223388', ['I45129253']], ['A286781772', ['I45129253']]], 'cited_by_count': 1641, 'concepts': [['C146849305', '0.79844105'], ['C154945302', '0.7888902'], ['C23379248', '0.7821758'], ['C65909025', '0.72590804'], ['C41008148', '0.70734054']], 'referenced_works': ['W48578105', 'W1590518343', 'W1899309388', 'W1905829557', 'W1923848918', 'W1975089519', 'W1992178727', 'W1999156278', 'W2008331211', 'W2008806336', 'W2060280062', 'W2074254947', 'W2083047701', 'W2103380420', 'W2104974755', 'W2120657032', 'W2132947399', 'W2133665775', 'W2150066425', 'W2151996626', 'W2171943915', 'W2179918789', 'W2194775991', 'W2221366145', 'W2340897893', 'W2427448504', 'W2440384215', 'W2444097022', 'W2474531669', 'W2488179576', 'W2535388113', 'W2963591054', 'W3100388886', 'W4206760982'], 'abstract': 'Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage. We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. Ex-ploiting epipolar geometry constraints, we generate disparity images by training our network with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth.', 'counts_by_year': [[2022, 150], [2021, 365], [2020, 475], [2019, 404], [2018, 223], [2017, 22]]}, {'id': 'W2289252105', 'doi': 'https://doi.org/10.1109/jssc.2016.2616357', 'title': 'Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks', 'type': 'journal-article', 'publication_date': '2017-01-01', 'host_venue': 'V83637746', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2212861991', ['I63966007']], ['A2170293669', ['I63966007']], ['A2103115969', ['I63966007']], ['A2093716610', ['I63966007']]], 'cited_by_count': 1640, 'concepts': [['C81363708', '0.7752147'], ['C41008148', '0.67078835'], ['C118524514', '0.5314882'], ['C186370098', '0.43790096'], ['C108583219', '0.41552848']], 'referenced_works': ['W1964471912', 'W1992781178', 'W2001598633', 'W2009832130', 'W2015861736', 'W2044535169', 'W2048266589', 'W2067523571', 'W2070167224', 'W2088079057', 'W2096645269', 'W2097117768', 'W2102605133', 'W2112796928', 'W2117539524', 'W2117696986', 'W2152839228', 'W2155893237', 'W2194775991', 'W2257979135', 'W2290132443', 'W2919115771', 'W3024621361'], 'abstract': 'Eyeriss is an accelerator for state-of-the-art deep convolutional neural networks (CNNs). It optimizes for the energy efficiency of the entire system, including the accelerator chip and off-chip DRAM, for various CNN shapes by reconfiguring the architecture. CNNs are widely used in modern AI systems but also bring challenges on throughput and energy efficiency to the underlying hardware. This is because its computation requires a large amount of data, creating significant data movement from on-chip and off-chip that is more energy-consuming than computation. Minimizing data movement energy cost for any CNN shape, therefore, is the key to high throughput and energy efficiency. Eyeriss achieves these goals by using a proposed processing dataflow, called row stationary (RS), on a spatial architecture with 168 processing elements. RS dataflow reconfigures the computation mapping of a given shape, which optimizes energy efficiency by maximally reusing data locally to reduce expensive data movement, such as DRAM accesses. Compression and data gating are also applied to further improve energy efficiency. Eyeriss processes the convolutional layers at 35 frames/s and 0.0029 DRAM access/multiply and accumulation (MAC) for AlexNet at 278 mW (batch size    $N = 4$   ), and 0.7 frames/s and 0.0035 DRAM access/MAC for VGG-16 at 236 mW (   $N = 3$   ).', 'counts_by_year': [[2022, 188], [2021, 385], [2020, 387], [2019, 334], [2018, 244], [2017, 75], [2016, 27], [2012, 1]]}, {'id': 'W2393319904', 'doi': 'https://doi.org/10.1145/2939672.2939753', 'title': 'Structural Deep Network Embedding', 'type': 'proceedings-article', 'publication_date': '2016-08-13', 'host_venue': 'V4306420424', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2099175083', ['I99065089']], ['A2113115369', ['I99065089']], ['A2111511002', ['I99065089']]], 'cited_by_count': 1629, 'concepts': [['C41008148', '0.77453077'], ['C41608201', '0.5721467'], ['C154945302', '0.5616084'], ['C165696696', '0.53839123'], ['C137753397', '0.5287853']], 'referenced_works': ['W2001141328', 'W2035299679', 'W2037933327', 'W2046253692', 'W2053186076', 'W2062797058', 'W2084809122', 'W2090891622', 'W2097308346', 'W2105543219', 'W2108614537', 'W2135598826', 'W2136922672', 'W2160815625', 'W2163922914', 'W2913932916', 'W2997183031', 'W3001645704', 'W3104097132', 'W3105705953', 'W4231109964'], 'abstract': 'Network embedding is an important method to learn low-dimensional representations of vertexes in networks, aiming to capture and preserve the network structure. Almost all the existing network embedding methods adopt shallow models. However, since the underlying network structure is complex, shallow models cannot capture the highly non-linear network structure, resulting in sub-optimal network representations. Therefore, how to find a method that is able to effectively capture the highly non-linear network structure and preserve the global and local structure is an open yet important problem. To solve this problem, in this paper we propose a Structural Deep Network Embedding method, namely SDNE. More specifically, we first propose a semi-supervised deep model, which has multiple layers of non-linear functions, thereby being able to capture the highly non-linear network structure. Then we propose to exploit the first-order and second-order proximity jointly to preserve the network structure. The second-order proximity is used by the unsupervised component to capture the global network structure. While the first-order proximity is used as the supervised information in the supervised component to preserve the local network structure. By jointly optimizing them in the semi-supervised deep model, our method can preserve both the local and global network structure and is robust to sparse networks. Empirically, we conduct the experiments on five real-world networks, including a language network, a citation network and three social networks. The results show that compared to the baselines, our method can reconstruct the original network significantly better and achieves substantial gains in three applications, i.e. multi-label classification, link prediction and visualization.', 'counts_by_year': [[2022, 200], [2021, 351], [2020, 422], [2019, 339], [2018, 225], [2017, 83], [2016, 7]]}, {'id': 'W2969424089', 'doi': 'https://doi.org/10.1109/twc.2019.2936025', 'title': 'Intelligent Reflecting Surface Enhanced Wireless Network via Joint Active and Passive Beamforming', 'type': 'journal-article', 'publication_date': '2019-08-23', 'host_venue': 'V63459445', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2311797224', ['I165932596']], ['A2413204075', ['I165932596']]], 'cited_by_count': 1626, 'concepts': [['C54197355', '0.92236686'], ['C41008148', '0.7336168'], ['C555944384', '0.62588525'], ['C32022120', '0.5460976'], ['C65422117', '0.50925505']], 'referenced_works': ['W1971443733', 'W1987380770', 'W1997834106', 'W2037866776', 'W2053521124', 'W2082432933', 'W2103972037', 'W2109005286', 'W2126351339', 'W2140578794', 'W2143927285', 'W2161473225', 'W2167282237', 'W2272804037', 'W2343823234', 'W2735938380', 'W2795167939', 'W2891354184', 'W2963466796', 'W2964076076', 'W2964129085', 'W4236826464'], 'abstract': "Intelligent reflecting surface (IRS) is envisioned to be a new and revolutionizing technology for achieving spectrum and energy efficient wireless communication networks cost-effectively in the future. Specifically, an IRS consists of a large number of low-cost passive elements each reflecting the incident signal with a certain phase shift to collaboratively achieve beamforming and/or interference suppression at designated receivers. In this paper, we study an IRS-aided multiuser multiple-input single-output (MISO) wireless system where one IRS is deployed to assist in the communication from a multi-antenna access point (AP) to multiple single-antenna users. As such, each user receives the superposed signals from the AP as well as the IRS via its reflection. We aim to minimize the total transmit power at the AP by jointly optimizing the transmit beamforming by active antenna array at the AP and reflect beamforming by passive phase shifters at the IRS, subject to users' individual signal-to-interference-plus-noise ratio (SINR) constraints. However, the formulated problem is non-convex and difficult to be solved optimally.", 'counts_by_year': [[2022, 411], [2021, 621], [2020, 490], [2019, 101], [2018, 2]]}, {'id': 'W2898598946', 'doi': 'https://doi.org/10.1093/nar/gky955', 'title': 'GENCODE reference annotation for the human and mouse genomes', 'type': 'journal-article', 'publication_date': '2019-01-08', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2062101058', ['I1303153112']], ['A694919984', ['I185103710']], ['A2424480457', ['I97565354']], ['A2154963224', ['I118564535']], ['A1995362950', ['I63966007']], ['A2071231283', ['I1303153112']], ['A2105487607', ['I1303153112']], ['A910319322', ['I32971472']], ['A2310642199', []], ['A2147675546', ['I185103710']], ['A3207976444', ['I1303153112']], ['A2485143726', ['I1303153112']], ['A2090361279', ['I1303153112']], ['A3111311845', []], ['A1959033204', ['I97565354']], ['A2210664129', ['I1303153112']], ['A2008511597', []], ['A2420748758', ['I1303153112']], ['A2067776390', ['I185103710']], ['A2800604722', ['I1303153112']], ['A2619102107', ['I1303153112']], ['A2251964615', ['I1303153112']], ['A2974532436', ['I1303153112']], ['A106429203', ['I1303153112']], ['A2923277057', ['I1303153112']], ['A2233366894', ['I1303153112']], ['A2893571406', []], ['A2342696407', ['I1303153112']], ['A2810045523', []], ['A2898161320', ['I1303153112']], ['A2108971833', ['I32971472']], ['A2588976913', ['I32971472']], ['A2547459221', ['I1303153112']], ['A2954421807', ['I32971472']], ['A2995835315', []], ['A1513018408', ['I1303153112']], ['A2303925854', ['I1303153112']], ['A2898148562', ['I1303153112']], ['A2118341164', ['I1303153112']], ['A2898600627', ['I1303153112']], ['A2804005669', ['I4654613']], ['A2912807758', ['I32971472']], ['A2249352692', ['I1303153112']], ['A1980315147', ['I1303153112']], ['A3035996561', ['I32971472']], ['A2135537546', ['I1303153112']], ['A2171709290', []], ['A2141155646', ['I32971472']], ['A338841611', ['I170486558']], ['A2151554911', ['I183935753']], ['A257558059', ['I63966007']], ['A162893701', ['I185103710']], ['A1703685181', ['I97565354']], ['A2023016288', []], ['A2121717666', ['I1303153112']]], 'cited_by_count': 1621, 'concepts': [['C141674004', '0.98482513'], ['C2776321320', '0.7500594'], ['C2777002779', '0.65210295'], ['C62177273', '0.6238966'], ['C141231307', '0.597465']], 'referenced_works': ['W1551073541', 'W1956454952', 'W2003787247', 'W2023978699', 'W2055043387', 'W2077327925', 'W2079671119', 'W2092170757', 'W2096465161', 'W2102304447', 'W2104549677', 'W2115692041', 'W2117688129', 'W2121906867', 'W2127143129', 'W2139480055', 'W2139658526', 'W2141831115', 'W2149429041', 'W2166825456', 'W2169674658', 'W2172189660', 'W2247766769', 'W2256016639', 'W2259938310', 'W2323608404', 'W2323717928', 'W2414206081', 'W2464717012', 'W2475020780', 'W2487518698', 'W2544816272', 'W2556805640', 'W2761275051', 'W2766459919', 'W2766848373', 'W2767232866', 'W2787788280', 'W2803944601', 'W2807521829', 'W2810156064', 'W2894305622', 'W2951972570', 'W2952306768', 'W2953263404', 'W4210702584', 'W4213108508', 'W4250359879'], 'abstract': 'The accurate identification and description of the genes in the human and mouse genomes is a fundamental requirement for high quality analysis of data informing both genome biology and clinical genomics. Over the last 15 years, the GENCODE consortium has been producing reference quality gene annotations to provide this foundational resource. The GENCODE consortium includes both experimental and computational biology groups who work together to improve and extend the GENCODE gene annotation. Specifically, we generate primary data, create bioinformatics tools and provide analysis to support the work of expert manual gene annotators and automated gene annotation pipelines. In addition, manual and computational annotation workflows use any and all publicly available data and analysis, along with the research literature to identify and characterise gene loci to the highest standard. GENCODE gene annotations are accessible via the Ensembl and UCSC Genome Browsers, the Ensembl FTP site, Ensembl Biomart, Ensembl Perl and REST APIs as well as https://www.gencodegenes.org.', 'counts_by_year': [[2022, 400], [2021, 625], [2020, 434], [2019, 157], [2018, 4]]}, {'id': 'W2964304707', 'doi': 'https://doi.org/10.1109/cvpr.2016.511', 'title': 'Convolutional Pose Machines', 'type': 'proceedings-article', 'publication_date': '2016-01-30', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2157229433', ['I74973139']], ['A2018566553', ['I74973139']], ['A2140539176', ['I74973139']], ['A2146369081', ['I74973139']]], 'cited_by_count': 1621, 'concepts': [['C52102323', '0.8199888'], ['C41008148', '0.79764915'], ['C2776214188', '0.7259333'], ['C154945302', '0.710981'], ['C2780451532', '0.6381784']], 'referenced_works': ['W23953656', 'W602397586', 'W1903029394', 'W1936750108', 'W1993149133', 'W1994529670', 'W1996478295', 'W1997500560', 'W2013640163', 'W2030536784', 'W2070797946', 'W2080873731', 'W2097151019', 'W2103015390', 'W2107878631', 'W2128271252', 'W2131263044', 'W2135533529', 'W2143158307', 'W2143478373', 'W2143487029', 'W2155893237', 'W2157939923'], 'abstract': 'Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.', 'counts_by_year': [[2022, 134], [2021, 338], [2020, 356], [2019, 416], [2018, 257], [2017, 102], [2016, 17]]}, {'id': 'W2424778531', 'doi': 'https://doi.org/10.1109/cvpr.2016.110', 'title': 'Social LSTM: Human Trajectory Prediction in Crowded Spaces', 'type': 'proceedings-article', 'publication_date': '2016-06-27', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2060491216', ['I97018004']], ['A2492472186', ['I97018004']], ['A2137039922', ['I97018004']], ['A2274990849', ['I97018004']], ['A1984838606', ['I97018004']], ['A1618661958', ['I97018004']]], 'cited_by_count': 1616, 'concepts': [['C13662910', '0.8328873'], ['C41008148', '0.7908281'], ['C154945302', '0.6496741'], ['C2780451532', '0.62579656'], ['C2778112365', '0.62050325']], 'referenced_works': ['W46519926', 'W66452226', 'W100367037', 'W259727857', 'W1518259947', 'W1689909837', 'W1711926650', 'W1922838137', 'W1967777429', 'W1968466370', 'W1970206276', 'W1972696612', 'W1982185844', 'W1997715798', 'W2020209171', 'W2024479254', 'W2040395913', 'W2047286693', 'W2056120433', 'W2064675550', 'W2070999216', 'W2071005201', 'W2074995863', 'W2079291076', 'W2082585576', 'W2090229683', 'W2092272787', 'W2093655440', 'W2099320314', 'W2101415982', 'W2101821104', 'W2118527252', 'W2124592697', 'W2124609748', 'W2128534087', 'W2133235827', 'W2134944993', 'W2146183743', 'W2147615062', 'W2150704630', 'W2154642173', 'W2158839502', 'W2161686298', 'W2164489414', 'W2165609887', 'W2167052694', 'W2221625691', 'W2532516272', 'W2911273949', 'W2951183276', 'W4239127346'], 'abstract': 'Pedestrians follow different trajectories to avoid obstacles and accommodate fellow pedestrians. Any autonomous vehicle navigating such a scene should be able to foresee the future positions of pedestrians and accordingly adjust its path to avoid collisions. This problem of trajectory prediction can be viewed as a sequence generation task, where we are interested in predicting the future trajectory of people based on their past positions. Following the recent success of Recurrent Neural Network (RNN) models for sequence prediction tasks, we propose an LSTM model which can learn general human movement and predict their future trajectories. This is in contrast to traditional approaches which use hand-crafted functions such as Social forces. We demonstrate the performance of our method on several public datasets. Our model outperforms state-of-the-art methods on some of these datasets. We also analyze the trajectories predicted by our model to demonstrate the motion behaviour learned by our model.', 'counts_by_year': [[2022, 174], [2021, 467], [2020, 426], [2019, 303], [2018, 154], [2017, 79], [2016, 12]]}, {'id': 'W2770125280', 'doi': 'https://doi.org/10.1016/j.cej.2017.11.059', 'title': 'Activation of persulfate (PS) and peroxymonosulfate (PMS) and application for the degradation of emerging contaminants', 'type': 'journal-article', 'publication_date': '2018-02-15', 'host_venue': 'V149937609', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2174633270', ['I4210128533']], ['A2782672289', ['I4210128533']]], 'cited_by_count': 1611, 'concepts': [['C2778729139', '0.92020714'], ['C2779679103', '0.8297454'], ['C112570922', '0.58463854'], ['C185592680', '0.58143866'], ['C107872376', '0.5683632']], 'referenced_works': ['W314323317', 'W966624069', 'W1127712305', 'W1138025054', 'W1455023259', 'W1578382804', 'W1623565050', 'W1627032112', 'W1824788921', 'W1826196581', 'W1833604444', 'W1870131130', 'W1921442749', 'W1966769789', 'W1967091842', 'W1968193214', 'W1972119071', 'W1976675246', 'W1977608255', 'W1978412154', 'W1982187838', 'W1983215067', 'W1983879225', 'W1987323646', 'W1988794823', 'W1990670192', 'W1992411968', 'W1992850724', 'W1994444701', 'W1996340661', 'W1997283834', 'W1997854946', 'W1997960382', 'W1999414626', 'W2001057261', 'W2003515398', 'W2009230362', 'W2009604608', 'W2011296196', 'W2012871851', 'W2012998347', 'W2016311367', 'W2019635083', 'W2020802278', 'W2022103444', 'W2022697878', 'W2026465309', 'W2027322558', 'W2027585298', 'W2032652071', 'W2032994663', 'W2036169956', 'W2038173949', 'W2038291677', 'W2040134513', 'W2040456488', 'W2040534411', 'W2045174428', 'W2045342647', 'W2051873653', 'W2054759372', 'W2056424411', 'W2058862209', 'W2062618008', 'W2065303656', 'W2067119346', 'W2067441129', 'W2067692229', 'W2069916097', 'W2070780800', 'W2071047140', 'W2071552394', 'W2073564719', 'W2078985995', 'W2079253547', 'W2080020076', 'W2081114557', 'W2082169563', 'W2088083245', 'W2088398848', 'W2088684041', 'W2090548033', 'W2091002202', 'W2093076236', 'W2093131958', 'W2094731216', 'W2096094438', 'W2106830986', 'W2116492037', 'W2131403420', 'W2146613601', 'W2150288531', 'W2160363483', 'W2166875310', 'W2182513531', 'W2183157033', 'W2194089638', 'W2195207925', 'W2205765569', 'W2208020852', 'W2211736630', 'W2220876274', 'W2236020494', 'W2236164259', 'W2244578615', 'W2254684565', 'W2266953687', 'W2267404598', 'W2277482974', 'W2286206949', 'W2287486004', 'W2289069106', 'W2298431938', 'W2300158998', 'W2301228662', 'W2302471798', 'W2315405796', 'W2315776822', 'W2321178107', 'W2321363019', 'W2322918343', 'W2328207157', 'W2333946702', 'W2335221168', 'W2340751996', 'W2341209051', 'W2342509013', 'W2342738239', 'W2344036593', 'W2344324216', 'W2345038963', 'W2345998342', 'W2347123726', 'W2356279173', 'W2394887079', 'W2406361769', 'W2406549447', 'W2408333260', 'W2411103551', 'W2413145276', 'W2413402613', 'W2461217359', 'W2461265466', 'W2467074667', 'W2468785275', 'W2470191317', 'W2477941524', 'W2488010376', 'W2489436287', 'W2495744501', 'W2498744255', 'W2498938028', 'W2509538092', 'W2514139663', 'W2516478127', 'W2517782333', 'W2518726113', 'W2519718304', 'W2521524492', 'W2522645846', 'W2531593090', 'W2534737939', 'W2550172918', 'W2552557957', 'W2556302772', 'W2557021920', 'W2562076183', 'W2566048126', 'W2567633496', 'W2575044501', 'W2575382487', 'W2576372938', 'W2586783714', 'W2588011371', 'W2588227475', 'W2589246173', 'W2591822179', 'W2595839172', 'W2599593032', 'W2600473920', 'W2602801623', 'W2603445336', 'W2603965999', 'W2605734696', 'W2608059137', 'W2609672420', 'W2610470215', 'W2610977844', 'W2612513953', 'W2618590668', 'W2619454685', 'W2626083158', 'W2650259773', 'W2726071294', 'W2952353036', 'W4256189854'], 'abstract': 'Abstract   Sulfate radical-based advanced oxidation processes (AOPs) have been received increasing attention in recent years due to their high capability and adaptability for the degradation of emerging contaminants. Persulfate (PS, S2O82−) and peroxymonosulfate (PMS, HSO5−) can be activated by thermal, alkaline, ultraviolet light, activated carbon, transition metal (such as Fe0, Fe2+, Cu2+, Co2+, Ag+), ultrasound and hydrogen peroxide to form sulfate radical (SO4 −), which is strong oxidant and capable of effectively degrading emerging pollutants. Sulfate radical-based AOPs have a series of advantages in comparison with  OH-based methods, for example: higher oxidation potential, higher selectivity and efficiency to oxidize pollutants containing unsaturated bonds or aromatic ring, wider pH range. Therefore, sulfate radicals are capable of removing the emerging contaminants more efficiently. In this review paper, various methods for the activation of PS and PMS were introduced, including, thermal, alkaline, radiation, transition metal ions and metal oxide, carbonaceous-based materials activation and so on; and their possible activation mechanisms were discussed. In addition, the application of activated PS and PMS for the degradation of emerging contaminants and the influencing factors were summarized. Finally, the concluding remarks and perspectives are made for future study on the activation of PS and PMS. This review can provide an overview for the activation and application of PS and PMS for the degradation of emerging contaminants, as well as for the deep understanding of the activation mechanisms of PS and PMS by various methods.', 'counts_by_year': [[2022, 585], [2021, 451], [2020, 338], [2019, 182], [2018, 32]]}, {'id': 'W2996428491', 'doi': 'https://doi.org/10.48550/arxiv.1909.11942', 'title': 'ALBERT: A Lite BERT for Self-supervised Learning of Language\n  Representations', 'type': 'posted-content', 'publication_date': '2020-04-30', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2115082539', ['I1291425158']], ['A2804578048', ['I160992636']], ['A2562257464', ['I1291425158']], ['A2000806699', ['I57206974']], ['A2101143821', ['I1291425158']], ['A2404746938', ['I1291425158']]], 'cited_by_count': 1610, 'concepts': [['C41008148', '0.8245199'], ['C2777530160', '0.6384818'], ['C137293760', '0.6328026'], ['C154945302', '0.5156709'], ['C2776760102', '0.5115112']], 'referenced_works': ['W131533222', 'W1486649854', 'W1599016936', 'W1991145433', 'W2124741472', 'W2130158090', 'W2153579005', 'W2170973209', 'W2250539671', 'W2251939518', 'W2338908902', 'W2396767181', 'W2525127255', 'W2606964149', 'W2610858497', 'W2786396726', 'W2805206884', 'W2896457183', 'W2899663614', 'W2911109671', 'W2930786691', 'W2940744433', 'W2945667196', 'W2949547296', 'W2950813464', 'W2952750383', 'W2962734576', 'W2962753370', 'W2963026768', 'W2963034893', 'W2963250244', 'W2963310665', 'W2963323070', 'W2963403868', 'W2963631907', 'W2963641307', 'W2963684275', 'W2963748441', 'W2963756346', 'W2964025273', 'W2964350391', 'W2965373594', 'W2966892770', 'W2969601108', 'W2970454332', 'W2970900903', 'W2978670439', 'W2998579922', 'W3104033643'], 'abstract': 'Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.', 'counts_by_year': [[2022, 34], [2021, 882], [2020, 659], [2019, 35]]}, {'id': 'W2475334473', 'doi': 'https://doi.org/10.1145/2988450.2988454', 'title': 'Wide &amp; Deep Learning for Recommender Systems', 'type': 'proceedings-article', 'publication_date': '2016-09-15', 'host_venue': 'V4306418092', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2144885571', ['I1291425158']], ['A2530104481', ['I1291425158']], ['A2213002574', ['I1291425158']], ['A2261353415', ['I1291425158']], ['A2150538384', ['I1291425158']], ['A2539339469', ['I1291425158']], ['A2227195560', ['I1291425158']], ['A1994222016', ['I1291425158']], ['A2251141449', ['I1291425158']], ['A2636063181', ['I1291425158']], ['A2532951969', ['I1291425158']], ['A2533063171', ['I1291425158']], ['A2114438626', ['I1291425158']], ['A2532281685', ['I1291425158']], ['A2932543382', ['I1291425158']], ['A2768161902', ['I1291425158']]], 'cited_by_count': 1608, 'concepts': [['C41008148', '0.811403'], ['C2778827112', '0.7626437'], ['C154945302', '0.7234413'], ['C2776401178', '0.7231115'], ['C108583219', '0.7223289']], 'referenced_works': ['W1965154800', 'W2046586564', 'W2094286023', 'W2157881433', 'W2194775991', 'W2998508934'], 'abstract': 'Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.', 'counts_by_year': [[2022, 306], [2021, 439], [2020, 394], [2019, 282], [2018, 123], [2017, 59], [2016, 2]]}, {'id': 'W2899070097', 'doi': 'https://doi.org/10.1093/nar/gky1033', 'title': 'PubChem 2019 update: improved access to chemical data', 'type': 'journal-article', 'publication_date': '2019-01-08', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2617071591', ['I4210109390']], ['A2674214034', ['I4210109390']], ['A2531633765', ['I4210109390']], ['A106891110', ['I4210109390']], ['A2899195951', ['I4210109390']], ['A2155512249', ['I4210109390']], ['A2228709497', ['I4210109390']], ['A2085205320', ['I4210109390']], ['A2087373738', ['I4210109390']], ['A2710640738', ['I4210109390']], ['A2045354893', ['I4210109390']], ['A2663009298', ['I4210109390']], ['A2137313570', ['I4210109390']]], 'cited_by_count': 1608, 'concepts': [['C158180186', '0.99120843'], ['C3017942907', '0.49595866'], ['C136764020', '0.4534585'], ['C41008148', '0.4475959'], ['C86803240', '0.44165325']], 'referenced_works': ['W2013469333', 'W2023839852', 'W2052250375', 'W2091506983', 'W2103017472', 'W2127358324', 'W2176758925', 'W2177317049', 'W2204695023', 'W2224056471', 'W2304702132', 'W2409470256', 'W2479356696', 'W2542446037', 'W2544136853', 'W2558999090', 'W2559191318', 'W2559588208', 'W2604808360', 'W2767891136', 'W2771328536', 'W2801555904', 'W2887459817', 'W2915789149', 'W2916534270', 'W2918011602', 'W4210702584'], 'abstract': 'PubChem (https://pubchem.ncbi.nlm.nih.gov) is a key chemical information resource for the biomedical research community. Substantial improvements were made in the past few years. New data content was added, including spectral information, scientific articles mentioning chemicals, and information for food and agricultural chemicals. PubChem released new web interfaces, such as PubChem Target View page, Sources page, Bioactivity dyad pages and Patent View page. PubChem also released a major update to PubChem Widgets and introduced a new programmatic access interface, called PUG-View. This paper describes these new developments in PubChem.', 'counts_by_year': [[2022, 341], [2021, 612], [2020, 484], [2019, 165], [2018, 2], [2017, 1]]}, {'id': 'W2884430236', 'doi': 'https://doi.org/10.1038/s41586-018-0337-2', 'title': 'Machine learning for molecular and materials science', 'type': 'journal-article', 'publication_date': '2018-07-26', 'host_venue': 'V137773608', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2149547153', ['I1286704778']], ['A2559625270', ['I51601045']], ['A1895377932', ['I40120149']], ['A2651470702', ['I114027177']], ['A2135464874', ['I47508984', 'I193775966']]], 'cited_by_count': 1605, 'concepts': [['C41008148', '0.5662577'], ['C9652623', '0.52594924'], ['C2780841128', '0.4657211'], ['C36503486', '0.44598782'], ['C154945302', '0.43476853']], 'referenced_works': ['W617139115', 'W1492999010', 'W1510073064', 'W1625305535', 'W1757990252', 'W1865667476', 'W1968761064', 'W1976492731', 'W1979769287', 'W1982589276', 'W1982598895', 'W1992985800', 'W1997974358', 'W2007400012', 'W2020786104', 'W2025679679', 'W2030843415', 'W2030976617', 'W2034097448', 'W2036524141', 'W2052226480', 'W2052891002', 'W2057069496', 'W2058452634', 'W2063007245', 'W2067250248', 'W2074616700', 'W2076063813', 'W2081413236', 'W2083415705', 'W2104489082', 'W2123306226', 'W2124234891', 'W2127168465', 'W2128245586', 'W2134164499', 'W2138178257', 'W2157886206', 'W2164524421', 'W2194321275', 'W2217912240', 'W2230728100', 'W2310703973', 'W2324964582', 'W2325264655', 'W2328928309', 'W2337082154', 'W2338402873', 'W2346180883', 'W2346400664', 'W2347129741', 'W2349487082', 'W2418973097', 'W2468638527', 'W2478294658', 'W2503662290', 'W2509907061', 'W2520500207', 'W2521267242', 'W2525748878', 'W2531602199', 'W2537064446', 'W2541404351', 'W2559394418', 'W2565684601', 'W2580919858', 'W2591366729', 'W2606044016', 'W2611413954', 'W2619580215', 'W2620846205', 'W2621738901', 'W2621742623', 'W2734520197', 'W2740407088', 'W2746244909', 'W2747592475', 'W2752180799', 'W2753962198', 'W2766362701', 'W2789615344', 'W2964116922', 'W3037315640', 'W3099950071', 'W4252359241'], 'abstract': 'Here we summarize recent progress in machine learning for the chemical sciences. We outline machine-learning techniques that are suitable for addressing research questions in this domain, as well as future directions for the field. We envisage a future in which the design, synthesis, characterization and application of molecules and materials is accelerated by artificial intelligence.', 'counts_by_year': [[2022, 398], [2021, 513], [2020, 423], [2019, 236], [2018, 27]]}, {'id': 'W1857884451', 'doi': 'https://doi.org/10.1109/cvpr.2016.465', 'title': 'Learning Multi-domain Convolutional Neural Networks for Visual Tracking', 'type': 'proceedings-article', 'publication_date': '2016-06-27', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2120661233', ['I123900574']], ['A2114374559', ['I123900574']]], 'cited_by_count': 1602, 'concepts': [['C41008148', '0.83575445'], ['C81363708', '0.75519204'], ['C154945302', '0.66486275'], ['C36503486', '0.65786046'], ['C2775936607', '0.5737043']], 'referenced_works': ['W1745334888', 'W1895577753', 'W1903029394', 'W1915599933', 'W1964846093', 'W2000326692', 'W2016075127', 'W2069057437', 'W2089961441', 'W2098941887', 'W2102605133', 'W2109579504', 'W2110518760', 'W2113325037', 'W2115609520', 'W2117539524', 'W2124211486', 'W2139047213', 'W2145287260', 'W2154889144', 'W2158592639', 'W2159686933', 'W2168117308', 'W2168356304', 'W2210596465', 'W2613779721', 'W2915935370', 'W2964138017'], 'abstract': 'We propose a novel visual tracking algorithm based on the representations from a discriminatively trained Convolutional Neural Network (CNN). Our algorithm pretrains a CNN using a large set of videos with tracking ground-truths to obtain a generic target representation. Our network is composed of shared layers and multiple branches of domain-specific layers, where domains correspond to individual training sequences and each branch is responsible for binary classification to identify the target in each domain. We train the network with respect to each domain iteratively to obtain generic target representations in the shared layers. When tracking a target in a new sequence, we construct a new network by combining the shared layers in the pretrained CNN with a new binary classification layer, which is updated online. Online tracking is performed by evaluating the candidate windows randomly sampled around the previous target state. The proposed algorithm illustrates outstanding performance compared with state-of-the-art methods in existing tracking benchmarks.', 'counts_by_year': [[2022, 151], [2021, 311], [2020, 317], [2019, 356], [2018, 286], [2017, 164], [2016, 16], [2015, 1]]}, {'id': 'W2952045272', 'doi': 'https://doi.org/10.1093/bioinformatics/bty407', 'title': 'Nextstrain: real-time tracking of pathogen evolution', 'type': 'journal-article', 'publication_date': '2018-12-01', 'host_venue': 'V52395412', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2264870885', ['I4210089486']], ['A2768378022', ['I4210089486']], ['A2769033584', ['I201448701', 'I4210089486']], ['A2170537447', ['I201448701', 'I4210089486']], ['A2611429274', ['I4210089486']], ['A2768149858', ['I4210089486']], ['A2656717011', ['I4210112458']], ['A2274632334', ['I4210089486']], ['A2054961175', ['I1850255', 'I4210112458', 'I12708293']]], 'cited_by_count': 1599, 'concepts': [['C21833749', '0.73286134'], ['C36464697', '0.6795306'], ['C544833334', '0.6475965'], ['C519991488', '0.6401944'], ['C141231307', '0.5386607']], 'referenced_works': ['W1817385113', 'W1976755807', 'W2004396083', 'W2155924876', 'W2259815689', 'W2608156331', 'W2618268848', 'W2625936141', 'W2951878836', 'W3187962553'], 'abstract': 'Understanding the spread and evolution of pathogens is important for effective public health measures and surveillance. Nextstrain consists of a database of viral genomes, a bioinformatics pipeline for phylodynamics analysis, and an interactive visualization platform. Together these present a real-time view into the evolution and spread of a range of viral pathogens of high public health importance. The visualization integrates sequence data with other data types such as geographic information, serology, or host species. Nextstrain compiles our current understanding into a single accessible location, open to health professionals, epidemiologists, virologists and the public alike.All code (predominantly JavaScript and Python) is freely available from github.com/nextstrain and the web-application is available at nextstrain.org.', 'counts_by_year': [[2022, 466], [2021, 638], [2020, 431], [2019, 49], [2018, 14]]}, {'id': 'W2472333518', 'doi': 'https://doi.org/10.1109/jiot.2016.2584538', 'title': 'Fog and IoT: An Overview of Research Opportunities', 'type': 'journal-article', 'publication_date': '2016-06-23', 'host_venue': 'V2480266640', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2143724125', ['I20089843']], ['A3217685898', ['I4210129566']]], 'cited_by_count': 1591, 'concepts': [['C41008148', '0.73312783'], ['C81860439', '0.43141833'], ['C38652104', '0.36631054'], ['C2522767166', '0.36278445'], ['C76155785', '0.33692288']], 'referenced_works': ['W647459346', 'W948830735', 'W1646619842', 'W1926090661', 'W1980134781', 'W2025399718', 'W2029812135', 'W2031771125', 'W2036569531', 'W2043854726', 'W2070268576', 'W2095059414', 'W2097281067', 'W2111418154', 'W2135099885', 'W2158893758'], 'abstract': 'Fog is an emergent architecture for computing, storage, control, and networking that distributes these services closer to end users along the cloud-to-things continuum. It covers both mobile and wireline scenarios, traverses across hardware and software, resides on network edge but also over access networks and among end users, and includes both data plane and control plane. As an architecture, it supports a growing variety of applications, including those in the Internet of Things (IoT), fifth-generation (5G) wireless systems, and embedded artificial intelligence (AI). This survey paper summarizes the opportunities and challenges of fog, focusing primarily in the networking context of IoT.', 'counts_by_year': [[2022, 150], [2021, 270], [2020, 348], [2019, 401], [2018, 297], [2017, 120], [2016, 5]]}, {'id': 'W2206071891', 'doi': 'https://doi.org/10.1099/ijsem.0.000760', 'title': 'OrthoANI: An improved algorithm and software for calculating average nucleotide identity', 'type': 'journal-article', 'publication_date': '2016-02-01', 'host_venue': 'V102181007', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2137050395', ['I139264467']], ['A2219927634', ['I139264467']], ['A2112748698', ['I139264467']], ['A2114569273', ['I139264467']]], 'cited_by_count': 1583, 'concepts': [['C2777742833', '0.8470799'], ['C86803240', '0.7629431'], ['C141231307', '0.75903654'], ['C103278499', '0.6372167'], ['C2778355321', '0.6241899']], 'referenced_works': ['W1967192350', 'W2015509339', 'W2073925485', 'W2079222081', 'W2092050554', 'W2095513921', 'W2097477547', 'W2116206245', 'W2119888547', 'W2121055398', 'W2127036970', 'W2134375163', 'W2158714788', 'W2168104781'], 'abstract': 'Species demarcation in Bacteria and Archaea is mainly based on overall genome relatedness, which serves a framework for modern microbiology. Current practice for obtaining these measures between two strains is shifting from experimentally determined similarity obtained by DNA–DNA hybridization (DDH) to genome-sequence-based similarity. Average nucleotide identity (ANI) is a simple algorithm that mimics DDH. Like DDH, ANI values between two genome sequences may be different from each other when reciprocal calculations are compared. We compared 63 690 pairs of genome sequences and found that the differences in reciprocal ANI values are significantly high, exceeding 1 % in some cases. To resolve this problem of not being symmetrical, a new algorithm, named OrthoANI, was developed to accommodate the concept of orthology for which both genome sequences were fragmented and only orthologous fragment pairs taken into consideration for calculating nucleotide identities. OrthoANI is highly correlated with ANI (using BLASTn) and the former showed approximately 0.1 % higher values than the latter. In conclusion, OrthoANI provides a more robust and faster means of calculating average nucleotide identity for taxonomic purposes. The standalone software tools are freely available at http://www.ezbiocloud.net/sw/oat.', 'counts_by_year': [[2022, 290], [2021, 365], [2020, 366], [2019, 305], [2018, 151], [2017, 76], [2016, 30]]}, {'id': 'W2342662179', 'doi': 'https://doi.org/10.1109/cvpr.2016.213', 'title': 'Convolutional Two-Stream Network Fusion for Video Action Recognition', 'type': 'proceedings-article', 'publication_date': '2016-06-27', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2276051864', ['I4092182']], ['A1996858318', ['I4092182']], ['A2469405535', ['I40120149']]], 'cited_by_count': 1581, 'concepts': [['C188441871', '0.8915847'], ['C41008148', '0.8253069'], ['C141353440', '0.78800005'], ['C81363708', '0.73430264'], ['C70437156', '0.6861754']], 'referenced_works': ['W1522734439', 'W1797109199', 'W1909952827', 'W1915485278', 'W1923332106', 'W1936750108', 'W1944615693', 'W1947481528', 'W1963882359', 'W1983364832', 'W2097117768', 'W2104657103', 'W2105101328', 'W2126579184', 'W2139501017', 'W2142194269', 'W2964191259', 'W3099206234'], 'abstract': 'Recent applications of Convolutional Neural Networks (ConvNets) for human action recognition in videos have proposed different solutions for incorporating the appearance and motion information. We study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. We make the following findings: (i) that rather than fusing at the softmax layer, a spatial and temporal network can be fused at a convolution layer without loss of performance, but with a substantial saving in parameters, (ii) that it is better to fuse such networks spatially at the last convolutional layer than earlier, and that additionally fusing at the class prediction layer can boost accuracy, finally (iii) that pooling of abstract convolutional features over spatiotemporal neighbourhoods further boosts performance. Based on these studies we propose a new ConvNet architecture for spatiotemporal fusion of video snippets, and evaluate its performance on standard benchmarks where this architecture achieves state-of-the-art results.', 'counts_by_year': [[2022, 145], [2021, 339], [2020, 350], [2019, 354], [2018, 256], [2017, 128], [2016, 7]]}, {'id': 'W2470394683', 'doi': 'https://doi.org/10.1007/978-3-319-48881-3_56', 'title': 'Fully-Convolutional Siamese Networks for Object Tracking', 'type': 'book-chapter', 'publication_date': '2016-10-08', 'host_venue': 'V106296714', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2291497107', ['I40120149']], ['A1404050242', ['I40120149']], ['A2213684462', ['I40120149']], ['A332962150', ['I40120149']], ['A2107932047', ['I40120149']]], 'cited_by_count': 1577, 'concepts': [['C41008148', '0.8675592'], ['C2781238097', '0.5659527'], ['C154945302', '0.50655186'], ['C31972630', '0.48735958'], ['C202474056', '0.41147965']], 'referenced_works': ['W764651262', 'W818325216', 'W1677182931', 'W1857884451', 'W1904671147', 'W1955055330', 'W1955514522', 'W1963882359', 'W1997121481', 'W2062118960', 'W2066513826', 'W2089961441', 'W2098941887', 'W2117539524', 'W2124211486', 'W2126302311', 'W2127589108', 'W2154071538', 'W2154889144', 'W2211629196', 'W2214352687', 'W2244956674', 'W2315667193', 'W2325939864', 'W2408241409', 'W2421627342', 'W2440384215', 'W2474599091', 'W2964111344', 'W2964253307', 'W3099206234', 'W3102624093'], 'abstract': 'The problem of arbitrary object tracking has traditionally been tackled by learning a model of the object’s appearance exclusively online, using as sole training data the video itself. Despite the success of these methods, their online-only approach inherently limits the richness of the model they can learn. Recently, several attempts have been made to exploit the expressive power of deep convolutional networks. However, when the object to track is not known beforehand, it is necessary to perform Stochastic Gradient Descent online to adapt the weights of the network, severely compromising the speed of the system. In this paper we equip a basic tracking algorithm with a novel fully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset for object detection in video. Our tracker operates at frame-rates beyond real-time and, despite its extreme simplicity, achieves state-of-the-art performance in multiple benchmarks.', 'counts_by_year': [[2022, 224], [2021, 399], [2020, 378], [2019, 323], [2018, 176], [2017, 75], [2016, 2]]}, {'id': 'W3106445841', 'doi': 'https://doi.org/10.1109/comst.2017.2682318', 'title': 'Mobile Edge Computing: A Survey on Architecture and Computation Offloading', 'type': 'journal-article', 'publication_date': '2017-02-17', 'host_venue': 'V23688054', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2011496452', ['I44504214']], ['A2306738916', ['I44504214']]], 'cited_by_count': 1577, 'concepts': [['C41008148', '0.87481487'], ['C2781041963', '0.8709525'], ['C79974875', '0.8031432'], ['C2776061582', '0.7812036'], ['C2779191767', '0.67514056']], 'referenced_works': ['W974042900', 'W1486093464', 'W1513178448', 'W1533110991', 'W1534518406', 'W1551426553', 'W1558773556', 'W1564901575', 'W1576160514', 'W1595535199', 'W1596596203', 'W1970750044', 'W1980959920', 'W1989480108', 'W1993997180', 'W2001027366', 'W2008794526', 'W2011224733', 'W2013317495', 'W2015867069', 'W2023380813', 'W2025652426', 'W2025725145', 'W2025725209', 'W2032129959', 'W2035309464', 'W2037409218', 'W2040340473', 'W2055376552', 'W2055929537', 'W2064969374', 'W2069211379', 'W2073673930', 'W2073765234', 'W2075410792', 'W2080017588', 'W2087214956', 'W2088692353', 'W2089657212', 'W2093843984', 'W2101788345', 'W2104237724', 'W2112212746', 'W2114623221', 'W2116175219', 'W2119598581', 'W2125213524', 'W2125593153', 'W2129021586', 'W2135099885', 'W2143488688', 'W2146848644', 'W2146981582', 'W2159619460', 'W2160886020', 'W2168962181', 'W2181360866', 'W2187775781', 'W2195423816', 'W2199213865', 'W2243451334', 'W2262390119', 'W2268062820', 'W2281422343', 'W2288033236', 'W2291697181', 'W2320494822', 'W2336451237', 'W2344423009', 'W2344607369', 'W2344695221', 'W2345301610', 'W2397723600', 'W2400861403', 'W2401898190', 'W2404643895', 'W2463406648', 'W2483192650', 'W2485510852', 'W2487267831', 'W2490612150', 'W2496693342', 'W2511425482', 'W2516316490', 'W2520473783', 'W2547727199', 'W2562344159', 'W2564810971', 'W2567074046', 'W2568817620', 'W2576752910', 'W2962804709', 'W2963508401', 'W2963529364', 'W2964335916', 'W3104952626', 'W3111145724'], 'abstract': 'Technological evolution of mobile user equipments (UEs), such as smartphones or laptops, goes hand-in-hand with evolution of new mobile applications. However, running computationally demanding applications at the UEs is constrained by limited battery capacity and energy consumption of the UEs. Suitable solution extending the battery life-time of the UEs is to offload the applications demanding huge processing to a conventional centralized cloud (CC). Nevertheless, this option introduces significant execution delay consisting in delivery of the offloaded applications to the cloud and back plus time of the computation at the cloud. Such delay is inconvenient and make the offloading unsuitable for real-time applications. To cope with the delay problem, a new emerging concept, known as mobile edge computing (MEC), has been introduced. The MEC brings computation and storage resources to the edge of mobile network enabling to run the highly demanding applications at the UE while meeting strict delay requirements. The MEC computing resources can be exploited also by operators and third parties for specific purposes. In this paper, we first describe major use cases and reference scenarios where the MEC is applicable. After that we survey existing concepts integrating MEC functionalities to the mobile networks and discuss current advancement in standardization of the MEC. The core of this survey is, then, focused on user-oriented use case in the MEC, i.e., computation offloading. In this regard, we divide the research on computation offloading to three key areas: i) decision on computation offloading, ii) allocation of computing resource within the MEC, and iii) mobility management. Finally, we highlight lessons learned in area of the MEC and we discuss open research challenges yet to be addressed in order to fully enjoy potentials offered by the MEC.', 'counts_by_year': [[2022, 267], [2021, 411], [2020, 442], [2019, 288], [2018, 144], [2017, 20]]}, {'id': 'W2559028527', 'doi': 'https://doi.org/10.1093/nar/gkw1133', 'title': 'The new NHGRI-EBI Catalog of published genome-wide association studies (GWAS Catalog)', 'type': 'journal-article', 'publication_date': '2017-01-04', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A1971827112', ['I1303153112']], ['A2584546638', ['I1303153112']], ['A2801005943', ['I1303153112']], ['A2711093843', ['I1303153112']], ['A2240922891', ['I4210090236']], ['A2385062526', ['I1303153112']], ['A1997756573', ['I4210090236']], ['A2509215578', ['I1303153112']], ['A2683453324', ['I1303153112']], ['A2271944062', ['I1303153112']], ['A2558383750', ['I1303153112']], ['A2137344641', ['I1303153112']], ['A1968424439', ['I1303153112']], ['A194598713', ['I4210090236']], ['A2121717666', ['I1303153112']], ['A2210664129', ['I1303153112']], ['A2046771686', ['I1303153112']]], 'cited_by_count': 1575, 'concepts': [['C106208931', '0.6728586'], ['C86803240', '0.5755908'], ['C113843644', '0.51922995'], ['C10590036', '0.45048875'], ['C2522767166', '0.42976284']], 'referenced_works': ['W183866246', 'W1973798273', 'W1977846646', 'W1980725116', 'W2051674484', 'W2065018089', 'W2077955816', 'W2096465161', 'W2100203037', 'W2113784410', 'W2116868464', 'W2117446594', 'W2135665336', 'W2136101247', 'W2147018385', 'W2148749248', 'W2165064122', 'W2183091920', 'W2238851414', 'W2396557861', 'W4252232449'], 'abstract': 'The NHGRI-EBI GWAS Catalog has provided data from published genome-wide association studies since 2008. In 2015, the database was redesigned and relocated to EMBL-EBI. The new infrastructure includes a new graphical user interface (www.ebi.ac.uk/gwas/), ontology supported search functionality and an improved curation interface. These developments have improved the data release frequency by increasing automation of curation and providing scaling improvements. The range of available Catalog data has also been extended with structured ancestry and recruitment information added for all studies. The infrastructure improvements also support scaling for larger arrays, exome and sequencing studies, allowing the Catalog to adapt to the needs of evolving study design, genotyping technologies and user needs in the future.', 'counts_by_year': [[2022, 140], [2021, 239], [2020, 277], [2019, 440], [2018, 349], [2017, 127], [2016, 1], [2015, 2]]}, {'id': 'W2891503716', 'doi': 'https://doi.org/10.1109/access.2018.2870052', 'title': 'Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)', 'type': 'journal-article', 'publication_date': '2018-09-17', 'host_venue': 'V2485537415', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2895832206', ['I81605866']], ['A2310408958', ['I81605866']]], 'cited_by_count': 1575, 'concepts': [['C41008148', '0.58203846'], ['C94966114', '0.55350554'], ['C154945302', '0.41473126']], 'referenced_works': ['W1505356189', 'W1787224781', 'W1915485278', 'W1972577679', 'W1975675278', 'W1994606570', 'W1996796871', 'W2001767453', 'W2013587512', 'W2026805626', 'W2059554529', 'W2061895235', 'W2063046703', 'W2066625485', 'W2084341220', 'W2084701050', 'W2091801169', 'W2113211233', 'W2116767760', 'W2118022153', 'W2119051895', 'W2125847307', 'W2126982030', 'W2134553571', 'W2135695572', 'W2148001258', 'W2154958885', 'W2158585626', 'W2164878629', 'W2195388612', 'W2266263576', 'W2282821441', 'W2284729062', 'W2295107390', 'W2295680652', 'W2318937711', 'W2339204188', 'W2549514639', 'W2596720518', 'W2604602969', 'W2613800022', 'W2613888037', 'W2615896489', 'W2749348810', 'W2751039952', 'W2766447205', 'W2786513456', 'W2795530988', 'W2802093198', 'W2806598546', 'W2809925683', 'W2963749936', 'W2963773574', 'W2963798744', 'W2964060211', 'W2964333534', 'W3099006712', 'W3101609372', 'W3121452939'], 'abstract': 'At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.', 'counts_by_year': [[2022, 478], [2021, 668], [2020, 341], [2019, 80], [2018, 2], [2012, 1]]}, {'id': 'W2171265988', 'doi': 'https://doi.org/10.1002/9781119508557.ch12', 'title': 'Introduction to Bayesian Networks', 'type': 'book-chapter', 'publication_date': '2019-03-27', 'host_venue': 'V62401924', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2198869620', []]], 'cited_by_count': 1574, 'concepts': [['C33724603', '0.76022434'], ['C41008148', '0.6508743'], ['C144559511', '0.6422392'], ['C49937458', '0.5855761'], ['C154945302', '0.56679916']], 'referenced_works': ['W2006258746', 'W2019963883', 'W2024332750', 'W2117812871', 'W2128299927'], 'abstract': 'Computational modelling of probability has become a major part of automated decision support systems. In this book, the principal ideas of probabilistic reasoning - known as Bayesian networks - are outlined and their practical implications illustrated. The book is intended for MSc students in knowledge-based systems, artificial intelligence and statistics, and for professionals in decision support systems applications and research.', 'counts_by_year': [[2021, 8], [2020, 16], [2019, 12], [2018, 13], [2017, 30], [2016, 32], [2015, 46], [2014, 48], [2013, 61], [2012, 64]]}, {'id': 'W2508069933', 'doi': 'https://doi.org/10.1016/s0140-6736(16)30958-8', 'title': 'Lung cancer: current therapies and new targeted treatments', 'type': 'journal-article', 'publication_date': '2017-01-21', 'host_venue': 'V49861241', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2776075750', ['I4210134151']], ['A2181774071', ['I55143463']], ['A2235575308', ['I1285301757']], ['A2680429803', ['I921990950']], ['A2131473987', ['I150468666']], ['A2530732794', ['I2799425052']], ['A722556301', ['I4210101691']]], 'cited_by_count': 1570, 'concepts': [['C71924100', '0.8252766'], ['C2776256026', '0.8206952'], ['C2777843972', '0.5673302'], ['C121608353', '0.5592919'], ['C177713679', '0.5313386']], 'referenced_works': ['W130099911', 'W1189434757', 'W1531732632', 'W1546552389', 'W1601800607', 'W1677365952', 'W1757407923', 'W1846586690', 'W1925282974', 'W1966752577', 'W1968899146', 'W1974903221', 'W1980497534', 'W1982488143', 'W1982907799', 'W1990626198', 'W1994539763', 'W1995867131', 'W2002127983', 'W2010619956', 'W2013768132', 'W2014346057', 'W2021481801', 'W2030227571', 'W2035845984', 'W2038901755', 'W2042480152', 'W2046189285', 'W2050249255', 'W2051021899', 'W2052546787', 'W2055402151', 'W2065293650', 'W2066671159', 'W2070059866', 'W2076178766', 'W2096156234', 'W2096198395', 'W2096216990', 'W2096322385', 'W2099530756', 'W2099904227', 'W2100912832', 'W2101441514', 'W2102192131', 'W2102625113', 'W2104347254', 'W2104504726', 'W2104701073', 'W2104830962', 'W2105820269', 'W2107211259', 'W2111580893', 'W2111662961', 'W2112621029', 'W2112854176', 'W2113149312', 'W2113876238', 'W2114635796', 'W2114784462', 'W2115646671', 'W2119317963', 'W2121110176', 'W2123394258', 'W2124427232', 'W2124967121', 'W2125665528', 'W2125735484', 'W2126709309', 'W2127129900', 'W2129068191', 'W2129360604', 'W2129590156', 'W2130488043', 'W2131988196', 'W2132578211', 'W2133149289', 'W2134787029', 'W2136023414', 'W2136601558', 'W2139110945', 'W2139804581', 'W2140980839', 'W2143488876', 'W2143859186', 'W2144918562', 'W2151116840', 'W2152481222', 'W2152939220', 'W2156353875', 'W2159060826', 'W2159192794', 'W2160074439', 'W2160304102', 'W2160725767', 'W2160889088', 'W2166164739', 'W2166618121', 'W2167191456', 'W2168404690', 'W2168691181', 'W2169624569', 'W2170030318', 'W2172010139', 'W2180425334', 'W2182482244', 'W2185920764', 'W2190649833', 'W2198093519', 'W2198405808', 'W2208535430', 'W2218718059', 'W2224190773', 'W2224395257', 'W2225799198', 'W2228221433', 'W2230051264', 'W2236653151', 'W2263280562', 'W2268387081', 'W2275877493', 'W2293531514', 'W2294012882', 'W2301564518', 'W2310252196', 'W2335671095', 'W2339311613', 'W2341288331', 'W2341730833', 'W2342552267', 'W2344034377', 'W2403654892', 'W2439107762', 'W2464088142', 'W2466008653', 'W2472128711', 'W2503552011', 'W2517665762', 'W2527615339', 'W2586885021', 'W2604216729', 'W4235389138'], 'abstract': 'Lung cancer is the most frequent cause of cancer-related deaths worldwide. Every year, 1·8 million people are diagnosed with lung cancer, and 1·6 million people die as a result of the disease. 5-year survival rates vary from 4-17% depending on stage and regional differences. In this Seminar, we discuss existing treatment for patients with lung cancer and the promise of precision medicine, with special emphasis on new targeted therapies. Some subgroups, eg-patients with poor performance status and elderly patients-are not specifically addressed, because these groups require special treatment considerations and no frameworks have been established in terms of new targeted therapies. We discuss prevention and early detection of lung cancer with an emphasis on lung cancer screening. Although we acknowledge the importance of smoking prevention and cessation, this is a large topic beyond the scope of this Seminar.', 'counts_by_year': [[2022, 391], [2021, 462], [2020, 368], [2019, 220], [2018, 75], [2017, 52], [2016, 1]]}, {'id': 'W2754529701', 'doi': 'https://doi.org/10.1109/bigdatacongress.2017.85', 'title': 'An Overview of Blockchain Technology: Architecture, Consensus, and Future Trends', 'type': 'proceedings-article', 'publication_date': '2017-06-25', 'host_venue': 'V4306419964', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A3164421333', ['I157773358']], ['A2756284913', ['I157773358']], ['A2161461259', ['I111950717']], ['A2304902609', ['I157773358']], ['A2131230917', ['I170215575']]], 'cited_by_count': 1559, 'concepts': [['C2779687700', '0.99105984'], ['C48044578', '0.7010303'], ['C41008148', '0.6877885'], ['C38652104', '0.5071666'], ['C48798503', '0.4744861']], 'referenced_works': ['W69553326', 'W1589846406', 'W1819513546', 'W1988600100', 'W2013613544', 'W2013686672', 'W2022932831', 'W2043007983', 'W2092068344', 'W2100522554', 'W2127612127', 'W2295506186', 'W2335755599', 'W2517744317', 'W3137092842'], 'abstract': 'Blockchain, the foundation of Bitcoin, has received extensive attentions recently. Blockchain serves as an immutable ledger which allows transactions take place in a decentralized manner. Blockchain-based applications are springing up, covering numerous fields including financial services, reputation system and Internet of Things (IoT), and so on. However, there are still many challenges of blockchain technology such as scalability and security problems waiting to be overcome. This paper presents a comprehensive overview on blockchain technology. We provide an overview of blockchain architechture firstly and compare some typical consensus algorithms used in different blockchains. Furthermore, technical challenges and recent advances are briefly listed. We also lay out possible future trends for blockchain.', 'counts_by_year': [[2022, 302], [2021, 424], [2020, 404], [2019, 326], [2018, 100], [2017, 1]]}, {'id': 'W2518099942', 'doi': 'https://doi.org/10.1038/nenergy.2016.141', 'title': 'A solid future for battery development', 'type': 'journal-article', 'publication_date': '2016-09-08', 'host_venue': 'V2764528046', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A1775886856', ['I200763008', 'I4210123207']], ['A2190676008', ['I200763008']]], 'cited_by_count': 1558, 'concepts': [['C555008776', '0.5333991'], ['C192562407', '0.3823421'], ['C39432304', '0.3746343'], ['C41008148', '0.33853334'], ['C119599485', '0.32347527']], 'referenced_works': ['W1606528480', 'W1964940765', 'W2007811913', 'W2017265410', 'W2021172010', 'W2022940027', 'W2042374222', 'W2051352594', 'W2056146245', 'W2058761265', 'W2063040771', 'W2065124153', 'W2077244146', 'W2107907073', 'W2153511586', 'W2153690895', 'W2190291396', 'W2193795818', 'W2202135044', 'W2237868774', 'W2275977556', 'W2294978050', 'W2317313911', 'W2321696461', 'W2322684643', 'W2329970688'], 'abstract': 'Solid-state batteries have recently attracted great interest as potentially safe and stable high-energy storage systems. However, key issues remain unsolved, hindering full-scale commercialization.', 'counts_by_year': [[2022, 360], [2021, 382], [2020, 333], [2019, 238], [2018, 170], [2017, 70], [2016, 3]]}, {'id': 'W2890019883', 'doi': 'https://doi.org/10.1021/acs.chemrev.8b00252', 'title': 'Design and Mechanisms of Asymmetric Supercapacitors', 'type': 'journal-article', 'publication_date': '2018-09-11', 'host_venue': 'V41143188', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2154473047', ['I181326427']], ['A2223538985', []], ['A2945737742', ['I3923682']], ['A2133785086', ['I181326427']], ['A2156237967', ['I181326427']], ['A2151288384', ['I181326427']], ['A2244908893', ['I181326427']], ['A2096842463', ['I142207940']], ['A2146299780', ['I142207940']]], 'cited_by_count': 1551, 'concepts': [['C6585489', '0.9632864'], ['C171250308', '0.63247794'], ['C73916439', '0.61540866'], ['C138331895', '0.5054364'], ['C26517878', '0.47101533']], 'referenced_works': ['W69522030', 'W1111595532', 'W1165364267', 'W1492545666', 'W1513767627', 'W1517069337', 'W1525197209', 'W1557441567', 'W1566582213', 'W1585567441', 'W1588886718', 'W1593570934', 'W1630361221', 'W1757205207', 'W1874505208', 'W1949231392', 'W1963496313', 'W1964151094', 'W1964708960', 'W1966097563', 'W1967486418', 'W1968636847', 'W1968721958', 'W1970320842', 'W1970393884', 'W1970996323', 'W1972462650', 'W1973095864', 'W1973101250', 'W1973231138', 'W1973408623', 'W1974447796', 'W1974449770', 'W1974463460', 'W1975129924', 'W1976111692', 'W1976459492', 'W1978126078', 'W1978177237', 'W1978232133', 'W1978586909', 'W1978692671', 'W1978749092', 'W1979171606', 'W1979189818', 'W1979198089', 'W1979664712', 'W1980790675', 'W1980853278', 'W1981051545', 'W1981231461', 'W1981652014', 'W1982179841', 'W1982345975', 'W1982446964', 'W1982685288', 'W1982897742', 'W1982904970', 'W1984355645', 'W1984440922', 'W1984917535', 'W1985877259', 'W1986268907', 'W1987014824', 'W1987305843', 'W1987494872', 'W1987722243', 'W1988080928', 'W1988648915', 'W1990198527', 'W1990249892', 'W1991291267', 'W1991780439', 'W1992486962', 'W1993243068', 'W1994821381', 'W1995151712', 'W1995264557', 'W1996003894', 'W1997430185', 'W1997594313', 'W1998561739', 'W1999359379', 'W1999768189', 'W2000320449', 'W2000617141', 'W2002380056', 'W2002539133', 'W2002601474', 'W2003236628', 'W2003394504', 'W2003489030', 'W2003850244', 'W2004432407', 'W2005624388', 'W2005758178', 'W2005824291', 'W2005956456', 'W2006318126', 'W2006477082', 'W2007435578', 'W2008176070', 'W2008294721', 'W2009074647', 'W2011339307', 'W2011452759', 'W2011529372', 'W2012044503', 'W2012170343', 'W2013274529', 'W2013408784', 'W2013974338', 'W2014025437', 'W2014154560', 'W2014254367', 'W2014355304', 'W2014677591', 'W2015071668', 'W2016464593', 'W2016944066', 'W2017751495', 'W2017894021', 'W2018568449', 'W2018652376', 'W2019123470', 'W2020017918', 'W2021076295', 'W2021094821', 'W2021172010', 'W2022010275', 'W2022097579', 'W2022171069', 'W2022430271', 'W2022506474', 'W2025127766', 'W2025479387', 'W2025481766', 'W2026346936', 'W2026529627', 'W2026575951', 'W2027629142', 'W2027785472', 'W2027883622', 'W2028799061', 'W2030124799', 'W2030585182', 'W2030858464', 'W2030917118', 'W2031587962', 'W2032047972', 'W2032100925', 'W2032638239', 'W2032774512', 'W2033522533', 'W2033573537', 'W2033912134', 'W2034623548', 'W2035206741', 'W2035306575', 'W2036119403', 'W2036148824', 'W2036843304', 'W2036921299', 'W2037502361', 'W2038239334', 'W2038337648', 'W2038361512', 'W2039786475', 'W2039861687', 'W2040858138', 'W2041140079', 'W2042102413', 'W2042438728', 'W2043188665', 'W2043512589', 'W2045072060', 'W2047668423', 'W2047915652', 'W2049965679', 'W2051509069', 'W2052319966', 'W2052595659', 'W2053219565', 'W2053328406', 'W2053488191', 'W2053526231', 'W2054488215', 'W2054806846', 'W2055486200', 'W2058391670', 'W2058685071', 'W2058755128', 'W2059438431', 'W2060666214', 'W2063065493', 'W2063994047', 'W2064490853', 'W2064797523', 'W2065042378', 'W2065146676', 'W2066220610', 'W2066536277', 'W2070258945', 'W2070378895', 'W2070783676', 'W2072419120', 'W2072545580', 'W2072920650', 'W2074262287', 'W2075021245', 'W2075348731', 'W2075904417', 'W2076115285', 'W2077717615', 'W2078072074', 'W2078858935', 'W2081248537', 'W2081870623', 'W2083176007', 'W2085072678', 'W2086041767', 'W2086738370', 'W2086765412', 'W2087606978', 'W2087656378', 'W2087781958', 'W2089008003', 'W2089061129', 'W2089083068', 'W2089470822', 'W2089525884', 'W2090631375', 'W2090778152', 'W2091543551', 'W2091730557', 'W2091740599', 'W2093182648', 'W2093348440', 'W2093831851', 'W2095116311', 'W2096683902', 'W2097536703', 'W2097634770', 'W2097924219', 'W2098490343', 'W2098887811', 'W2101262472', 'W2102002349', 'W2102892245', 'W2103035009', 'W2104330751', 'W2104827573', 'W2105038077', 'W2106222858', 'W2106248046', 'W2106875576', 'W2108165585', 'W2108989992', 'W2109111609', 'W2109593963', 'W2110321398', 'W2110376406', 'W2111022026', 'W2111399906', 'W2114013071', 'W2115421428', 'W2116327492', 'W2118196965', 'W2118617310', 'W2122709469', 'W2123942020', 'W2124075915', 'W2124766968', 'W2131218991', 'W2131423686', 'W2131799910', 'W2132482504', 'W2132624238', 'W2133226943', 'W2134288491', 'W2135575245', 'W2136347857', 'W2139402501', 'W2139731327', 'W2139897690', 'W2139921601', 'W2142595200', 'W2142841029', 'W2143429349', 'W2143890708', 'W2143952856', 'W2145191789', 'W2145742276', 'W2146183594', 'W2146617620', 'W2147508004', 'W2149053345', 'W2149119196', 'W2150673614', 'W2151760332', 'W2152411537', 'W2152690962', 'W2153917923', 'W2155474329', 'W2156254108', 'W2156962388', 'W2157298237', 'W2157808906', 'W2159378660', 'W2162677524', 'W2164512959', 'W2165149676', 'W2165150159', 'W2167815070', 'W2170401117', 'W2170749250', 'W2170962008', 'W2172177289', 'W2187594337', 'W2220719432', 'W2229124923', 'W2264338402', 'W2289162424', 'W2292848292', 'W2297069771', 'W2313702111', 'W2315077573', 'W2316555084', 'W2319682219', 'W2320598980', 'W2322262925', 'W2322934746', 'W2323852127', 'W2324981427', 'W2326058938', 'W2326642132', 'W2326852907', 'W2332109073', 'W2333181595', 'W2333910605', 'W2334072737', 'W2340338681', 'W2396325454', 'W2404711663', 'W2404725544', 'W2405918300', 'W2408092447', 'W2415044037', 'W2415435445', 'W2462774717', 'W2498519584', 'W2509883496', 'W2525823194', 'W2531080479', 'W2533840362', 'W2549029156', 'W2549843691', 'W2556723530', 'W2558590332', 'W2559817462', 'W2575597094', 'W2594796904', 'W2601907750', 'W2603131419', 'W2613241133', 'W2613472177', 'W2613748579', 'W2618056566', 'W2622043388', 'W2709233531', 'W2732116070', 'W2752395163', 'W2753377532', 'W2755314573', 'W2755966471', 'W2756435273', 'W2757041592', 'W2759535314', 'W2767672125', 'W2773162330', 'W2781867630', 'W2789863381', 'W2791201420', 'W2791623637', 'W2793197584', 'W2795176435', 'W2803550761', 'W3023435578', 'W3102454905', 'W4211046987', 'W4230016485', 'W4239864304', 'W4245158813'], 'abstract': 'Ongoing technological advances in diverse fields including portable electronics, transportation, and green energy are often hindered by the insufficient capability of energy-storage devices. By taking advantage of two different electrode materials, asymmetric supercapacitors can extend their operating voltage window beyond the thermodynamic decomposition voltage of electrolytes while enabling a solution to the energy storage limitations of symmetric supercapacitors. This review provides comprehensive knowledge to this field. We first look at the essential energy-storage mechanisms and performance evaluation criteria for asymmetric supercapacitors to understand the wide-ranging research conducted in this area. Then we move to the recent progress made for the design and fabrication of electrode materials and the overall structure of asymmetric supercapacitors in different categories. We also highlight several key scientific challenges and present our perspectives on enhancing the electrochemical performance of future asymmetric supercapacitors.', 'counts_by_year': [[2022, 429], [2021, 510], [2020, 413], [2019, 178], [2018, 4]]}, {'id': 'W2398139753', 'doi': 'https://doi.org/10.1093/jmammal/gyw078', 'title': '2016 Guidelines of the American Society of Mammalogists for the use of wild mammals in research and education:', 'type': 'journal-article', 'publication_date': '2016-06-09', 'host_venue': 'V165569574', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A1995818627', ['I102401767']]], 'cited_by_count': 1545, 'concepts': [['C206345919', '0.47191125'], ['C161191863', '0.44356418'], ['C29376679', '0.43069398'], ['C205649164', '0.38436407'], ['C17744445', '0.36702234']], 'referenced_works': ['W1965628923', 'W1975003532', 'W1976217011', 'W1979723009', 'W1992043629', 'W2002345256', 'W2002380766', 'W2013019703', 'W2023071829', 'W2038318567', 'W2043180237', 'W2075901050', 'W2087830543', 'W2115388023', 'W2119978637', 'W2130206481', 'W2142468960', 'W2144346440', 'W2144909109', 'W2145234285', 'W2163657311', 'W2171659204', 'W2176746262', 'W2190982433', 'W2285581860', 'W2290800182', 'W2313489393', 'W2313973174', 'W2315005662', 'W2316644749', 'W2316850849', 'W2320975585', 'W2322134088', 'W2323605994', 'W2325672065', 'W2326192479', 'W2326853044', 'W2333873577', 'W2334563472', 'W2408895522', 'W2923666106', 'W4237266689', 'W4237348020', 'W4245136990', 'W4250181719', 'W4253512500', 'W4299838924'], 'abstract': 'Guidelines for use of wild mammal species in research are updated from Sikes et al. (2011) . These guidelines cover current professional techniques and regulations involving the use of mammals in research and teaching; they also incorporate new resources, procedural summaries, and reporting requirements. Included are details on capturing, marking, housing, and humanely killing wild mammals. It is recommended that Institutional Animal Care and Use Committees (IACUCs), regulatory agencies, and investigators use these guidelines as a resource for protocols involving wild mammals, whether studied in the field or in captivity. These guidelines were prepared and approved by the American Society of Mammalogists (ASM), in consultation with professional veterinarians experienced in wildlife research and IACUCs, whose collective expertise provides a broad and comprehensive understanding of the biology of nondomesticated mammals. The current version of these guidelines and any subsequent modifications are available online on the Animal Care and Use Committee page of the ASM website ( http://mammalogy.org/uploads/committee_files/CurrentGuidelines.pdf ). Additional resources pertaining to the use of wild animals in research are available at: http://www.mammalsociety.org/committees/animal-care-and-use#tab3 .Los lineamientos para el uso de especies de mamíferos de vida silvestre en la investigación con base en Sikes et al. (2011) se actualizaron. Dichos lineamientos cubren técnicas y regulaciones profesionales actuales que involucran el uso de mamíferos en la investigación y enseñanza; también incorporan recursos nuevos, resúmenes de procedimientos y requisitos para reportes. Se incluyen detalles acerca de captura, marcaje, manutención en cautiverio y eutanasia de mamíferos de vida silvestre. Se recomienda que los comités institucionales de uso y cuidado animal (cifras en inglés: IACUCs), las agencias reguladoras y los investigadores se adhieran a dichos lineamientos como fuente base de protocolos que involucren mamíferos de vida silvestre, ya sea investigaciones de campo o en cautiverio. Dichos lineamientos fueron preparados y aprobados por la ASM, en consulta con profesionales veterinarios experimentados en investigaciones de vida silvestre y IACUCS, de quienes cuya experiencia colectiva provee un entendimiento amplio y exhaustivo de la biología de mamíferos no-domesticados. La presente versión de los lineamientos y modificaciones posteriores están disponibles en línea en la página web de la ASM, bajo Cuidado Animal y Comité de Uso: ( http://mammalogy.org/uploads/committee_files/CurrentGuidelines.pdf ). Recursos adicionales relacionados con el uso de animales de vida silvestre para la investigación se encuentran disponibles en ( http://www.mammalsociety.org/committees/animal-care-and-use#tab3 ).', 'counts_by_year': [[2022, 234], [2021, 298], [2020, 276], [2019, 278], [2018, 213], [2017, 142], [2016, 52], [2015, 43], [2014, 6], [2013, 1]]}, {'id': 'W2622826443', 'doi': 'https://doi.org/10.1162/neco_a_00990', 'title': 'Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review', 'type': 'journal-article', 'publication_date': '2017-09-01', 'host_venue': 'V207023548', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2623878839', ['I165390105']], ['A2153138036', ['I165390105']]], 'cited_by_count': 1545, 'concepts': [['C81363708', '0.81778'], ['C52069626', '0.80472505'], ['C108583219', '0.79429185'], ['C2780586970', '0.75584984'], ['C154945302', '0.6669825']], 'referenced_works': ['W20283819', 'W51946390', 'W114517082', 'W1487564550', 'W1498436455', 'W1502609557', 'W1523493493', 'W1524680991', 'W1534665966', 'W1576278180', 'W1606858007', 'W1613249581', 'W1677182931', 'W1840106123', 'W1849277567', 'W1895577753', 'W1919191429', 'W1932198206', 'W1932847118', 'W1934184906', 'W1936750108', 'W1938425378', 'W1950843348', 'W1963826206', 'W1969067576', 'W1973895663', 'W1980287119', 'W1984309565', 'W1985835389', 'W1993482030', 'W1994197834', 'W1994906459', 'W1996901117', 'W1998808035', 'W2000723898', 'W2012220983', 'W2015861736', 'W2019464758', 'W2021354639', 'W2025183033', 'W2040868835', 'W2047389622', 'W2053229256', 'W2058641082', 'W2062118960', 'W2063632045', 'W2064675550', 'W2074772891', 'W2076063813', 'W2082855665', 'W2091987367', 'W2097117768', 'W2098554822', 'W2099171762', 'W2100495367', 'W2101926813', 'W2102605133', 'W2103212315', 'W2104978738', 'W2107878631', 'W2111244981', 'W2112284959', 'W2112796928', 'W2114588272', 'W2115733720', 'W2116064496', 'W2116360511', 'W2117539524', 'W2117876524', 'W2117951582', 'W2125653371', 'W2126833203', 'W2128154306', 'W2130325614', 'W2132424367', 'W2134557905', 'W2136922672', 'W2138621090', 'W2139427956', 'W2140262144', 'W2141125852', 'W2141200610', 'W2144354855', 'W2145287260', 'W2147800946', 'W2155893237', 'W2156163116', 'W2156297475', 'W2156909104', 'W2157364932', 'W2158778629', 'W2159291644', 'W2159737176', 'W2162262658', 'W2162915993', 'W2163922914', 'W2164370980', 'W2165828254', 'W2169805405', 'W2171590421', 'W2172654076', 'W2176950688', 'W2179352600', 'W2194775991', 'W2285660444', 'W2300242332', 'W2302255633', 'W2331143823', 'W2345474290', 'W2404540148', 'W2468059673', 'W2470322391', 'W2474608001', 'W2536626143', 'W2546302380', 'W2578935686', 'W2919115771', 'W2962719052', 'W2962734882', 'W2963003451', 'W2963012631', 'W2963234238', 'W2964082701', 'W3022436500', 'W3099206234', 'W3101287485', 'W4205947740', 'W4231109964', 'W4232280717', 'W4238284510'], 'abstract': 'Convolutional neural networks (CNNs) have been applied to visual tasks since the late 1980s. However, despite a few scattered applications, they were dormant until the mid-2000s when developments in computing power and the advent of large amounts of labeled data, supplemented by improved algorithms, contributed to their advancement and brought them to the forefront of a neural network renaissance that has seen rapid progression since 2012. In this review, which focuses on the application of CNNs to image classification tasks, we cover their development, from their predecessors up to recent state-of-the-art deep learning systems. Along the way, we analyze (1) their early successes, (2) their role in the deep learning renaissance, (3) selected symbolic works that have contributed to their recent popularity, and (4) several improvement attempts by reviewing contributions and challenges of over 300 publications. We also introduce some of their current trends and remaining challenges.', 'counts_by_year': [[2022, 359], [2021, 508], [2020, 393], [2019, 211], [2018, 61], [2017, 6]]}, {'id': 'W2964101377', 'doi': 'https://doi.org/10.1109/cvpr.2018.00262', 'title': 'Residual Dense Network for Image Super-Resolution', 'type': 'proceedings-article', 'publication_date': '2018-06-18', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2297316014', ['I12912129']], ['A2429400013', ['I5388228']], ['A2101737198', ['I12912129']], ['A2102375896', ['I12912129']], ['A2123131494', ['I12912129']]], 'cited_by_count': 1541, 'concepts': [['C81363708', '0.8132411'], ['C185798385', '0.77895933'], ['C155512373', '0.76233995'], ['C41008148', '0.7448003'], ['C2776401178', '0.6749372']], 'referenced_works': ['W1885185971', 'W1930824406', 'W1950594372', 'W1983781364', 'W2047920195', 'W2057065563', 'W2121927366', 'W2133665775', 'W2150081556', 'W2157190232', 'W2192954843', 'W2194775991', 'W2214802144', 'W2242218935', 'W2263468737', 'W2476548250', 'W2607041014', 'W2613155248', 'W2739757502', 'W2747898905', 'W2780544323', 'W2963372104', 'W2963470893', 'W2964125708', 'W2964277374'], 'abstract': 'A very deep convolutional neural network (CNN) has recently achieved great success for image super-resolution (SR) and offered hierarchical features as well. However, most deep CNN based SR models do not make full use of the hierarchical features from the original low-resolution (LR) images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in image SR. We fully exploit the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via dense connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory (CM) mechanism. Local feature fusion in RDB is then used to adaptively learn more effective features from preceding and current local features and stabilizes the training of wider network. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Experiments on benchmark datasets with different degradation models show that our RDN achieves favorable performance against state-of-the-art methods.', 'counts_by_year': [[2022, 265], [2021, 521], [2020, 469], [2019, 250], [2018, 30]]}, {'id': 'W2971836485', 'doi': 'https://doi.org/10.1103/physrevx.9.031040', 'title': 'GWTC-1: A Gravitational-Wave Transient Catalog of Compact Binary Mergers Observed by LIGO and Virgo during the First and Second Observing Runs', 'type': 'journal-article', 'publication_date': '2019-09-04', 'host_venue': 'V137042341', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A3178754429', ['I122411786']], ['A2931258442', []], ['A2898869343', []], ['A2899004101', []], ['A1989600986', []], ['A2102662472', []], ['A3130519413', []], ['A2120816080', []], ['A2643273335', ['I149899117']], ['A2949309055', ['I149899117']], ['A2063771739', []], ['A2196045220', []], ['A2149021947', []], ['A3214412835', []], ['A2013867193', []], ['A3188632010', []], ['A2590089022', []], ['A2116332243', []], ['A3187237540', []], ['A2330323686', []], ['A1977554423', []], ['A2097044971', []], ['A3192986197', []], ['A3124361617', []], ['A3212470564', []], ['A3153306750', []], ['A3206652915', []], ['A3193127485', []], ['A2730442849', []], ['A3192504475', []], ['A3187769929', []], ['A2884359312', []], ['A2872020220', []], ['A2259678854', []], ['A2947746710', []], ['A2113463528', []], ['A2031868467', []], ['A2022335298', []], ['A2806163201', []], ['A1986532585', []], ['A3212970892', []], ['A2763707440', []], ['A2899345576', []], ['A2984619410', []], ['A2624902964', ['I149899117']], ['A2776483237', []], ['A3214511802', []], ['A2980672605', []], ['A3212616752', []], ['A3037946954', []], ['A3212949844', []], ['A2079318338', []], ['A3206141467', []], ['A3177099217', []], ['A3192154922', []], ['A2596778389', []], ['A2973721591', []], ['A2161639735', []], ['A3099877015', []], ['A2997780418', []], ['A2136366644', []], ['A2580889767', []], ['A2041338530', []], ['A2862977381', []], ['A3190637119', []], ['A2106758119', []], ['A2061148210', []], ['A2030123867', []], ['A2440901623', []], ['A3208170591', []], ['A3094403503', []], ['A2463697456', []], ['A3048686063', []], ['A2943352481', []], ['A3213399745', []], ['A2561527376', []], ['A3184339089', []], ['A2598481214', []], ['A3188724226', ['I149899117']], ['A884709112', []], ['A2763783792', []], ['A2167365184', []], ['A416185457', []], ['A2902171395', []], ['A2083399793', []], ['A3211899950', []], ['A3211257290', []], ['A2305878762', []], ['A2806336849', []], ['A2592016208', ['I181647926']], ['A3206513768', []], ['A2916176969', []], ['A3188721990', []], ['A1685694851', []], ['A3160198059', []], ['A3189789049', []], ['A1977286476', []], ['A2963003197', []], ['A3037119030', []], ['A2137817755', []], ['A2573281761', []], ['A2110062665', []], ['A3183534842', []], ['A3036941851', []], ['A2734424565', ['I149899117']], ['A2510367362', []], ['A3212234500', []], ['A3187225073', []], ['A2656910661', []], ['A2942746322', []], ['A401219682', []], ['A2998504994', ['I149899117']], ['A2340744523', []], ['A3191148886', []], ['A3176036958', []], ['A2950710615', []], ['A2752961157', []], ['A3135996568', []], ['A3212010517', []], ['A3213584691', []], ['A3188004614', []], ['A2161572305', []], ['A3187368144', []], ['A2133056318', []], ['A3214496640', []], ['A1649922120', []], ['A2590013777', []], ['A2762264107', []], ['A3214274015', []], ['A2919895144', []], ['A3213443375', []], ['A3022976532', ['I149899117']], ['A3174630990', []], ['A2964759560', []], ['A2916988494', []], ['A2828582320', []], ['A3214576486', []], ['A1920643890', []], ['A2049455156', []], ['A3192455471', []], ['A1949674659', ['I149899117']], ['A2955044138', []], ['A3112433548', []], ['A3160394671', []], ['A2047849238', []], ['A2816657102', ['I149899117']], ['A1985904051', []], ['A2595927603', ['I100532134']], ['A3211859875', []], ['A3212580957', []], ['A3214030669', []], ['A2284239319', []], ['A3020614285', []], ['A2983595523', []], ['A3213387392', []], ['A2946402245', []], ['A2552037243', []], ['A2119394733', []], ['A2633311535', []], ['A2942460859', []], ['A3168193628', []], ['A3211925547', []], ['A3184056980', []], ['A2253635596', []], ['A3183896882', []], ['A2853512116', []], ['A2428212893', []], ['A2215987423', []], ['A3137638649', []], ['A2095557066', []], ['A3213895913', []], ['A3213777938', []], ['A2993381879', []], ['A2837841027', []], ['A3121094524', []], ['A3171906794', []], ['A3081522446', []], ['A2293492264', []], ['A3213317439', []], ['A2555126875', []], ['A2355490837', []], ['A2812767936', []], ['A2906057127', []], ['A2949330054', []], ['A3212324755', []], ['A3187385361', []], ['A2298498211', []], ['A3175291849', []], ['A3213698473', []], ['A2906531714', []], ['A2765447218', []], ['A1966111217', []], ['A2026320856', []], ['A3212951553', []], ['A3135857111', []], ['A3211378190', []], ['A2165216277', []], ['A2302273048', []], ['A3198901737', []], ['A2805294218', []], ['A2876288685', []], ['A2886637637', []], ['A3213144574', []], ['A3191252368', []], ['A2807562062', []], ['A3211866706', []], ['A3089244475', []], ['A2806972367', []], ['A3212210347', []], ['A3192959517', []], ['A2561889523', []], ['A2793250799', []], ['A2629010976', []], ['A2567194190', []], ['A3095537412', []], ['A2979791816', []], ['A1993090682', ['I157674565']], ['A2608252036', []], ['A2946504086', []], ['A2949908670', []], ['A3186867372', []], ['A2980616383', []], ['A2743835024', []], ['A3088498399', []], ['A3212659004', []], ['A3196881120', []], ['A2343198265', []], ['A2104489708', []], ['A1997433525', []], ['A2309834725', []], ['A2807515302', ['I149899117']], ['A2794293094', []], ['A3088771138', []], ['A3182150140', []], ['A2973073773', []], ['A2607649874', []], ['A2879856988', []], ['A2252855434', []], ['A2008373628', []], ['A3087786845', []], ['A3190232124', []], ['A3212834775', []], ['A2277296262', []], ['A3129173276', []], ['A2324162412', []], ['A2906062181', []], ['A2829642999', []], ['A3213875306', []], ['A2144671710', []], ['A3216622180', []], ['A2783475783', []], ['A2044934989', []], ['A3211522277', []], ['A3185887323', ['I149899117']], ['A3093770069', []], ['A1979152311', ['I149899117']], ['A2135574779', []], ['A3180139287', []], ['A3167738374', []], ['A2146738652', []], ['A3211521442', []], ['A2168664669', []], ['A3214511527', []], ['A2015763309', []], ['A3214707733', []], ['A2806255016', []], ['A3010691662', []], ['A2304156027', []], ['A1418452008', []], ['A2116577483', []], ['A3197349342', []], ['A2766695860', []], ['A2569824804', ['I149899117']], ['A2766499774', []], ['A2805214195', []], ['A2141040907', []], ['A2767037644', []], ['A2880828452', []], ['A2766248084', ['I149899117']], ['A3213061463', []], ['A2977241735', []], ['A3037162861', []], ['A3212049868', []], ['A3147297631', []], ['A2793899711', []], ['A3011099301', []], ['A2805388938', []], ['A3214657051', []], ['A3212506750', []], ['A3144773985', []], ['A3212560564', []], ['A2274420929', []], ['A2147618017', []], ['A2598058840', []], ['A3197431885', ['I149899117']], ['A2981499915', []], ['A2305700984', []], ['A2241179355', []], ['A2862851245', []], ['A2230265955', []], ['A2905805660', []], ['A3212099532', []], ['A3197027745', []], ['A2899846597', []], ['A3160246366', []], ['A3091867744', []], ['A3197649087', []], ['A3214447848', []], ['A3119048600', []], ['A3016124112', []], ['A2806793317', []], ['A2791266189', []], ['A2467688294', []], ['A3189148401', []], ['A2807888253', []], ['A2028554557', []], ['A3190793682', []], ['A2917506697', []], ['A2948178927', []], ['A3206727944', []], ['A2592053980', []], ['A2307450509', []], ['A2820447793', []], ['A2876331988', []], ['A2641626098', []], ['A2641626098', []], ['A3213456684', []], ['A3211611348', []], ['A1999419386', []], ['A3211743260', []], ['A2766675109', []], ['A3211840338', []], ['A2149221475', []], ['A3105301088', []], ['A3108351319', []], ['A1833794549', []], ['A3175940303', []], ['A2906592182', []], ['A1998320771', []], ['A1965779292', []], ['A2984447949', []], ['A3103283984', []], ['A2558490475', []], ['A3196836202', []], ['A2806973963', []], ['A2766026765', []], ['A2792463995', []], ['A3212026953', []], ['A2116294106', []], ['A3191197435', []], ['A2142222472', []], ['A3213800390', []], ['A2591496726', []], ['A2139140847', []], ['A2653205707', []], ['A3213489065', []], ['A2026851826', []], ['A2167050613', []], ['A3214360346', []], ['A2112263148', []], ['A2782723294', []], ['A3173498059', []], ['A3191281857', []], ['A3000397202', []], ['A3200044225', []], ['A2884489641', []], ['A2951363368', []], ['A2608772347', []], ['A687187703', []], ['A3213055845', []], ['A2128417274', []], ['A2765354106', []], ['A2848021311', []], ['A2766944085', []], ['A3197661198', []], ['A3214056200', []], ['A2607714946', []], ['A3000744708', []], ['A3164183435', []], ['A3184262436', []], ['A3123928773', []], ['A2282535663', []], ['A2436983654', []], ['A2472445730', []], ['A3213672858', []], ['A2224979328', []], ['A2196472791', []], ['A3210093540', []], ['A2087067879', []], ['A2977042228', []], ['A2974774671', []], ['A1931051767', []], ['A2896549439', []], ['A3027403836', []], ['A2970255773', []], ['A3212738087', []], ['A2588028427', []], ['A2120371615', []], ['A2800816236', []], ['A3213909439', []], ['A2267511333', []], ['A2021276475', []], ['A1968023185', []], ['A2873961809', []], ['A3187762700', []], ['A3193155114', []], ['A2982931411', []], ['A3211802894', []], ['A1859510812', []], ['A2137278808', []], ['A2951105902', []], ['A2906245780', []], ['A2837878796', []], ['A2502810578', []], ['A3164983791', []], ['A2470660414', []], ['A3115672102', []], ['A2766716900', []], ['A2134513389', []], ['A3018296462', []], ['A3211337918', ['I149899117']], ['A2743903848', []], ['A3192147838', []], ['A2805840153', []], ['A2750548298', []], ['A2221050282', []], ['A3037040773', []], ['A3122707594', []], ['A3212331716', []], ['A3098422003', []], ['A3197882586', []], ['A3213215445', []], ['A2946415084', []], ['A3213838636', []], ['A2420107187', []], ['A3164587974', []], ['A3196871450', ['I149899117']], ['A2959089861', []], ['A2776268936', []], ['A2332457619', []], ['A1990656394', []], ['A2141423093', []], ['A2973814101', []], ['A2485895420', []], ['A2157477235', []], ['A2503627946', []], ['A2042136083', ['I149899117']], ['A2973622029', []], ['A2974203684', []], ['A3004597787', []], ['A2135192745', []], ['A2236585722', []], ['A2985328053', []], ['A2344571799', []], ['A3116516404', []], ['A2151255333', []], ['A2303437377', []], ['A2083397653', []], ['A2287281526', ['I149899117']], ['A2805556525', []], ['A3179915334', []], ['A2042955919', ['I149899117']], ['A3211603209', []], ['A3165639304', []], ['A2894018816', []], ['A2912162295', ['I149899117']], ['A2013597997', []], ['A2773864941', []], ['A3214027899', []], ['A3214534791', []], ['A2067057196', []], ['A2149005230', []], ['A2744851237', []], ['A1920053454', []], ['A2120138609', []], ['A3213728926', []], ['A2766920907', []], ['A2443672252', []], ['A2771330512', []], ['A2761304767', []], ['A3214494604', []], ['A2807398788', []], ['A2053798579', []], ['A3213909048', []], ['A2915743262', []], ['A3200779726', []], ['A3196459003', []], ['A2807150143', []], ['A3212112723', []], ['A3211694138', []], ['A2884218867', []], ['A2334165722', []], ['A3179832972', []], ['A2763511771', []], ['A3122260426', []], ['A2651934103', []], ['A2260744060', []], ['A2906382202', []], ['A2112891699', []], ['A2906645834', []], ['A2075323785', []], ['A3212179902', []], ['A2906433992', []], ['A3096406494', []], ['A470399586', []], ['A2905952050', []], ['A3188168968', []], ['A2361649979', []], ['A3149395330', []], ['A2137027242', []], ['A3214701367', ['I149899117']], ['A2966236035', []], ['A2998692115', []], ['A2564053002', []], ['A2567819330', []], ['A2812052584', []], ['A2974510088', []], ['A3025655894', []], ['A3131816118', []], ['A2324032376', ['I149899117']], ['A3214417641', []], ['A2763762051', []], ['A2088094235', []], ['A2951704019', []], ['A3014997762', []], ['A3106411652', []], ['A2946076134', []], ['A2805263574', []], ['A2144846099', []], ['A2375511350', []], ['A2332964699', []], ['A2893661853', []], ['A3214404249', []], ['A2805279307', []], ['A3193063450', []], ['A2797091175', ['I149899117']], ['A2105246234', []], ['A2973371795', []], ['A2906421031', []], ['A3144260476', []], ['A3205387863', []], ['A2992126523', []], ['A2896102104', []], ['A3169792634', []], ['A2834197742', []], ['A3037258224', []], ['A3036613123', []], ['A3214571791', []], ['A3213936075', []], ['A3169129870', []], ['A2703584364', ['I149899117']], ['A3137194014', []], ['A2974393256', []], ['A2905975918', []], ['A2976354671', []], ['A3214619139', []], ['A2108037976', ['I149899117']], ['A3198669105', ['I149899117']], ['A3212113571', []], ['A3172560174', []], ['A2973973483', []], ['A3213815220', []], ['A2917182657', ['I149899117']], ['A2833732568', []], ['A3021014529', []], ['A1685051018', []], ['A3094131044', []], ['A2838408147', ['I149899117']], ['A2906294454', []], ['A2426512568', []], ['A3198109926', ['I149899117']], ['A3199681967', []], ['A2234325469', []], ['A2949671386', []], ['A2581106287', []], ['A2131558627', []], ['A3175777026', []], ['A2090376259', []], ['A2834120332', ['I149899117']], ['A3126151952', []], ['A2906572499', []], ['A2955755552', []], ['A3212656223', []], ['A2660334908', []], ['A2791156957', []], ['A2306815261', []], ['A3214689789', []], ['A3212669462', []], ['A1973573521', []], ['A3147653104', []], ['A3113890189', []], ['A1826541738', []], ['A2021249184', []], ['A2549372245', ['I149899117']], ['A3167281730', []], ['A2809015571', []], ['A3212667814', []], ['A2095771935', []], ['A2103134365', []], ['A2482502493', []], ['A1169343919', []], ['A2142495155', ['I149899117']], ['A3121789463', []], ['A3129373350', []], ['A3131819562', []], ['A2795157789', []], ['A2968747388', []], ['A2443561061', []], ['A2884995774', []], ['A2913636264', []], ['A2900991861', []], ['A3124919011', []], ['A3213376923', []], ['A2682071947', []], ['A3198756242', []], ['A3123561278', []], ['A3212508305', []], ['A425630834', []], ['A2027578991', ['I29607241']], ['A3212259363', []], ['A2135441924', []], ['A2953771372', ['I29607241']], ['A3190696561', []], ['A2031087335', []], ['A3101521172', ['I149899117']], ['A2615368805', []], ['A2547743495', []], ['A2899354435', []], ['A3189629458', ['I149899117']], ['A2745238556', []], ['A2826849062', []], ['A2984096928', []], ['A2131331663', []], ['A2765766625', []], ['A3213828037', []], ['A2063177140', []], ['A3211713138', []], ['A3189077882', []], ['A2953110040', []], ['A2425517206', ['I29607241']], ['A3201457288', []], ['A1954864400', []], ['A3190282054', ['I29607241']], ['A3196387853', []], ['A2764146561', []], ['A2530083674', []], ['A3170207227', []], ['A3099062606', []], ['A3213923457', []], ['A2209475002', []], ['A2072248821', []], ['A2788508077', []], ['A3217471248', []], ['A3165376653', []], ['A2016056937', []], ['A3199007853', []], ['A2105787019', []], ['A3185243625', []], ['A2081131462', []], ['A2348623712', ['I149899117']], ['A3189577759', []], ['A2102079205', []], ['A2778954463', []], ['A3213878273', []], ['A2986016436', []], ['A2765896026', []], ['A3188738125', []], ['A3127618124', []], ['A3214530577', []], ['A3187908163', []], ['A2105490290', ['I149899117']], ['A1663768879', []], ['A3213275891', []], ['A2469800559', []], ['A3183609872', []], ['A3212939944', []], ['A2465526937', []], ['A1973105240', []], ['A3212080398', []], ['A3213784548', []], ['A2949277257', []], ['A2121893416', []], ['A3213209345', []], ['A2993123622', []], ['A2650871170', []], ['A1994734163', []], ['A2975777987', []], ['A2989034818', ['I149899117']], ['A2950169272', []], ['A2902246656', []], ['A2001351290', []], ['A3196574167', []], ['A2533606300', []], ['A2807423593', []], ['A2943856697', []], ['A3170098099', []], ['A3190502927', []], ['A2468204009', []], ['A2428639699', []], ['A3018553469', []], ['A2304030444', []], ['A2798531098', []], ['A2761792207', []], ['A2910186728', []], ['A2070676417', []], ['A2794983599', []], ['A2596795233', []], ['A2118056577', []], ['A2780469136', []], ['A3213614579', []], ['A2766026859', []], ['A2148062765', []], ['A2012213346', []], ['A3198651051', []], ['A2963708514', ['I24676775']], ['A2906041042', []], ['A3217497777', []], ['A2136639717', []], ['A3213995003', []], ['A2123327683', []], ['A3128870394', []], ['A2767114523', []], ['A2765084922', []], ['A3211526149', []], ['A2576892988', []], ['A3213265322', []], ['A3213760675', []], ['A2970106919', []], ['A3004627767', []], ['A2952436053', []], ['A2893928364', []], ['A3185592983', ['I149899117']], ['A2595464485', []], ['A3135606165', []], ['A2266688588', []], ['A3067585087', []], ['A1982961642', ['I84909340']], ['A3212714246', []], ['A2767201966', []], ['A2834318602', []], ['A3178757429', ['I84909340']], ['A2957965052', []], ['A2005208586', []], ['A2599763636', []], ['A2097849493', []], ['A2439563356', []], ['A3206407848', []], ['A2863447672', ['I149899117']], ['A3212653464', []], ['A3211423238', []], ['A2805860137', []], ['A3184351269', []], ['A1969889460', []], ['A2781815556', ['I149899117']], ['A2272353480', []], ['A2023299128', ['I149899117']], ['A2963376855', []], ['A2306413170', []], ['A2871160851', []], ['A2167727370', []], ['A2988703613', []], ['A3212830524', []], ['A2768289150', []], ['A2975435233', []], ['A3103100839', []], ['A3121867565', []], ['A2949236356', ['I149899117']], ['A2806731212', []], ['A3023393658', []], ['A2975609283', []], ['A3172241110', ['I149899117']], ['A3214157093', []], ['A2780453633', []], ['A3208341317', []], ['A2745157023', []], ['A2108716544', []], ['A2834474587', ['I149899117']], ['A2579844043', []], ['A3191279662', []], ['A2104041286', []], ['A3022666979', []], ['A3187619492', []], ['A2743962002', []], ['A2976108310', []], ['A3212223573', []], ['A3197931354', []], ['A3066450176', []], ['A3206518448', []], ['A2840184033', []], ['A2953715031', []], ['A3149136633', []], ['A2743103778', []], ['A2521298114', []], ['A1967288566', []], ['A3213312486', []], ['A2024848060', []], ['A3095169362', []], ['A2766819460', ['I149899117']], ['A2767198558', []], ['A3039043906', []], ['A2253053690', []], ['A2915091928', []], ['A3189327450', []], ['A3192106124', []], ['A2162010643', []], ['A2611783856', ['I100532134']], ['A2269450469', []], ['A2807529995', []], ['A2974034227', []], ['A1975865521', ['I100532134']], ['A2142834634', []], ['A3214407301', []], ['A2104112402', []], ['A3125804629', []], ['A2954703215', []], ['A2155309926', ['I149899117']], ['A2833209380', ['I149899117']], ['A3188680033', []], ['A2272538144', []], ['A3187223402', []], ['A415854352', []], ['A3206472845', []], ['A3189702922', []], ['A3213013926', []], ['A2147448641', []], ['A395586697', []], ['A2905689109', []], ['A2905626028', []], ['A1654852912', []], ['A2970243390', []], ['A2140290691', []], ['A2906049513', []], ['A2974524012', []], ['A3092437567', []], ['A2956798651', []], ['A3019035701', []], ['A3198640294', []], ['A3103146582', ['I149899117']], ['A3206970968', []], ['A3198074476', []], ['A3100080145', ['I149899117']], ['A3188415666', []], ['A2469000994', []], ['A2976081679', ['I149899117']], ['A3157012711', []], ['A3197230867', []], ['A2906654577', []], ['A2118448810', []], ['A1963716262', []], ['A2972396686', []], ['A3088945647', []], ['A2906359521', []], ['A3172586782', []], ['A3212947902', []], ['A3212689698', []], ['A3198039102', []], ['A3003397437', []], ['A3196868043', []], ['A3198553987', []], ['A2614585417', []], ['A2184182716', []], ['A362084126', []], ['A2918218855', []], ['A1884058422', []], ['A3173173182', []], ['A3207885158', []], ['A3183286780', []], ['A2462241501', []], ['A2089513116', []], ['A2765135632', []], ['A2461163460', []], ['A3176648456', []], ['A3188331298', []], ['A1782561727', []], ['A3214715012', []], ['A2941682954', []], ['A3205324612', []], ['A2805807800', []], ['A3160767430', []], ['A3183674337', []], ['A3007455155', []], ['A2878446963', []], ['A2812330481', []], ['A3214401601', []], ['A3210570954', []], ['A2100129471', []], ['A3214716399', []], ['A3088823900', []], ['A3212711058', []], ['A2946809466', []], ['A3214718166', []], ['A2744100075', []], ['A2469241872', []], ['A3206323586', ['I149899117']], ['A2962726707', []], ['A2766898098', []], ['A2147981533', []], ['A2773400098', []], ['A2959485217', []], ['A3211557392', []], ['A2974564501', []], ['A2271697176', []], ['A2957078916', []], ['A1386464591', ['I149899117']], ['A3135876474', []], ['A3125232773', []], ['A2596842744', []], ['A3214219958', []], ['A3009423763', []], ['A2973649355', []], ['A2976615018', []], ['A2905854981', []], ['A3106077237', []], ['A3094318201', []], ['A2917981810', []], ['A1994600236', []], ['A3187848715', []], ['A2528509270', []], ['A3198562296', []], ['A2150227725', []], ['A2091781350', []], ['A2263080307', []], ['A3213085247', []], ['A2949097735', []], ['A2895669897', []], ['A3099795724', ['I149899117']], ['A2743706024', ['I149899117']], ['A2170966916', []], ['A2226082458', []], ['A3021341707', []], ['A3002678630', []], ['A2099037435', []], ['A3088163438', []], ['A3126429971', []], ['A2414665270', ['I149899117']], ['A3115898744', []], ['A3210228010', []], ['A2972136177', []], ['A2267468254', ['I149899117']], ['A2311189936', []], ['A3214605660', []], ['A2169968594', []], ['A2765653448', []], ['A2885759830', ['I149899117']], ['A2895170385', []], ['A3212511150', []], ['A3211448949', []], ['A2905668040', []], ['A2791879238', []], ['A3205204008', []], ['A2906016004', []], ['A3176609274', []], ['A2610409491', []], ['A3180273036', []], ['A2575681443', []], ['A3042780887', []], ['A2237733577', []], ['A3140490383', []], ['A3165812064', []], ['A3213487519', []], ['A3213907600', []], ['A2059989788', []], ['A3213738079', []], ['A2872422862', []], ['A2973327313', []], ['A2138374417', []], ['A3196301643', []], ['A2038388809', []], ['A2924118733', []], ['A2029544047', []], ['A2906619186', []], ['A2500174149', []], ['A3142380466', []], ['A3211653756', []], ['A2767122797', []], ['A3214718701', []], ['A2916776118', ['I149899117']], ['A2193867565', []], ['A2976094966', ['I149899117']], ['A1541186076', []], ['A2976816135', []], ['A3198399133', ['I149899117']], ['A2591107197', []], ['A2806090049', []], ['A3083537935', []], ['A1976720645', []], ['A2580845160', []], ['A400490453', []], ['A2065534013', []], ['A2766309083', []], ['A2975410111', []], ['A3176384363', []], ['A2265138170', []], ['A3213747351', []], ['A3187158056', []], ['A3213214962', []], ['A2766855397', []], ['A3177261447', []], ['A2042879894', []], ['A2762642778', []], ['A2282471978', []], ['A3212765078', []], ['A2919506958', []], ['A3211900768', []], ['A2040393559', []], ['A3212099725', []], ['A2995618979', ['I149899117']], ['A3213965564', []], ['A2129706933', []], ['A2765751982', ['I149899117']], ['A2162845826', []], ['A3212450814', []], ['A2905736953', []], ['A3141516437', []], ['A2109240833', []], ['A3107409158', []], ['A3037906761', []], ['A2953014576', []], ['A3212614577', []], ['A2115179272', []], ['A3007345822', []], ['A3105308735', ['I149899117']], ['A3139912005', []], ['A3133310043', []], ['A2478539643', []], ['A3088769739', []], ['A3095261960', []], ['A3188046254', []], ['A2271925699', []], ['A2905975129', []], ['A3212807160', []], ['A3128012180', []], ['A3213155827', []], ['A3213806971', []], ['A3212056422', []], ['A2976055498', []], ['A3207720097', []], ['A3168011387', []], ['A2680674295', []], ['A3145841316', []], ['A3118211531', []], ['A2087571514', []], ['A3213960926', []], ['A3101698037', []], ['A2358827619', []], ['A2159263755', []], ['A2139670350', []], ['A2164731926', []], ['A3171630263', []], ['A2122667816', []], ['A2258435089', []], ['A3213982056', []], ['A2963217010', []], ['A3211531267', []], ['A2976862466', []], ['A3148351075', []], ['A2232612787', []], ['A2874945874', []], ['A2617753731', []], ['A2422158808', []], ['A3087785070', []], ['A3212186086', []], ['A3190535392', []], ['A3187337232', []], ['A2076617041', []], ['A3135550114', []], ['A3214699647', []], ['A3198816670', []], ['A2794769002', []], ['A3213082654', []], ['A1990234647', []], ['A3088317034', []], ['A2806245680', []], ['A3211750527', []], ['A2869745026', []], ['A2163347761', []], ['A2743303635', []], ['A2766389493', []], ['A2974763369', []], ['A2997733971', []], ['A2763702278', []], ['A2097660264', []], ['A3215552898', []], ['A3140778378', []], ['A2980424894', []], ['A2195821694', []], ['A2906309789', []], ['A2951717091', []], ['A2806399133', []], ['A2561660945', []], ['A3211483640', []], ['A3131437222', ['I149899117']], ['A3212444959', ['I149899117']], ['A3197332458', []], ['A2405613006', []], ['A2994044587', ['I149899117']], ['A2993470130', []], ['A2919554698', []], ['A3133253250', ['I149899117']], ['A3205844333', []], ['A2014299767', []], ['A2097961538', []], ['A3093294401', []], ['A2469540581', []], ['A2567643794', []], ['A3004058091', ['I149899117']], ['A2996670930', []], ['A3145747630', []], ['A2518612237', []], ['A2893230900', ['I149899117']], ['A2999412613', ['I149899117']], ['A2157982768', ['I149899117']], ['A3113580033', []], ['A3037870288', ['I149899117']], ['A1914572088', []], ['A2988580849', ['I149899117']], ['A3214192475', []], ['A2793555429', []], ['A3207359307', []], ['A2609393006', ['I149899117']], ['A2765830911', []], ['A2906296242', []], ['A3124858029', []], ['A3088353587', []], ['A3181357828', []], ['A2605415677', []], ['A3211905128', []], ['A3212998959', []], ['A2563109565', []], ['A2563109565', []], ['A2906162563', []], ['A1434570200', []], ['A2881695771', []], ['A2973270439', []], ['A2770177904', []], ['A2745106814', []], ['A2984455146', []], ['A3211822923', []], ['A3084538508', []], ['A3048868603', []], ['A2761741246', []], ['A2635620662', []], ['A3028549774', []], ['A2419807468', []], ['A2852550122', []], ['A3186584125', []], ['A320109892', []], ['A2622566261', []], ['A3213510191', []]], 'cited_by_count': 1541, 'concepts': [['C121332964', '0.9038948'], ['C190330329', '0.89031553'], ['C2780688901', '0.86338234'], ['C192887742', '0.75981534'], ['C32602459', '0.6886997']], 'referenced_works': ['W275843794', 'W632363818', 'W1134494461', 'W1522168210', 'W1559490316', 'W1606642326', 'W1783545336', 'W1790289060', 'W1838270195', 'W1868059750', 'W1963491799', 'W1965555277', 'W1967852555', 'W1968985209', 'W1971068715', 'W1973724272', 'W1974046066', 'W1977459118', 'W1978501285', 'W1978916504', 'W1981281886', 'W1983668354', 'W1984734622', 'W1985536898', 'W1989170879', 'W1990978371', 'W1993039780', 'W1994870116', 'W1995114597', 'W1995875735', 'W1996004845', 'W1997367763', 'W1997881416', 'W2005424667', 'W2014304222', 'W2014522010', 'W2017542306', 'W2017576190', 'W2021459457', 'W2023599662', 'W2027859446', 'W2028399298', 'W2030004612', 'W2032095233', 'W2036334887', 'W2037939815', 'W2039964660', 'W2044542193', 'W2048695882', 'W2051449498', 'W2051539854', 'W2051952627', 'W2052321554', 'W2052907476', 'W2054745122', 'W2055151270', 'W2055344669', 'W2061930234', 'W2064672687', 'W2064816908', 'W2065913386', 'W2066813507', 'W2071998148', 'W2076480618', 'W2077003551', 'W2081275439', 'W2081895114', 'W2083366520', 'W2088544735', 'W2095105526', 'W2096077283', 'W2097212390', 'W2100622876', 'W2101680365', 'W2102464939', 'W2103860658', 'W2103931654', 'W2104863352', 'W2104942321', 'W2105234305', 'W2106532428', 'W2106723786', 'W2114922502', 'W2115305599', 'W2115462982', 'W2116108818', 'W2118840719', 'W2123567195', 'W2128132769', 'W2131373099', 'W2138965529', 'W2139823471', 'W2146950091', 'W2147286576', 'W2147686177', 'W2147698438', 'W2148574866', 'W2148994792', 'W2150357442', 'W2152377128', 'W2153135464', 'W2154984826', 'W2155065285', 'W2155802808', 'W2162520084', 'W2163389455', 'W2163983676', 'W2165591362', 'W2167572849', 'W2169650135', 'W2198581161', 'W2207139482', 'W2214545089', 'W2237829933', 'W2252795400', 'W2254980152', 'W2256597892', 'W2261794778', 'W2266746071', 'W2268129260', 'W2268511610', 'W2273686191', 'W2273772273', 'W2273991482', 'W2277521171', 'W2277666539', 'W2277737850', 'W2339792708', 'W2342129039', 'W2412899893', 'W2415259147', 'W2437571239', 'W2479210508', 'W2482351672', 'W2512413576', 'W2523032431', 'W2545747680', 'W2546409652', 'W2547616757', 'W2550934118', 'W2555668341', 'W2561919945', 'W2571156602', 'W2581294054', 'W2606613431', 'W2611629309', 'W2617872130', 'W2620263135', 'W2621266390', 'W2621786732', 'W2622058034', 'W2624566613', 'W2626418343', 'W2669423079', 'W2736993946', 'W2744176152', 'W2754075545', 'W2756153650', 'W2756512191', 'W2758557427', 'W2758785873', 'W2759460094', 'W2764113317', 'W2765081049', 'W2765170919', 'W2767506920', 'W2768286292', 'W2768878487', 'W2769299006', 'W2771411612', 'W2771626761', 'W2783132980', 'W2783391180', 'W2787204134', 'W2792803858', 'W2792856352', 'W2805726701', 'W2806591638', 'W2806836424', 'W2807309204', 'W2807614683', 'W2848380992', 'W2885835694', 'W2897059916', 'W2911700471', 'W2928880801', 'W2949551701', 'W2953142974', 'W3098069234', 'W3098343980', 'W3098414694', 'W3098472840', 'W3098569287', 'W3099071754', 'W3099145134', 'W3099186672', 'W3099518669', 'W3099689639', 'W3100108344', 'W3100383693', 'W3100591142', 'W3101078310', 'W3101191906', 'W3101866315', 'W3102004940', 'W3102493421', 'W3102842056', 'W3102905749', 'W3102942362', 'W3102987010', 'W3103271996', 'W3103591927', 'W3103811778', 'W3104042330', 'W3104489202', 'W3104629333', 'W3105515930', 'W3105665039', 'W3106000950', 'W3106112360', 'W3106442961', 'W3124935603', 'W4230900352', 'W4234856688', 'W4234933822', 'W4292875581', 'W4299551239'], 'abstract': 'We present the results from three gravitational-wave searches for coalescing compact binaries with component masses above 1$\\mathrm{M}_\\odot$ during the first and second observing runs of the Advanced gravitational-wave detector network. During the first observing run (O1), from September $12^\\mathrm{th}$, 2015 to January $19^\\mathrm{th}$, 2016, gravitational waves from three binary black hole mergers were detected. The second observing run (O2), which ran from November $30^\\mathrm{th}$, 2016 to August $25^\\mathrm{th}$, 2017, saw the first detection of gravitational waves from a binary neutron star inspiral, in addition to the observation of gravitational waves from a total of seven binary black hole mergers, four of which we report here for the first time: GW170729, GW170809, GW170818 and GW170823. For all significant gravitational-wave events, we provide estimates of the source properties. The detected binary black holes have total masses between $18.6_{-0.7}^{+3.2}\\mathrm{M}_\\odot$, and $84.4_{-11.1}^{+15.8} \\mathrm{M}_\\odot$, and range in distance between $320_{-110}^{+120}$ Mpc and $2840_{-1360}^{+1400}$ Mpc. No neutron star - black hole mergers were detected. In addition to highly significant gravitational-wave events, we also provide a list of marginal event candidates with an estimated false alarm rate less than 1 per 30 days. From these results over the first two observing runs, which include approximately one gravitational-wave detection per 15 days of data searched, we infer merger rates at the 90% confidence intervals of $110\\, -\\, 3840$ $\\mathrm{Gpc}^{-3}\\,\\mathrm{y}^{-1}$ for binary neutron stars and $9.7\\, -\\, 101$ $\\mathrm{Gpc}^{-3}\\,\\mathrm{y}^{-1}$ for binary black holes assuming fixed population distributions, and determine a neutron star - black hole merger rate 90% upper limit of $610$ $\\mathrm{Gpc}^{-3}\\,\\mathrm{y}^{-1}$.', 'counts_by_year': [[2022, 354], [2021, 556], [2020, 520], [2019, 109], [2018, 2]]}, {'id': 'W2230320310', 'doi': 'https://doi.org/10.1093/nar/gkv1507', 'title': 'TCGAbiolinks: an R/Bioconductor package for integrative analysis of TCGA data', 'type': 'journal-article', 'publication_date': '2016-05-05', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2279973115', ['I132053463']], ['A2622548974', ['I17974374']], ['A2222249800', ['I132053463']], ['A2650209538', ['I16337185']], ['A2105977853', ['I4210095629']], ['A2233702244', ['I4210149433']], ['A1766746767', ['I17974374']], ['A2075357857', ['I17974374']], ['A1915890641', ['I16337185']], ['A1718920857', ['I4210095629']], ['A2145453858', ['I28200790']], ['A2295622617', ['I132053463']], ['A66562203', ['I17974374']]], 'cited_by_count': 1534, 'concepts': [['C2779694297', '0.9523246'], ['C86803240', '0.7333993'], ['C121912465', '0.580434'], ['C70721500', '0.536993'], ['C177212765', '0.4493655']], 'referenced_works': ['W1544691147', 'W1812256879', 'W1848939527', 'W1936755136', 'W1967220865', 'W1973094248', 'W1991415686', 'W2005837044', 'W2010991736', 'W2025183726', 'W2044702943', 'W2074746593', 'W2084160423', 'W2095463689', 'W2096283457', 'W2105528101', 'W2105776484', 'W2109816625', 'W2110065044', 'W2114104545', 'W2114843025', 'W2115462905', 'W2124558732', 'W2126547130', 'W2131050852', 'W2131478115', 'W2141425631', 'W2146512944', 'W2158485828', 'W2160697532', 'W2168342464', 'W2180481128', 'W2262414037', 'W2263206910', 'W2560367415', 'W2917738795', 'W4213257593', 'W4232835811'], 'abstract': "The Cancer Genome Atlas (TCGA) research network has made public a large collection of clinical and molecular phenotypes of more than 10 000 tumor patients across 33 different tumor types. Using this cohort, TCGA has published over 20 marker papers detailing the genomic and epigenomic alterations associated with these tumor types. Although many important discoveries have been made by TCGA's research network, opportunities still exist to implement novel methods, thereby elucidating new biological pathways and diagnostic markers. However, mining the TCGA data presents several bioinformatics challenges, such as data retrieval and integration with clinical data and other molecular data types (e.g. RNA and DNA methylation). We developed an R/Bioconductor package called TCGAbiolinks to address these challenges and offer bioinformatics solutions by using a guided workflow to allow users to query, download and perform integrative analyses of TCGA data. We combined methods from computer science and statistics into the pipeline and incorporated methodologies developed in previous TCGA marker studies and in our own group. Using four different TCGA tumor types (Kidney, Brain, Breast and Colon) as examples, we provide case studies to illustrate examples of reproducibility, integrative analysis and utilization of different Bioconductor packages to advance and accelerate novel discoveries.", 'counts_by_year': [[2022, 431], [2021, 460], [2020, 331], [2019, 181], [2018, 78], [2017, 41], [2016, 10], [2015, 1]]}, {'id': 'W2346834216', 'doi': 'https://doi.org/10.1136/bmj.i2139', 'title': 'Medical error—the third leading cause of death in the US', 'type': 'journal-article', 'publication_date': '2016-05-03', 'host_venue': 'V4210185579', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A1492816019', ['I2799853436']], ['A2563645637', ['I2799853436']]], 'cited_by_count': 1531, 'concepts': [['C29374701', '0.565567'], ['C41008148', '0.3968164'], ['C71924100', '0.3817153'], ['C2522767166', '0.36801657'], ['C142724271', '0.12656796']], 'referenced_works': ['W70273200', 'W79429584', 'W1590074823', 'W2028577022', 'W2041653029', 'W2044588535', 'W2124717344', 'W2139326546', 'W2161554615', 'W2166858912', 'W2170574565', 'W2612994670', 'W2754265708'], 'abstract': 'Medical error is not included on death certificates or in rankings of cause of death.  Martin Makary  and  Michael Daniel  assess its contribution to mortality and call for better reporting', 'counts_by_year': [[2022, 150], [2021, 231], [2020, 301], [2019, 306], [2018, 244], [2017, 236], [2016, 63]]}, {'id': 'W2108646579', 'doi': 'https://doi.org/10.1007/978-1-4899-7687-1_907', 'title': 'Sentiment Analysis and Opinion Mining', 'type': 'book-chapter', 'publication_date': '2017-01-01', 'host_venue': 'V4306463950', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2785343825', ['I39422238']]], 'cited_by_count': 1530, 'concepts': [['C66402592', '0.87501943'], ['C26011011', '0.70222723'], ['C518677369', '0.6286113'], ['C9652623', '0.57060754'], ['C2522767166', '0.53244245']], 'referenced_works': ['W1964613733', 'W2044429219', 'W2084046180', 'W2086277751', 'W2096110600', 'W2097726431', 'W2100362224', 'W2108420397', 'W2112422413', 'W2112744748', 'W2129294185', 'W2133341045', 'W2136680862', 'W2139517011', 'W2153353890', 'W2155328222', 'W2159021374', 'W2159457224', 'W2160409620', 'W2160660844', 'W2166706824', 'W2167660864', 'W2168493061', 'W2612769033', 'W2787893582', 'W4233906183'], 'abstract': "Sentiment analysis and opinion mining is the field of study that analyzes people's opinions, sentiments, evaluations, attitudes, and emotions from written language. It is one of the most active research areas in natural language processing and is also widely studied in data mining, Web mining, and text mining. In fact, this research has spread outside of computer science to the management sciences and social sciences due to its importance to business and society as a whole. The growing importance of sentiment analysis coincides with the growth of social media such as reviews, forum discussions, blogs, micro-blogs, Twitter, and social networks. For the first time in human history, we now have a huge volume of opinionated data recorded in digital form for analysis. Sentiment analysis systems are being applied in almost every business and social domain because opinions are central to almost all human activities and are key influencers of our behaviors. Our beliefs and perceptions of reality, and the choices we make, are largely conditioned on how others see and evaluate the world. For this reason, when we need to make a decision we often seek out the opinions of others. This is true not only for individuals but also for organizations. This book is a comprehensive introductory and survey text. It covers all important topics and the latest developments in the field with over 400 references. It is suitable for students, researchers and practitioners who are interested in social media analysis in general and sentiment analysis in particular. Lecturers can readily use it in class for courses on natural language processing, social media analysis, text mining, and data mining. Lecture slides are also available online.", 'counts_by_year': [[2022, 8], [2021, 147], [2020, 213], [2019, 144], [2018, 167], [2017, 121], [2016, 181], [2015, 258], [2014, 202], [2013, 81], [2012, 8]]}, {'id': 'W2512971201', 'doi': 'https://doi.org/10.1145/2959100.2959190', 'title': 'Deep Neural Networks for YouTube Recommendations', 'type': 'proceedings-article', 'publication_date': '2016-09-07', 'host_venue': 'V4306418092', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2645023824', ['I1291425158']], ['A3092959256', ['I1291425158']], ['A2231269731', ['I1291425158']]], 'cited_by_count': 1530, 'concepts': [['C108583219', '0.8240584'], ['C41008148', '0.7909056'], ['C189430467', '0.7376669'], ['C192209626', '0.69512784'], ['C2984842247', '0.670595']], 'referenced_works': ['W1720514416', 'W2000493066', 'W2019077769', 'W2040367556', 'W2076618162', 'W2087398646', 'W2114079787', 'W2157881433', 'W2166237624'], 'abstract': 'YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous user-facing impact.', 'counts_by_year': [[2022, 249], [2021, 375], [2020, 378], [2019, 263], [2018, 173], [2017, 89], [2016, 2]]}, {'id': 'W2772723798', 'doi': 'https://doi.org/10.1001/jama.2017.14585', 'title': 'Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer', 'type': 'journal-article', 'publication_date': '2017-12-12', 'host_venue': 'V172573765', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2125607297', ['I145872427']], ['A1991178247', ['I83019370']], ['A2762893385', ['I193662353']], ['A1994494219', ['I145872427']], ['A2461676442', ['I145872427']], ['A2031367385', ['I145872427']], ['A1495806369', ['I145872427']], ['A2525713780', ['I145872427']], ['A2772074683', ['I193662353']], ['A2613277450', ['I145872427']], ['A329422269', ['I145872427']], ['A2774681303', ['I193662353']], ['A2469131153', []], ['A2051767270', ['I145872427']], ['A2041748604', ['I1316535847']], ['A2160246052', ['I1316535847']], ['A2715453789', ['I1316535847']], ['A2011491954', ['I63966007']], ['A2684941238', []], ['A2303123772', ['I1316535847']], ['A2765465414', ['I136199984']], ['A2226427118', ['I136199984']], ['A2135999815', ['I136199984']], ['A2597581349', ['I177725633']], ['A2741319727', ['I177725633']], ['A2166341251', ['I177725633']], ['A3173255164', []], ['A2126161882', []], ['A3175438315', ['I37101380']], ['A230029459', ['I201799495']], ['A2949146575', ['I201799495']], ['A2008777834', ['I201799495']], ['A2619649749', []], ['A3214493005', []], ['A3175451781', []], ['A2310754593', ['I185261750']], ['A2742012565', ['I39555362']], ['A2094809659', ['I39555362']], ['A2343837867', ['I60342839']], ['A385875820', ['I39555362']], ['A2959503375', ['I39555362']], ['A2011583136', ['I878227088']], ['A2795002931', ['I145872427']], ['A2962839551', ['I122228004']], ['A1967650061', ['I122228004']], ['A2411797541', ['I150589677']], ['A1095227096', ['I122228004']], ['A2306245081', ['I166825849']], ['A339676566', ['I122228004']], ['A2183060016', ['I122228004']], ['A2038107283', ['I62916508']], ['A3175019453', ['I62916508']], ['A3175908851', ['I62916508']], ['A2097230586', ['I62916508']], ['A572302453', ['I62916508']], ['A3003231365', ['I98285908']], ['A2139500777', ['I98285908']], ['A2264901118', ['I98285908']], ['A2115782275', ['I98285908']], ['A253638804', ['I2613432']], ['A2618235017', ['I151823869']], ['A2716232370', ['I151823869']], ['A2713796784', ['I151823869']], ['A2157461436', ['I79189158']], ['A2406088723', ['I79189158']], ['A2296648026', ['I79189158']], ['A2109044308', ['I79189158']], ['A1470173428', ['I39804081']], ['A3177405722', ['I39804081']]], 'cited_by_count': 1529, 'concepts': [['C71924100', '0.9778627'], ['C530470458', '0.6857183'], ['C2780849966', '0.5765808'], ['C143998085', '0.4410292'], ['C121608353', '0.43039727']], 'referenced_works': ['W183723164', 'W1528968815', 'W1592225892', 'W1901129140', 'W2009260241', 'W2029694543', 'W2044428447', 'W2044465660', 'W2082384201', 'W2097117768', 'W2097841139', 'W2104955141', 'W2117897510', 'W2119821739', 'W2122441470', 'W2124592697', 'W2126406304', 'W2151103935', 'W2153855001', 'W2163352848', 'W2194775991', 'W2264887978', 'W2339885376', 'W2401520370', 'W2470965540', 'W2557738935', 'W2564463480', 'W2581082771', 'W2618530766', 'W2911964244'], 'abstract': 'Importance  Application of deep learning algorithms to whole-slide pathology images can potentially improve diagnostic accuracy and efficiency.  Objective  Assess the performance of automated deep learning algorithms at detecting metastases in hematoxylin and eosin–stained tissue sections of lymph nodes of women with breast cancer and compare it with pathologists’ diagnoses in a diagnostic setting.  Design, Setting, and Participants  Researcher challenge competition (CAMELYON16) to develop automated solutions for detecting lymph node metastases (November 2015-November 2016). A training data set of whole-slide images from 2 centers in the Netherlands with (n\u2009=\u2009110) and without (n\u2009=\u2009160) nodal metastases verified by immunohistochemical staining were provided to challenge participants to build algorithms. Algorithm performance was evaluated in an independent test set of 129 whole-slide images (49 with and 80 without metastases). The same test set of corresponding glass slides was also evaluated by a panel of 11 pathologists with time constraint (WTC) from the Netherlands to ascertain likelihood of nodal metastases for each slide in a flexible 2-hour session, simulating routine pathology workflow, and by 1 pathologist without time constraint (WOTC).  Exposures  Deep learning algorithms submitted as part of a challenge competition or pathologist interpretation.  Main Outcomes and Measures  The presence of specific metastatic foci and the absence vs presence of lymph node metastasis in a slide or image using receiver operating characteristic curve analysis. The 11 pathologists participating in the simulation exercise rated their diagnostic confidence as definitely normal, probably normal, equivocal, probably tumor, or definitely tumor.  Results  The area under the receiver operating characteristic curve (AUC) for the algorithms ranged from 0.556 to 0.994. The top-performing algorithm achieved a lesion-level, true-positive fraction comparable with that of the pathologist WOTC (72.4% [95% CI, 64.3%-80.4%]) at a mean of 0.0125 false-positives per normal whole-slide image. For the whole-slide image classification task, the best algorithm (AUC, 0.994 [95% CI, 0.983-0.999]) performed significantly better than the pathologists WTC in a diagnostic simulation (mean AUC, 0.810 [range, 0.738-0.884];P\u2009  Conclusions and Relevance  In the setting of a challenge competition, some deep learning algorithms achieved better diagnostic performance than a panel of 11 pathologists participating in a simulation exercise designed to mimic routine pathology workflow; algorithm performance was comparable with an expert pathologist interpreting whole-slide images without time constraints. Whether this approach has clinical utility will require evaluation in a clinical setting.', 'counts_by_year': [[2022, 271], [2021, 434], [2020, 394], [2019, 312], [2018, 114], [2017, 4]]}, {'id': 'W2513329101', 'doi': 'https://doi.org/10.1016/j.ijsu.2016.08.014', 'title': 'The SCARE Statement: Consensus-based surgical case report guidelines', 'type': 'journal-article', 'publication_date': '2016-10-01', 'host_venue': 'V67965910', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A1940434747', ['I200166805']], ['A1937107987', ['I200166805']], ['A2566493213', ['I200166805']], ['A2502323122', ['I47508984']], ['A2253223765', ['I47508984']], ['A719935200', ['I1283280774']]], 'cited_by_count': 1526, 'concepts': [['C71924100', '0.8121265'], ['C105776082', '0.80124086'], ['C2779356329', '0.79807293'], ['C2780182762', '0.71596575'], ['C2777286243', '0.6981904']], 'referenced_works': ['W1710887510', 'W1965767365', 'W1976576973', 'W1982511475', 'W2025585639', 'W2092282228', 'W2093038967', 'W2111367525', 'W2150267992', 'W2159101055', 'W2267867972', 'W2906355678'], 'abstract': 'Case reports have been a long held tradition within the surgical literature. Reporting guidelines can improve transparency and reporting quality. However, recent consensus-based guidelines for case reports (CARE) are not surgically focused. Our objective was to develop surgical case report guidelines.The CARE statement was used as the basis for a Delphi consensus. The Delphi questionnaire was administered via Google Forms and conducted using standard Delphi methodology. A multidisciplinary group of surgeons and others with expertise in the reporting of case reports were invited to participate. In round one, participants stated how each item of the CARE statement should be changed and what additional items were needed. Revised and additional items from round one were put forward into a further round, where participants voted on the extent of their agreement with each item, using a nine-point Likert scale, as proposed by the Grading of Recommendations, Assessment, Development and Evaluations (GRADE) working group.In round one, there was a 64% (38/59) response rate. Following adjustment of the guideline with the incorporation of recommended changes, round two commenced and there was an 83% (49/59) response rate. All but one of the items were approved by the participants, with Likert scores 7-9 awarded by >70% of respondents. The final guideline consists of a 14-item checklist.We present the SCARE Guideline, consisting of a 14-item checklist that will improve the reporting quality of surgical case reports.', 'counts_by_year': [[2022, 17], [2021, 29], [2020, 72], [2019, 169], [2018, 635], [2017, 564], [2016, 40]]}, {'id': 'W2963433607', 'doi': 'https://doi.org/10.1137/16m1080173', 'title': 'Optimization Methods for Large-Scale Machine Learning', 'type': 'journal-article', 'publication_date': '2018-05-08', 'host_venue': 'V160107561', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A712415982', ['I2252078561']], ['A2148115191', ['I186143895']], ['A2070638347', ['I4210155590']]], 'cited_by_count': 1523, 'concepts': [['C2778755073', '0.5916172'], ['C41008148', '0.5846473'], ['C154945302', '0.39245188'], ['C119857082', '0.37109303'], ['C126255220', '0.3477876']], 'referenced_works': ['W1498436455', 'W1970789124', 'W1972711404', 'W1979089199', 'W1979382551', 'W1988720110', 'W1988795359', 'W1991083751', 'W1992208280', 'W1994377487', 'W1994616650', 'W2000389939', 'W2005126631', 'W2005136695', 'W2006903949', 'W2010581677', 'W2020107577', 'W2038210983', 'W2039050532', 'W2042173174', 'W2043919728', 'W2045079045', 'W2045334340', 'W2051434435', 'W2051669046', 'W2053742104', 'W2058532290', 'W2058916297', 'W2061570747', 'W2078409719', 'W2086161653', 'W2095984592', 'W2096840748', 'W2100556411', 'W2101159990', 'W2103111465', 'W2105934661', 'W2107501462', 'W2109339818', 'W2109449402', 'W2112796928', 'W2112820497', 'W2115706991', 'W2119058682', 'W2119647652', 'W2122203560', 'W2125993116', 'W2126607811', 'W2146842127', 'W2154451187', 'W2155192981', 'W2162870776', 'W2167820643', 'W2169713291', 'W2221583060', 'W2287259016', 'W2322150470', 'W2611328865', 'W2962748029', 'W2963060476', 'W2963156201', 'W3103657382', 'W3104398353', 'W4205213118', 'W4233413206', 'W4237883889', 'W4238404964', 'W4239510810'], 'abstract': 'This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques th...', 'counts_by_year': [[2022, 173], [2021, 489], [2020, 452], [2019, 300], [2018, 88], [2017, 17], [2016, 1]]}, {'id': 'W2962772482', 'doi': 'https://doi.org/10.1145/3236009', 'title': 'A Survey of Methods for Explaining Black Box Models', 'type': 'journal-article', 'publication_date': '2018-08-22', 'host_venue': 'V157921468', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2137291674', ['I108290504']], ['A22253233', ['I108290504']], ['A1968619696', ['I108290504']], ['A2241902680', ['I108290504']], ['A3216012384', ['I122991210']], ['A7769909', ['I108290504']]], 'cited_by_count': 1518, 'concepts': [['C2781067378', '0.94785345'], ['C94966114', '0.8800904'], ['C41008148', '0.829651'], ['C12713177', '0.6734272'], ['C2522767166', '0.53871167']], 'referenced_works': ['W1531743498', 'W1560484089', 'W1623342295', 'W1787224781', 'W1915485278', 'W1932198206', 'W1976101156', 'W1982428585', 'W1988160630', 'W1990088911', 'W1996796871', 'W2005297674', 'W2009049007', 'W2011402106', 'W2013587512', 'W2014181466', 'W2026019770', 'W2026905436', 'W2027582332', 'W2027857686', 'W2046945713', 'W2051734503', 'W2054640944', 'W2056751685', 'W2059554529', 'W2061895235', 'W2063046703', 'W2084701050', 'W2090026354', 'W2094839437', 'W2095350947', 'W2100788563', 'W2104046064', 'W2106100979', 'W2107222792', 'W2118022153', 'W2123497007', 'W2125847307', 'W2128420091', 'W2148001258', 'W2149033360', 'W2158585626', 'W2166454173', 'W2168635375', 'W2190008860', 'W2195388612', 'W2240067561', 'W2282821441', 'W2295107390', 'W2367397349', 'W2394669110', 'W2396009330', 'W2510508396', 'W2548509278', 'W2549514639', 'W2556340658', 'W2584281025', 'W2588936615', 'W2614087582', 'W2745133928', 'W2770363731', 'W2913420921', 'W2962977265', 'W2963287333', 'W3009845143', 'W3102161834', 'W3123427206', 'W3124443940', 'W3157172840', 'W4240252876'], 'abstract': 'In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.', 'counts_by_year': [[2022, 307], [2021, 572], [2020, 388], [2019, 219], [2018, 29]]}, {'id': 'W2596636257', 'doi': 'https://doi.org/10.1109/jiot.2017.2683200', 'title': 'A Survey on Internet of Things: Architecture, Enabling Technologies, Security and Privacy, and Applications', 'type': 'journal-article', 'publication_date': '2017-03-15', 'host_venue': 'V2480266640', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2619523000', ['I87445476']], ['A2503200132', ['I4322298']], ['A2166589344', ['I193531525']], ['A2144672251', ['I87445476']], ['A2163097695', ['I143413998']], ['A2443267613', ['I204512498']]], 'cited_by_count': 1511, 'concepts': [['C41008148', '0.76709354'], ['C123657996', '0.59221756'], ['C38652104', '0.5915599'], ['C110875604', '0.5553702'], ['C123201435', '0.53785574']], 'referenced_works': ['W175294107', 'W643058322', 'W1538382963', 'W1570018622', 'W1571394812', 'W1605101819', 'W1818232037', 'W1902748306', 'W1965638547', 'W1969299781', 'W1970701130', 'W1973767013', 'W1978659553', 'W1980035202', 'W1988037376', 'W1989374345', 'W1992900191', 'W1994669355', 'W2005280675', 'W2006528490', 'W2007388932', 'W2009038479', 'W2011206559', 'W2012574038', 'W2017021984', 'W2019190697', 'W2019269619', 'W2024949003', 'W2026203830', 'W2026892459', 'W2031926216', 'W2034732732', 'W2035203720', 'W2036569531', 'W2038194220', 'W2039049774', 'W2041785419', 'W2045371716', 'W2045819916', 'W2058401212', 'W2059276169', 'W2066064155', 'W2067027467', 'W2069612517', 'W2072641892', 'W2072669749', 'W2074213526', 'W2078837499', 'W2079934812', 'W2080765376', 'W2084330204', 'W2089160186', 'W2094403981', 'W2095366857', 'W2098897917', 'W2099613071', 'W2103851297', 'W2104021812', 'W2104208424', 'W2104959866', 'W2105103777', 'W2105126063', 'W2106267320', 'W2109156380', 'W2114623221', 'W2115018301', 'W2115710963', 'W2116286374', 'W2117227183', 'W2117496982', 'W2117668875', 'W2118972724', 'W2119046642', 'W2120629158', 'W2123770198', 'W2123990969', 'W2124861999', 'W2126993465', 'W2127414816', 'W2128372565', 'W2129061840', 'W2130403046', 'W2130512269', 'W2134295053', 'W2134306616', 'W2135126436', 'W2137100320', 'W2143514931', 'W2146148058', 'W2146910660', 'W2147623910', 'W2148481610', 'W2153500642', 'W2154161193', 'W2154387512', 'W2163413582', 'W2165491783', 'W2165887572', 'W2168267147', 'W2170018742', 'W2173530811', 'W2173788200', 'W2174904793', 'W2233085856', 'W2289218694', 'W2291413744', 'W2292021756', 'W2324437139', 'W2325877089', 'W2335888457', 'W2344239871', 'W2344594340', 'W2350448019', 'W2416799949', 'W2470808257', 'W2476363507', 'W2487500832', 'W2500020422', 'W2526910689', 'W2534687077', 'W2549146204', 'W2551730448', 'W2551959053', 'W2555431798', 'W2561310382', 'W2575501069', 'W2730523305', 'W2736394344', 'W2762398138', 'W2964345729', 'W3144583911', 'W3161358203', 'W4235105762', 'W4253709609'], 'abstract': 'Fog/edge computing has been proposed to be integrated with Internet of Things (IoT) to enable computing services devices deployed at network edge, aiming to improve the user’s experience and resilience of the services in case of failures. With the advantage of distributed architecture and close to end-users, fog/edge computing can provide faster response and greater quality of service for IoT applications. Thus, fog/edge computing-based IoT becomes future infrastructure on IoT development. To develop fog/edge computing-based IoT infrastructure, the architecture, enabling techniques, and issues related to IoT should be investigated first, and then the integration of fog/edge computing and IoT should be explored. To this end, this paper conducts a comprehensive overview of IoT with respect to system architecture, enabling technologies, security and privacy issues, and present the integration of fog/edge computing and IoT, and applications. Particularly, this paper first explores the relationship between cyber-physical systems and IoT, both of which play important roles in realizing an intelligent cyber-physical world. Then, existing architectures, enabling technologies, and security and privacy issues in IoT are presented to enhance the understanding of the state of the art IoT development. To investigate the fog/edge computing-based IoT, this paper also investigate the relationship between IoT and fog/edge computing, and discuss issues in fog/edge computing-based IoT. Finally, several applications, including the smart grid, smart transportation, and smart cities, are presented to demonstrate how fog/edge computing-based IoT to be implemented in real-world applications.', 'counts_by_year': [[2022, 211], [2021, 385], [2020, 345], [2019, 360], [2018, 168], [2017, 41]]}, {'id': 'W2982770724', 'doi': 'https://doi.org/10.1109/iccv.2019.00972', 'title': 'FCOS: Fully Convolutional One-Stage Object Detection', 'type': 'proceedings-article', 'publication_date': '2019-04-02', 'host_venue': 'V4306419272', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2564728138', ['I5681781']], ['A2139473605', ['I5681781']], ['A3168993789', ['I5681781']], ['A2478334157', ['I5681781']]], 'cited_by_count': 1504, 'concepts': [['C41008148', '0.8012805'], ['C2776151529', '0.72392344'], ['C94915269', '0.71934086'], ['C2776760102', '0.6424173'], ['C89600930', '0.60750306']], 'referenced_works': ['W1536680647', 'W1803059841', 'W1903029394', 'W2108598243', 'W2194775991', 'W2517615595', 'W2549139847', 'W2557728737', 'W2565639579', 'W2570343428', 'W2605982830', 'W2952787292', 'W2954054736', 'W2962986948', 'W2963037989', 'W2963299996', 'W2963351448', 'W2964297864', 'W2990946490', 'W3098090606'], 'abstract': 'We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at:Code is available at: https://tinyurl.com/FCOSv1', 'counts_by_year': [[2022, 462], [2021, 653], [2020, 341], [2019, 48]]}, {'id': 'W2214802144', 'doi': 'https://doi.org/10.1109/cvpr.2016.181', 'title': 'Deeply-Recursive Convolutional Network for Image Super-Resolution', 'type': 'proceedings-article', 'publication_date': '2016-06-27', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2293552381', ['I139264467']], ['A2598630691', ['I139264467']], ['A2163009075', ['I139264467']]], 'cited_by_count': 1496, 'concepts': [['C168773036', '0.80071104'], ['C41008148', '0.74520034'], ['C774472', '0.72232985'], ['C45347329', '0.6479201'], ['C115961682', '0.6008266']], 'referenced_works': ['W1677182931', 'W1791560514', 'W1934184906', 'W1950594372', 'W2047920195', 'W2087380704', 'W2107878631', 'W2112796928', 'W2118963448', 'W2121927366', 'W2150081556', 'W2534320940'], 'abstract': 'We propose an image super-resolution method (SR) using a deeply-recursive convolutional network (DRCN). Our network has a very deep recursive layer (up to 16 recursions). Increasing recursion depth can improve performance without introducing new parameters for additional convolutions. Albeit advantages, learning a DRCN is very hard with a standard gradient descent method due to exploding/vanishing gradients. To ease the difficulty of training, we propose two extensions: recursive-supervision and skip-connection. Our method outperforms previous methods by a large margin.', 'counts_by_year': [[2022, 173], [2021, 351], [2020, 360], [2019, 335], [2018, 186], [2017, 77], [2016, 10]]}, {'id': 'W2471962767', 'doi': 'https://doi.org/10.1109/cvpr.2016.445', 'title': 'Structure-from-Motion Revisited', 'type': 'proceedings-article', 'publication_date': '2016-06-27', 'host_venue': 'V4306417987', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A3173908332', ['I114027177']], ['A2579267572', ['I114027177']]], 'cited_by_count': 1495, 'concepts': [['C41008148', '0.7432228'], ['C63479239', '0.74243355'], ['C48044578', '0.7053874'], ['C43521106', '0.56995267'], ['C146159030', '0.5686345']], 'referenced_works': ['W29546508', 'W148241443', 'W1514108073', 'W1552779774', 'W1820096659', 'W1908016767', 'W1953588267', 'W1980635903', 'W1991544872', 'W2001790138', 'W2006138321', 'W2013472030', 'W2026888851', 'W2027724697', 'W2030370643', 'W2033086776', 'W2059496137', 'W2063955637', 'W2069971775', 'W2079376982', 'W2080629029', 'W2089024363', 'W2089888558', 'W2105303354', 'W2109757902', 'W2125795712', 'W2133192850', 'W2135973942', 'W2137568841', 'W2143116602', 'W2151103935', 'W2156598602', 'W2158054358', 'W2162137128', 'W2211196523', 'W2217143704', 'W2255534844', 'W2335800409', 'W2536680313'], 'abstract': 'Incremental Structure-from-Motion is a prevalent strategy for 3D reconstruction from unordered image collections. While incremental reconstruction systems have tremendously advanced in all regards, robustness, accuracy, completeness, and scalability remain the key problems towards building a truly general-purpose pipeline. We propose a new SfM technique that improves upon the state of the art to make a further step towards this ultimate goal. The full reconstruction pipeline is released to the public as an open-source implementation.', 'counts_by_year': [[2022, 174], [2021, 450], [2020, 408], [2019, 252], [2018, 125], [2017, 70], [2016, 16]]}]
[{'id': 'W3101380508', 'doi': 'https://doi.org/10.1080/01621459.2017.1285773', 'title': 'Variational Inference: A Review for Statisticians', 'type': 'journal-article', 'publication_date': '2016-01-04', 'host_venue': 'V62401924', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2289542319', ['I78577930']], ['A2268548611', ['I78577930']], ['A2073808159', ['I95457486']]], 'cited_by_count': 1494, 'concepts': [['C55974624', '0.7365477'], ['C111350023', '0.62200963'], ['C2776214188', '0.62170446'], ['C160234255', '0.5493757'], ['C41008148', '0.52656317']], 'referenced_works': ['W203276351', 'W1483883706', 'W1496451467', 'W1504869774', 'W1516111018', 'W1520448186', 'W1528056001', 'W1536343953', 'W1575578922', 'W1580282541', 'W1586357756', 'W1590183771', 'W1845773340', 'W1965555277', 'W1969408578', 'W1970789124', 'W1981796042', 'W1983428013', 'W1989879829', 'W1991237518', 'W1994616650', 'W2000116444', 'W2009094355', 'W2009849957', 'W2010532309', 'W2018102971', 'W2020999234', 'W2025994725', 'W2030179284', 'W2031988475', 'W2032454358', 'W2032929634', 'W2037769578', 'W2043623344', 'W2045887127', 'W2048612325', 'W2049657715', 'W2056044742', 'W2056760934', 'W2057177057', 'W2069570104', 'W2070521069', 'W2071415508', 'W2077606580', 'W2078703947', 'W2083875149', 'W2095775020', 'W2096996605', 'W2103551323', 'W2107418674', 'W2108907664', 'W2116433231', 'W2126819395', 'W2127498532', 'W2130843763', 'W2131481495', 'W2137808289', 'W2138309709', 'W2138556224', 'W2138996412', 'W2141200443', 'W2143143555', 'W2144824356', 'W2151835407', 'W2153305362', 'W2156474069', 'W2160255741', 'W2162995719', 'W2165001851', 'W2170897798', 'W2171911691', 'W2174706414', 'W2567948266', 'W3099640513', 'W3102845826', 'W3106518001', 'W4232632925', 'W4237791300', 'W4291230573', 'W4292691288', 'W4293052541', 'W4302033506'], 'abstract': 'One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.', 'counts_by_year': [[2022, 319], [2021, 435], [2020, 365], [2019, 228], [2018, 98], [2017, 25], [2016, 17], [2012, 3]]}, {'id': 'W3099176913', 'doi': 'https://doi.org/10.1016/0370-2693(93)90726-x', 'title': 'Exact evolution equation for the effective potential', 'type': 'journal-article', 'publication_date': '2017-10-16', 'host_venue': 'V173952182', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2075233551', ['I223822909']]], 'cited_by_count': 1493, 'concepts': [['C121332964', '0.8879223'], ['C54937798', '0.77254975'], ['C106195933', '0.67507255'], ['C99874945', '0.5940785'], ['C121864883', '0.5595385']], 'referenced_works': ['W1597171048', 'W1991356019', 'W1995688361', 'W2007728513', 'W2024803134', 'W2067520844', 'W2079216472', 'W2086860422', 'W2319680621'], 'abstract': 'We derive a new exact evolution equation for the scale dependence of an effective action. The corresponding equation for the effective potential permits a useful truncation. This allows one to deal with the infrared problems of theories with massless modes in less than four dimensions which are relevant for the high temperature phase transition in particle physics or the computation of critical exponents in statistical mechanics.', 'counts_by_year': [[2022, 76], [2021, 106], [2020, 114], [2019, 111], [2018, 113], [2017, 92], [2016, 97], [2015, 84], [2014, 66], [2013, 82], [2012, 78]]}, {'id': 'W2346143774', 'doi': 'https://doi.org/10.1093/nar/gkw343', 'title': 'The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2016 update', 'type': 'journal-article', 'publication_date': '2016-07-08', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A837107254', ['I145311948']], ['A2165697970', ['I145311948']], ['A2520192813', ['I4210107326']], ['A2229110458', ['I130769515']], ['A2518815875', ['I130769515']], ['A2336641661', ['I130769515']], ['A2109325469', ['I130769515']], ['A2212237980', ['I145311948']], ['A2008526058', ['I130769515']], ['A2521022893', ['I145311948']], ['A2327525373', ['I161046081']], ['A256748975', ['I145311948']], ['A2011731670', ['I130769515']], ['A2155812305', ['I130769515']], ['A2176000030', ['I91045830']], ['A2763753394', ['I4210100714']], ['A2397631753', ['I145311948']], ['A2895040106', ['I145311948']], ['A99770489', ['I130769515']], ['A2002910627', ['I193531525']]], 'cited_by_count': 1492, 'concepts': [['C2780615836', '0.71222377'], ['C2522767166', '0.7004826'], ['C206588197', '0.6012129'], ['C41008148', '0.57717353'], ['C86803240', '0.38129425']], 'referenced_works': ['W1547888106', 'W1583240815', 'W1969739015', 'W1977859620', 'W1993202658', 'W1998933811', 'W2001938458', 'W2013434113', 'W2037086031', 'W2041281952', 'W2097464861', 'W2115646700', 'W2136482257', 'W2141541659', 'W2146545525', 'W2161645441', 'W2168103533', 'W2176686025', 'W2225557190'], 'abstract': "High-throughput data production technologies, particularly 'next-generation' DNA sequencing, have ushered in widespread and disruptive changes to biomedical research. Making sense of the large datasets produced by these technologies requires sophisticated statistical and computational methods, as well as substantial computational power. This has led to an acute crisis in life sciences, as researchers without informatics training attempt to perform computation-dependent analyses. Since 2005, the Galaxy project has worked to address this problem by providing a framework that makes advanced computational tools usable by non experts. Galaxy seeks to make data-intensive research more accessible, transparent and reproducible by providing a Web-based environment in which users can perform computational analyses and have all of the details automatically tracked for later inspection, publication, or reuse. In this report we highlight recently added features enabling biomedical analyses on a large scale.", 'counts_by_year': [[2022, 126], [2021, 206], [2020, 210], [2019, 279], [2018, 350], [2017, 272], [2016, 49]]}, {'id': 'W2284386481', 'doi': 'https://doi.org/10.1016/j.npls.2016.01.001', 'title': 'How to plan and perform a qualitative study using content analysis', 'type': 'journal-article', 'publication_date': '2016-01-01', 'host_venue': 'V2898392220', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2113540867', ['I183111857']]], 'cited_by_count': 1490, 'concepts': [['C162446236', '0.6003112'], ['C2776505523', '0.5438815'], ['C3018587665', '0.45755187'], ['C41008148', '0.45275167'], ['C190248442', '0.4342359']], 'referenced_works': ['W79229537', 'W1502456417', 'W1979005473', 'W2019011442', 'W2053384257', 'W2053946009', 'W2063930111', 'W2067404301', 'W2092822028', 'W2109345701', 'W2119366833', 'W2122328334', 'W2140472019', 'W2162623797'], 'abstract': 'Abstract  This paper describes the research process – from planning to presentation, with the emphasis on credibility throughout the whole process – when the methodology of qualitative content analysis is chosen in a qualitative study. The groundwork for the credibility initiates when the planning of the study begins. External and internal resources have to be identified, and the researcher must consider his or her experience of the phenomenon to be studied in order to minimize any bias of his/her own influence. The purpose of content analysis is to organize and elicit meaning from the data collected and to draw realistic conclusions from it. The researcher must choose whether the analysis should be of a broad surface structure ( a manifest analysis ) or of a deep structure ( a latent analysis).  Four distinct main stages are described in this paper: the decontextualisation, the recontextualisation, the categorization, and the compilation. This description of qualitative content analysis offers one approach that shows how the general principles of the method can be used.', 'counts_by_year': [[2022, 334], [2021, 452], [2020, 351], [2019, 210], [2018, 102], [2017, 32], [2016, 8]]}, {'id': 'W1837982953', 'doi': 'https://doi.org/10.1016/j.future.2015.09.021', 'title': 'Integration of Cloud computing and Internet of Things: A survey', 'type': 'journal-article', 'publication_date': '2016-03-01', 'host_venue': 'V186357190', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2026526041', ['I71267560']], ['A2013120679', ['I71267560']], ['A1832128115', ['I71267560']], ['A307383784', ['I71267560']]], 'cited_by_count': 1488, 'concepts': [['C41008148', '0.89934206'], ['C79974875', '0.783455'], ['C81860439', '0.56030434'], ['C110875604', '0.53222215'], ['C136764020', '0.43992898']], 'referenced_works': ['W154256836', 'W1531453604', 'W1587464373', 'W1963742947', 'W1964408636', 'W1966836595', 'W1972808979', 'W1979254854', 'W1983786230', 'W1987941050', 'W2002570983', 'W2008094782', 'W2015931693', 'W2019672983', 'W2022709631', 'W2025783348', 'W2026156467', 'W2029100038', 'W2045936147', 'W2053509435', 'W2054162984', 'W2056035392', 'W2060437593', 'W2062527204', 'W2069514153', 'W2083533476', 'W2086393213', 'W2100320843', 'W2100359107', 'W2104237724', 'W2105103777', 'W2108395864', 'W2109026747', 'W2111619626', 'W2114296561', 'W2122917000', 'W2129368140', 'W2130743022', 'W2131646073', 'W2132247362', 'W2143538196', 'W2153457047', 'W2168452204', 'W2502428178'], 'abstract': 'Cloud computing and Internet of Things (IoT) are two very different technologies that are both already part of our life. Their adoption and use are expected to be more and more pervasive, making them important components of the Future Internet. A novel paradigm where Cloud and IoT are merged together is foreseen as disruptive and as an enabler of a large number of application scenarios.In this paper, we focus our attention on the integration of Cloud and IoT, which is what we call the CloudIoT paradigm. Many works in literature have surveyed Cloud and IoT separately and, more precisely, their main properties, features, underlying technologies, and open issues. However, to the best of our knowledge, these works lack a detailed analysis of the new CloudIoT paradigm, which involves completely new applications, challenges, and research issues. To bridge this gap, in this paper we provide a literature survey on the integration of Cloud and IoT. Starting by analyzing the basics of both IoT and Cloud Computing, we discuss their complementarity, detailing what is currently driving to their integration. Thanks to the adoption of the CloudIoT paradigm a number of applications are gaining momentum: we provide an up-to-date picture of CloudIoT applications in literature, with a focus on their specific research challenges. These challenges are then analyzed in details to show where the main body of research is currently heading. We also discuss what is already available in terms of platforms-both proprietary and open source-and projects implementing the CloudIoT paradigm. Finally, we identify open issues and future directions in this field, which we expect to play a leading role in the landscape of the Future Internet. Vision and motivations for the integration of Cloud computing and Internet of Things (IoT).Applications stemming from the integration of Cloud computing and IoT.Hot research topics and challenges in the integrated scenario of Cloud computing and IoT.Open issues and future directions for research in this scenario.', 'counts_by_year': [[2022, 112], [2021, 249], [2020, 288], [2019, 323], [2018, 251], [2017, 182], [2016, 77], [2015, 4]]}, {'id': 'W2902907165', 'doi': 'https://doi.org/10.1126/science.aar6404', 'title': 'A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play', 'type': 'journal-article', 'publication_date': '2018-12-07', 'host_venue': 'V3880285', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2593774290', ['I45129253']], ['A2766000132', ['I4210090411']], ['A2512603362', ['I4210090411']], ['A291373107', ['I4210090411']], ['A2480833549', ['I4210090411']], ['A2015134264', ['I4210090411']], ['A2107049243', ['I4210090411']], ['A2697048878', ['I4210090411']], ['A2196286353', ['I4210090411']], ['A2032008572', ['I4210090411']], ['A2952465156', ['I4210090411']], ['A2033942250', ['I4210090411']], ['A4302276', ['I4210090411']]], 'cited_by_count': 1488, 'concepts': [['C41008148', '0.6592012'], ['C97541855', '0.5794534'], ['C12713177', '0.55669945'], ['C154945302', '0.4148512'], ['C107457646', '0.3719102']], 'referenced_works': ['W1587022413', 'W1970647289', 'W1977989560', 'W2000759616', 'W2014932765', 'W2020844044', 'W2041367235', 'W2079303600', 'W2099001564', 'W2157803532', 'W2161608691', 'W2165999534', 'W2257979135', 'W2302255633', 'W2766447205', 'W2911296969', 'W2913201078', 'W3098637454'], 'abstract': 'One program to rule them all Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system. Science , this issue p. 1140 ; see also pp. 1087 and 1118', 'counts_by_year': [[2022, 280], [2021, 494], [2020, 444], [2019, 257], [2018, 10]]}, {'id': 'W2756512191', 'doi': 'https://doi.org/10.1103/physrevlett.119.141101', 'title': 'GW170814: A Three-Detector Observation of Gravitational Waves from a Binary Black Hole Coalescence', 'type': 'journal-article', 'publication_date': '2017-10-06', 'host_venue': 'V24807848', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A3178754429', ['I122411786']], ['A2931258442', ['I122411786']], ['A2898869343', ['I121820613']], ['A1989600986', ['I131729948']], ['A2102662472', ['I33213144']], ['A3130519413', []], ['A2209450666', []], ['A2420842909', ['I16337185']], ['A2120816080', ['I122411786']], ['A2643273335', ['I1285433949']], ['A3169239100', ['I1285433949']], ['A2625603523', ['I368840534']], ['A2606179050', ['I157725225']], ['A2063771739', ['I241749']], ['A2196045220', []], ['A2149021947', ['I63966007']], ['A2603756616', ['I80849659']], ['A2013867193', []], ['A2155388422', ['I59781447']], ['A2590089022', ['I11947397']], ['A3131221433', ['I43579087']], ['A2116332243', ['I157725225']], ['A3187237540', ['I108290504']], ['A1977554423', ['I118347636']], ['A2097044971', []], ['A2782877838', ['I122411786']], ['A2690326216', ['I122411786']], ['A2582221634', ['I43579087']], ['A2763529038', ['I868834043']], ['A2554449514', []], ['A2559717680', ['I122411786']], ['A2730442849', ['I122411786']], ['A2113287107', ['I122411786']], ['A2031093257', ['I142934699']], ['A2304354506', []], ['A2259678854', ['I19149307']], ['A2947746710', ['I116067653']], ['A2113463528', ['I1285433949']], ['A2899650056', ['I159176309']], ['A2031868467', []], ['A2022335298', []], ['A2761138110', ['I79510175']], ['A3211606981', []], ['A354908839', ['I1285433949']], ['A2620942280', ['I84475105']], ['A3123185368', ['I121820613']], ['A2557756023', ['I142934699']], ['A2624902964', ['I1285433949']], ['A2776483237', []], ['A2980672605', []], ['A2624797780', ['I878022262']], ['A3037946954', ['I12097938']], ['A404213974', ['I27483092']], ['A2079318338', []], ['A2431706484', ['I70983195']], ['A2605419875', ['I130238516']], ['A589382308', ['I122411786']], ['A2596778389', ['I7882870']], ['A2951079984', ['I122411786']], ['A2161639735', []], ['A962187625', ['I122411786']], ['A2136366644', ['I131729948']], ['A2580889767', ['I7882870']], ['A2041338530', ['I63966007']], ['A2862977381', []], ['A2277627506', []], ['A2906954143', ['I1306266525']], ['A2106758119', []], ['A2061148210', ['I33213144']], ['A2030123867', ['I97018004']], ['A2440901623', ['I108290504']], ['A2137004321', []], ['A1998565893', []], ['A2761325050', ['I7882870']], ['A2463697456', ['I138689650']], ['A3048686063', ['I106118109']], ['A2761866993', ['I1285433949']], ['A2943352481', ['I99542240']], ['A2882607536', []], ['A2561527376', ['I7882870']], ['A3149347399', ['I122411786']], ['A2826001191', ['I1285433949']], ['A2763783792', ['I155173764']], ['A2167365184', ['I79619799']], ['A416185457', []], ['A2902171395', []], ['A2083399793', []], ['A2294285147', ['I70983195']], ['A2222729917', []], ['A2305878762', ['I19880235']], ['A2592016208', ['I122411786']], ['A2259017461', ['I33213144']], ['A2332769670', []], ['A2916176969', ['I181647926']], ['A252456976', ['I1285433949']], ['A1685694851', ['I63966007']], ['A3160198059', ['I56590836']], ['A2242844166', ['I114112103']], ['A1977286476', []], ['A2784445771', ['I70983195']], ['A2191991431', []], ['A2146764893', ['I122411786']], ['A2792656203', ['I122411786']], ['A2573281761', ['I122411786']], ['A2110062665', ['I177877127']], ['A2479864622', []], ['A3036941851', ['I145872427']], ['A2471586637', ['I149899117']], ['A2734424565', ['I1285433949']], ['A2510367362', []], ['A3187225073', []], ['A2976419461', ['I1285433949']], ['A2656910661', []], ['A2765681291', ['I97018004']], ['A401219682', []], ['A2340744523', []], ['A2245142053', ['I149899117']], ['A2950710615', []], ['A2752961157', ['I59781447']], ['A2868717839', []], ['A2781016760', []], ['A2161572305', []], ['A3187368144', []], ['A2133056318', ['I43579087']], ['A1649922120', []], ['A2590013777', ['I181233156']], ['A2762264107', []], ['A3213443375', []], ['A2987127593', ['I1285433949']], ['A1664647747', []], ['A2266672070', ['I43579087']], ['A3177457120', ['I188497080']], ['A2461687777', ['I122411786']], ['A3202162351', ['I5681781']], ['A2828582320', ['I5681781']], ['A2947399517', ['I122411786']], ['A2188286533', ['I121820613']], ['A1920643890', ['I63966007']], ['A2049455156', ['I4654613']], ['A3192455471', ['I865915315']], ['A1949674659', ['I149899117']], ['A2955044138', []], ['A2989870371', []], ['A2047849238', ['I97018004']], ['A2816657102', ['I1285433949']], ['A1985904051', ['I130701444']], ['A2595927603', ['I100532134']], ['A1971896208', ['I122411786']], ['A2131239672', ['I130701444']], ['A3161920399', ['I122411786']], ['A2284239319', []], ['A2107701862', ['I1306266525']], ['A2252054064', ['I83816512']], ['A2215283952', ['I145872427']], ['A2946402245', ['I74801974']], ['A2552037243', ['I5681781']], ['A2119394733', ['I99065089']], ['A3208452685', ['I149899117']], ['A2633311535', []], ['A2942460859', []], ['A2958567712', ['I12315562']], ['A2743089085', ['I166972335']], ['A2253635596', []], ['A3183896882', ['I116067653']], ['A2853512116', ['I43579087']], ['A2428212893', ['I368840534']], ['A2215987423', []], ['A3137638649', []], ['A2095557066', []], ['A2593215993', ['I122411786']], ['A2005930892', ['I16097986']], ['A3213777938', ['I108290504']], ['A2993381879', []], ['A2552948661', ['I130769515']], ['A3214217541', ['I7882870']], ['A2791863069', ['I25846049']], ['A2158094367', ['I153230381']], ['A2555126875', ['I111979921']], ['A2355490837', []], ['A2812767936', ['I43579087']], ['A2949330054', ['I185261750']], ['A2246948601', ['I12097938']], ['A2550028113', ['I40347166']], ['A2298498211', ['I177877127']], ['A3175291849', ['I122411786']], ['A2107266570', ['I33213144']], ['A2765447218', ['I33213144']], ['A1966111217', []], ['A2026320856', []], ['A2595735354', ['I166972335']], ['A3135857111', ['I4921948']], ['A2267524564', ['I66946132']], ['A1996499331', ['I118347636']], ['A2165216277', []], ['A2302273048', ['I177877127']], ['A3200379016', ['I241749']], ['A3198901737', []], ['A3131885961', ['I177725633']], ['A2252018950', ['I177877127']], ['A2471669862', ['I33213144']], ['A3000654889', ['I875825670']], ['A2742982685', ['I97018004']], ['A2742256190', ['I83816512']], ['A2113263115', []], ['A2104149412', ['I130701444']], ['A2766007607', ['I165779595']], ['A3192959517', []], ['A2561889523', ['I368840534']], ['A2793250799', []], ['A2629010976', []], ['A2567194190', []], ['A2050847768', ['I861853513']], ['A1993090682', ['I132053463']], ['A2946504086', ['I158011677']], ['A2091580292', ['I80849659']], ['A2345183096', []], ['A2893597284', ['I79619799']], ['A2743835024', []], ['A2466503393', ['I121820613']], ['A2634077886', ['I16097986']], ['A2744959338', ['I78577930']], ['A2343198265', ['I23732399']], ['A2104489708', ['I12315562']], ['A1997433525', []], ['A2984511297', ['I80849659']], ['A2794293094', ['I122411786']], ['A3088771138', ['I111979921']], ['A3182150140', []], ['A3015068072', ['I78577930']], ['A2781364731', ['I122411786']], ['A2879856988', ['I905677539']], ['A2252855434', ['I130701444']], ['A2008373628', ['I177877127']], ['A2017395160', []], ['A2578295048', ['I122411786']], ['A2868493386', ['I12315562']], ['A2277296262', ['I43579087']], ['A2507807532', ['I2802326326']], ['A2324162412', ['I121820613']], ['A2829642999', ['I174135032']], ['A2819008340', ['I121820613']], ['A2144671710', ['I7882870']], ['A2577600451', ['I7882870']], ['A2783475783', []], ['A2044934989', ['I1306266525']], ['A2766428488', ['I106118109']], ['A3104715842', ['I114112103']], ['A2042635623', []], ['A1979152311', ['I149899117']], ['A3183164914', []], ['A2516309085', ['I33213144']], ['A2146738652', []], ['A2153454514', []], ['A2168664669', []], ['A2138862428', ['I70983195']], ['A2015763309', ['I91136226']], ['A2612479978', ['I130701444']], ['A2828709917', ['I70983195']], ['A1988922277', ['I97018004']], ['A3010691662', []], ['A2304156027', []], ['A1418452008', []], ['A2116577483', ['I108290504']], ['A2766695860', ['I63966007']], ['A2975633334', ['I149899117']], ['A2569824804', ['I149899117']], ['A2766499774', ['I124601658']], ['A2789344678', ['I149899117']], ['A2141040907', []], ['A2595328712', []], ['A2767037644', []], ['A2880828452', ['I27825529']], ['A3177960049', ['I149899117']], ['A2616433085', ['I868834043']], ['A678187721', ['I59781447']], ['A2977241735', ['I2802326326']], ['A2120516790', []], ['A2267448327', []], ['A2917561175', []], ['A2512693162', ['I108290504']], ['A2265221633', []], ['A3212506750', ['I149899117']], ['A3144773985', ['I108290504']], ['A2274420929', ['I40347166']], ['A2797796199', []], ['A2147618017', ['I63966007']], ['A2598058840', ['I149899117']], ['A1744413271', ['I149899117']], ['A2564105237', ['I79510175']], ['A2424897147', ['I7882870']], ['A2560970344', ['I79619799']], ['A2305700984', ['I43579087']], ['A2241179355', ['I149899117']], ['A3126698273', ['I149899117']], ['A2598542268', []], ['A2230265955', ['I99065089']], ['A2761346687', []], ['A2765940484', ['I7882870']], ['A2107317751', []], ['A2317069749', ['I91136226']], ['A2565748458', ['I188497080']], ['A2063698960', []], ['A2778232883', ['I149899117']], ['A2056008936', ['I122411786']], ['A3148340371', ['I122411786']], ['A2530629287', ['I33213144']], ['A2791266189', ['I63966007']], ['A2467688294', ['I63966007']], ['A2807888253', []], ['A2028554557', ['I12097938']], ['A2601060555', ['I122411786']], ['A2917506697', ['I63966007']], ['A2953877943', []], ['A1806296415', ['I78577930']], ['A3206727944', []], ['A2592053980', ['I70983195']], ['A2307450509', ['I79510175']], ['A2147865693', ['I99065089']], ['A2876331988', []], ['A2641626098', ['I40347166']], ['A2641626098', ['I79619799']], ['A2561355786', ['I79510175']], ['A2037446865', ['I166088655']], ['A1999419386', ['I79510175']], ['A2766675109', ['I166972335']], ['A2773897234', ['I149899117']], ['A2745233370', ['I122411786']], ['A2149221475', ['I97018004']], ['A2762583035', ['I63966007']], ['A1833794549', ['I108290504']], ['A2116510464', ['I80849659']], ['A1998320771', []], ['A1965779292', ['I108290504']], ['A2765741566', ['I70983195']], ['A2984447949', []], ['A940872858', []], ['A2558490475', ['I40347166']], ['A3152383050', ['I70983195']], ['A2766026765', ['I130238516']], ['A2792463995', []], ['A2681436121', ['I7882870']], ['A3213279077', ['I185261750']], ['A2142222472', ['I16097986']], ['A2742352285', ['I118347636']], ['A2697261290', ['I130701444']], ['A2591496726', []], ['A2139140847', []], ['A2653205707', []], ['A1924538289', ['I106118109']], ['A2026851826', ['I79619799']], ['A2133367798', ['I181233156']], ['A2580419259', []], ['A2946183167', ['I122411786']], ['A2112263148', ['I63966007']], ['A2893006435', []], ['A648929643', ['I33213144']], ['A438297632', []], ['A2829257807', ['I7882870']], ['A3200044225', ['I59781447']], ['A2567071063', ['I79619799']], ['A2340803116', ['I98677209']], ['A2608772347', ['I27483092']], ['A687187703', ['I5681781']], ['A2215402173', ['I59781447']], ['A2765354106', ['I905677539']], ['A2848021311', []], ['A2766944085', []], ['A3197661198', ['I84475105']], ['A2417734273', []], ['A2607714946', ['I122996671']], ['A2419685366', ['I1306266525']], ['A3000744708', []], ['A3164183435', []], ['A3184262436', []], ['A2149564232', ['I157725225']], ['A3196982953', []], ['A2436983654', []], ['A2472445730', ['I2799516425']], ['A2904429710', ['I130701444']], ['A2224979328', ['I11947397']], ['A2196472791', []], ['A3210093540', ['I43579087']], ['A2894592520', ['I121820613']], ['A1637814842', []], ['A1931051767', []], ['A2978132810', ['I84475105']], ['A2970255773', ['I27825529']], ['A2588028427', ['I27837315']], ['A2120371615', ['I33213144']], ['A2743085654', ['I79510175']], ['A2800816236', ['I56590836']], ['A2128477673', ['I121820613']], ['A2267511333', ['I108290504']], ['A2021276475', ['I11947397']], ['A1968023185', ['I19880235']], ['A2116344843', ['I122411786']], ['A2430134528', []], ['A3193155114', ['I2799516425']], ['A2982931411', []], ['A2616057256', []], ['A1859510812', ['I1294671590']], ['A2137278808', ['I7882870']], ['A2951105902', ['I63966007']], ['A2765051707', []], ['A3164983791', ['I190397597']], ['A2470660414', ['I79619799']], ['A2766716900', ['I84475105']], ['A2134513389', ['I145872427']], ['A3018296462', ['I149899117']], ['A2895613899', []], ['A2743903848', ['I277688954']], ['A3192147838', ['I190397597']], ['A2488799353', ['I99065089']], ['A2580259573', ['I130769515']], ['A3037040773', []], ['A2082107602', ['I122411786']], ['A2018645323', ['I122411786']], ['A2592154046', ['I27837315']], ['A2766391760', []], ['A2302614570', ['I72951846']], ['A2946415084', ['I63966007']], ['A2767121680', ['I79510175']], ['A2420107187', ['I7882870']], ['A2776749675', ['I202697423']], ['A2806435661', ['I149899117']], ['A2275643125', []], ['A2776268936', ['I130769515']], ['A2332457619', ['I79510175']], ['A1990656394', ['I177725633']], ['A2765542636', []], ['A1977732146', ['I121820613']], ['A2157477235', []], ['A2503627946', ['I181401687']], ['A2042136083', ['I149899117']], ['A2190268344', ['I7882870']], ['A2973622029', ['I185261750']], ['A1836536504', ['I7882870']], ['A3023329593', []], ['A2236585722', ['I2746051580']], ['A1980042119', []], ['A2344571799', []], ['A3116516404', ['I277688954']], ['A2151255333', []], ['A2303437377', ['I7882870']], ['A2083397653', ['I7882870']], ['A2287281526', ['I7882870']], ['A396076215', ['I122411786']], ['A2042955919', ['I149899117']], ['A2046589782', ['I7882870']], ['A3165639304', ['I145872427']], ['A2138917923', []], ['A2013597997', ['I1294671590']], ['A2517767075', []], ['A2067057196', ['I40347166']], ['A2149005230', ['I79510175']], ['A2744851237', ['I43579087']], ['A1920053454', ['I7882870']], ['A2138217739', ['I7882870']], ['A2595902692', ['I177877127']], ['A2966114616', ['I149899117']], ['A2771330512', []], ['A2761304767', ['I277688954']], ['A2846634780', ['I84475105']], ['A2053798579', ['I905677539']], ['A2580008531', ['I7882870']], ['A1994649786', []], ['A2882041359', ['I149899117']], ['A1581467037', ['I12315562']], ['A1898921624', []], ['A2334165722', []], ['A2273340091', ['I2746051580']], ['A2763511771', ['I122411786']], ['A1997196459', ['I11947397']], ['A2651934103', []], ['A2260744060', ['I2746051580']], ['A2112891699', ['I130701444']], ['A2075323785', ['I108511484']], ['A2096689809', ['I181647926']], ['A2947756923', ['I905677539']], ['A2893095224', ['I121820613']], ['A470399586', ['I241749']], ['A2129196187', ['I43439940']], ['A2468505319', ['I7882870']], ['A3149395330', []], ['A2137027242', ['I177877127']], ['A3214701367', ['I149899117']], ['A2474110466', ['I79510175']], ['A2998692115', ['I111979921']], ['A2564053002', ['I122411786']], ['A2567819330', []], ['A2150621295', ['I878022262']], ['A2974510088', ['I122411786']], ['A3025655894', ['I43579087']], ['A2957347368', ['I181233156']], ['A2947380208', ['I149899117']], ['A2763762051', ['I121820613']], ['A2745139675', ['I157725225']], ['A2775102250', ['I63966007']], ['A3014997762', []], ['A2169656381', ['I114112103']], ['A2946076134', []], ['A2144846099', ['I114983960']], ['A2375511350', ['I7882870']], ['A2076592991', ['I157725225']], ['A2332964699', ['I91136226']], ['A2743957451', ['I79510175']], ['A2893661853', ['I201448701']], ['A2150745972', ['I19880235']], ['A2946979426', []], ['A2949655747', ['I149899117']], ['A2117989385', []], ['A2973371795', []], ['A2255034731', ['I118347636']], ['A3205387863', ['I2800676788']], ['A2992126523', ['I104338594']], ['A2896102104', ['I177725633']], ['A2569516458', ['I5681781']], ['A2834197742', []], ['A3037258224', ['I4921948']], ['A1978017669', ['I130701444']], ['A2158194427', ['I5681781']], ['A2980724654', []], ['A2766309262', ['I181401687']], ['A2703584364', ['I149899117']], ['A2778226645', []], ['A2257335508', ['I159176309']], ['A2024514381', ['I33213144']], ['A2789321292', ['I12097938']], ['A2108037976', ['I149899117']], ['A2779389584', ['I149899117']], ['A2500365072', []], ['A2114391012', ['I122411786']], ['A2431304288', ['I63966007']], ['A2002141085', ['I159176309']], ['A2586345805', ['I122411786']], ['A1685051018', ['I4654613']], ['A2590008495', ['I122411786']], ['A3021257910', ['I149899117']], ['A2838408147', ['I149899117']], ['A2198090973', ['I149899117']], ['A2426512568', []], ['A2780020188', ['I149899117']], ['A2234325469', ['I185261750']], ['A2809203402', []], ['A2581106287', ['I11947397']], ['A2254396243', ['I25846049']], ['A3175777026', []], ['A2789823989', ['I43579087']], ['A2834120332', ['I149899117']], ['A3126151952', ['I177725633']], ['A2605199855', []], ['A2660334908', ['I39913132']], ['A2791156957', ['I155173764']], ['A2306815261', ['I97018004']], ['A2561810233', []], ['A2561192544', ['I277688954']], ['A1973573521', ['I56590836']], ['A2433897488', []], ['A2408956840', ['I122411786']], ['A1826541738', []], ['A2021249184', ['I149899117']], ['A2549372245', ['I7882870']], ['A2308070415', ['I4921948']], ['A2990015612', ['I4575257']], ['A2153355722', ['I139264467']], ['A2103134365', ['I104338594']], ['A3001018922', ['I7882870']], ['A2142495155', ['I149899117']], ['A3005172750', ['I12097938']], ['A2119509461', []], ['A2546432156', []], ['A3131819562', ['I2799516425']], ['A2795157789', ['I56590836']], ['A2884995774', ['I177725633']], ['A2886708460', ['I27825529']], ['A2682071947', ['I1294504835']], ['A3198756242', ['I177877127']], ['A3010430677', ['I177725633']], ['A425630834', []], ['A2027578991', ['I79510175']], ['A2626309292', ['I70983195']], ['A2135441924', ['I29607241']], ['A2953771372', ['I1294671590']], ['A1978127905', []], ['A2031087335', []], ['A3101521172', ['I149899117']], ['A2615368805', ['I155173764']], ['A2547743495', ['I142934699']], ['A2849484998', ['I149899117']], ['A2745238556', ['I116067653']], ['A2826849062', ['I149899117']], ['A2984096928', ['I63966007']], ['A2131331663', ['I122411786']], ['A2765766625', ['I79510175']], ['A2829489375', ['I868834043']], ['A357350526', ['I149899117']], ['A2063177140', ['I63966007']], ['A2442311525', ['I79510175']], ['A2744105030', ['I43579087']], ['A1499064879', ['I70983195']], ['A2425517206', ['I70983195']], ['A3201457288', ['I130769515']], ['A1954864400', ['I29607241']], ['A3190282054', []], ['A2764146561', ['I114983960']], ['A2530083674', ['I130238516']], ['A3170207227', ['I7882870']], ['A2973259274', ['I118347636']], ['A2440179721', ['I43579087']], ['A2209475002', []], ['A2072248821', []], ['A2788508077', ['I2799516425']], ['A2432693565', ['I78577930']], ['A3165376653', ['I78577930']], ['A2016056937', ['I157725225']], ['A2770679456', ['I97018004']], ['A2105787019', ['I122411786']], ['A379122203', ['I122411786']], ['A2081131462', ['I16097986']], ['A2576170318', ['I201448701']], ['A3189577759', []], ['A201038113', ['I114983960']], ['A2102079205', ['I7882870']], ['A2309715647', ['I166088655']], ['A2163983367', ['I63966007']], ['A2986016436', ['I63966007']], ['A2765896026', []], ['A3188738125', ['I2799516425']], ['A3172795602', ['I122411786']], ['A2263492593', ['I7882870']], ['A3187908163', []], ['A2105490290', ['I130238516']], ['A1663768879', ['I122411786']], ['A2203868986', ['I78577930']], ['A2469800559', ['I63966007']], ['A3183609872', ['I72951846']], ['A2465526937', []], ['A1973105240', ['I118347636']], ['A2125448681', []], ['A2294070359', ['I63966007']], ['A2591893922', ['I145610796']], ['A2580450533', ['I122411786']], ['A2121893416', ['I122411786']], ['A2607098049', ['I118347636']], ['A3047356865', ['I56590836']], ['A2993123622', ['I118347636']], ['A2650871170', ['I12097938']], ['A2976632078', ['I130769515']], ['A2975777987', ['I149899117']], ['A2989034818', ['I149899117']], ['A2902246656', []], ['A2742568143', ['I16337185']], ['A2001351290', ['I165779595']], ['A2218083907', []], ['A2948494050', ['I43579087']], ['A2270695881', []], ['A2240465705', ['I114983960']], ['A2574097560', ['I122411786']], ['A2468204009', ['I7882870']], ['A2428639699', []], ['A3018553469', ['I2746051580']], ['A2304030444', ['I130238516']], ['A2798531098', []], ['A2761792207', ['I1294671590']], ['A2910186728', ['I79619799']], ['A2070676417', ['I16285277']], ['A2794983599', []], ['A2596795233', ['I33213144']], ['A2562441733', ['I111979921']], ['A2634403777', ['I63966007']], ['A2780469136', ['I23732399']], ['A2766026859', []], ['A2148062765', ['I114983960']], ['A2012213346', []], ['A2842074435', ['I149899117']], ['A2420206282', ['I24676775']], ['A2117529880', ['I59781447']], ['A2136639717', ['I19880235']], ['A2899302788', ['I33213144']], ['A2123327683', ['I63966007']], ['A2767114523', ['I166972335']], ['A2608603388', []], ['A2765084922', ['I368840534']], ['A2099520814', []], ['A3211526149', ['I63966007']], ['A2576892988', ['I190397597']], ['A3014210212', ['I241749']], ['A2012427454', []], ['A2784806291', []], ['A3004627767', ['I74801974']], ['A2082008669', []], ['A2952436053', ['I2799516425']], ['A2893928364', ['I79619799']], ['A2108725442', ['I33213144']], ['A2763198395', ['I79510175']], ['A3185592983', ['I149899117']], ['A2595464485', ['I43579087']], ['A3135606165', ['I2802326326']], ['A2266688588', ['I59781447']], ['A2919488214', []], ['A1982961642', ['I5681781']], ['A3212714246', ['I70983195']], ['A2767201966', ['I84475105']], ['A2818447739', ['I7882870']], ['A2616521045', ['I130701444']], ['A2957965052', ['I116067653']], ['A2005208586', []], ['A2599763636', ['I127439422']], ['A2766653834', ['I27825529']], ['A2439563356', []], ['A2921463104', []], ['A2601398211', ['I149899117']], ['A2547973123', ['I27837315']], ['A2857017133', ['I122411786']], ['A2109619239', ['I181401687']], ['A2162966607', ['I7882870']], ['A2972243581', ['I177725633']], ['A2171044953', ['I118347636']], ['A1969889460', ['I145872427']], ['A2781815556', ['I149899117']], ['A2272353480', []], ['A2023299128', ['I149899117']], ['A2882558418', ['I149899117']], ['A2963376855', []], ['A2097564241', []], ['A2306413170', ['I79510175']], ['A2871160851', ['I79510175']], ['A2272207797', []], ['A2768289150', ['I27825529']], ['A2849436941', ['I138943879']], ['A2305605118', []], ['A3121867565', []], ['A2949236356', ['I149899117']], ['A3023393658', ['I80849659']], ['A2235812901', ['I905677539']], ['A2985611607', ['I149899117']], ['A2140161256', []], ['A2579244377', []], ['A2744566729', ['I130238516']], ['A2745157023', ['I33213144']], ['A2108716544', ['I155173764']], ['A2834474587', ['I149899117']], ['A2579844043', ['I5681781']], ['A2056767313', []], ['A2104041286', ['I12315562']], ['A2616629501', ['I130769515']], ['A3204430090', ['I1294504835']], ['A2743962002', ['I177877127']], ['A2976108310', ['I122996671']], ['A2780831007', []], ['A2258687281', ['I181233156']], ['A2055008422', []], ['A3206518448', []], ['A2840184033', ['I159176309']], ['A2127597866', ['I25846049']], ['A2953715031', ['I25846049']], ['A2547307271', ['I122411786']], ['A2983268837', ['I177725633']], ['A2521298114', ['I111979921']], ['A1967288566', ['I79510175']], ['A2427216197', []], ['A2024848060', []], ['A2316050742', []], ['A2766819460', ['I149899117']], ['A2767198558', ['I59781447']], ['A3103155373', []], ['A2253053690', ['I7882870']], ['A2915091928', []], ['A3189327450', ['I108290504']], ['A3192106124', []], ['A2162010643', ['I99542240']], ['A2611783856', []], ['A2946318513', ['I100532134']], ['A2587175082', ['I122411786']], ['A1975865521', ['I1294671590']], ['A2084820644', ['I70983195']], ['A2142834634', []], ['A2070179702', ['I143104139']], ['A2103984972', []], ['A2954703215', ['I122411786']], ['A2574613773', ['I111979921']], ['A2155309926', ['I149899117']], ['A2778574024', []], ['A2272538144', []], ['A3187223402', ['I114983960']], ['A415854352', ['I190397597']], ['A2151351881', []], ['A3206472845', []], ['A3189702922', ['I1294671590']], ['A2091317358', ['I16337185']], ['A2766409451', []], ['A2147448641', ['I7882870']], ['A2617729782', ['I43579087']], ['A395586697', []], ['A1654852912', []], ['A2970243390', ['I169173203']], ['A2975120789', ['I149899117']], ['A2140290691', ['I7882870']], ['A2554014841', ['I59781447']], ['A2022981214', ['I84475105']], ['A3019035701', ['I905677539']], ['A1915066629', ['I79510175']], ['A3198640294', ['I43579087']], ['A2075222951', ['I149899117']], ['A2575409521', ['I16337185']], ['A3103146582', ['I149899117']], ['A1975441365', ['I149899117']], ['A3206970968', []], ['A3147674427', ['I19880235']], ['A3100080145', ['I149899117']], ['A3188415666', []], ['A2469000994', []], ['A2976081679', ['I149899117']], ['A3157012711', ['I43579087']], ['A3197230867', ['I2802326326']], ['A2118448810', ['I122411786']], ['A1963716262', ['I181233156']], ['A2921152201', []], ['A2614875211', ['I118347636']], ['A355872337', []], ['A2306114148', ['I106118109']], ['A2134809853', []], ['A2515858894', []], ['A3198039102', ['I12315562']], ['A3197632095', ['I2802326326']], ['A3196868043', ['I2802326326']], ['A2765174062', ['I905677539']], ['A362084126', []], ['A2918218855', ['I149899117']], ['A1884058422', []], ['A3173173182', ['I142934699']], ['A3207885158', ['I114983960']], ['A2098395136', []], ['A2462241501', ['I181647926']], ['A3166907651', ['I122411786']], ['A3017399678', ['I157725225']], ['A2323751444', ['I70983195']], ['A2461163460', []], ['A1782561727', ['I157725225']], ['A2896877003', ['I149899117']], ['A2941682954', ['I27837315']], ['A3205324612', ['I155173764']], ['A2805807800', []], ['A2108733834', ['I7882870']], ['A3188567202', []], ['A3007455155', []], ['A2878446963', ['I2799516425']], ['A2089289871', ['I122411786']], ['A2100500661', ['I181233156']], ['A3023317979', ['I2802326326']], ['A2100129471', ['I131729948']], ['A2742318693', []], ['A2777681260', []], ['A2946809466', ['I99542240']], ['A2744100075', ['I201448701']], ['A2469241872', ['I7882870']], ['A2497839463', ['I149899117']], ['A2962726707', []], ['A2766898098', ['I868834043']], ['A2038817311', []], ['A2773400098', ['I122411786']], ['A372378086', []], ['A1454296234', ['I43579087']], ['A2222685358', ['I183935753']], ['A2271697176', []], ['A2957078916', ['I122996671']], ['A3134223752', ['I149899117']], ['A3135876474', ['I127439422']], ['A3125232773', ['I56590836']], ['A2143863161', ['I111979921']], ['A2596842744', ['I122411786']], ['A2893374340', ['I122411786']], ['A3009423763', ['I16097986']], ['A2793876488', []], ['A2924289912', ['I70983195']], ['A3094318201', ['I1294671590']], ['A2917981810', ['I79510175']], ['A1994600236', ['I70983195']], ['A2923680840', ['I27837315']], ['A2528509270', []], ['A2942673923', ['I159176309']], ['A2256275616', ['I181233156']], ['A2150227725', ['I122411786']], ['A2091781350', ['I111979921']], ['A2170197708', ['I149899117']], ['A2263080307', ['I122411786']], ['A2132247024', ['I149899117']], ['A2262095931', ['I181233156']], ['A1928098798', ['I159176309']], ['A2780652161', ['I149899117']], ['A2975822744', ['I149899117']], ['A2823398554', ['I149899117']], ['A2170966916', ['I149899117']], ['A2226082458', ['I84475105']], ['A2473231171', ['I7882870']], ['A3002678630', ['I118347636']], ['A2099037435', ['I157725225']], ['A2304026211', []], ['A3126429971', ['I27674431']], ['A3115898744', []], ['A3210228010', []], ['A2954896656', []], ['A2311189936', ['I118347636']], ['A2610678376', []], ['A3164919411', ['I1294504835']], ['A2169968594', ['I111979921']], ['A2765653448', ['I27825529']], ['A2885759830', ['I149899117']], ['A2576581198', ['I97018004']], ['A3132031979', ['I66946132']], ['A2143397848', ['I43579087']], ['A2791879238', ['I63966007']], ['A2997064189', ['I130701444']], ['A3147198915', ['I130701444']], ['A2590111591', ['I43579087']], ['A2610409491', ['I99542240']], ['A2683727804', []], ['A2575681443', ['I80849659']], ['A3042780887', ['I1306266525']], ['A3013619970', ['I149899117']], ['A3140490383', []], ['A3165812064', ['I905677539']], ['A2059989788', ['I118347636']], ['A2761446687', []], ['A3159832477', ['I142934699']], ['A2973327313', ['I122411786']], ['A2138374417', ['I65181880']], ['A2311932885', []], ['A2742862949', ['I43579087']], ['A2038388809', ['I7882870']], ['A2924118733', []], ['A2029544047', ['I59781447']], ['A2500174149', ['I7882870']], ['A2225816550', []], ['A2767122797', ['I84475105']], ['A2597221171', ['I78577930']], ['A2976094966', ['I149899117']], ['A1541186076', ['I159176309']], ['A2976816135', ['I149899117']], ['A2778070955', ['I149899117']], ['A2591107197', ['I79619799']], ['A3083537935', ['I2802326326']], ['A1976720645', ['I79619799']], ['A2580845160', ['I7882870']], ['A400490453', ['I190397597']], ['A2065534013', ['I19880235']], ['A2766309083', []], ['A2975410111', ['I35046152']], ['A2017918644', []], ['A2028434539', ['I102298084']], ['A2789987291', ['I165779595']], ['A2212093568', []], ['A2983755655', ['I59781447']], ['A2121561893', ['I79510175']], ['A2042879894', []], ['A2762642778', ['I84475105']], ['A2282471978', []], ['A2767160252', ['I7882870']], ['A2919506958', ['I56590836']], ['A2816315007', ['I181233156']], ['A2040393559', ['I33213144']], ['A1983407470', ['I227486990']], ['A2995618979', ['I149899117']], ['A2766702730', ['I188497080']], ['A2153356882', ['I1294504835']], ['A2134195473', ['I122411786']], ['A2160250974', ['I143104139']], ['A2975623048', ['I149899117']], ['A3157438058', ['I149899117']], ['A3213845937', ['I79619799']], ['A2395485183', []], ['A2781019202', []], ['A2774501621', []], ['A2109240833', ['I56590836']], ['A3107409158', []], ['A2953014576', ['I79510175']], ['A443913643', []], ['A2460100039', ['I7882870']], ['A2115179272', ['I108290504']], ['A2317330835', ['I7882870']], ['A3105308735', ['I16097986']], ['A2582553718', ['I122411786']], ['A2256758043', ['I79619799']], ['A2478539643', []], ['A2087966639', []], ['A2768379193', ['I33213144']], ['A2606584123', []], ['A2271925699', []], ['A2939166178', []], ['A3128012180', ['I63966007']], ['A2783896328', ['I122411786']], ['A2765118388', ['I74801974']], ['A2766505808', ['I74801974']], ['A2976055498', ['I2802326326']], ['A3207720097', ['I43579087']], ['A735303264', ['I60060512']], ['A355478588', ['I11947397']], ['A3145841316', ['I122411786']], ['A2241826757', ['I79510175']], ['A2087571514', ['I149899117']], ['A420791329', ['I122411786']], ['A2310694307', ['I121820613']], ['A2072883853', ['I122411786']], ['A2358827619', []], ['A2159263755', []], ['A2139670350', []], ['A2164731926', []], ['A2248622543', ['I70983195']], ['A2258435089', []], ['A2122667816', []], ['A2013806816', ['I7882870']], ['A2255402467', ['I138689650']], ['A2625376048', ['I122411786']], ['A2589417994', ['I122411786']], ['A3148351075', []], ['A2232612787', ['I79619799']], ['A2874945874', []], ['A2617753731', ['I7882870']], ['A2422158808', ['I5681781']], ['A2049394008', []], ['A1995694786', ['I122411786']], ['A3190535392', ['I2799516425']], ['A3187337232', ['I190397597']], ['A2076617041', ['I190397597']], ['A3135550114', ['I43579087']], ['A2259884945', []], ['A2779629177', ['I868834043']], ['A3198816670', ['I114983960']], ['A2794769002', ['I63966007']], ['A2829398367', ['I70983195']], ['A1990234647', ['I27483092']], ['A1968203033', []], ['A2554150283', ['I19880235']], ['A2190794208', ['I122411786']], ['A2869745026', ['I166972335']], ['A2163347761', ['I166972335']], ['A2743303635', []], ['A3198547128', ['I142934699']], ['A2563614108', ['I122411786']], ['A2997733971', ['I149899117']], ['A2763702278', []], ['A2306205450', ['I79619799']], ['A3215552898', ['I130769515']], ['A3140778378', ['I2802326326']], ['A2634783434', ['I177725633']], ['A2195821694', []], ['A2163404454', []], ['A2806399133', ['I2799516425']], ['A2561660945', ['I132053463']], ['A2763069523', []], ['A2763611573', ['I149899117']], ['A2779871521', ['I149899117']], ['A3197332458', ['I122411786']], ['A2405613006', ['I63966007']], ['A2993470130', ['I177877127']], ['A2919554698', ['I157725225']], ['A2780408220', ['I149899117']], ['A2563878330', ['I149899117']], ['A2472787669', ['I149899117']], ['A2014299767', ['I149899117']], ['A2097961538', ['I149899117']], ['A2561376831', ['I122411786']], ['A2469540581', ['I33213144']], ['A3123187546', ['I56590836']], ['A2991035697', ['I149899117']], ['A2996670930', ['I7882870']], ['A2175238087', ['I122411786']], ['A2942710400', ['I145872427']], ['A3196515928', ['I122411786']], ['A2893230900', ['I149899117']], ['A2697219847', ['I149899117']], ['A2157982768', ['I149899117']], ['A2590603314', ['I122411786']], ['A2090992818', ['I149899117']], ['A1914572088', ['I7882870']], ['A2596947182', ['I149899117']], ['A2743764686', ['I155173764']], ['A2744444273', ['I177725633']], ['A2122129026', []], ['A2588464670', ['I7882870']], ['A2609393006', ['I149899117']], ['A2765830911', ['I155173764']], ['A2766362220', ['I122411786']], ['A2810030146', ['I122411786']], ['A2161109715', ['I66946132']], ['A3122599703', ['I92446798']], ['A2605415677', ['I118347636']], ['A2765550308', ['I33213144']], ['A2563109565', []], ['A2563109565', ['I63966007']], ['A1434570200', ['I2799516425']], ['A2076993918', []], ['A2211178064', ['I84475105']], ['A2745106814', []], ['A2984455146', []], ['A3103749501', ['I111979921']], ['A3190207947', ['I122411786']], ['A2776586902', ['I16285277']], ['A2761741246', ['I7882870']], ['A3094532032', ['I155173764']], ['A2635620662', ['I177877127']], ['A2610356264', ['I111979921']], ['A2419807468', ['I111979921']], ['A2939358957', ['I149899117']], ['A2852550122', ['I56590836']], ['A3186584125', ['I185261750']], ['A2622566261', ['I122411786']], ['A2078501612', ['I122411786']]], 'cited_by_count': 1479, 'concepts': [['C121332964', '0.90786135'], ['C190330329', '0.810699'], ['C2780688901', '0.8093442'], ['C94915269', '0.67760795'], ['C44870925', '0.5927352']], 'referenced_works': ['W275843794', 'W632363818', 'W1134494461', 'W1531620101', 'W1559490316', 'W1606642326', 'W1783545336', 'W1790289060', 'W1825257214', 'W1868059750', 'W1965555277', 'W1971068715', 'W1982649138', 'W1984734622', 'W1988621345', 'W1989170879', 'W1992161867', 'W1993183337', 'W1994870116', 'W1995114597', 'W1996004845', 'W1997367763', 'W2002430529', 'W2007563335', 'W2011658404', 'W2014522010', 'W2014804681', 'W2016708572', 'W2022385453', 'W2030491042', 'W2036334887', 'W2037939815', 'W2042418150', 'W2043645849', 'W2044542193', 'W2047309522', 'W2047446109', 'W2051539854', 'W2051952627', 'W2052438420', 'W2054109161', 'W2054257393', 'W2070717869', 'W2071998148', 'W2075152653', 'W2091980335', 'W2095105526', 'W2096077283', 'W2103860658', 'W2105234305', 'W2106723786', 'W2116108818', 'W2116970717', 'W2131373099', 'W2138965529', 'W2139823471', 'W2140909732', 'W2142520301', 'W2147286576', 'W2148257829', 'W2148994792', 'W2153135464', 'W2154984826', 'W2162520084', 'W2166436681', 'W2169650135', 'W2170550880', 'W2198581161', 'W2214545089', 'W2222169896', 'W2252795400', 'W2254226187', 'W2256597892', 'W2263465265', 'W2268511610', 'W2273488465', 'W2273686191', 'W2273892807', 'W2277666539', 'W2277737850', 'W2292484606', 'W2298582657', 'W2308616882', 'W2339792708', 'W2342129039', 'W2342680266', 'W2343709646', 'W2412899893', 'W2415259147', 'W2416868275', 'W2437571239', 'W2513431819', 'W2535713222', 'W2546409652', 'W2547616757', 'W2550934118', 'W2555668341', 'W2571156602', 'W2571248007', 'W2604732792', 'W2606613431', 'W2615661122', 'W2621266390', 'W2621644807', 'W2951135371', 'W2952358176', 'W2953142974', 'W2993727403', 'W3099071754', 'W3099518669', 'W3102004940', 'W3102942362', 'W3105874581', 'W3106112360', 'W3106374464', 'W3106384049', 'W3124639462', 'W4234856688', 'W4292875581'], 'abstract': 'On August 14, 2017 at 10:30:43 UTC, the Advanced Virgo detector and the two Advanced LIGO detectors coherently observed a transient gravitational-wave signal produced by the coalescence of two stellar mass black holes, with a false-alarm-rate of $\\lesssim$ 1 in 27000 years. The signal was observed with a three-detector network matched-filter signal-to-noise ratio of 18. The inferred masses of the initial black holes are $30.5_{-3.0}^{+5.7}$ Msun and $25.3_{-4.2}^{+2.8}$ Msun (at the 90% credible level). The luminosity distance of the source is $540_{-210}^{+130}~\\mathrm{Mpc}$, corresponding to a redshift of $z=0.11_{-0.04}^{+0.03}$. A network of three detectors improves the sky localization of the source, reducing the area of the 90% credible region from 1160 deg$^2$ using only the two LIGO detectors to 60 deg$^2$ using all three detectors. For the first time, we can test the nature of gravitational wave polarizations from the antenna response of the LIGO-Virgo network, thus enabling a new class of phenomenological tests of gravity.', 'counts_by_year': [[2022, 92], [2021, 211], [2020, 252], [2019, 388], [2018, 486], [2017, 48], [2016, 1], [2013, 1]]}, {'id': 'W2395626624', 'doi': 'https://doi.org/10.1126/sciadv.1600200', 'title': 'Fundamentals of cancer metabolism', 'type': 'journal-article', 'publication_date': '2016-05-01', 'host_venue': 'V2737427234', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A286673543', ['I867280407']], ['A2081551422', ['I111979921']]], 'cited_by_count': 1477, 'concepts': [['C62231903', '0.466397'], ['C121608353', '0.4600392'], ['C70721500', '0.44314712'], ['C41008148', '0.36959344'], ['C86803240', '0.3136667']], 'referenced_works': ['W900373682', 'W1483430612', 'W1541421641', 'W1553978925', 'W1673751863', 'W1772301402', 'W1824112533', 'W1829589101', 'W1881064736', 'W1890766043', 'W1900467627', 'W1916910865', 'W1932780354', 'W1965265512', 'W1967291345', 'W1968620004', 'W1969524738', 'W1970766774', 'W1972254548', 'W1975215468', 'W1975566288', 'W1976949137', 'W1979887297', 'W1980084094', 'W1980181091', 'W1982219839', 'W1982324715', 'W1982330633', 'W1982789056', 'W1983107759', 'W1984882020', 'W1985106139', 'W1988377666', 'W1988756388', 'W1992524428', 'W1992724001', 'W1993238514', 'W1995069275', 'W1995103178', 'W1995675594', 'W1996311090', 'W1996365670', 'W1998166655', 'W2000144783', 'W2001999652', 'W2002583208', 'W2004574994', 'W2005327657', 'W2005703314', 'W2008279952', 'W2009397177', 'W2009999719', 'W2011593105', 'W2012442095', 'W2012588998', 'W2014988241', 'W2015012139', 'W2015755329', 'W2016133306', 'W2016253024', 'W2017035524', 'W2017746705', 'W2019560775', 'W2021492010', 'W2021787243', 'W2022899908', 'W2023103376', 'W2023217997', 'W2023300506', 'W2023947368', 'W2026967508', 'W2031022790', 'W2035010201', 'W2036659083', 'W2038003361', 'W2038009564', 'W2039212784', 'W2039945243', 'W2040641918', 'W2041237929', 'W2041596211', 'W2041729369', 'W2044515618', 'W2044596456', 'W2045122835', 'W2046178144', 'W2051014867', 'W2051181260', 'W2052543023', 'W2055811314', 'W2056015556', 'W2058218780', 'W2061059838', 'W2063632665', 'W2067107606', 'W2069376956', 'W2070923004', 'W2072451938', 'W2072569792', 'W2073331259', 'W2074448607', 'W2074847845', 'W2078972833', 'W2080452140', 'W2085288412', 'W2086528604', 'W2086816631', 'W2087377825', 'W2089818796', 'W2092152920', 'W2093575933', 'W2093985232', 'W2095860292', 'W2095987251', 'W2096752472', 'W2098131661', 'W2099711117', 'W2100511713', 'W2100628426', 'W2101027575', 'W2101228747', 'W2101774229', 'W2102186262', 'W2102536168', 'W2104014368', 'W2104450983', 'W2104696999', 'W2104852749', 'W2105665764', 'W2114805483', 'W2115420718', 'W2116516144', 'W2117692326', 'W2118107482', 'W2118519201', 'W2119115728', 'W2119437574', 'W2122341467', 'W2122776638', 'W2123014930', 'W2126511952', 'W2126524529', 'W2126817554', 'W2127063807', 'W2129917557', 'W2130784515', 'W2131719921', 'W2131860315', 'W2134061682', 'W2134258065', 'W2134294998', 'W2135617839', 'W2137386173', 'W2138271341', 'W2139847043', 'W2140814324', 'W2140983197', 'W2142898716', 'W2144235090', 'W2144542772', 'W2146414563', 'W2146544786', 'W2147058114', 'W2148332345', 'W2149282639', 'W2149343603', 'W2151358436', 'W2151980465', 'W2154313451', 'W2157099092', 'W2161877652', 'W2162790428', 'W2162989569', 'W2163404652', 'W2163471981', 'W2164219809', 'W2164422311', 'W2166583179', 'W2168656281', 'W2168735777', 'W2169516568', 'W2169903226', 'W2172697812', 'W2173466127', 'W2174118934', 'W2179192278', 'W2186118939', 'W2190340505', 'W2196885702', 'W2200781451', 'W2207569459', 'W2207799751', 'W2210628912', 'W2216910489', 'W2234115940', 'W2258066491', 'W2264411128', 'W2312291090', 'W2336096370', 'W2337267398', 'W4237937003', 'W4243903519', 'W4252021281'], 'abstract': 'Researchers provide a conceptual framework to understand current knowledge of the fundamentals of cancer metabolism.', 'counts_by_year': [[2022, 277], [2021, 389], [2020, 322], [2019, 235], [2018, 144], [2017, 99], [2016, 9]]}, {'id': 'W2936349953', 'doi': 'https://doi.org/10.3847/2041-8213/ab0ec7', 'title': 'First M87 Event Horizon Telescope Results. I. The Shadow of the Supermassive Black Hole', 'type': 'journal-article', 'publication_date': '2019-04-10', 'host_venue': 'V4210175824', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2162272575', []], ['A2029672378', ['I134820265']], ['A1966534324', ['I149899117']], ['A2099122403', ['I1311099166']], ['A2935952427', ['I16097986']], ['A2536036580', ['I149899117']], ['A2496120967', ['I138006243']], ['A1268505207', ['I136199984']], ['A2108686535', ['I63966007']], ['A2864562759', []], ['A2539854995', ['I136199984']], ['A2937997121', []], ['A2196651897', ['I122411786']], ['A2947297489', ['I1311099166']], ['A2899209117', []], ['A2081267117', ['I145872427']], ['A2463910648', ['I136199984']], ['A2051565586', ['I149899117']], ['A2054508998', ['I151746483']], ['A2939844179', []], ['A2641019927', ['I145872427']], ['A2194400045', ['I2800676788']], ['A2158894009', ['I40347166']], ['A2915543468', ['I136199984']], ['A2855814200', ['I138006243']], ['A2552849962', ['I205783295']], ['A2940389913', ['I887064364']], ['A2108362735', ['I1311099166']], ['A3175291849', ['I19820366']], ['A2651768324', ['I2800676788']], ['A2124161232', ['I136199984']], ['A2758025067', ['I66862912']], ['A2145462817', ['I205783295']], ['A2947297489', ['I63966007']], ['A2935724678', ['I199525922']], ['A2947446220', ['I145872427']], ['A2037410514', ['I71267560']], ['A2912819914', ['I69552723']], ['A2877955790', []], ['A1975161900', ['I149899117']], ['A2196171785', ['I149899117']], ['A2638968421', ['I136199984']], ['A388278477', ['I149899117']], ['A2078475988', ['I145872427']], ['A2023622334', ['I63966007']], ['A2798826376', ['I2609998029']], ['A2938951676', ['I145872427']], ['A2112716162', ['I63966007']], ['A2942904384', []], ['A2946847674', ['I114090438']], ['A2114972991', ['I134820265']], ['A2267060652', ['I136199984']], ['A2015655279', ['I157725225']], ['A2139706867', []], ['A2938454137', []], ['A2940248907', ['I151746483']], ['A2871483393', ['I121797337']], ['A2549383106', ['I114090438']], ['A2128704681', ['I19820366']], ['A2018533581', ['I136199984']], ['A2083994979', ['I199525922']], ['A2286133576', ['I63966007']], ['A2012070561', ['I169381384']], ['A2162103941', ['I20231570']], ['A2250409157', ['I1311099166']], ['A2318455978', ['I199525922']], ['A2435950833', ['I1311099166']], ['A2592967798', ['I19820366']], ['A2706179837', ['I39824353']], ['A2217901261', []], ['A2108122243', ['I1311099166']], ['A2910819636', ['I145872427']], ['A2888894956', ['I136199984']], ['A2156992784', ['I138006243']], ['A2159225451', ['I145872427']], ['A2797718235', ['I151746483']], ['A2239042677', ['I2799714274']], ['A2786641629', ['I136199984']], ['A661355302', ['I111088046']], ['A2227238057', ['I2800676788']], ['A2555515727', ['I151746483']], ['A1448525415', ['I149899117']], ['A2170937078', ['I199525922']], ['A2938580577', ['I136199984']], ['A2935716214', ['I80584463']], ['A2655976860', ['I149899117']], ['A2496790422', ['I138006243']], ['A2608212457', ['I2800676788']], ['A2084307070', ['I116465919']], ['A2179286472', ['I1311099166']], ['A2108037976', ['I1311099166']], ['A2947440290', ['I1311099166']], ['A2106930737', ['I149899117']], ['A2474704753', []], ['A2120288441', ['I149899117']], ['A2291559160', ['I142974352']], ['A2256505404', []], ['A2513267473', ['I2800676788']], ['A2107030656', []], ['A2307354850', ['I881766915']], ['A2186277005', ['I66862912']], ['A2768535928', ['I149899117']], ['A2773542218', []], ['A2533335160', ['I16733864']], ['A3212584424', ['I149899117']], ['A714682511', ['I8961855']], ['A2798352692', ['I63966007']], ['A2343642260', ['I149899117']], ['A2947826291', ['I149899117']], ['A2963270781', ['I19820366']], ['A2029185228', ['I887064364']], ['A2640667263', ['I138006243']], ['A1148853569', ['I111088046']], ['A388937666', ['I66862912']], ['A2006643769', ['I1311099166']], ['A2400059025', ['I63966007']], ['A2565635236', ['I138006243']], ['A2133794038', ['I149899117']], ['A2121793394', ['I114090438']], ['A2022771990', []], ['A3212424106', ['I136199984']], ['A2935744394', ['I63966007']], ['A2979212391', ['I145872427']], ['A2947563438', ['I145872427']], ['A1984016274', ['I199525922']], ['A2148228031', ['I172787465']], ['A2234714853', ['I1311099166']], ['A2118843672', ['I136199984']], ['A2109987196', ['I24603500']], ['A2912196781', ['I203238179']], ['A2432352201', []], ['A2939602616', ['I151746483']], ['A2947022729', ['I149899117']], ['A2939797645', ['I74801974']], ['A2583257953', ['I114090438']], ['A1976925341', ['I149899117']], ['A2947256138', ['I199525922']], ['A2096381090', ['I138006243']], ['A2938319471', ['I136199984']], ['A2134376587', ['I136199984']], ['A1527925734', ['I109736498']], ['A2938866743', ['I136199984']], ['A2306144916', []], ['A1623665298', ['I95457486']], ['A2936636250', ['I24603500']], ['A2248445458', ['I114090438']], ['A2935867865', ['I157725225']], ['A2912129947', ['I183974707']], ['A2304685091', ['I138006243']], ['A2265219047', ['I183974707']], ['A2936157661', ['I172787465']], ['A2164478084', ['I1311099166']], ['A2638947131', []], ['A2124625990', ['I136199984']], ['A215060322', ['I114090438']], ['A2955222483', ['I114090438']], ['A2551786108', ['I145872427']], ['A3097411339', ['I63966007']], ['A2496945629', ['I149899117']], ['A2937898374', ['I138006243']], ['A2936554726', ['I138006243']], ['A2117107991', ['I149899117']], ['A2558875888', ['I149899117']], ['A2938272281', ['I63966007']], ['A2552540946', ['I1343871089']], ['A2053887763', []], ['A2549459435', []], ['A1990107472', ['I39824353']], ['A2021593490', ['I113306721']], ['A2055457037', ['I9927081']], ['A1926823185', ['I24603500']], ['A2441234508', []], ['A2885759830', ['I20231570']], ['A2675344839', ['I19820366']], ['A2940162130', ['I80584463']], ['A2115842056', ['I193775966']], ['A676358105', ['I63966007']], ['A1964546684', ['I199525922']], ['A2508729197', ['I151746483']], ['A2793380711', ['I121797337']], ['A2101973788', ['I63966007']], ['A2084263155', ['I201537933']], ['A2693513849', ['I149899117']], ['A2938195545', ['I138006243']], ['A2947234236', ['I139264467']], ['A2937330348', ['I199525922']], ['A2937729882', ['I80584463']], ['A2069708846', ['I80584463']], ['A2092153939', ['I145872427']], ['A2430256265', ['I149899117']], ['A2466201611', ['I6902469']], ['A200671804', ['I136199984']], ['A2304685091', ['I149899117']], ['A2906137205', ['I149899117']], ['A2910486076', ['I136199984']], ['A2937258773', ['I157725225']], ['A2900006127', ['I47720641']], ['A2135055977', ['I136199984']], ['A2947281253', ['I145872427']], ['A2548140060', ['I45129253']], ['A2200956326', ['I19820366']], ['A3037102273', ['I126520041']], ['A424174943', ['I149899117']], ['A2484630069', ['I2800676788']], ['A2939461234', ['I145872427']], ['A2899620188', ['I136199984']], ['A2575317714', ['I33849332']], ['A2939582245', ['I111236770']], ['A2607673522', ['I2609998029']], ['A3013553924', ['I7863295']], ['A2162214129', ['I149899117']], ['A2023270877', ['I63966007']], ['A2045130775', ['I63966007']], ['A2122246463', ['I40347166']], ['A2421364213', []], ['A2970159277', ['I80584463']], ['A3093650352', ['I136199984']], ['A2939150305', ['I39824353']], ['A2056698261', ['I63966007']], ['A3205902390', ['I39824353']], ['A2338741240', ['I189290911']], ['A2677343172', ['I1311099166']], ['A3035435415', ['I189290911']], ['A3035037825', ['I1311099166']], ['A2844271944', ['I1311099166']], ['A2118970958', []], ['A2936740703', ['I136199984']], ['A2764367515', []], ['A2125883440', ['I40347166']], ['A2986678664', ['I63966007']], ['A2939891703', []], ['A2338656979', ['I63966007']], ['A2836503346', ['I95457486']], ['A2920979296', ['I149899117']], ['A2078476558', ['I63966007']], ['A2949435734', ['I149899117']], ['A1822667284', ['I149899117']], ['A2293701907', ['I63966007']], ['A2160648057', ['I24603500']], ['A2126416114', ['I188538660']], ['A2857125139', ['I125749732']], ['A2938725495', ['I33434090']], ['A2938858404', ['I24603500']], ['A2803508639', ['I138006243']], ['A2538319206', ['I138006243']], ['A2098482267', ['I138006243']], ['A2740725374', ['I39824353']], ['A2500660101', ['I39824353']], ['A2618423258', ['I149899117']], ['A2655384567', ['I1309140791']], ['A2678074148', ['I149899117']], ['A2105926353', ['I138006243']], ['A2435097781', ['I24603500']], ['A2950329732', []], ['A2164368547', ['I5023651']], ['A2049595690', ['I188538660']], ['A3207863010', ['I1311099166']], ['A2904070870', ['I189290911']], ['A2855290702', ['I189290911']], ['A2303239817', ['I1311099166']], ['A2846731344', ['I40347166']], ['A2300516104', ['I8961855']], ['A1695994553', ['I1321434546']], ['A85522135', ['I149899117']], ['A1996342444', ['I199525922']], ['A2626664965', []], ['A3206213529', ['I1311099166']], ['A2910179685', ['I1311099166']], ['A2104942175', ['I1311099166']], ['A2107445692', ['I136199984']], ['A2952146151', ['I40347166']], ['A2998116428', ['I1311099166']], ['A2100566382', ['I199525922']], ['A2078631736', []], ['A2939707515', []], ['A2335893400', ['I2609998029']], ['A2971260499', ['I28407311']], ['A3212234505', ['I40347166']], ['A2419729726', ['I1311099166']], ['A2823632669', ['I48566637']], ['A2835026350', ['I189290911']], ['A2768535928', ['I1311099166']], ['A2846568673', ['I189290911']], ['A2559309441', ['I2609998029']], ['A2950390040', ['I1311099166']], ['A2803653287', ['I138006243']], ['A2939323286', []], ['A2938743129', ['I138006243']], ['A1980743682', ['I63966007']], ['A2039869108', ['I1321434546']], ['A2805954046', ['I922237871']], ['A3014232925', ['I40347166']], ['A3131734629', ['I39824353']], ['A2939926536', []], ['A2940239440', ['I2609998029']], ['A1996393398', ['I149899117']], ['A2431488515', ['I157725225']], ['A2663564724', []], ['A1966748011', ['I7863295']], ['A3090027357', ['I138006243']], ['A2170049350', ['I1311099166']], ['A2305753403', ['I136199984']], ['A2096572362', ['I204465549']], ['A1934386288', ['I1311099166']], ['A2179278582', ['I15807432']], ['A1468150737', ['I1311099166']], ['A1993555439', []], ['A2128090057', ['I136199984']], ['A2937179294', []], ['A3105097121', ['I1321434546']], ['A2533906031', ['I63966007']], ['A2057387891', ['I1311099166']], ['A2951696649', []], ['A2013895227', ['I1311099166']], ['A1978754084', ['I40347166']], ['A2012259697', ['I138006243']], ['A2842677362', []], ['A2936721848', []], ['A2939886147', ['I2609998029']], ['A2938729922', []], ['A2032583161', ['I20089843']], ['A2813330171', ['I1311099166']], ['A2936919556', []], ['A2342578584', ['I136199984']], ['A2482772195', ['I130701444']], ['A2629226111', ['I1311099166']], ['A2050870505', ['I24603500']], ['A2935762399', ['I63966007']], ['A2780627191', ['I136199984']], ['A2949426153', ['I1311099166']], ['A2938427966', []], ['A2428138863', ['I136199984']], ['A3167810791', ['I97018004']], ['A1978052051', ['I145872427']], ['A2952363048', ['I136199984']], ['A2124900880', []], ['A3034802093', ['I1311099166']], ['A3027027496', ['I161318765']], ['A2776432738', ['I63966007']], ['A2081428928', ['I122411786']], ['A2559329630', []], ['A2143795888', ['I95457486']], ['A2938793662', ['I136199984']], ['A3034230815', ['I1311099166']], ['A2153612993', ['I39824353']], ['A2602906871', ['I63966007']], ['A228816188', ['I138006243']]], 'cited_by_count': 1477, 'concepts': [['C121332964', '0.9746363'], ['C173120059', '0.859637'], ['C44870925', '0.7569369'], ['C50341732', '0.6617963'], ['C148292158', '0.64579993']], 'referenced_works': ['W1551745221', 'W1560414242', 'W1633529777', 'W1651593697', 'W1665346145', 'W1765032695', 'W1852220742', 'W1875404698', 'W1912595871', 'W1931297695', 'W1965336810', 'W1965834693', 'W1967790056', 'W1968456825', 'W1972402856', 'W1973497701', 'W1979140081', 'W1981111503', 'W1984055282', 'W1984403449', 'W1985415508', 'W1986451051', 'W1988751690', 'W1990311414', 'W1991108342', 'W1995921363', 'W1997902577', 'W1998548404', 'W1999417864', 'W2000246796', 'W2002442941', 'W2003210682', 'W2003710193', 'W2006905260', 'W2009568911', 'W2010362241', 'W2013352950', 'W2015011282', 'W2017901093', 'W2018370556', 'W2019108554', 'W2023347320', 'W2024532173', 'W2031106768', 'W2033188665', 'W2035396512', 'W2035538351', 'W2035637895', 'W2037245193', 'W2042039095', 'W2042122167', 'W2042155338', 'W2046703480', 'W2049013365', 'W2050276909', 'W2050444942', 'W2052045615', 'W2057928525', 'W2062839084', 'W2062949828', 'W2064174537', 'W2067381051', 'W2067915101', 'W2073862476', 'W2078887476', 'W2080912662', 'W2085576844', 'W2085701313', 'W2087315141', 'W2092975024', 'W2093190121', 'W2100168895', 'W2106198430', 'W2108628220', 'W2109112103', 'W2114169704', 'W2116923776', 'W2116990644', 'W2120683449', 'W2123042076', 'W2123527757', 'W2123710893', 'W2124422401', 'W2127804216', 'W2128089313', 'W2129947695', 'W2133778146', 'W2141145035', 'W2150151189', 'W2152059463', 'W2154223176', 'W2156621121', 'W2165875176', 'W2173563237', 'W2175694974', 'W2181359111', 'W2195797723', 'W2219142959', 'W2252795400', 'W2281136567', 'W2328619392', 'W2479388119', 'W2507204106', 'W2508092792', 'W2520222101', 'W2539239875', 'W2551043800', 'W2593213383', 'W2593379902', 'W2593489710', 'W2594279543', 'W2739053710', 'W2744067488', 'W2775381858', 'W2787075170', 'W2788112096', 'W2804564056', 'W2896165040', 'W2936095933', 'W2937056002', 'W2938000962', 'W2939024351', 'W2939038312', 'W2950251174', 'W2963076714', 'W3098270385', 'W3098333541', 'W3098438575', 'W3098517098', 'W3098544497', 'W3098774414', 'W3100868871', 'W3101250889', 'W3101955839', 'W3102313407', 'W3102615744', 'W3103204359', 'W3103204423', 'W3103261082', 'W3103357807', 'W3103517951', 'W3103524523', 'W3103581061', 'W3103659626', 'W3103921749', 'W3103974864', 'W3104283018', 'W3104409603', 'W3104457818', 'W3104584790', 'W3104664494', 'W3104765358', 'W3105804180', 'W3105829701', 'W3105891255', 'W3106157386', 'W3106312846', 'W3106331127', 'W3121157417', 'W3124533373', 'W4293256937', 'W4297902343'], 'abstract': 'When surrounded by a transparent emission region, black holes are expected to reveal a dark shadow caused by gravitational light bending and photon capture at the event horizon. To image and study this phenomenon, we have assembled the Event Horizon Telescope, a global very long baseline interferometry array observing at a wavelength of 1.3 mm. This allows us to reconstruct event-horizon-scale images of the supermassive black hole candidate in the center of the giant elliptical galaxy M87. We have resolved the central compact radio source as an asymmetric bright emission ring with a diameter of 42+/-3 micro-as, which is circular and encompasses a central depression in brightness with a flux ratio ~10:1. The emission ring is recovered using different calibration and imaging schemes, with its diameter and width remaining stable over four different observations carried out in different days. Overall, the observed image is consistent with expectations for the shadow of a Kerr black hole as predicted by general relativity. The asymmetry in brightness in the ring can be explained in terms of relativistic beaming of the emission from a plasma rotating close to the speed of light around a black hole. We compare our images to an extensive library of ray-traced general-relativistic magnetohydrodynamic simulations of black holes and derive a central mass of M = (6.5+/-0.7) x 10^9 Msun. Our radio-wave observations thus provide powerful evidence for the presence of supermassive black holes in centers of galaxies and as the central engines of active galactic nuclei. They also present a new tool to explore gravity in its most extreme limit and on a mass scale that was so far not accessible.', 'counts_by_year': [[2022, 375], [2021, 489], [2020, 423], [2019, 188]]}, {'id': 'W3025238321', 'doi': 'https://doi.org/10.1016/s0140-6736(17)32366-8', 'title': 'Global, regional, and national comparative risk assessment of 84 behavioural, environmental and occupational, and metabolic risks or clusters of risks, 1990–2016: a systematic analysis for the Global Burden of Disease Study 2016', 'type': 'journal-article', 'publication_date': '2017-09-16', 'host_venue': 'V49861241', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2336267196', []], ['A2526738623', []], ['A1261223370', []], ['A2104043555', []], ['A2167087174', []], ['A1586390273', []], ['A2026292620', []], ['A275226674', []], ['A1950551245', []], ['A2989320974', []], ['A2525751138', []], ['A2524659107', []], ['A2755910248', []], ['A1828904284', []], ['A2337146604', []], ['A2151274743', []], ['A2209012582', []], ['A1244030808', []], ['A2132116411', []], ['A2755349297', []], ['A2755240926', []], ['A2756237734', []], ['A2546993401', []], ['A2562109587', []], ['A1968471205', []], ['A2343865831', []], ['A2279206299', []], ['A2525984107', []], ['A2754876412', []], ['A1917517771', []], ['A1991399651', []], ['A2899528386', []], ['A2251799706', []], ['A1896914120', []], ['A2709848888', []], ['A109783649', []], ['A2952949420', []], ['A290882514', []], ['A2318996425', []], ['A2075525165', []], ['A2116969515', []], ['A2291839830', []], ['A2018956803', []], ['A2658307528', []], ['A2917564806', []], ['A2033215675', []], ['A2109138503', []], ['A1902482174', []], ['A2893513547', []], ['A174217572', []], ['A1949092674', []], ['A3211254539', []], ['A1974818969', []], ['A2754942240', []], ['A2607074190', []], ['A116052509', []], ['A2515935880', []], ['A2424735554', []], ['A2807634735', []], ['A2053519999', []], ['A2016239449', []], ['A2289255636', []], ['A2075268468', []], ['A2026359138', []], ['A2103380712', []], ['A660088416', []], ['A3213045101', []], ['A1981440893', []], ['A2220936201', []], ['A2170988738', []], ['A2001825501', []], ['A2034318265', []], ['A3191566627', []], ['A308811693', []], ['A2156669191', []], ['A2108298034', []], ['A1895530724', []], ['A2135382290', []], ['A2155770780', []], ['A2616542806', []], ['A2082872497', []], ['A2255678021', []], ['A2100094052', []], ['A2483563741', []], ['A2527205794', []], ['A1680462067', []], ['A2524309682', []], ['A2142082663', []], ['A2015186656', []], ['A2123150680', []], ['A1954828074', []], ['A1695632349', []], ['A2054394482', []], ['A2424262092', []], ['A2755823268', []], ['A2755232509', []], ['A2158980598', []], ['A678399540', []], ['A2130905594', []], ['A621791706', []], ['A2755867325', []], ['A3176002406', []], ['A138047837', []], ['A2345520266', []], ['A2105920767', []], ['A2306491945', []], ['A2064694482', []], ['A2366254680', []], ['A2854778701', []], ['A2123807283', []], ['A781531929', []], ['A2494376796', []], ['A2559996202', []], ['A2589248567', []], ['A2128665858', []], ['A2609800051', []], ['A2424892588', []], ['A3024771790', []], ['A2140769477', []], ['A2212624663', []], ['A2528576217', []], ['A2912226948', []], ['A935523974', []], ['A2009078738', []], ['A3019388298', []], ['A344498437', []], ['A2568267441', []], ['A2145883831', []], ['A1661044825', []], ['A2015830873', []], ['A2006538287', []], ['A256209369', []], ['A2061034448', []], ['A2754324203', []], ['A692807803', []], ['A4431029', []], ['A3094181563', []], ['A1944064886', []], ['A2023264609', []], ['A2092117536', []], ['A2789395593', []], ['A2036512884', []], ['A2014559487', []], ['A66691543', []], ['A2303827053', []], ['A2171790375', []], ['A2162480515', []], ['A2304245645', []], ['A3190043835', []], ['A2134754651', []], ['A2225675537', []], ['A2419696075', []], ['A3214475530', []], ['A2444505863', []], ['A2024673501', []], ['A1981266779', []], ['A1485388775', []], ['A1991156927', []], ['A2139978954', []], ['A325119662', []], ['A2525638264', []], ['A2147768707', []], ['A2170374040', []], ['A3010334873', []], ['A130753112', []], ['A2599128148', []], ['A2719099999', []], ['A3156606865', []], ['A2637625157', []], ['A2517957986', []], ['A2239987130', []], ['A2433616363', []], ['A2058142950', []], ['A1989487947', []], ['A2029234275', []], ['A2198161829', []], ['A248647053', []], ['A2180474783', []], ['A2160364156', []], ['A2309765197', []], ['A2098038954', []], ['A419432919', []], ['A2517658965', []], ['A2177286373', []], ['A2798490169', []], ['A2755160274', []], ['A2476985414', []], ['A1971808073', []], ['A2264474294', []], ['A1976610169', []], ['A2174002162', []], ['A2661338970', []], ['A2527676953', []], ['A2508616232', []], ['A1991954648', []], ['A2336558418', []], ['A2045839628', []], ['A2907358776', []], ['A2409117092', []], ['A1998697755', []], ['A2136108649', []], ['A2078916635', []], ['A1987498663', []], ['A2900340207', []], ['A2295826387', []], ['A2147249232', []], ['A2190367765', []], ['A2122588423', []], ['A439920156', []], ['A1934246530', []], ['A2527252649', []], ['A2003644413', []], ['A2170943676', []], ['A2752875094', []], ['A69356327', []], ['A2087097567', []], ['A2145855513', []], ['A2008056385', []], ['A261494640', []], ['A1999104575', []], ['A1975482828', []], ['A2754965555', []], ['A2159017802', []], ['A1987290921', []], ['A2059841237', []], ['A2702278152', []], ['A2416794896', []], ['A2159703735', []], ['A2063878478', []], ['A1684365439', []], ['A110332668', []], ['A2145691735', []], ['A2091436485', []], ['A2113762876', []], ['A2964652680', []], ['A2103856476', []], ['A2100571447', []], ['A1933433957', []], ['A2755112910', []], ['A2099842673', []], ['A2756137649', []], ['A1984201670', []], ['A2165675880', []], ['A2510023083', []], ['A2090105886', []], ['A2305773610', []], ['A2112064235', []], ['A2230245500', []], ['A2887837994', []], ['A2145525382', []], ['A1900504705', []], ['A2048675361', []], ['A2133846596', []], ['A2755115224', []], ['A2755509893', []], ['A2097761319', []], ['A2058870430', []], ['A1967675247', []], ['A2132501245', []], ['A2098444608', []], ['A2804266614', []], ['A2146891038', []], ['A1975324464', []], ['A2525534507', []], ['A2756478490', []], ['A2576156219', []], ['A2737505702', []], ['A2310038103', []], ['A2066511934', []], ['A3157605970', []], ['A2468155649', []], ['A1963193313', []], ['A2298408023', []], ['A1875942229', []], ['A242742814', []], ['A2107678894', []], ['A2899947101', []], ['A3177409340', []], ['A2632825197', []], ['A3120043664', []], ['A954872853', []], ['A706716812', []], ['A2946320960', []], ['A1969029766', []], ['A1169616326', []], ['A2192301710', []], ['A3214522656', []], ['A2160882886', []], ['A2591497774', []], ['A2087695571', []], ['A2221155485', []], ['A2605157327', []], ['A2131372770', []], ['A2137545149', []], ['A2560788212', []], ['A2402779843', []], ['A2527007444', []], ['A2038807793', []], ['A188612772', []], ['A2126648517', []], ['A1989521457', []], ['A3190491666', []], ['A2304074508', []], ['A3160405201', []], ['A2095606354', []], ['A2139270141', []], ['A2101549818', []], ['A2656209814', []], ['A2805183671', []], ['A2618847948', []], ['A2140496955', []], ['A2755975474', []], ['A2106144155', []], ['A2565902731', []], ['A2102161365', []], ['A1243347398', []], ['A2174321888', []], ['A3189366274', []], ['A2128434403', []], ['A2562465928', []], ['A2040328808', []], ['A2792833490', []], ['A1830555489', []], ['A2585818880', []], ['A2106392805', []], ['A2426028861', []], ['A2598709781', []], ['A1727635955', []], ['A2133394427', []], ['A1631637737', []], ['A2945866049', []], ['A1964422715', []], ['A2103078029', []], ['A2756291383', []], ['A2937964638', []], ['A2508058704', []], ['A2146427514', []], ['A2462423758', []], ['A1286028393', []], ['A2107937508', []], ['A2171983489', []], ['A2213128263', []], ['A1847310112', []], ['A3211925812', []], ['A3190196087', []], ['A2269532386', []], ['A2950321715', []], ['A2664379405', []], ['A1969880502', []], ['A3037227835', []], ['A202526925', []], ['A2102508135', []], ['A2943224566', []], ['A2165933833', []], ['A1935550320', []], ['A2320193999', []], ['A2598037195', []], ['A1311980873', []], ['A2701840363', []], ['A2430086965', []], ['A2573321743', []], ['A2109432494', []], ['A2641135659', []], ['A2644442497', []], ['A2023451452', []], ['A2217126065', []], ['A2138863258', []], ['A2124962734', []], ['A2756135132', []], ['A2126042519', []], ['A2110381418', []], ['A2287785407', []], ['A224987146', []], ['A1860009983', []], ['A2072898194', []], ['A2696008654', []], ['A2078422342', []], ['A2156641128', []], ['A2754774276', []], ['A2608759177', []], ['A1460372564', []], ['A2755060371', []], ['A2316804223', []], ['A2173061812', []], ['A2134948551', []], ['A2113265520', []], ['A2018982985', []], ['A2059048655', []], ['A2755691198', []], ['A878870270', []], ['A1809145585', []], ['A2133215214', []], ['A2949180028', []], ['A2756247572', []], ['A2164729008', []], ['A3144227121', []], ['A2755026186', []], ['A2709541152', []], ['A2592397181', []], ['A2755628027', []], ['A650968989', []], ['A2028028389', []], ['A2040971506', []], ['A2005122984', []], ['A2529893633', []], ['A2598867768', []], ['A2602401922', []], ['A2201699675', []], ['A2799989877', []], ['A2558895513', []], ['A2527901436', []], ['A2045444862', []], ['A2527585475', []], ['A1859188192', []], ['A1990140758', []], ['A2288782178', []], ['A2007049068', []], ['A2130487955', []], ['A2203942795', []], ['A2109200798', []], ['A2623549860', []], ['A2754216068', []], ['A1900504705', []], ['A1899471802', []], ['A2113374822', []], ['A2212815995', []], ['A2098591421', []], ['A2024441155', []], ['A2128235974', []], ['A2157298618', []], ['A138278537', []], ['A1991284286', []], ['A3094348129', []], ['A2780731423', []], ['A2526120338', []], ['A1714729726', []], ['A2133877409', []], ['A1976173630', []], ['A2797166399', []], ['A2306642447', []], ['A2099909626', []], ['A3212338639', []], ['A2673842419', []], ['A2559374945', []], ['A2804116351', []], ['A1587262111', []], ['A2162568637', []], ['A2112367615', []], ['A1224992126', []], ['A251309730', []], ['A2303031201', []], ['A2156989373', []], ['A2177398213', []], ['A2572175195', []], ['A2525102276', []], ['A2436480564', []], ['A395835951', []], ['A2152681086', []], ['A1975406000', []], ['A2159204427', []], ['A2117136343', []], ['A2183182747', []], ['A689632033', []], ['A2952209971', []], ['A299447943', []], ['A2251493392', []], ['A2116791245', []], ['A2132455678', []], ['A2796797587', []], ['A2618069995', []], ['A2564633991', []], ['A1972229181', []], ['A2133495838', []], ['A2169176658', []], ['A76907487', []], ['A2755582526', []], ['A1976543349', []], ['A2606161589', []], ['A2596156213', []], ['A2266022754', []], ['A2694501703', []], ['A2279835831', []], ['A2099944687', []], ['A2104279826', []], ['A2151569403', []], ['A2251827248', []], ['A2288782178', []], ['A2205227065', []], ['A2483337482', []], ['A2082872497', []], ['A2101248102', []], ['A2089080248', []], ['A2043004940', []], ['A1968728195', []], ['A2126156898', []], ['A2103273789', []], ['A2126457318', []], ['A1972757265', []], ['A1890927917', []], ['A2013181484', []], ['A1570052640', []], ['A1920971052', []], ['A2257395298', []], ['A798807942', []], ['A119010230', []], ['A2148901667', []], ['A1849683455', []], ['A2560759403', []], ['A2034445190', []], ['A2263369703', []], ['A2144421627', []], ['A2653082139', []], ['A2997373551', []], ['A2121364159', []], ['A2163412007', []], ['A2157030991', []], ['A2102328879', []], ['A2755742307', []], ['A2756321290', []], ['A2121553805', []], ['A2221776659', []], ['A2085533449', []], ['A2082798115', []], ['A2008617460', []], ['A2663722086', []], ['A341443032', []], ['A329907494', []], ['A2097837126', []], ['A2758241083', []], ['A3211886523', []], ['A2167506362', []], ['A2788832255', []], ['A2163382776', []], ['A1907465466', []], ['A2050379650', []], ['A2627886379', []], ['A2754570444', []], ['A2755255890', []], ['A614647486', []], ['A1977245344', []], ['A1978294612', []], ['A1852563117', []], ['A2308578969', []], ['A2754950566', []], ['A2311673811', []], ['A2108967599', []], ['A2187417062', []], ['A2811276577', []], ['A2112093150', []], ['A2754092363', []], ['A2604742042', []], ['A2617635383', []], ['A2632166907', []], ['A2527981122', []], ['A2115643939', []], ['A2304636401', []], ['A2001912647', []], ['A1259793373', []], ['A2151397838', []], ['A2235522935', []], ['A2650374411', []], ['A3214586569', []], ['A2249472255', []], ['A2755538372', []], ['A2166148018', []], ['A2395891654', []], ['A3193264458', []], ['A1997328309', []], ['A2754725730', []], ['A2124281492', []], ['A267907075', []], ['A2224875839', []], ['A2753949767', []], ['A2556729383', []], ['A2132834798', []], ['A1980880360', []], ['A1874049041', []], ['A1239443135', []], ['A2044166451', []], ['A2060903916', []], ['A2591217870', []], ['A2619345531', []], ['A2804786586', []], ['A2099142247', []], ['A1575105442', []], ['A3209593969', []], ['A2104826947', []], ['A2756124656', []], ['A1784439412', []], ['A2952451616', []], ['A2607599903', []], ['A2218714182', []], ['A2604723853', []], ['A284438857', []], ['A1976008987', []], ['A2237400685', []], ['A3165398205', []], ['A2005500094', []], ['A2892883214', []], ['A3049497679', []], ['A2040123927', []], ['A2963961637', []], ['A1973406440', []], ['A2888942052', []], ['A2894260413', []], ['A2754603567', []], ['A2685385024', []], ['A2123033560', []], ['A2103680419', []], ['A2082324935', []], ['A2025574871', []], ['A1967961136', []], ['A2123946502', []], ['A772519725', []], ['A2299946479', []], ['A2182410343', []], ['A1965223436', []], ['A2615580838', []], ['A2105129513', []], ['A2243130012', []], ['A2709168440', []], ['A2144154959', []], ['A2112328861', []], ['A2796586280', []], ['A1760979461', []], ['A2275319484', []], ['A2949845258', []], ['A2716683888', []], ['A2139945721', []], ['A2560293163', []], ['A2169155597', []], ['A1827643105', []], ['A2439370755', []], ['A2238864982', []], ['A2112779362', []], ['A2755317805', []], ['A2756138928', []], ['A2803173578', []], ['A2530073150', []], ['A1852087164', []]], 'cited_by_count': 1477, 'concepts': [['C99454951', '0.72729695'], ['C71924100', '0.609449'], ['C12174686', '0.6055141'], ['C2779343474', '0.5757851'], ['C16851059', '0.53036404']], 'referenced_works': ['W1516252722', 'W1556903744', 'W1617145133', 'W1898188068', 'W1910215337', 'W1947392633', 'W1992484917', 'W1995299403', 'W2017966330', 'W2020528012', 'W2023294425', 'W2027759577', 'W2056180219', 'W2073508948', 'W2080054984', 'W2101997013', 'W2109003937', 'W2110052313', 'W2116515517', 'W2116647894', 'W2122349850', 'W2131745222', 'W2133699833', 'W2133911610', 'W2133979009', 'W2137074485', 'W2139530732', 'W2140173726', 'W2141970008', 'W2142514276', 'W2143342890', 'W2148044501', 'W2150278133', 'W2154137484', 'W2162775913', 'W2163710303', 'W2166266395', 'W2168581177', 'W2225912682', 'W2272263214', 'W2313998826', 'W2320118694', 'W2321364247', 'W2341244791', 'W2440172825', 'W2460922963', 'W2462161029', 'W2464212024', 'W2470481127', 'W2493534573', 'W2524655967', 'W2546882876', 'W2567182858', 'W2582602949', 'W2605474532', 'W2735180763', 'W3143437408', 'W3150595609', 'W4210288536', 'W4210308876', 'W4253369196', 'W4255375128'], 'abstract': 'The Global Burden of Diseases, Injuries, and Risk Factors Study 2016 (GBD 2016) provides a comprehensive assessment of risk factor exposure and attributable burden of disease. By providing estimates over a long time series, this study can monitor risk exposure trends critical to health surveillance and inform policy debates on the importance of addressing risks in context.We used the comparative risk assessment framework developed for previous iterations of GBD to estimate levels and trends in exposure, attributable deaths, and attributable disability-adjusted life-years (DALYs), by age group, sex, year, and location for 84 behavioural, environmental and occupational, and metabolic risks or clusters of risks from 1990 to 2016. This study included 481 risk-outcome pairs that met the GBD study criteria for convincing or probable evidence of causation. We extracted relative risk (RR) and exposure estimates from 22 717 randomised controlled trials, cohorts, pooled cohorts, household surveys, census data, satellite data, and other sources, according to the GBD 2016 source counting methods. Using the counterfactual scenario of theoretical minimum risk exposure level (TMREL), we estimated the portion of deaths and DALYs that could be attributed to a given risk. Finally, we explored four drivers of trends in attributable burden: population growth, population ageing, trends in risk exposure, and all other factors combined.Since 1990, exposure increased significantly for 30 risks, did not change significantly for four risks, and decreased significantly for 31 risks. Among risks that are leading causes of burden of disease, child growth failure and household air pollution showed the most significant declines, while metabolic risks, such as body-mass index and high fasting plasma glucose, showed significant increases. In 2016, at Level 3 of the hierarchy, the three leading risk factors in terms of attributable DALYs at the global level for men were smoking (124·1 million DALYs [95% UI 111·2 million to 137·0 million]), high systolic blood pressure (122·2 million DALYs [110·3 million to 133·3 million], and low birthweight and short gestation (83·0 million DALYs [78·3 million to 87·7 million]), and for women, were high systolic blood pressure (89·9 million DALYs [80·9 million to 98·2 million]), high body-mass index (64·8 million DALYs [44·4 million to 87·6 million]), and high fasting plasma glucose (63·8 million DALYs [53·2 million to 76·3 million]). In 2016 in 113 countries, the leading risk factor in terms of attributable DALYs was a metabolic risk factor. Smoking remained among the leading five risk factors for DALYs for 109 countries, while low birthweight and short gestation was the leading risk factor for DALYs in 38 countries, particularly in sub-Saharan Africa and South Asia. In terms of important drivers of change in trends of burden attributable to risk factors, between 2006 and 2016 exposure to risks explains an 9·3% (6·9-11·6) decline in deaths and a 10·8% (8·3-13·1) decrease in DALYs at the global level, while population ageing accounts for 14·9% (12·7-17·5) of deaths and 6·2% (3·9-8·7) of DALYs, and population growth for 12·4% (10·1-14·9) of deaths and 12·4% (10·1-14·9) of DALYs. The largest contribution of trends in risk exposure to disease burden is seen between ages 1 year and 4 years, where a decline of 27·3% (24·9-29·7) of the change in DALYs between 2006 and 2016 can be attributed to declines in exposure to risks.Increasingly detailed understanding of the trends in risk exposure and the RRs for each risk-outcome pair provide insights into both the magnitude of health loss attributable to risks and how modification of risk exposure has contributed to health trends. Metabolic risks warrant particular policy attention, due to their large contribution to global disease burden, increasing trends, and variable patterns across countries at the same level of development. GBD 2016 findings show that, while it has huge potential to improve health, risk modification has played a relatively small part in the past decade.The Bill & Melinda Gates Foundation, Bloomberg Philanthropies.', 'counts_by_year': [[2022, 216], [2021, 253], [2020, 283], [2019, 424], [2018, 284], [2017, 17]]}, {'id': 'W2183182206', 'doi': 'https://doi.org/10.1109/tpami.2015.2437384', 'title': 'Region-Based Convolutional Networks for Accurate Object Detection and Segmentation', 'type': 'journal-article', 'publication_date': '2016-01-01', 'host_venue': 'V199944782', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2473549963', ['I4210164937']], ['A2133088636', ['I95457486']], ['A2174985400', ['I95457486']], ['A2136556746', ['I95457486']]], 'cited_by_count': 1474, 'concepts': [['C75608658', '0.8343264'], ['C41008148', '0.7618476'], ['C2776151529', '0.71721613'], ['C154945302', '0.6994661'], ['C153180895', '0.6664056']], 'referenced_works': ['W1566135517', 'W1614862348', 'W1948751323', 'W1958328135', 'W1964005749', 'W1991367009', 'W2010181071', 'W2020308406', 'W2022508996', 'W2031489346', 'W2046382188', 'W2056695679', 'W2061035601', 'W2066624635', 'W2068730032', 'W2088049833', 'W2101926813', 'W2102605133', 'W2110226160', 'W2112301665', 'W2112796928', 'W2115150266', 'W2122146326', 'W2141200610', 'W2143890915', 'W2144794286', 'W2147800946', 'W2151049637', 'W2151103935', 'W2155871590', 'W2161106546', 'W2162741153', 'W2163922914', 'W2165698076', 'W2168356304', 'W2217896605', 'W4256462001'], 'abstract': 'Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.', 'counts_by_year': [[2022, 191], [2021, 279], [2020, 278], [2019, 275], [2018, 230], [2017, 146], [2016, 72], [2015, 3]]}, {'id': 'W2522833408', 'doi': 'https://doi.org/10.1038/nenergy.2016.142', 'title': 'Polymer-templated nucleation and crystal growth of perovskite films for solar cells with efficiency greater than 21%', 'type': 'journal-article', 'publication_date': '2016-09-19', 'host_venue': 'V2764528046', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2570991860', ['I5124864']], ['A2146953444', ['I5124864']], ['A2172039777', ['I5124864']], ['A1901465113', ['I5124864']], ['A2159454752', ['I5124864']], ['A45966780', ['I5124864']], ['A2435034426', ['I5124864']], ['A14922024', ['I5124864']], ['A2012626572', ['I5124864']]], 'cited_by_count': 1474, 'concepts': [['C61048295', '0.8347067'], ['C192562407', '0.75579417'], ['C521977710', '0.652747'], ['C155011858', '0.60334563'], ['C26456737', '0.50725955']], 'referenced_works': ['W1819923046', 'W1993509513', 'W2001574526', 'W2011472471', 'W2036694628', 'W2081879098', 'W2098254896', 'W2125469086', 'W2135422920', 'W2139843659', 'W2143338675', 'W2154311504', 'W2169328609', 'W2172511679', 'W2190169384', 'W2259658805', 'W2266656507', 'W2268497559', 'W2269355212', 'W2284771844', 'W2318514767', 'W2327499546', 'W2346415244', 'W2417861834'], 'abstract': 'The past several years have witnessed the rapid emergence of a class of solar cells based on mixed organic–inorganic halide perovskites. Today’s state-of-the-art perovskite solar cells (PSCs) employ various methods to enhance nucleation and improve the smoothness of the perovskite films formed via solution processing. However, the lack of precise control over the crystallization process creates a risk of forming unwanted defects, for example, pinholes and grain boundaries. Here, we introduce an approach to prepare perovskite films of high electronic quality by using poly(methyl methacrylate) (PMMA) as a template to control nucleation and crystal growth. We obtain shiny smooth perovskite films of excellent electronic quality, as manifested by a remarkably long photoluminescence lifetime. We realize stable PSCs with excellent reproducibility showing a power conversion efficiency (PCE) of up to 21.6% and a certified PCE of 21.02% under standard AM 1.5G reporting conditions. Controlling the crystallization process of perovskite films is crucial to obtaining high efficiency in perovskite solar cells. Bi\xa0et\xa0al.\xa0propose the use of poly(methyl methacrylate) as a template for the controlled nucleation and growth of perovskite crystals achieving efficiency of 21.6%.', 'counts_by_year': [[2022, 176], [2021, 275], [2020, 263], [2019, 284], [2018, 275], [2017, 194], [2016, 5]]}, {'id': 'W2500751094', 'doi': 'https://doi.org/10.1109/tgrs.2016.2584107', 'title': 'Deep Feature Extraction and Classification of Hyperspectral Images Based on Convolutional Neural Networks', 'type': 'journal-article', 'publication_date': '2016-07-18', 'host_venue': 'V111326731', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2103621309', ['I204983213']], ['A2556451228', ['I204983213']], ['A2423068874', ['I204983213']], ['A2111460500', ['I31746571']], ['A2795124944', ['I2898391981']]], 'cited_by_count': 1473, 'concepts': [['C159078339', '0.8112262'], ['C81363708', '0.75919193'], ['C52622490', '0.7321567'], ['C154945302', '0.72075266'], ['C153180895', '0.68609446']], 'referenced_works': ['W1584663654', 'W1990895816', 'W1990914742', 'W1992484904', 'W2001141328', 'W2001298023', 'W2004104348', 'W2016860790', 'W2019377328', 'W2029316659', 'W2041227601', 'W2043665634', 'W2044439250', 'W2049520143', 'W2053186076', 'W2063385051', 'W2087263574', 'W2090424610', 'W2097915756', 'W2098057602', 'W2100285320', 'W2100495367', 'W2102605133', 'W2103094532', 'W2109395398', 'W2111838090', 'W2112796928', 'W2114819256', 'W2131864940', 'W2136922672', 'W2140991832', 'W2153635508', 'W2159874418', 'W2162698522', 'W2163922914', 'W2167685974', 'W2179290474', 'W2257307118', 'W4255455317'], 'abstract': 'Due to the advantages of deep learning, in this paper, a regularized deep feature extraction (FE) method is presented for hyperspectral image (HSI) classification using a convolutional neural network (CNN). The proposed approach employs several convolutional and pooling layers to extract deep features from HSIs, which are nonlinear, discriminant, and invariant. These features are useful for image classification and target detection. Furthermore, in order to address the common issue of imbalance between high dimensionality and limited availability of training samples for the classification of HSI, a few strategies such as L2 regularization and dropout are investigated to avoid overfitting in class data modeling. More importantly, we propose a 3-D CNN-based FE model with combined regularization to extract effective spectral-spatial features of hyperspectral imagery. Finally, in order to further improve the performance, a virtual sample enhanced method is proposed. The proposed approaches are carried out on three widely used hyperspectral data sets: Indian Pines, University of Pavia, and Kennedy Space Center. The obtained results reveal that the proposed models with sparse constraints provide competitive results to state-of-the-art methods. In addition, the proposed deep FE opens a new window for further research.', 'counts_by_year': [[2022, 268], [2021, 342], [2020, 334], [2019, 311], [2018, 152], [2017, 63], [2016, 1]]}, {'id': 'W3023564401', 'doi': 'https://doi.org/10.1016/s0140-6736(20)31094-1', 'title': 'Hyperinflammatory shock in children during COVID-19 pandemic', 'type': 'journal-article', 'publication_date': '2020-05-23', 'host_venue': 'V49861241', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2577436227', ['I4210144851']], ['A3023268799', ['I4210144851']], ['A3021789398', ['I4210144851']], ['A2687727856', ['I4210144851']], ['A3022155694', ['I4210144851']]], 'cited_by_count': 1469, 'concepts': [['C2781423770', '0.7915292'], ['C71924100', '0.76215446'], ['C2781300812', '0.7561232'], ['C3008058167', '0.7107897'], ['C89623803', '0.66490203']], 'referenced_works': ['W2777772497', 'W2914899652', 'W3015438945'], 'abstract': 'South Thames Retrieval Service in London, UK, provides paediatric intensive care support and retrieval to 2 million children in South East England. During a period of 10 days in mid-April, 2020, we noted an unprecedented cluster of eight children with hyperinflammatory shock, showing features similar to atypical Kawasaki disease, Kawasaki disease shock syndrome,1 or toxic shock syndrome (typical number is one or two children per week). This case cluster formed the basis of a national alert.', 'counts_by_year': [[2022, 293], [2021, 620], [2020, 555]]}, {'id': 'W2999044305', 'doi': 'https://doi.org/10.1038/s41586-019-1923-7', 'title': 'Improved protein structure prediction using potentials from deep learning', 'type': 'journal-article', 'publication_date': '2020-01-15', 'host_venue': 'V137773608', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2045282652', ['I4210090411']], ['A2587667305', ['I4210090411']], ['A2640229920', ['I4210090411']], ['A2425779167', ['I4210090411']], ['A2697048878', ['I4210090411']], ['A2101543821', ['I4210090411']], ['A2898900647', ['I4210090411']], ['A2949153525', ['I4210090411']], ['A2659864187', ['I4210090411']], ['A2979965215', ['I4210090411']], ['A2568807268', ['I4210090411']], ['A2646187725', ['I4210090411']], ['A2033942250', ['I4210090411']], ['A2232573376', ['I4210090411']], ['A2141745054', ['I4210090411']], ['A2902024811', ['I2801081054', 'I45129253']], ['A2593774290', ['I4210090411']], ['A1111049960', ['I4210090411']], ['A4302276', ['I4210090411']]], 'cited_by_count': 1460, 'concepts': [['C18051474', '0.55947924'], ['C41008148', '0.3924037'], ['C154945302', '0.38936263'], ['C70721500', '0.35902876'], ['C47701112', '0.314299']], 'referenced_works': ['W1955046737', 'W1967292572', 'W1970973025', 'W1976666280', 'W2008545402', 'W2008708467', 'W2024060531', 'W2032050148', 'W2051210555', 'W2051434435', 'W2052935924', 'W2053069893', 'W2060421713', 'W2082667898', 'W2102245393', 'W2107867854', 'W2114340287', 'W2127793522', 'W2129020329', 'W2130479394', 'W2132275076', 'W2136799255', 'W2140564438', 'W2140673705', 'W2146134187', 'W2154106236', 'W2158714788', 'W2161151688', 'W2166701319', 'W2194775991', 'W2213268248', 'W2557595285', 'W2558272290', 'W2765742208', 'W2766710027', 'W2783006100', 'W2801109052', 'W2902663357', 'W2949867299', 'W2960589734', 'W2962831078', 'W2967999860', 'W2968295962', 'W2971003495', 'W2980272550', 'W3199799076', 'W4241935390'], 'abstract': 'Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence1. This problem is of fundamental importance as the structure of a protein largely determines its function2; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures3. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force4 that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures. The resulting system, named AlphaFold, achieves high accuracy, even for sequences with fewer homologous sequences. In the recent Critical Assessment of Protein Structure Prediction5 (CASP13)-a blind assessment of the state of the field-AlphaFold created high-accuracy structures (with template modelling (TM) scores6 of 0.7 or higher) for 24 out of 43 free modelling domains, whereas the next best method, which used sampling and contact information, achieved such accuracy for only 14 out of 43 domains. AlphaFold represents a considerable advance in protein-structure prediction. We expect this increased accuracy to enable insights into the function and malfunction of proteins, especially in cases for which no structures for homologous proteins have been experimentally determined7.', 'counts_by_year': [[2022, 421], [2021, 777], [2020, 249], [2019, 8]]}, {'id': 'W2310992461', 'doi': 'https://doi.org/10.1109/tmi.2016.2538465', 'title': 'Brain Tumor Segmentation Using Convolutional Neural Networks in MRI Images', 'type': 'journal-article', 'publication_date': '2016-03-04', 'host_venue': 'V58069681', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2169918884', ['I99682543']], ['A2275363802', ['I99682543']], ['A2105511238', ['I99682543']], ['A2997811833', ['I99682543']]], 'cited_by_count': 1456, 'concepts': [['C89600930', '0.78123444'], ['C154945302', '0.7442138'], ['C41008148', '0.74151707'], ['C81363708', '0.6576806'], ['C153180895', '0.57294446']], 'referenced_works': ['W1865761', 'W33787415', 'W850948285', 'W1524094261', 'W1597605992', 'W1641498739', 'W1763905908', 'W1964123122', 'W1987869189', 'W1996595747', 'W2000269560', 'W2028494674', 'W2031223143', 'W2037316890', 'W2044738244', 'W2112796928', 'W2115862526', 'W2117340355', 'W2120109874', 'W2123498585', 'W2132358780', 'W2147800946', 'W2152175008', 'W2154158661', 'W2160382843', 'W2163922914', 'W2241207628', 'W2293130453', 'W2546302380', 'W2919115771', 'W4245256608'], 'abstract': 'Among brain tumors, gliomas are the most common and aggressive, leading to a very short life expectancy in their highest grade. Thus, treatment planning is a key stage to improve the quality of life of oncological patients. Magnetic resonance imaging (MRI) is a widely used imaging technique to assess these tumors, but the large amount of data produced by MRI prevents manual segmentation in a reasonable time, limiting the use of precise quantitative measurements in the clinical practice. So, automatic and reliable segmentation methods are required; however, the large spatial and structural variability among brain tumors make automatic segmentation a challenging problem. In this paper, we propose an automatic segmentation method based on Convolutional Neural Networks (CNN), exploring small 3 ×3 kernels. The use of small kernels allows designing a deeper architecture, besides having a positive effect against overfitting, given the fewer number of weights in the network. We also investigated the use of intensity normalization as a pre-processing step, which though not common in CNN-based segmentation methods, proved together with data augmentation to be very effective for brain tumor segmentation in MRI images. Our proposal was validated in the Brain Tumor Segmentation Challenge 2013 database (BRATS 2013), obtaining simultaneously the first position for the complete, core, and enhancing regions in Dice Similarity Coefficient metric (0.88, 0.83, 0.77) for the Challenge data set. Also, it obtained the overall first position by the online evaluation platform. We also participated in the on-site BRATS 2015 Challenge using the same model, obtaining the second place, with Dice Similarity Coefficient metric of 0.78, 0.65, and 0.75 for the complete, core, and enhancing regions, respectively.', 'counts_by_year': [[2022, 215], [2021, 314], [2020, 333], [2019, 268], [2018, 209], [2017, 104], [2016, 9]]}, {'id': 'W2790979755', 'doi': 'https://doi.org/10.1016/j.compag.2018.02.016', 'title': 'Deep learning in agriculture: A survey', 'type': 'journal-article', 'publication_date': '2018-04-01', 'host_venue': 'V116775814', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A162198452', ['I4210100913']], ['A2488055944', ['I4210100913']]], 'cited_by_count': 1449, 'concepts': [['C108583219', '0.87947136'], ['C154945302', '0.7419363'], ['C41008148', '0.68278074'], ['C119857082', '0.6336602'], ['C118518473', '0.5222217']], 'referenced_works': ['W1185880853', 'W1490788488', 'W1973552978', 'W1978331315', 'W1994168328', 'W2010456657', 'W2014847057', 'W2015386604', 'W2029316659', 'W2076063813', 'W2097117768', 'W2108598243', 'W2118023920', 'W2123229215', 'W2136848157', 'W2156710280', 'W2157005989', 'W2165140361', 'W2165698076', 'W2185489349', 'W2261465687', 'W2292390047', 'W2345726069', 'W2470368200', 'W2470803522', 'W2473156356', 'W2501369945', 'W2518392921', 'W2520364485', 'W2531609949', 'W2553151007', 'W2578363764', 'W2587218622', 'W2603641813', 'W2604086375', 'W2610544234', 'W2611227133', 'W2620742659', 'W2742878349', 'W2752192487', 'W2757734040', 'W2761140038', 'W2919115771', 'W2952533583', 'W2963523428', 'W2963554310', 'W2963801405', 'W4205947740'], 'abstract': 'Deep learning constitutes a recent, modern technique for image processing and data analysis, with promising results and large potential. As deep learning has been successfully applied in various domains, it has recently entered also the domain of agriculture. In this paper, we perform a survey of 40 research efforts that employ deep learning techniques, applied to various agricultural and food production challenges. We examine the particular agricultural problems under study, the specific models and frameworks employed, the sources, nature and pre-processing of data used, and the overall performance achieved according to the metrics used at each work under study. Moreover, we study comparisons of deep learning with other existing popular techniques, in respect to differences in classification or regression performance. Our findings indicate that deep learning provides high accuracy, outperforming existing commonly used image processing techniques.', 'counts_by_year': [[2022, 383], [2021, 452], [2020, 365], [2019, 212], [2018, 30]]}, {'id': 'W2964024144', 'doi': 'https://doi.org/10.1109/iccv.2017.629', 'title': 'StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks', 'type': 'proceedings-article', 'publication_date': '2017-10-01', 'host_venue': 'V4306419272', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A3168992401', ['I102322142']], ['A2624022264', ['I186143895']], ['A2141739595', ['I177725633']]], 'cited_by_count': 1448, 'concepts': [['C41008148', '0.7849777'], ['C185798385', '0.7095605'], ['C39890363', '0.6750637'], ['C2781238097', '0.6408626'], ['C2779231336', '0.6309496']], 'referenced_works': ['W2183341477', 'W2194775991', 'W2398118205', 'W2533598788'], 'abstract': 'Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing textto- image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256.256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.', 'counts_by_year': [[2022, 114], [2021, 329], [2020, 392], [2019, 381], [2018, 176], [2017, 54], [2016, 1]]}, {'id': 'W2886368623', 'doi': 'https://doi.org/10.1016/j.tjem.2018.08.001', 'title': "User's guide to correlation coefficients", 'type': 'journal-article', 'publication_date': '2018-08-07', 'host_venue': 'V2764591159', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2133980741', ['I74897591']]], 'cited_by_count': 1446, 'concepts': [['C117220453', '0.6934328'], ['C77350462', '0.55338883'], ['C55078378', '0.50338715'], ['C2986115478', '0.47873232'], ['C41008148', '0.42675805']], 'referenced_works': ['W1534350900', 'W2150182805'], 'abstract': 'When writing a manuscript, we often use words such as perfect, strong, good or weak to name the strength of the relationship between variables. However, it is unclear where a good relationship turns into a strong one. The same strength of r is named differently by several researchers. Therefore, there is an absolute necessity to explicitly report the strength and direction of r while reporting correlation coefficients in manuscripts. This article aims to familiarize medical readers with several different correlation coefficients reported in medical manuscripts, clarify confounding aspects and summarize the naming practices for the strength of correlation coefficients.', 'counts_by_year': [[2022, 538], [2021, 541], [2020, 290], [2019, 70], [2018, 3]]}, {'id': 'W2803072880', 'doi': 'https://doi.org/10.1038/s41588-018-0099-7', 'title': 'Detection of widespread horizontal pleiotropy in causal relationships inferred from Mendelian randomization between complex traits and diseases', 'type': 'journal-article', 'publication_date': '2018-04-23', 'host_venue': 'V137905309', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2052991623', ['I98704320']], ['A2129991891', ['I4210087915']], ['A2013033188', ['I4210087915', 'I107606265']], ['A2160936674', ['I98704320']]], 'cited_by_count': 1445, 'concepts': [['C2779901538', '0.9467468'], ['C26207810', '0.94104373'], ['C86803240', '0.72893244'], ['C54355233', '0.47146776'], ['C106934330', '0.4550922']], 'referenced_works': ['W1902575419', 'W1905058580', 'W1977846646', 'W1996299724', 'W1998096270', 'W2005648153', 'W2067539811', 'W2080225144', 'W2101737264', 'W2104549677', 'W2106036991', 'W2108842111', 'W2113699335', 'W2116908385', 'W2118743532', 'W2139875520', 'W2143282831', 'W2154439634', 'W2155615155', 'W2163969089', 'W2195783463', 'W2207115540', 'W2324862792', 'W2401811918', 'W2475125285', 'W2503030347', 'W2518986366', 'W2519445944', 'W2521215217', 'W2528302205', 'W2559028527', 'W2581245397', 'W2581452190', 'W2587804655', 'W2599197229', 'W2613603864', 'W2724817460', 'W2744463954', 'W4210703904'], 'abstract': "Horizontal pleiotropy occurs when the variant has an effect on disease outside of its effect on the exposure in Mendelian randomization (MR). Violation of the 'no horizontal pleiotropy' assumption can cause severe bias in MR. We developed the Mendelian randomization pleiotropy residual sum and outlier (MR-PRESSO) test to identify horizontal pleiotropic outliers in multi-instrument summary-level MR testing. We showed using simulations that the MR-PRESSO test is best suited when horizontal pleiotropy occurs in <50% of instruments. Next we applied the MR-PRESSO test, along with several other MR tests, to complex traits and diseases and found that horizontal pleiotropy (i) was detectable in over 48% of significant causal relationships in MR; (ii) introduced distortions in the causal estimates in MR that ranged on average from -131% to 201%; (iii) induced false-positive causal relationships in up to 10% of relationships; and (iv) could be corrected in some but not all instances.", 'counts_by_year': [[2022, 525], [2021, 439], [2020, 282], [2019, 156], [2018, 40], [2017, 1], [2016, 1]]}, {'id': 'W2525984666', 'doi': 'https://doi.org/10.1056/nejmp1606181', 'title': 'Predicting the Future — Big Data, Machine Learning, and Clinical Medicine', 'type': 'journal-article', 'publication_date': '2016-09-28', 'host_venue': 'V62468778', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A847559205', ['I1283280774']], ['A47466971', ['I79576946']]], 'cited_by_count': 1439, 'concepts': [['C71924100', '0.8432541'], ['C75684735', '0.65154624'], ['C2522767166', '0.4714756'], ['C154945302', '0.44481725'], ['C119857082', '0.44362256']], 'referenced_works': ['W2020506948', 'W2103018059', 'W2203167467', 'W2341256599'], 'abstract': 'The algorithms of machine learning, which can sift through vast numbers of variables looking for combinations that reliably predict outcomes, will improve prognosis, displace much of the work of radiologists and anatomical pathologists, and improve diagnostic accuracy.', 'counts_by_year': [[2022, 244], [2021, 360], [2020, 317], [2019, 258], [2018, 176], [2017, 80]]}, {'id': 'W2770092353', 'doi': 'https://doi.org/10.1038/s41467-017-01261-5', 'title': 'Functional mapping and annotation of genetic associations with FUMA', 'type': 'journal-article', 'publication_date': '2017-11-28', 'host_venue': 'V64187185', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2596105402', ['I865915315']], ['A2785832463', ['I4210159958', 'I911458345']], ['A833637969', ['I865915315']], ['A1989856754', ['I4210108594', 'I4210159958']]], 'cited_by_count': 1438, 'concepts': [['C106208931', '0.77427673'], ['C2776321320', '0.7305582'], ['C168393362', '0.59590435'], ['C35605836', '0.51576996'], ['C186413461', '0.44177508']], 'referenced_works': ['W97382084', 'W1533942137', 'W1605145036', 'W1636205509', 'W1713135633', 'W1925388892', 'W1984068087', 'W1984636340', 'W2001280003', 'W2003283154', 'W2006609152', 'W2010457001', 'W2018363492', 'W2039153962', 'W2040316499', 'W2051314902', 'W2074199852', 'W2076154138', 'W2078059415', 'W2082704080', 'W2094379190', 'W2099055805', 'W2101357408', 'W2104549677', 'W2106779017', 'W2116868464', 'W2118742454', 'W2119017552', 'W2125972459', 'W2126997165', 'W2135199713', 'W2136322773', 'W2139852278', 'W2141820415', 'W2152127168', 'W2160126774', 'W2160995259', 'W2161633633', 'W2162151166', 'W2163013660', 'W2163924952', 'W2167806431', 'W2170146596', 'W2182733429', 'W2195359644', 'W2202219635', 'W2254879037', 'W2256016639', 'W2259938310', 'W2346626662', 'W2417483443', 'W2506960987', 'W2534005127', 'W2535910303', 'W2545941295', 'W2553838260', 'W2557097554', 'W2560048039', 'W2596789401', 'W2951325776', 'W2951856336'], 'abstract': 'A main challenge in genome-wide association studies (GWAS) is to pinpoint possible causal variants. Results from GWAS typically do not directly translate into causal variants because the majority of hits are in non-coding or intergenic regions, and the presence of linkage disequilibrium leads to effects being statistically spread out across multiple variants. Post-GWAS annotation facilitates the selection of most likely causal variant(s). Multiple resources are available for post-GWAS annotation, yet these can be time consuming and do not provide integrated visual aids for data interpretation. We, therefore, develop FUMA: an integrative web-based platform using information from multiple biological resources to facilitate functional annotation of GWAS results, gene prioritization and interactive visualization. FUMA accommodates positional, expression quantitative trait loci (eQTL) and chromatin interaction mappings, and provides gene-based, pathway and tissue enrichment results. FUMA results directly aid in generating hypotheses that are testable in functional experiments aimed at proving causal relations.', 'counts_by_year': [[2022, 362], [2021, 413], [2020, 312], [2019, 228], [2018, 114], [2017, 7]]}, {'id': 'W2963095307', 'doi': 'https://doi.org/10.1016/j.artint.2018.07.007', 'title': 'Explanation in artificial intelligence: Insights from the social sciences', 'type': 'journal-article', 'publication_date': '2019-02-01', 'host_venue': 'V196139623', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2160430097', ['I165779595']]], 'cited_by_count': 1438, 'concepts': [['C132010649', '0.684149'], ['C169900460', '0.5402086'], ['C2780233690', '0.46281755'], ['C66045256', '0.45863283'], ['C188147891', '0.45265833']], 'referenced_works': ['W1636554345', 'W1936290004', 'W1963959269', 'W1965562882', 'W1968340857', 'W1968843430', 'W1971143134', 'W1971771969', 'W1974864724', 'W1976127883', 'W1976584123', 'W1983870751', 'W1984670080', 'W1984735186', 'W1988271510', 'W1994655610', 'W1997694951', 'W2008186371', 'W2015240128', 'W2016030371', 'W2018241419', 'W2019175150', 'W2027515211', 'W2030721126', 'W2031233243', 'W2033031653', 'W2034637247', 'W2039739384', 'W2040611514', 'W2041029272', 'W2045024942', 'W2049931818', 'W2050227612', 'W2052916960', 'W2054034297', 'W2064189954', 'W2067122120', 'W2069743909', 'W2071894919', 'W2074416001', 'W2077177590', 'W2077871908', 'W2085629789', 'W2089249982', 'W2090497191', 'W2090673483', 'W2094837707', 'W2099550726', 'W2100758964', 'W2102578078', 'W2106748507', 'W2108309071', 'W2108403623', 'W2110085097', 'W2113019196', 'W2113211233', 'W2116151961', 'W2117852736', 'W2118314955', 'W2120302568', 'W2130464748', 'W2132920211', 'W2133671386', 'W2137819283', 'W2142470178', 'W2142910214', 'W2145704119', 'W2154594412', 'W2154988829', 'W2156311465', 'W2159186600', 'W2159933571', 'W2162651730', 'W2162829624', 'W2165201543', 'W2321972046', 'W2401878968', 'W2412691362', 'W2512823917', 'W2594336441', 'W2618132268', 'W2747394361', 'W2765191723', 'W2795329138', 'W3026587103', 'W3122240369', 'W3122497069', 'W3165003809', 'W4230634779', 'W4231806624', 'W4231827019', 'W4233188212', 'W4248308907'], 'abstract': "There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.", 'counts_by_year': [[2022, 359], [2021, 565], [2020, 345], [2019, 151], [2018, 12], [2012, 2]]}, {'id': 'W2982083293', 'doi': 'https://doi.org/10.1109/iccv.2019.00140', 'title': 'Searching for MobileNetV3', 'type': 'proceedings-article', 'publication_date': '2019-05-06', 'host_venue': 'V4306419272', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2612173839', ['I1291425158']], ['A2056797953', ['I1291425158']], ['A2107777167', ['I1291425158']], ['A2911865271', ['I1291425158']], ['A1973675100', ['I1291425158']], ['A2601901235', ['I1291425158']], ['A2612843733', ['I1291425158']], ['A2127898042', ['I1291425158']], ['A2231381296', ['I1291425158']], ['A2944112373', ['I1291425158']], ['A2089062156', ['I1291425158']], ['A2127971021', ['I1291425158']]], 'cited_by_count': 1438, 'concepts': [['C41008148', '0.832896'], ['C89600930', '0.6853139'], ['C2776151529', '0.61341995'], ['C154945302', '0.61206263'], ['C70437156', '0.59184444']], 'referenced_works': ['W1610060839', 'W1903029394', 'W1949049686', 'W2117539524', 'W2294370754', 'W2340897893', 'W2560023338', 'W2606722458', 'W2920974670', 'W2963122961', 'W2963418739'], 'abstract': 'We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2% more accurate on ImageNet classification while reducing latency by 20% compared to MobileNetV2. MobileNetV3-Small is 6.6% more accurate compared to a MobileNetV2 model with comparable latency. MobileNetV3-Large detection is over 25% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 34% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.', 'counts_by_year': [[2022, 478], [2021, 626], [2020, 297], [2019, 30], [2012, 1]]}, {'id': 'W2988396470', 'doi': 'https://doi.org/10.1186/s13059-019-1832-y', 'title': 'OrthoFinder: phylogenetic orthology inference for comparative genomics', 'type': 'journal-article', 'publication_date': '2019-11-14', 'host_venue': 'V81160022', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A1966961424', ['I40120149']], ['A2156933072', ['I40120149']]], 'cited_by_count': 1438, 'concepts': [['C2776214188', '0.795534'], ['C193252679', '0.78696406'], ['C86803240', '0.7220677'], ['C105176652', '0.56613404'], ['C70721500', '0.56335145']], 'referenced_works': ['W1889423108', 'W1900937478', 'W1972157500', 'W1997272729', 'W2008856488', 'W2012035190', 'W2012220164', 'W2016681664', 'W2030317329', 'W2031611770', 'W2045204781', 'W2048448217', 'W2061646232', 'W2074432684', 'W2082202339', 'W2092672051', 'W2099321785', 'W2100203037', 'W2108039804', 'W2111647009', 'W2114676910', 'W2120836981', 'W2124105487', 'W2136258184', 'W2137084536', 'W2141052558', 'W2142678478', 'W2159266221', 'W2160378127', 'W2161973008', 'W2162327194', 'W2167188257', 'W2277953252', 'W2281531600', 'W2316262244', 'W2766147358', 'W2793316497', 'W2796130155', 'W2882992890', 'W2949367299', 'W2950954328', 'W2952556895', 'W4230266413', 'W4247412012'], 'abstract': 'Abstract Here, we present a major advance of the OrthoFinder method. This extends OrthoFinder’s high accuracy orthogroup inference to provide phylogenetic inference of orthologs, rooted gene trees, gene duplication events, the rooted species tree, and comparative genomics statistics. Each output is benchmarked on appropriate real or simulated datasets, and where comparable methods exist, OrthoFinder is equivalent to or outperforms these methods. Furthermore, OrthoFinder is the most accurate ortholog inference method on the Quest for Orthologs benchmark test. Finally, OrthoFinder’s comprehensive phylogenetic analysis is achieved with equivalent speed and scalability to the fastest, score-based heuristic methods. OrthoFinder is available at https://github.com/davidemms/OrthoFinder .', 'counts_by_year': [[2022, 634], [2021, 549], [2020, 240], [2019, 10]]}, {'id': 'W2342408547', 'doi': 'https://doi.org/10.1109/comst.2015.2494502', 'title': 'A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection', 'type': 'journal-article', 'publication_date': '2016-01-22', 'host_venue': 'V23688054', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A47266448', ['I2802946424']], ['A2140819898', ['I2802946424']]], 'cited_by_count': 1437, 'concepts': [['C41008148', '0.8086029'], ['C35525427', '0.732347'], ['C38652104', '0.53771067'], ['C10511746', '0.4182866'], ['C124101348', '0.3959482']], 'referenced_works': ['W4348436', 'W166342712', 'W1525180324', 'W1541288193', 'W1554891714', 'W1670263352', 'W1777713421', 'W1856750239', 'W1937068078', 'W1952056635', 'W1966809779', 'W1971751469', 'W1975791498', 'W1978779053', 'W1979877030', 'W1986334900', 'W1991237739', 'W1993717606', 'W1999427165', 'W2002016471', 'W2014712522', 'W2017528992', 'W2030553727', 'W2033626294', 'W2036959577', 'W2038819341', 'W2040870580', 'W2066664409', 'W2071221494', 'W2076384720', 'W2077574412', 'W2081707439', 'W2082550445', 'W2091576221', 'W2094138658', 'W2096118443', 'W2100537916', 'W2102201073', 'W2104593144', 'W2112090702', 'W2114996745', 'W2123619513', 'W2129018774', 'W2133941447', 'W2133990480', 'W2137983211', 'W2139212933', 'W2139669429', 'W2142384583', 'W2142889610', 'W2143893259', 'W2145052282', 'W2146082061', 'W2152195021', 'W2154929945', 'W2158454296', 'W2160598920', 'W2166559705', 'W2167917621', 'W2171331105', 'W2173213060', 'W2203709713', 'W2911964244', 'W4211007335', 'W4211064163', 'W4213332169', 'W4231918628', 'W4232872328', 'W4236137412', 'W4249991499', 'W4299670631'], 'abstract': 'This survey paper describes a focused literature survey of machine learning (ML) and data mining (DM) methods for cyber analytics in support of intrusion detection. Short tutorial descriptions of each ML/DM method are provided. Based on the number of citations or the relevance of an emerging method, papers representing each method were identified, read, and summarized. Because data are so important in ML/DM approaches, some well-known cyber data sets used in ML/DM are described. The complexity of ML/DM algorithms is addressed, discussion of challenges for using ML/DM for cyber security is presented, and some recommendations on when to use a given method are provided.', 'counts_by_year': [[2022, 227], [2021, 348], [2020, 352], [2019, 267], [2018, 173], [2017, 59], [2016, 7], [2015, 1]]}, {'id': 'W2951254987', 'doi': 'https://doi.org/10.1038/s41467-018-07641-9', 'title': 'High throughput ANI analysis of 90K prokaryotic genomes reveals clear species boundaries', 'type': 'journal-article', 'publication_date': '2018-11-30', 'host_venue': 'V64187185', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2228676561', ['I130701444']], ['A1769116373', ['I130701444']], ['A47992352', ['I4210090236']], ['A947296715', ['I130701444']], ['A672311968', ['I130701444']]], 'cited_by_count': 1437, 'concepts': [['C141231307', '0.86019874'], ['C193252679', '0.6124715'], ['C86803240', '0.6043086'], ['C78458016', '0.5114278'], ['C184898388', '0.48643565']], 'referenced_works': ['W1579534339', 'W1587277135', 'W1661708608', 'W1991330646', 'W2041579725', 'W2045204781', 'W2051838613', 'W2055693471', 'W2059410416', 'W2066818550', 'W2078064634', 'W2091232725', 'W2101962198', 'W2104253405', 'W2107282968', 'W2119888547', 'W2121055398', 'W2121139063', 'W2124351063', 'W2127036970', 'W2127303481', 'W2134375163', 'W2136145671', 'W2158714788', 'W2160932169', 'W2163786118', 'W2168334721', 'W2206071891', 'W2585955926', 'W2588775853', 'W2699862099', 'W2736920791', 'W2753961085', 'W2950150251', 'W2950354111'], 'abstract': 'Abstract A fundamental question in microbiology is whether there is continuum of genetic diversity among genomes, or clear species boundaries prevail instead. Whole-genome similarity metrics such as Average Nucleotide Identity (ANI) help address this question by facilitating high resolution taxonomic analysis of thousands of genomes from diverse phylogenetic lineages. To scale to available genomes and beyond, we present FastANI, a new method to estimate ANI using alignment-free approximate sequence mapping. FastANI is accurate for both finished and draft genomes, and is up to three orders of magnitude faster compared to alignment-based approaches. We leverage FastANI to compute pairwise ANI values among all prokaryotic genomes available in the NCBI database. Our results reveal clear genetic discontinuity, with 99.8% of the total 8 billion genome pairs analyzed conforming to &gt;95% intra-species and &lt;83% inter-species ANI values. This discontinuity is manifested with or without the most frequently sequenced species, and is robust to historic additions in the genome databases.', 'counts_by_year': [[2022, 492], [2021, 503], [2020, 296], [2019, 139], [2018, 5], [2017, 2]]}, {'id': 'W2327501763', 'doi': 'https://doi.org/10.1109/icassp.2016.7472621', 'title': 'Listen, attend and spell: A neural network for large vocabulary conversational speech recognition', 'type': 'proceedings-article', 'publication_date': '2016-03-20', 'host_venue': 'V4306419087', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2281662583', ['I74973139']], ['A2066236689', ['I1291425158']], ['A2148448995', ['I1291425158']], ['A2969484184', ['I1291425158']]], 'cited_by_count': 1432, 'concepts': [['C41008148', '0.814112'], ['C28490314', '0.8092643'], ['C2780844864', '0.741797'], ['C23224414', '0.6778644'], ['C137293760', '0.59645635']], 'referenced_works': ['W1533416326', 'W2005708641', 'W2064675550', 'W2112796928', 'W2143612262', 'W2157331557', 'W2160815625', 'W2293858598', 'W2963211739'], 'abstract': 'We present Listen, Attend and Spell (LAS), a neural speech recognizer that transcribes speech utterances directly to characters without pronunciation models, HMMs or other components of traditional speech recognizers. In LAS, the neural network architecture subsumes the acoustic, pronunciation and language models making it not only an end-to-end trained system but an end-to-end model. In contrast to DNN-HMM, CTC and most other models, LAS makes no independence assumptions about the probability distribution of the output character sequences given the acoustic sequence. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits each character conditioned on all previous characters, and the entire acoustic sequence. On a Google voice search task, LAS achieves a WER of 14.1% without a dictionary or an external language model and 10.3% with language model rescoring over the top 32 beams. In comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0% on the same set.', 'counts_by_year': [[2022, 111], [2021, 366], [2020, 370], [2019, 314], [2018, 182], [2017, 73], [2016, 15]]}, {'id': 'W2774178591', 'doi': 'https://doi.org/10.1016/j.jmb.2017.12.007', 'title': 'A Completely Reimplemented MPI Bioinformatics Toolkit with a New HHpred Server at its Core', 'type': 'journal-article', 'publication_date': '2017-12-01', 'host_venue': 'V46058472', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A3117484830', ['I4210112458']], ['A2774064367', ['I4210112458']], ['A2344234897', ['I4210112458']], ['A2775727438', ['I4210112458']], ['A2771828097', ['I4210112458']], ['A2772076158', ['I4210112458']], ['A2775332319', ['I4210112458']], ['A2306469526', ['I4210131661']], ['A218930724', ['I4210112458']], ['A2123207086', ['I4210112458']]], 'cited_by_count': 1431, 'concepts': [['C41008148', '0.76479673'], ['C184898388', '0.55073386'], ['C11392498', '0.4756455'], ['C45484198', '0.46943405'], ['C61053724', '0.45925802']], 'referenced_works': ['W1590240800', 'W1794209129', 'W1803102843', 'W1810634920', 'W1965827393', 'W1971147414', 'W1993175766', 'W1998445752', 'W2012534599', 'W2015546050', 'W2026478054', 'W2036149515', 'W2037312364', 'W2049059215', 'W2051210555', 'W2097015301', 'W2102424972', 'W2105961968', 'W2111211467', 'W2127322768', 'W2129508689', 'W2131617076', 'W2132926880', 'W2137450588', 'W2138122982', 'W2138872556', 'W2139881678', 'W2142678478', 'W2143519602', 'W2144305952', 'W2144362290', 'W2145268834', 'W2149525061', 'W2152770371', 'W2153187042', 'W2154479185', 'W2158623906', 'W2158714788', 'W2160378127', 'W2161199282', 'W2162085776', 'W2162980545', 'W2168770860', 'W2173732482', 'W2211193336', 'W2343212594', 'W2411213844', 'W2540069603', 'W2557595285', 'W2559738700', 'W2950954328', 'W2963457143'], 'abstract': 'The MPI Bioinformatics Toolkit (https://toolkit.tuebingen.mpg.de) is a free, one-stop web service for protein bioinformatic analysis. It currently offers 34 interconnected external and in-house tools, whose functionality covers sequence similarity searching, alignment construction, detection of sequence features, structure prediction, and sequence classification. This breadth has made the Toolkit an important resource for experimental biology and for teaching bioinformatic inquiry. Recently, we replaced the first version of the Toolkit, which was released in 2005 and had served around 2.5 million queries, with an entirely new version, focusing on improved features for the comprehensive analysis of proteins, as well as on promoting teaching. For instance, our popular remote homology detection server, HHpred, now allows pairwise comparison of two sequences or alignments and offers additional profile HMMs for several model organisms and domain databases. Here, we introduce the new version of our Toolkit and its application to the analysis of proteins.', 'counts_by_year': [[2022, 273], [2021, 428], [2020, 374], [2019, 277], [2018, 76], [2017, 2], [2016, 1]]}, {'id': 'W2609883120', 'doi': 'https://doi.org/10.1109/cvpr.2017.700', 'title': 'Unsupervised Learning of Depth and Ego-Motion from Video', 'type': 'proceedings-article', 'publication_date': '2017-04-25', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2120864060', ['I95457486']], ['A3177694685', ['I1291425158']], ['A1737721025', ['I1291425158']], ['A2104328312', ['I1291425158']]], 'cited_by_count': 1426, 'concepts': [['C154945302', '0.7918481'], ['C41008148', '0.73534405'], ['C65909025', '0.6833192'], ['C31972630', '0.6433656'], ['C146849305', '0.60832006']], 'referenced_works': ['W219040644', 'W1520997877', 'W1776042733', 'W1803059841', 'W1836533770', 'W1938714998', 'W1971719398', 'W2008706659', 'W2071499765', 'W2074254947', 'W2083047701', 'W2098362450', 'W2108134361', 'W2132947399', 'W2146293109', 'W2150066425', 'W2200124539', 'W2259631822', 'W2294015878', 'W2300779272', 'W2336968928', 'W2340897893', 'W2427448504', 'W2489710028', 'W2575671312', 'W2593414960', 'W2963591054', 'W2963739349', 'W3100388886', 'W3103648783', 'W4235111023', 'W4249522571'], 'abstract': 'We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. We achieve this by simultaneously training depth and camera pose estimation networks using the task of view synthesis as the supervisory signal. The networks are thus coupled via the view synthesis objective during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performing comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performing favorably with established SLAM systems under comparable input settings.', 'counts_by_year': [[2022, 127], [2021, 344], [2020, 410], [2019, 364], [2018, 173], [2017, 6], [2016, 1]]}, {'id': 'W2963336322', 'doi': 'https://doi.org/10.1109/jsac.2017.2725519', 'title': 'A Survey on Non-Orthogonal Multiple Access for 5G Networks: Research Challenges and Future Trends', 'type': 'journal-article', 'publication_date': '2017-07-11', 'host_venue': 'V90422530', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2121277316', ['I67415387']], ['A2558007693', ['I4800084']], ['A36717893', ['I21370196']], ['A2253850548', ['I181369854']], ['A2096046393', ['I31746571']], ['A2104822238', ['I141945490']]], 'cited_by_count': 1421, 'concepts': [['C41008148', '0.78452855'], ['C76155785', '0.45550302'], ['C31258907', '0.38669154'], ['C118524514', '0.37003928']], 'referenced_works': ['W1578217165', 'W1690504809', 'W1872003072', 'W1901462474', 'W1922937245', 'W1958281780', 'W1975499480', 'W2008104692', 'W2014006472', 'W2027563534', 'W2032372805', 'W2042519026', 'W2051943016', 'W2068712429', 'W2084872270', 'W2085486577', 'W2086559523', 'W2099111195', 'W2102241149', 'W2111505049', 'W2116334496', 'W2117890770', 'W2141682101', 'W2142165404', 'W2142962453', 'W2170531625', 'W2186171661', 'W2187903755', 'W2221113447', 'W2224294090', 'W2233021649', 'W2277419020', 'W2287326484', 'W2287831847', 'W2289097008', 'W2289565584', 'W2290377878', 'W2290894987', 'W2294228416', 'W2302349451', 'W2316695877', 'W2341471227', 'W2341725391', 'W2343021946', 'W2343463506', 'W2346122382', 'W2383626323', 'W2413850816', 'W2462659818', 'W2465568610', 'W2472080793', 'W2488958542', 'W2490443550', 'W2496222037', 'W2500307208', 'W2506399502', 'W2507603180', 'W2507748565', 'W2507790150', 'W2508215585', 'W2509696551', 'W2510173137', 'W2510754270', 'W2510906680', 'W2511874756', 'W2515756451', 'W2519194698', 'W2522880951', 'W2523122228', 'W2527867943', 'W2528407593', 'W2529223827', 'W2544882198', 'W2549116153', 'W2549558423', 'W2551126468', 'W2553781059', 'W2562812855', 'W2564149722', 'W2565158077', 'W2565396475', 'W2566155463', 'W2574228330', 'W2582447695', 'W2604203516', 'W2604437453', 'W2621878961', 'W2626422957', 'W2734523996', 'W2734578560', 'W2734589605', 'W2735094220', 'W2735344404', 'W2735475593', 'W2735500522', 'W2735816365', 'W2736019605', 'W2736111490', 'W2736591909', 'W2737159091', 'W2739171486', 'W2739324738', 'W2951205086', 'W2963126933', 'W2963291364', 'W2963357004', 'W2963634826', 'W2963691998', 'W2964008556', 'W2964019026', 'W2964044470', 'W2964053866', 'W2964123890', 'W2964153125', 'W2964241829', 'W3099378150', 'W3101650196', 'W3102873868', 'W3103423272', 'W3103537411', 'W3103720573', 'W3104193638', 'W3105798676'], 'abstract': 'Non-orthogonal multiple access (NOMA) is an essential enabling technology for the fifth-generation (5G) wireless networks to meet the heterogeneous demands on low latency, high reliability, massive connectivity, improved fairness, and high throughput. The key idea behind NOMA is to serve multiple users in the same resource block, such as a time slot, subcarrier, or spreading code. The NOMA principle is a general framework, and several recently proposed 5G multiple access schemes can be viewed as special cases. This survey provides an overview of the latest NOMA research and innovations as well as their applications. Thereby, the papers published in this special issue are put into the context of the existing literature. Future research challenges regarding NOMA in 5G and beyond are also discussed.', 'counts_by_year': [[2022, 159], [2021, 305], [2020, 344], [2019, 372], [2018, 226], [2017, 14]]}, {'id': 'W2991188715', 'doi': 'https://doi.org/10.1107/s1600576719014092', 'title': '<i>Mercury 4.0</i>: from visualization to analysis, design and prediction', 'type': 'journal-article', 'publication_date': '2020-02-01', 'host_venue': 'V79162954', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2143243980', ['I4210098980']], ['A2990363580', ['I4210098980']], ['A2342425976', ['I4210098980']], ['A123222184', ['I4210098980']], ['A2312179098', ['I4210098980']], ['A2122578653', ['I4210098980']], ['A2991069836', ['I4210098980']], ['A2299357340', ['I4210098980']], ['A2132256925', ['I4210098980']], ['A2571739477', ['I4210098980']], ['A2312664004', ['I4210098980']]], 'cited_by_count': 1421, 'concepts': [['C36464697', '0.8649868'], ['C2777777548', '0.72539186'], ['C59740354', '0.5157078'], ['C2777904410', '0.50956535'], ['C41008148', '0.49611768']], 'referenced_works': ['W1533988144', 'W1964855514', 'W1966239199', 'W1972954236', 'W1976683085', 'W1979368277', 'W1991210919', 'W1994400622', 'W1996502963', 'W1999269416', 'W1999302805', 'W2001517009', 'W2009032263', 'W2012580598', 'W2017702218', 'W2024022781', 'W2030679318', 'W2031051061', 'W2032842297', 'W2049663171', 'W2050456292', 'W2073462945', 'W2074127131', 'W2075423140', 'W2078114024', 'W2083487084', 'W2086945873', 'W2092369579', 'W2102060985', 'W2103467043', 'W2116248043', 'W2128745537', 'W2142802964', 'W2149413471', 'W2156118208', 'W2159878435', 'W2164640163', 'W2169133363', 'W2171478966', 'W2319902168', 'W2323065661', 'W2470382390', 'W2562158657', 'W2579628493', 'W2599075544', 'W2756905281', 'W2757095450', 'W2773362277', 'W2789059526', 'W2793377314', 'W2810760531', 'W2898142429', 'W2903130016'], 'abstract': 'The program Mercury , developed at the Cambridge Crystallographic Data Centre, was originally designed primarily as a crystal structure visualization tool. Over the years the fields and scientific communities of chemical crystallography and crystal engineering have developed to require more advanced structural analysis software. Mercury has evolved alongside these scientific communities and is now a powerful analysis, design and prediction platform which goes a lot further than simple structure visualization.', 'counts_by_year': [[2022, 504], [2021, 562], [2020, 348], [2019, 1]]}, {'id': 'W2535690855', 'doi': 'https://doi.org/10.1109/sp.2017.41', 'title': 'Membership Inference Attacks Against Machine Learning Models', 'type': 'proceedings-article', 'publication_date': '2017-05-22', 'host_venue': 'V4306418833', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2081295151', ['I205783295']], ['A2168184148', ['I1326498283']], ['A2727759672', ['I205783295']], ['A8790322', ['I205783295']]], 'cited_by_count': 1419, 'concepts': [['C2776214188', '0.8603926'], ['C41008148', '0.7678957'], ['C119857082', '0.76071346'], ['C154945302', '0.6450889'], ['C12713177', '0.56743693']], 'referenced_works': ['W1510952750', 'W1992926795', 'W2009733253', 'W2040228409', 'W2051267297', 'W2053637704', 'W2077905990', 'W2095272373', 'W2096633407', 'W2106313770', 'W2106463421', 'W2123147099', 'W2130051287', 'W2162379889', 'W2198253679', 'W2329660289', 'W2473418344', 'W2532520288', 'W2962835266', 'W3215186461', 'W4248649186'], 'abstract': "We quantitatively investigate how  learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of  learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial machine learning as a service providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.", 'counts_by_year': [[2022, 209], [2021, 454], [2020, 417], [2019, 226], [2018, 92], [2017, 20]]}, {'id': 'W2468965001', 'doi': 'https://doi.org/10.1038/nmat4671', 'title': 'Pursuing prosthetic electronic skin', 'type': 'journal-article', 'publication_date': '2016-09-01', 'host_venue': 'V103895331', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2076384713', ['I97018004']], ['A2554387265', ['I97018004']], ['A2095693262', ['I97018004']]], 'cited_by_count': 1417, 'concepts': [['C90130585', '0.75213027'], ['C165062768', '0.6462653'], ['C175933922', '0.5919223'], ['C138331895', '0.5903305'], ['C171250308', '0.5156928']], 'referenced_works': ['W432310612', 'W609855795', 'W996402342', 'W1590155876', 'W1899294427', 'W1900879954', 'W1907979007', 'W1942196050', 'W1963937305', 'W1964337419', 'W1964617104', 'W1968181716', 'W1970961037', 'W1972175108', 'W1977853894', 'W1982085124', 'W1982276321', 'W1987345946', 'W1987864047', 'W1991575272', 'W1993148840', 'W1995042675', 'W1998544072', 'W1999585120', 'W1999959880', 'W2004636727', 'W2009742623', 'W2010387091', 'W2012265686', 'W2015026123', 'W2016461817', 'W2016822963', 'W2017562410', 'W2018904782', 'W2030363736', 'W2030699080', 'W2032282517', 'W2032423464', 'W2032971557', 'W2033152648', 'W2033202265', 'W2033601397', 'W2036632895', 'W2038563595', 'W2044479329', 'W2045035245', 'W2045484898', 'W2050831077', 'W2051183487', 'W2051230789', 'W2051520896', 'W2052351374', 'W2054947395', 'W2056594748', 'W2057661532', 'W2057878036', 'W2060215837', 'W2061453480', 'W2062994334', 'W2063950368', 'W2065323729', 'W2068090639', 'W2069137854', 'W2071803318', 'W2074613775', 'W2074936002', 'W2076131235', 'W2076156211', 'W2077821607', 'W2078344108', 'W2095904636', 'W2097880175', 'W2100436521', 'W2100561453', 'W2103131770', 'W2103709281', 'W2105993389', 'W2106707841', 'W2107662531', 'W2114682092', 'W2116464943', 'W2117155287', 'W2117586085', 'W2118236587', 'W2118425652', 'W2119651407', 'W2121529660', 'W2123352686', 'W2124261887', 'W2131479747', 'W2132301122', 'W2134307143', 'W2134500330', 'W2137930184', 'W2147698153', 'W2151386142', 'W2152812611', 'W2154339880', 'W2157094999', 'W2157426451', 'W2158417770', 'W2159505440', 'W2161098413', 'W2162056974', 'W2164875840', 'W2170562875', 'W2171005488', 'W2171130677', 'W2171386063', 'W2174636528', 'W2202858174', 'W2212410734', 'W2216956381', 'W2256016128', 'W2268125433', 'W2268905166', 'W2296426322', 'W2324980468', 'W2334234243', 'W4213421750', 'W4230375040', 'W4242572675', 'W4243058651', 'W4248906240'], 'abstract': "Skin plays an important role in mediating our interactions with the world. Recreating the properties of skin using electronic devices could have profound implications for prosthetics and medicine. The pursuit of artificial skin has inspired innovations in materials to imitate skin's unique characteristics, including mechanical durability and stretchability, biodegradability, and the ability to measure a diversity of complex sensations over large areas. New materials and fabrication strategies are being developed to make mechanically compliant and multifunctional skin-like electronics, and improve brain/machine interfaces that enable transmission of the skin's signals into the body. This Review will cover materials and devices designed for mimicking the skin's ability to sense and generate biomimetic signals.", 'counts_by_year': [[2022, 241], [2021, 310], [2020, 265], [2019, 268], [2018, 212], [2017, 113], [2016, 4]]}, {'id': 'W2289629300', 'doi': 'https://doi.org/10.1039/c6cs00061d', 'title': 'Molecular imprinting: perspectives and applications', 'type': 'journal-article', 'publication_date': '2016-04-18', 'host_venue': 'V316438', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2141227072', ['I18452120', 'I4210108244']], ['A2585732457', ['I4210108244', 'I151013683']], ['A2115597053', ['I4210108244']], ['A2572073612', ['I4210108244']], ['A2992308801', ['I4210108244']]], 'cited_by_count': 1416, 'concepts': [['C94715292', '0.69008344'], ['C39442485', '0.66961867'], ['C171250308', '0.5617908'], ['C185592680', '0.36051214'], ['C41008148', '0.3480559']], 'referenced_works': ['W133576001', 'W297128690', 'W319648502', 'W621791958', 'W629278778', 'W768012932', 'W812768393', 'W815560347', 'W817004456', 'W821127453', 'W825874946', 'W854094384', 'W877111188', 'W1423700954', 'W1433287634', 'W1503551359', 'W1535310779', 'W1553045382', 'W1561460285', 'W1571336715', 'W1574687127', 'W1603986747', 'W1708235842', 'W1784245982', 'W1784941127', 'W1836476934', 'W1847620753', 'W1869934238', 'W1881070081', 'W1896272741', 'W1899265642', 'W1904352690', 'W1920514480', 'W1939297487', 'W1945767359', 'W1963845278', 'W1964067945', 'W1964139199', 'W1964258125', 'W1964824118', 'W1965267088', 'W1965667991', 'W1965745910', 'W1966544168', 'W1967028039', 'W1967083864', 'W1967665877', 'W1968275763', 'W1968394981', 'W1968450349', 'W1968535657', 'W1968832774', 'W1968850229', 'W1969355797', 'W1969610132', 'W1969663562', 'W1969775080', 'W1970063796', 'W1970111423', 'W1970121975', 'W1970135144', 'W1970290419', 'W1970968963', 'W1971071860', 'W1971188983', 'W1971241253', 'W1971530095', 'W1971966881', 'W1972241205', 'W1972376114', 'W1972453673', 'W1972700003', 'W1973035699', 'W1973370020', 'W1974729579', 'W1974893685', 'W1975138205', 'W1975214138', 'W1975471220', 'W1975687263', 'W1975796220', 'W1976233679', 'W1976902040', 'W1976955863', 'W1977078503', 'W1977611067', 'W1977897631', 'W1978037940', 'W1978087039', 'W1978098293', 'W1978161010', 'W1978282708', 'W1978446686', 'W1978527864', 'W1978530704', 'W1978808411', 'W1978824737', 'W1980439443', 'W1980451046', 'W1980477536', 'W1980553550', 'W1981000400', 'W1981542782', 'W1981793971', 'W1982111242', 'W1982409447', 'W1982508975', 'W1983288318', 'W1983378810', 'W1984155888', 'W1984204062', 'W1984328673', 'W1984697354', 'W1985794905', 'W1985886619', 'W1985920559', 'W1986286994', 'W1986617373', 'W1987196762', 'W1987534324', 'W1987554045', 'W1987979953', 'W1988098555', 'W1988151904', 'W1988437444', 'W1990257958', 'W1990676461', 'W1991026207', 'W1991208790', 'W1991268543', 'W1991514785', 'W1991935387', 'W1992250925', 'W1992333064', 'W1993250970', 'W1993260903', 'W1993598977', 'W1994406378', 'W1994760991', 'W1994774024', 'W1994931638', 'W1995081073', 'W1995692319', 'W1995737042', 'W1995834010', 'W1995972551', 'W1996026948', 'W1996917733', 'W1997005273', 'W1997099323', 'W1997286484', 'W1997382403', 'W1998046228', 'W1998369400', 'W1998749308', 'W1998789207', 'W1998950357', 'W1999937183', 'W2001040703', 'W2001659589', 'W2002130112', 'W2002206534', 'W2002521801', 'W2002718151', 'W2003066647', 'W2003188712', 'W2004003535', 'W2004080260', 'W2004353571', 'W2004766135', 'W2004839196', 'W2005604552', 'W2005934486', 'W2006482772', 'W2006974769', 'W2007017968', 'W2007712302', 'W2007936881', 'W2008267078', 'W2008469080', 'W2008492147', 'W2008569521', 'W2008945607', 'W2008987670', 'W2009074614', 'W2009167846', 'W2009376126', 'W2009782481', 'W2010288911', 'W2010483160', 'W2011244368', 'W2011554940', 'W2011639034', 'W2011776321', 'W2011953070', 'W2011994848', 'W2012133922', 'W2012322146', 'W2012668523', 'W2012685044', 'W2013034436', 'W2013155462', 'W2013223887', 'W2013566210', 'W2013584332', 'W2014297498', 'W2014416435', 'W2014714723', 'W2015609142', 'W2016209983', 'W2017869796', 'W2017877203', 'W2017931413', 'W2017977971', 'W2018474636', 'W2018914783', 'W2019118534', 'W2019156146', 'W2019705576', 'W2019747988', 'W2019860772', 'W2019908545', 'W2019952663', 'W2020065779', 'W2020685206', 'W2020889330', 'W2021082337', 'W2021245951', 'W2021410285', 'W2021443749', 'W2021906850', 'W2021952293', 'W2021986242', 'W2023214531', 'W2023364548', 'W2023413580', 'W2023684412', 'W2024074038', 'W2024201207', 'W2024598661', 'W2024679377', 'W2024697105', 'W2024827449', 'W2025370297', 'W2025776351', 'W2026168949', 'W2026520753', 'W2026647912', 'W2026760326', 'W2026799041', 'W2026928602', 'W2026972068', 'W2027360905', 'W2028283197', 'W2028625394', 'W2028652835', 'W2028759793', 'W2028886956', 'W2028895943', 'W2028913827', 'W2029263900', 'W2029401045', 'W2029746684', 'W2029783847', 'W2029970578', 'W2030047384', 'W2030866103', 'W2030917008', 'W2031246464', 'W2031302874', 'W2031312626', 'W2031721610', 'W2032169705', 'W2032587095', 'W2032764022', 'W2032909199', 'W2033139932', 'W2033152203', 'W2034537142', 'W2034628650', 'W2034803353', 'W2035047927', 'W2035183037', 'W2035191226', 'W2035450477', 'W2036168333', 'W2037094357', 'W2037324437', 'W2037475000', 'W2037798769', 'W2037828350', 'W2038635182', 'W2038878080', 'W2039304362', 'W2039406414', 'W2039583443', 'W2039625240', 'W2039674311', 'W2039993083', 'W2040085818', 'W2040221728', 'W2040297764', 'W2040448441', 'W2041620967', 'W2041954922', 'W2042610455', 'W2042734038', 'W2043496809', 'W2043575884', 'W2043603555', 'W2044342062', 'W2044597804', 'W2044726937', 'W2044848504', 'W2044915157', 'W2045255806', 'W2045517528', 'W2045678504', 'W2045859231', 'W2046866740', 'W2047153028', 'W2047270440', 'W2047313197', 'W2047772819', 'W2048320715', 'W2048454489', 'W2049177605', 'W2049342735', 'W2049448081', 'W2049735246', 'W2050261843', 'W2050412063', 'W2050625414', 'W2051030773', 'W2051361807', 'W2051736508', 'W2051780062', 'W2052454899', 'W2052806640', 'W2053049145', 'W2053280493', 'W2054719538', 'W2054914075', 'W2055393399', 'W2055863963', 'W2056724651', 'W2056829065', 'W2057014617', 'W2057224988', 'W2057327906', 'W2057330420', 'W2057610525', 'W2057662494', 'W2058118059', 'W2058378641', 'W2058472917', 'W2058864519', 'W2059103844', 'W2059647164', 'W2059912874', 'W2060383827', 'W2060434199', 'W2060458056', 'W2061063217', 'W2061280445', 'W2061448431', 'W2061550293', 'W2061566366', 'W2061679907', 'W2061783346', 'W2061926698', 'W2061982728', 'W2064008282', 'W2064121847', 'W2064329838', 'W2065234488', 'W2065639552', 'W2066755768', 'W2067538352', 'W2067849162', 'W2068356464', 'W2068480387', 'W2068630707', 'W2068732902', 'W2068878338', 'W2069039242', 'W2069953141', 'W2070358314', 'W2070888037', 'W2072159017', 'W2072267069', 'W2072466304', 'W2072865821', 'W2073150035', 'W2073610320', 'W2073617279', 'W2074251861', 'W2074880409', 'W2074996443', 'W2075136428', 'W2075458638', 'W2075836387', 'W2075928383', 'W2076577748', 'W2076720669', 'W2076987550', 'W2077063396', 'W2077310044', 'W2077358154', 'W2077628058', 'W2077891377', 'W2078577217', 'W2078623031', 'W2079643819', 'W2079657559', 'W2079770300', 'W2079807437', 'W2080241173', 'W2080389940', 'W2080563890', 'W2080747581', 'W2081126839', 'W2081300927', 'W2081594297', 'W2081701844', 'W2082243550', 'W2082337695', 'W2082573035', 'W2083213291', 'W2083241660', 'W2083333634', 'W2083370537', 'W2083794586', 'W2083808112', 'W2083975331', 'W2085231822', 'W2085790519', 'W2085880316', 'W2086041128', 'W2086049365', 'W2086361014', 'W2086404320', 'W2086437144', 'W2086782545', 'W2087087323', 'W2087090332', 'W2087165705', 'W2087619426', 'W2087938237', 'W2088448826', 'W2088561313', 'W2088623755', 'W2088731351', 'W2088843328', 'W2088890545', 'W2088906331', 'W2089018984', 'W2089023732', 'W2089099225', 'W2089233764', 'W2089589779', 'W2090169177', 'W2090433531', 'W2090783753', 'W2090856558', 'W2091099548', 'W2091372012', 'W2091758787', 'W2091766239', 'W2091954676', 'W2092062018', 'W2092200529', 'W2092225823', 'W2092282353', 'W2092354784', 'W2092468373', 'W2092502367', 'W2092619467', 'W2092626455', 'W2092744127', 'W2093012395', 'W2093136770', 'W2093357097', 'W2093450855', 'W2093512065', 'W2093582060', 'W2093739782', 'W2093873996', 'W2094037125', 'W2094164073', 'W2094410330', 'W2094676213', 'W2094712774', 'W2094767976', 'W2094912858', 'W2095186669', 'W2095404978', 'W2095530757', 'W2095706075', 'W2095840505', 'W2097015443', 'W2097445679', 'W2097707634', 'W2098023432', 'W2098449279', 'W2099936177', 'W2102650353', 'W2103352542', 'W2104714966', 'W2104915374', 'W2105398164', 'W2106222501', 'W2107986928', 'W2108249995', 'W2108711184', 'W2108727081', 'W2108849850', 'W2109302126', 'W2110401202', 'W2111427056', 'W2113062957', 'W2113120396', 'W2113233596', 'W2113408056', 'W2113522223', 'W2114317462', 'W2115155842', 'W2116925334', 'W2117376875', 'W2118142076', 'W2118539913', 'W2119262951', 'W2119536455', 'W2119947965', 'W2121669501', 'W2122347920', 'W2122653776', 'W2124919565', 'W2126801923', 'W2127177421', 'W2128136499', 'W2129102947', 'W2130374314', 'W2131078656', 'W2131827176', 'W2132006963', 'W2132853095', 'W2133318626', 'W2133744605', 'W2133989277', 'W2135831281', 'W2136805004', 'W2136863798', 'W2137577190', 'W2138401180', 'W2138514492', 'W2139577110', 'W2141398801', 'W2141655424', 'W2143067390', 'W2144518770', 'W2144589905', 'W2146012343', 'W2146374568', 'W2148531667', 'W2149265341', 'W2150184000', 'W2150771637', 'W2151890766', 'W2152670851', 'W2153234665', 'W2153408620', 'W2153705567', 'W2154714425', 'W2154811026', 'W2154986879', 'W2157441948', 'W2157524346', 'W2161004176', 'W2161006939', 'W2163311312', 'W2163610177', 'W2164219910', 'W2165790300', 'W2166934622', 'W2167581469', 'W2168888042', 'W2168992509', 'W2169127402', 'W2169390818', 'W2171523067', 'W2172294713', 'W2243383384', 'W2312260016', 'W2314136388', 'W2314588979', 'W2315514908', 'W2315535008', 'W2316625759', 'W2317295301', 'W2319169345', 'W2321064069', 'W2321125766', 'W2321157221', 'W2321723093', 'W2324065590', 'W2324368909', 'W2325470188', 'W2325915657', 'W2326126714', 'W2326738770', 'W2327021654', 'W2327148368', 'W2328163175', 'W2330258794', 'W2332844190', 'W2332870217', 'W2332963905', 'W2334850520', 'W2508661682', 'W2950556935', 'W4230989039', 'W4238835741', 'W4249495403', 'W4254870942', 'W4256503849'], 'abstract': 'This critical review presents a survey of recent developments in technologies and strategies for the preparation of MIPs, followed by the application of MIPs in sample pretreatment, chromatographic separation and chemical sensing.', 'counts_by_year': [[2022, 215], [2021, 276], [2020, 263], [2019, 257], [2018, 209], [2017, 149], [2016, 42]]}, {'id': 'W2742330194', 'doi': 'https://doi.org/10.1145/3137597.3137600', 'title': 'Fake News Detection on Social Media', 'type': 'journal-article', 'publication_date': '2017-09-01', 'host_venue': 'V4210176598', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2108363897', ['I55732556']], ['A2084520757', ['I4210149760']], ['A2122735199', ['I55732556']], ['A2147392410', ['I87216513']], ['A2122391114', ['I55732556']]], 'cited_by_count': 1410, 'concepts': [['C518677369', '0.80387664'], ['C2779756789', '0.78370637'], ['C41008148', '0.75637794'], ['C108827166', '0.59069824'], ['C529147693', '0.51212597']], 'referenced_works': ['W651477617', 'W1507075132', 'W1526882625', 'W1532503642', 'W1541280084', 'W1799750435', 'W1912982817', 'W1969944630', 'W1975594555', 'W1996665163', 'W2015462142', 'W2032897813', 'W2034968913', 'W2041614930', 'W2041709326', 'W2041946752', 'W2042038069', 'W2046881809', 'W2048102542', 'W2054476043', 'W2072715695', 'W2075585362', 'W2084591134', 'W2086456653', 'W2096555427', 'W2132553681', 'W2142869398', 'W2143046471', 'W2232384272', 'W2248267741', 'W2252350410', 'W2347127863', 'W2398287226', 'W2410465342', 'W2423682914', 'W2516338333', 'W2531862055', 'W2535287707', 'W2537054494', 'W2538371562', 'W2550819555', 'W2598689838', 'W2622849676', 'W2738406145', 'W2919115771', 'W2963908320', 'W3011865677', 'W3103442714', 'W3125182500', 'W3126072720', 'W4244037933', 'W4246826033'], 'abstract': 'Social media for news consumption is a double-edged sword. On the one hand, its low cost, easy access, and rapid dissemination of information lead people to seek out and consume news from social media. On the other hand, it enables the wide spread of \\fake news", i.e., low quality news with intentionally false information. The extensive spread of fake news has the potential for extremely negative impacts on individuals and society. Therefore, fake news detection on social media has recently become an emerging research that is attracting tremendous attention. Fake news detection on social media presents unique characteristics and challenges that make existing detection algorithms from traditional news media ine ective or not applicable. First, fake news is intentionally written to mislead readers to believe false information, which makes it difficult and nontrivial to detect based on news content; therefore, we need to include auxiliary information, such as user social engagements on social media, to help make a determination. Second, exploiting this auxiliary information is challenging in and of itself as users\' social engagements with fake news produce data that is big, incomplete, unstructured, and noisy. Because the issue of fake news detection on social media is both challenging and relevant, we conducted this survey to further facilitate research on the problem. In this survey, we present a comprehensive review of detecting fake news on social media, including fake news characterizations on psychology and social theories, existing algorithms from a data mining perspective, evaluation metrics and representative datasets. We also discuss related research areas, open problems, and future research directions for fake news detection on social media.', 'counts_by_year': [[2022, 253], [2021, 442], [2020, 379], [2019, 236], [2018, 97]]}, {'id': 'W2555618208', 'doi': 'https://doi.org/10.1109/cvpr.2017.691', 'title': 'Multi-view 3D Object Detection Network for Autonomous Driving', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2119051707', ['I99065089']], ['A2151001937', ['I98301712']], ['A2970353990', ['I98301712']], ['A2147803694', ['I98301712']]], 'cited_by_count': 1409, 'concepts': [['C131979681', '0.86888766'], ['C41008148', '0.7720189'], ['C154945302', '0.6762062'], ['C2776151529', '0.6666477'], ['C185798385', '0.6595114']], 'referenced_works': ['W7746136', 'W1536680647', 'W1914646313', 'W1946609740', 'W2046382188', 'W2071042563', 'W2114111978', 'W2129305389', 'W2145142957', 'W2150066425', 'W2229637417', 'W2293349265', 'W2463402750', 'W2468368736', 'W2473644495', 'W2474389331', 'W2962731536', 'W2963083779'], 'abstract': 'This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the birds eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25% and 30% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 14.9% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.', 'counts_by_year': [[2022, 172], [2021, 391], [2020, 416], [2019, 308], [2018, 109], [2017, 13]]}, {'id': 'W2569012662', 'doi': 'https://doi.org/10.1021/acs.chemrev.6b00644', 'title': 'Transition Metal-Catalyzed C–H Amination: Scope, Mechanism, and Applications', 'type': 'journal-article', 'publication_date': '2017-01-04', 'host_venue': 'V41143188', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2140785122', ['I4210104335']], ['A2633622516', ['I157485424']], ['A2150815937', ['I157485424']]], 'cited_by_count': 1403, 'concepts': [['C185592680', '0.9238926'], ['C198503264', '0.8871422'], ['C161790260', '0.7386954'], ['C21951064', '0.6808417'], ['C106773901', '0.6613272']], 'referenced_works': ['W255314532', 'W571730305', 'W1510912478', 'W1609593344', 'W1716063961', 'W1784010470', 'W1882777239', 'W1902427796', 'W1902428895', 'W1913219259', 'W1916308107', 'W1918291506', 'W1936419785', 'W1938899376', 'W1954803579', 'W1963034753', 'W1964487763', 'W1965015210', 'W1966930861', 'W1967254706', 'W1967431172', 'W1967498381', 'W1968205461', 'W1968517293', 'W1970252331', 'W1970910216', 'W1971288902', 'W1971425837', 'W1972117554', 'W1973292600', 'W1973963022', 'W1975064238', 'W1976559982', 'W1976801308', 'W1977484149', 'W1977988158', 'W1978353169', 'W1978856733', 'W1980402890', 'W1981072615', 'W1981130552', 'W1981288459', 'W1982732047', 'W1983515948', 'W1983815247', 'W1984012152', 'W1987808027', 'W1990194002', 'W1990284515', 'W1990957776', 'W1992012618', 'W1992259722', 'W1992842386', 'W1993653898', 'W1994390276', 'W1995898999', 'W1995928586', 'W1996941025', 'W1997770703', 'W1999725727', 'W2000267849', 'W2000354589', 'W2000865243', 'W2001989348', 'W2002861652', 'W2003610491', 'W2003754630', 'W2003876494', 'W2004174458', 'W2004604901', 'W2005661768', 'W2006184470', 'W2006383333', 'W2006947767', 'W2007309958', 'W2007470266', 'W2007594170', 'W2007620949', 'W2008616263', 'W2008850528', 'W2009858571', 'W2010101793', 'W2011298014', 'W2011552216', 'W2012314528', 'W2013012582', 'W2013062458', 'W2013760130', 'W2016902320', 'W2017255626', 'W2017847164', 'W2019052777', 'W2019568660', 'W2022708853', 'W2022908039', 'W2024559491', 'W2026159532', 'W2027429351', 'W2028653198', 'W2029501153', 'W2030647402', 'W2031655326', 'W2031659413', 'W2033442130', 'W2033511623', 'W2034537753', 'W2035607313', 'W2036099524', 'W2037025098', 'W2038452922', 'W2039397684', 'W2039705271', 'W2039804228', 'W2040538389', 'W2040694793', 'W2040713091', 'W2040938839', 'W2041860183', 'W2043011398', 'W2043834735', 'W2044178974', 'W2045516689', 'W2047246306', 'W2047648221', 'W2048022307', 'W2048802767', 'W2049407878', 'W2050379798', 'W2050511290', 'W2050791488', 'W2051429044', 'W2052015737', 'W2052118630', 'W2052859068', 'W2054785509', 'W2056637308', 'W2058142786', 'W2059302556', 'W2060227668', 'W2060872673', 'W2061142370', 'W2061423071', 'W2061848553', 'W2063443169', 'W2063674611', 'W2064259630', 'W2065382371', 'W2066428139', 'W2067626193', 'W2067767613', 'W2067826614', 'W2067946880', 'W2068749447', 'W2070760110', 'W2071555224', 'W2071936450', 'W2072356342', 'W2075073048', 'W2076044769', 'W2078277171', 'W2078787764', 'W2080183992', 'W2080187643', 'W2081070271', 'W2081360871', 'W2082909540', 'W2083061046', 'W2083150093', 'W2083768562', 'W2083790594', 'W2083990575', 'W2084452985', 'W2084833349', 'W2085172649', 'W2085939185', 'W2087236453', 'W2087655557', 'W2088309473', 'W2088386185', 'W2088723766', 'W2088869949', 'W2088948925', 'W2089412862', 'W2089839565', 'W2090212893', 'W2090280264', 'W2090517202', 'W2091788586', 'W2092172784', 'W2092361417', 'W2092638601', 'W2094075343', 'W2094844841', 'W2096915063', 'W2097148620', 'W2097789371', 'W2099127406', 'W2099363361', 'W2099665432', 'W2102676049', 'W2102838682', 'W2104093225', 'W2105062092', 'W2105722596', 'W2105947010', 'W2106557976', 'W2106868795', 'W2106994791', 'W2107469577', 'W2108072178', 'W2109841895', 'W2110312043', 'W2110953698', 'W2111118274', 'W2111347359', 'W2111653196', 'W2111876916', 'W2111988807', 'W2112254665', 'W2113278931', 'W2114944732', 'W2117374252', 'W2117772509', 'W2118740852', 'W2119783195', 'W2120523997', 'W2121253395', 'W2121674481', 'W2121722353', 'W2121821467', 'W2123366338', 'W2123771203', 'W2124213317', 'W2125772924', 'W2127484472', 'W2127706988', 'W2128969383', 'W2130141905', 'W2130665665', 'W2131634514', 'W2132030714', 'W2132280715', 'W2132626909', 'W2136935607', 'W2137098133', 'W2138216614', 'W2138445447', 'W2140158426', 'W2144168852', 'W2145630006', 'W2146049267', 'W2147599451', 'W2147750000', 'W2147925478', 'W2148492910', 'W2150338396', 'W2150639224', 'W2151864089', 'W2152083435', 'W2152570388', 'W2153410908', 'W2153484069', 'W2153701075', 'W2154210943', 'W2155390912', 'W2155530883', 'W2155808546', 'W2156032117', 'W2159456209', 'W2162157647', 'W2164023709', 'W2164118281', 'W2165352032', 'W2165711924', 'W2165858980', 'W2166144166', 'W2166466785', 'W2167149302', 'W2168564553', 'W2169371839', 'W2178754294', 'W2180058938', 'W2199406638', 'W2200436954', 'W2204286420', 'W2213924983', 'W2218266460', 'W2233694737', 'W2236752792', 'W2255596973', 'W2265756463', 'W2269558166', 'W2273211579', 'W2273989337', 'W2280353002', 'W2282184765', 'W2286618867', 'W2287571712', 'W2287902978', 'W2288881481', 'W2289478978', 'W2290916158', 'W2294245738', 'W2296454788', 'W2299327344', 'W2300667452', 'W2308509085', 'W2311561544', 'W2313384930', 'W2313737475', 'W2314018283', 'W2314034371', 'W2314586347', 'W2314685300', 'W2315684216', 'W2315990393', 'W2316218799', 'W2316230801', 'W2316510623', 'W2316864204', 'W2317020669', 'W2317115327', 'W2317348934', 'W2317383270', 'W2317406023', 'W2317771072', 'W2317919875', 'W2319465534', 'W2320065507', 'W2321141802', 'W2321904077', 'W2321928492', 'W2322034841', 'W2322146862', 'W2322705312', 'W2323189499', 'W2324081872', 'W2324391215', 'W2324901793', 'W2325072690', 'W2325370483', 'W2325882359', 'W2326200481', 'W2326473865', 'W2326535103', 'W2326587253', 'W2328176497', 'W2328935951', 'W2330081547', 'W2330273110', 'W2330631897', 'W2330973450', 'W2331229388', 'W2331372718', 'W2331452414', 'W2331550511', 'W2331844198', 'W2331980462', 'W2332191152', 'W2332216194', 'W2333020171', 'W2333335708', 'W2333506656', 'W2333700638', 'W2334054216', 'W2334372247', 'W2334569506', 'W2334705630', 'W2334712536', 'W2334968708', 'W2334994621', 'W2335140017', 'W2335579226', 'W2335645516', 'W2338202342', 'W2338698975', 'W2339051409', 'W2339876809', 'W2342772717', 'W2342892827', 'W2343349747', 'W2343607831', 'W2344013673', 'W2344476092', 'W2344832573', 'W2346205765', 'W2362306701', 'W2384909584', 'W2409253758', 'W2413111098', 'W2414578828', 'W2417851625', 'W2418331135', 'W2418991601', 'W2430497374', 'W2464886544', 'W2465616692', 'W2467606099', 'W2470357071', 'W2482434813', 'W2486572676', 'W2489366484', 'W2499592177', 'W2504256884', 'W2512887334', 'W2517093895', 'W2518378475', 'W2529657426', 'W2546620090', 'W2581321057', 'W2949422210', 'W2949514989', 'W2949900008', 'W2949925332', 'W2950030876', 'W2950041319', 'W2950082485', 'W2950122838', 'W2950978692', 'W2950982964', 'W2951066275', 'W2951176764', 'W2951406184', 'W2951912170', 'W2951927646', 'W2952028639', 'W2952078457', 'W2952218523', 'W2952567240', 'W2952567764', 'W2953222240', 'W4253683669'], 'abstract': 'Catalytic transformation of ubiquitous C-H bonds into valuable C-N bonds offers an efficient synthetic approach to construct N-functionalized molecules. Over the last few decades, transition metal catalysis has been repeatedly proven to be a powerful tool for the direct conversion of cheap hydrocarbons to synthetically versatile amino-containing compounds. This Review comprehensively highlights recent advances in intra- and intermolecular C-H amination reactions utilizing late transition metal-based catalysts. Initial discovery, mechanistic study, and additional applications were categorized on the basis of the mechanistic scaffolds and types of reactions. Reactivity and selectivity of novel systems are discussed in three sections, with each being defined by a proposed working mode.', 'counts_by_year': [[2022, 181], [2021, 301], [2020, 261], [2019, 288], [2018, 256], [2017, 116]]}, {'id': 'W2936774411', 'doi': 'https://doi.org/10.21437/interspeech.2019-2680', 'title': 'SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition', 'type': 'proceedings-article', 'publication_date': '2019-04-18', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2168891258', ['I1291425158']], ['A2281662583', ['I1291425158']], ['A3209865217', ['I63966007']], ['A2723467035', ['I1291425158']], ['A2655758810', ['I1291425158']], ['A1859220838', ['I1291425158']], ['A2148448995', ['I1291425158']]], 'cited_by_count': 1403, 'concepts': [['C28490314', '0.77626455'], ['C41008148', '0.7496079'], ['C137293760', '0.677117'], ['C2777402240', '0.6428777'], ['C2780957641', '0.56098044']], 'referenced_works': ['W97072897', 'W1485222997', 'W1494198834', 'W1524333225', 'W1915251500', 'W1922655562', 'W1989674786', 'W2099621636', 'W2102113734', 'W2108677974', 'W2112739286', 'W2121879602', 'W2143612262', 'W2147768505', 'W2166637769', 'W2183341477', 'W2184045248', 'W2184343439', 'W2327501763', 'W2397147568', 'W2407080277', 'W2514741789', 'W2520160253', 'W2521999726', 'W2577366047', 'W2597757402', 'W2608712415', 'W2617258110', 'W2746314669', 'W2759921250', 'W2781384251', 'W2782451907', 'W2799800213', 'W2804047946', 'W2887528618', 'W2888493875', 'W2889163603', 'W2889282842', 'W2898181186', 'W2904818793', 'W2905263927', 'W2912492482', 'W2962824709', 'W2962826786', 'W2963070863', 'W2963303028', 'W2963727906', 'W2963742216', 'W2973215447'], 'abstract': "We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8% WER on test-other without the use of a language model, and 5.8% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8%/14.1% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.", 'counts_by_year': [[2022, 222], [2021, 669], [2020, 426], [2019, 84], [2018, 1]]}, {'id': 'W2285660444', 'doi': 'https://doi.org/10.1145/3007787.3001163', 'title': 'EIE', 'type': 'journal-article', 'publication_date': '2016-06-18', 'host_venue': 'V4210193905', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2233699872', ['I97018004']], ['A2537574871', ['I97018004']], ['A2341927824', ['I97018004']], ['A2439915105', ['I97018004']], ['A1840085346', ['I97018004']], ['A2113577245', ['I97018004']], ['A2037541505', ['I1304085615']]], 'cited_by_count': 1399, 'concepts': [['C41008148', '0.79406077'], ['C162478608', '0.6361229'], ['C173608175', '0.5761491'], ['C7366592', '0.5685377'], ['C68043766', '0.54831004']], 'referenced_works': ['W179875071', 'W1536680647', 'W1902041153', 'W1905882502', 'W1981252059', 'W1990315422', 'W1991539813', 'W2004455575', 'W2048266589', 'W2064675550', 'W2067523571', 'W2094756095', 'W2097117768', 'W2108598243', 'W2128853364', 'W2145287260', 'W2152839228', 'W2155893237', 'W2172654076', 'W2276486856', 'W4243519499', 'W4245199738', 'W4245683599'], 'abstract': "State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets. While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power. Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by pruning the redundant connections and having multiple connections share the same weight. We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing. Going from DRAM to SRAM gives EIE 120× energy saving; Exploiting sparsity saves 10×; Weight sharing gives 8×; Skipping zero activations from ReLU saves another 3×. Evaluated on nine DNN benchmarks, EIE is 189× and 13× faster when compared to CPU and GPU implementations of the same DNN without compression. EIE has a processing power of 102 GOPS working directly on a compressed network, corresponding to 3 TOPS on an uncompressed network, and processes FC layers of AlexNet at 1.88×10 4 frames/sec with a power dissipation of only 600mW. It is 24,000× and 3,400× more energy efficient than a CPU and GPU respectively. Compared with DaDianNao, EIE has 2.9×, 19× and 3× better throughput, energy efficiency and area efficiency.", 'counts_by_year': [[2022, 115], [2021, 286], [2020, 335], [2019, 276], [2018, 269], [2017, 94], [2016, 21], [2012, 1]]}, {'id': 'W2543927648', 'doi': 'https://doi.org/10.1109/cvpr.2017.17', 'title': 'Universal Adversarial Perturbations', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306402144', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2342058533', ['I5124864']], ['A2153015520', ['I5124864']], ['A2015310755', ['I203339264']], ['A2063844727', ['I5124864']]], 'cited_by_count': 1397, 'concepts': [['C37736160', '0.6846787'], ['C41008148', '0.5417628'], ['C154945302', '0.27385783']], 'referenced_works': ['W9657784', 'W1932198206', 'W1999192586', 'W2097117768', 'W2117539524', 'W2145287260', 'W2155893237', 'W2194775991', 'W2963003451', 'W2963098487', 'W2963669006'], 'abstract': 'Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.', 'counts_by_year': [[2022, 136], [2021, 400], [2020, 394], [2019, 296], [2018, 138], [2017, 32]]}, {'id': 'W2964523010', 'doi': 'https://doi.org/10.1016/j.jbusres.2019.07.039', 'title': 'Literature review as a research methodology: An overview and guidelines', 'type': 'journal-article', 'publication_date': '2019-11-01', 'host_venue': 'V93284759', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2502783183', ['I181046868']]], 'cited_by_count': 1397, 'concepts': [['C539667460', '0.4428897'], ['C15744967', '0.3727326'], ['C55587333', '0.3505484'], ['C2522767166', '0.3253604'], ['C41008148', '0.324463']], 'referenced_works': ['W1764745664', 'W1979290264', 'W1982070737', 'W2002705479', 'W2005501262', 'W2012932483', 'W2079283960', 'W2081753990', 'W2082192234', 'W2094227722', 'W2096423181', 'W2103517563', 'W2107328434', 'W2113373290', 'W2121854318', 'W2129232060', 'W2136868285', 'W2138368559', 'W2159538290', 'W2238735419', 'W2279438569', 'W2288281658', 'W2305663577', 'W2595900916', 'W2761394648', 'W2772979648', 'W2785873055', 'W3125505924', 'W3126135051', 'W4238850195'], 'abstract': 'Abstract   Knowledge production within the field of business research is accelerating at a tremendous speed while at the same time remaining fragmented and interdisciplinary. This makes it hard to keep up with state-of-the-art and to be at the forefront of research, as well as to assess the collective evidence in a particular area of business research. This is why the literature review as a research method is more relevant than ever. Traditional literature reviews often lack thoroughness and rigor and are conducted ad hoc, rather than following a specific methodology. Therefore, questions can be raised about the quality and trustworthiness of these types of reviews. This paper discusses literature review as a methodology for conducting research and offers an overview of different types of reviews, as well as some guidelines to how to both conduct and evaluate a literature review paper. It also discusses common pitfalls and how to get literature reviews published.', 'counts_by_year': [[2022, 579], [2021, 602], [2020, 206], [2019, 6]]}, {'id': 'W2963727135', 'doi': 'https://doi.org/10.1109/cvpr.2018.00472', 'title': 'VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection', 'type': 'proceedings-article', 'publication_date': '2018-06-18', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2141213476', ['I1311269955']], ['A1967568329', ['I1311269955']]], 'cited_by_count': 1396, 'concepts': [['C131979681', '0.8864075'], ['C41008148', '0.80292106'], ['C154945302', '0.73778427'], ['C31972630', '0.61112094'], ['C52622490', '0.60709435']], 'referenced_works': ['W1493004075', 'W1519128923', 'W1563354748', 'W1966456026', 'W2033834476', 'W2052789223', 'W2071042563', 'W2099606917', 'W2114111978', 'W2116588463', 'W2126624994', 'W2136020167', 'W2145142957', 'W2160821342', 'W2162411291', 'W2172156083', 'W2194775991', 'W2211722331', 'W2293349265', 'W2481401919', 'W2558294288', 'W2565472677', 'W2963721253'], 'abstract': "Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.", 'counts_by_year': [[2022, 205], [2021, 457], [2020, 443], [2019, 253], [2018, 36], [2017, 2]]}, {'id': 'W414544266', 'doi': 'https://doi.org/10.1007/s00521-015-1920-1', 'title': 'Dragonfly algorithm: a new meta-heuristic optimization technique for solving single-objective, discrete, and multi-objective problems', 'type': 'journal-article', 'publication_date': '2016-05-01', 'host_venue': 'V147897268', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A3006208000', ['I2188195594']]], 'cited_by_count': 1396, 'concepts': [['C2988037039', '0.83360136'], ['C41008148', '0.6193314'], ['C173801870', '0.5919932'], ['C126255220', '0.5554197'], ['C68597687', '0.5423132']], 'referenced_works': ['W84428182', 'W1580022276', 'W1587294773', 'W1748133846', 'W1972271820', 'W1977712932', 'W1980048226', 'W1980843902', 'W1997600725', 'W2000621750', 'W2001979953', 'W2003961265', 'W2009207502', 'W2010882086', 'W2012691599', 'W2028885867', 'W2031183907', 'W2035117491', 'W2037000926', 'W2037567262', 'W2044371115', 'W2047156683', 'W2050044094', 'W2053498776', 'W2055631528', 'W2056561256', 'W2061438946', 'W2065401134', 'W2070843600', 'W2088036356', 'W2096166399', 'W2107941094', 'W2109364787', 'W2111393363', 'W2111416126', 'W2114652055', 'W2123808725', 'W2125213524', 'W2125899728', 'W2126105956', 'W2126554879', 'W2133188177', 'W2137514952', 'W2143451729', 'W2143560894', 'W2145113795', 'W2151554678', 'W2152195021', 'W2165171393', 'W2166028204', 'W2535351973', 'W4234406933'], 'abstract': 'A novel swarm intelligence optimization technique is proposed called dragonfly algorithm (DA). The main inspiration of the DA algorithm originates from the static and dynamic swarming behaviours of dragonflies in nature. Two essential phases of optimization, exploration and exploitation, are designed by modelling the social interaction of dragonflies in navigating, searching for foods, and avoiding enemies when swarming dynamically or statistically. The paper also considers the proposal of binary and multi-objective versions of DA called binary DA (BDA) and multi-objective DA (MODA), respectively. The proposed algorithms are benchmarked by several mathematical test functions and one real case study qualitatively and quantitatively. The results of DA and BDA prove that the proposed algorithms are able to improve the initial random population for a given problem, converge towards the global optimum, and provide very competitive results compared to other well-known algorithms in the literature. The results of MODA also show that this algorithm tends to find very accurate approximations of Pareto optimal solutions with high uniform distribution for multi-objective problems. The set of designs obtained for the submarine propeller design problem demonstrate the merits of MODA in solving challenging real problems with unknown true Pareto optimal front as well. Note that the source codes of the DA, BDA, and MODA algorithms are publicly available at http://www.alimirjalili.com/DA.html.', 'counts_by_year': [[2022, 311], [2021, 359], [2020, 289], [2019, 214], [2018, 121], [2017, 68], [2016, 26], [2015, 5]]}, {'id': 'W2542459869', 'doi': 'https://doi.org/10.1145/2934664', 'title': 'Apache Spark', 'type': 'journal-article', 'publication_date': '2016-10-28', 'host_venue': 'V103482838', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2009645378', ['I97018004']], ['A2059277171', ['I4210159714']], ['A2071950306', ['I4210159714']], ['A2098092316', ['I4210159714']], ['A2039995418', ['I4210159714']], ['A2502187356', ['I95457486']], ['A2298118675', ['I4210159714']], ['A3080324563', ['I4210159714']], ['A2232375382', ['I95457486']], ['A2523407221', ['I95457486']], ['A734473377', ['I95457486']], ['A2252171364', ['I95457486']], ['A719828399', ['I95457486']], ['A2161479384', ['I95457486']]], 'cited_by_count': 1395, 'concepts': [['C2781215313', '0.730567'], ['C41008148', '0.5342077'], ['C111919701', '0.36734897'], ['C199360897', '0.19420284']], 'referenced_works': ['W1868798185', 'W1976821017', 'W1989017925', 'W2000041758', 'W2013455564', 'W2038412523', 'W2045271686', 'W2074935284', 'W2078945459', 'W2100830825', 'W2113640782', 'W2139072600', 'W2163764145', 'W2170616854'], 'abstract': 'This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications.', 'counts_by_year': [[2022, 151], [2021, 310], [2020, 349], [2019, 306], [2018, 209], [2017, 67], [2016, 1]]}, {'id': 'W2544493586', 'doi': 'https://doi.org/10.1093/nar/gkw1004', 'title': 'CARD 2017: expansion and model-centric curation of the comprehensive antibiotic resistance database', 'type': 'journal-article', 'publication_date': '2017-01-04', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2635826898', ['I98251732']], ['A2535685820', ['I98251732']], ['A2302078333', ['I98251732']], ['A2049620891', ['I98251732']], ['A2541462301', ['I98251732']], ['A2539531868', ['I98251732']], ['A2539927968', ['I98251732']], ['A2543145553', ['I98251732']], ['A2187807620', ['I98251732']], ['A2608981262', ['I98251732']], ['A2635924261', ['I98251732']], ['A2225386764', ['I18014758']], ['A2484756016', ['I18014758']], ['A2129757592', ['I4210141533']], ['A2096490174', ['I4210141533']], ['A2542046496', ['I142762351']], ['A2289464704', ['I98251732']], ['A1981169186', ['I98251732']], ['A2154848232', ['I98251732']], ['A2124379914', ['I98251732']], ['A2188411458', ['I18014758']], ['A2149196411', ['I98251732']], ['A2698482611', ['I98251732']]], 'cited_by_count': 1395, 'concepts': [['C2776666601', '0.7120136'], ['C86803240', '0.6282898'], ['C91632574', '0.61767185'], ['C25810664', '0.59802705'], ['C70721500', '0.4681896']], 'referenced_works': ['W1974123350', 'W1993860350', 'W2008274231', 'W2027184506', 'W2077210509', 'W2078874321', 'W2099837668', 'W2118688136', 'W2134852597', 'W2142678478', 'W2148083369', 'W2151397621', 'W2157122545', 'W2163316049', 'W2175270296', 'W2203222511', 'W2222575139', 'W2300011769'], 'abstract': 'The Comprehensive Antibiotic Resistance Database (CARD; http://arpcard.mcmaster.ca) is a manually curated resource containing high quality reference data on the molecular basis of antimicrobial resistance (AMR), with an emphasis on the genes, proteins and mutations involved in AMR. CARD is ontologically structured, model centric, and spans the breadth of AMR drug classes and resistance mechanisms, including intrinsic, mutation-driven and acquired resistance. It is built upon the Antibiotic Resistance Ontology (ARO), a custom built, interconnected and hierarchical controlled vocabulary allowing advanced data sharing and organization. Its design allows the development of novel genome analysis tools, such as the Resistance Gene Identifier (RGI) for resistome prediction from raw genome sequence. Recent improvements include extensive curation of additional reference sequences and mutations, development of a unique Model Ontology and accompanying AMR detection models to power sequence analysis, new visualization tools, and expansion of the RGI for detection of emergent AMR threats. CARD curation is updated monthly based on an interplay of manual literature curation, computational text mining, and genome analysis.', 'counts_by_year': [[2022, 208], [2021, 292], [2020, 333], [2019, 318], [2018, 164], [2017, 78]]}, {'id': 'W2604808360', 'doi': 'https://doi.org/10.1093/nar/gkw1108', 'title': 'Expansion of the Gene Ontology knowledgebase and resources', 'type': 'journal-article', 'publication_date': '2017-01-04', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2114289465', ['I148283060']], ['A2806978745', ['I148283060']], ['A2806529328', ['I148283060']], ['A2104912896', ['I148283060']], ['A2190004771', ['I148283060']], ['A2806367832', ['I4210100400']], ['A2106328922', ['I4210113054']], ['A2128134571', ['I4210113054']]], 'cited_by_count': 1394, 'concepts': [['C25810664', '0.7646253'], ['C2987395477', '0.6606303'], ['C86803240', '0.6495491'], ['C206345919', '0.59853977'], ['C2522767166', '0.4950439']], 'referenced_works': ['W1490161904', 'W1832220924', 'W2068728659', 'W2102983252', 'W2111191870', 'W2117602634', 'W2127589936', 'W2128891846', 'W2134396951', 'W2140424919', 'W2163603537', 'W2164461702', 'W2171815113', 'W2276605232', 'W2282929546', 'W2339882774', 'W2346143774', 'W2514630101', 'W2517051411', 'W2559466477', 'W3102771275', 'W4251386234'], 'abstract': 'The Gene Ontology (GO) is a comprehensive resource of computable knowledge regarding the functions of genes and gene products. As such, it is extensively used by the biomedical research community for the analysis of -omics and related data. Our continued focus is on improving the quality and utility of the GO resources, and we welcome and encourage input from researchers in all areas of biology. In this update, we summarize the current contents of the GO knowledgebase, and present several new features and improvements that have been made to the ontology, the annotations and the tools. Among the highlights are 1) developments that facilitate access to, and application of, the GO knowledgebase, and 2) extensions to the resource as well as increasing support for descriptions of causal models of biological systems and network biology. To learn more, visit http://geneontology.org/.', 'counts_by_year': [[2022, 88], [2021, 176], [2020, 261], [2019, 468], [2018, 356], [2017, 44], [2016, 1]]}, {'id': 'W2795989238', 'doi': 'https://doi.org/10.1016/j.cell.2018.02.052', 'title': 'An Integrated TCGA Pan-Cancer Clinical Data Resource to Drive High-Quality Survival Outcome Analytics', 'type': 'journal-article', 'publication_date': '2018-04-05', 'host_venue': 'V110447773', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2331270146', ['I1321217177']], ['A1816800110', ['I1314135232']], ['A3205870363', ['I114027177']], ['A1863574459', ['I154057602']], ['A2101651291', ['I1343551460']], ['A180898208', ['I107606265']], ['A2224753070', ['I2800363604']], ['A1901144256', ['I1337826666']], ['A2239834603', ['I4210086933']], ['A2157539533', ['I170201317']], ['A2114585142', ['I1323236076']], ['A2259285298', ['I180670191']], ['A2967338647', ['I2800363604']], ['A2081585096', ['I3148997608']], ['A2142581098', ['I1321217177']]], 'cited_by_count': 1394, 'concepts': [['C86803240', '0.93046373'], ['C79158427', '0.4518258'], ['C206345919', '0.4385473'], ['C24756922', '0.42658168'], ['C2779530757', '0.4116785']], 'referenced_works': ['W1782486205', 'W1812256879', 'W1942578084', 'W1948838904', 'W1950829641', 'W1963505593', 'W1966714873', 'W1967220865', 'W2001529994', 'W2008706768', 'W2020016182', 'W2025183726', 'W2037668591', 'W2038981426', 'W2040738262', 'W2041440766', 'W2068523052', 'W2073618702', 'W2087910159', 'W2088201493', 'W2096283457', 'W2096287682', 'W2096864872', 'W2097255042', 'W2104389711', 'W2108696783', 'W2110647409', 'W2110847259', 'W2115108310', 'W2118600784', 'W2123696077', 'W2129925362', 'W2138154121', 'W2143396070', 'W2151337547', 'W2161112598', 'W2161289668', 'W2164090407', 'W2275877493', 'W2311052236', 'W2366536035', 'W2573152477', 'W2575837388', 'W2611684441', 'W4239330499'], 'abstract': 'For a decade, The Cancer Genome Atlas (TCGA) program collected clinicopathologic annotation data along with multi-platform molecular profiles of more than 11,000 human tumors across 33 different cancer types. TCGA clinical data contain key features representing the democratized nature of the data collection process. To ensure proper use of this large clinical dataset associated with genomic features, we developed a standardized dataset named the TCGA Pan-Cancer Clinical Data Resource (TCGA-CDR), which includes four major clinical outcome endpoints. In addition to detailing major challenges and statistical limitations encountered during the effort of integrating the acquired clinical data, we present a summary that includes endpoint usage recommendations for each cancer type. These TCGA-CDR findings appear to be consistent with cancer genomics studies independent of the TCGA effort and provide opportunities for investigating cancer biology using clinical correlates at an unprecedented scale.', 'counts_by_year': [[2022, 406], [2021, 426], [2020, 365], [2019, 165], [2018, 32]]}, {'id': 'W2270470215', 'doi': 'https://doi.org/10.3390/s16010115', 'title': 'Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition', 'type': 'journal-article', 'publication_date': '2016-01-18', 'host_venue': 'V101949793', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2144934268', ['I162608824']], ['A2100949428', ['I162608824']]], 'cited_by_count': 1387, 'concepts': [['C41008148', '0.7960399'], ['C121687571', '0.7594647'], ['C154945302', '0.72394854'], ['C81363708', '0.71423346'], ['C108583219', '0.66286105']], 'referenced_works': ['W1585811943', 'W1941659294', 'W1989496527', 'W1993761347', 'W2023302299', 'W2033310064', 'W2073401630', 'W2094149843', 'W2097575504', 'W2100495367', 'W2121269968', 'W2131368080', 'W2136435976', 'W2148857358'], 'abstract': "Human activity recognition (HAR) tasks have traditionally been solved using engineered features obtained by heuristic processes. Current research suggests that deep convolutional neural networks are suited to automate feature extraction from raw sensor inputs. However, human activities are made of complex sequences of motor movements, and capturing this temporal dynamics is fundamental for successful HAR. Based on the recent success of recurrent neural networks for time series domains, we propose a generic deep framework for activity recognition based on convolutional and LSTM recurrent units, which: (i) is suitable for multimodal wearable sensors; (ii) can perform sensor fusion naturally; (iii) does not require expert knowledge in designing features; and (iv) explicitly models the temporal dynamics of feature activations. We evaluate our framework on two datasets, one of which has been used in a public activity recognition challenge. Our results show that our framework outperforms competing deep non-recurrent networks on the challenge dataset by 4% on average; outperforming some of the previous reported results by up to 9%. Our results show that the framework can be applied to homogeneous sensor modalities, but can also fuse multimodal sensors to improve performance. We characterise key architectural hyperparameters' influence on performance to provide insights about their optimisation.", 'counts_by_year': [[2022, 191], [2021, 334], [2020, 315], [2019, 260], [2018, 185], [2017, 78], [2016, 19], [2015, 1]]}, {'id': 'W2563705555', 'doi': 'https://doi.org/10.1109/cvpr.2017.549', 'title': 'RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2151505639', ['I172675005']], ['A2099981787', ['I5681781']], ['A2139473605', ['I5681781']], ['A2120202076', ['I5681781']]], 'cited_by_count': 1380, 'concepts': [['C41008148', '0.8042723'], ['C70437156', '0.69807905'], ['C155512373', '0.67091596'], ['C154945302', '0.6281029'], ['C81363708', '0.6198949']], 'referenced_works': ['W1495267108', 'W1745334888', 'W1903029394', 'W1905829557', 'W1923115158', 'W1923184257', 'W2031489346', 'W2067912884', 'W2102605133', 'W2104408738', 'W2124592697', 'W2125215748', 'W2144794286', 'W2154815154', 'W2194775991', 'W2309415944', 'W2340897893', 'W2503931548', 'W2963108253', 'W2963563573'], 'abstract': 'Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.', 'counts_by_year': [[2022, 179], [2021, 347], [2020, 365], [2019, 346], [2018, 125], [2017, 15], [2015, 1]]}, {'id': 'W4211196614', 'doi': 'https://doi.org/10.1002/cpbi.3', 'title': 'Comparative Protein Structure Modeling Using MODELLER', 'type': 'journal-article', 'publication_date': '2016-06-01', 'host_venue': 'V4210177645', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2106342959', ['I180670191']], ['A2153528915', ['I180670191']]], 'cited_by_count': 1380, 'concepts': [['C193153401', '0.99342704'], ['C41008148', '0.6692521'], ['C66153294', '0.5628747'], ['C18051474', '0.5165795'], ['C82714645', '0.4485348']], 'referenced_works': ['W53898165', 'W1481137597', 'W1495099700', 'W1508596864', 'W1520470614', 'W1523469746', 'W1527979595', 'W1532202009', 'W1541134925', 'W1549740397', 'W1589812083', 'W1590020374', 'W1590648220', 'W1592870802', 'W1595347157', 'W1603416910', 'W1606011614', 'W1831325397', 'W1861776684', 'W1871470604', 'W1966040808', 'W1966679395', 'W1969051510', 'W1969211070', 'W1970197953', 'W1970999032', 'W1971149989', 'W1971853518', 'W1972436355', 'W1973503184', 'W1974885083', 'W1975591133', 'W1977689937', 'W1978304263', 'W1980374511', 'W1981264193', 'W1982481323', 'W1982533969', 'W1982679430', 'W1984530381', 'W1985408726', 'W1986191025', 'W1986665514', 'W1986743541', 'W1990698302', 'W1992192767', 'W1993418211', 'W1995413875', 'W1996253840', 'W1996357466', 'W1997075303', 'W1998586260', 'W1999569420', 'W1999613945', 'W1999645081', 'W2000464453', 'W2000661169', 'W2000878722', 'W2000908837', 'W2002992883', 'W2003331005', 'W2003852785', 'W2004228538', 'W2004500032', 'W2005631746', 'W2007176915', 'W2007804411', 'W2010458285', 'W2011752341', 'W2014719716', 'W2023477917', 'W2024912628', 'W2025204630', 'W2026185422', 'W2027408247', 'W2027536884', 'W2027638447', 'W2028079384', 'W2028590160', 'W2029377282', 'W2031901496', 'W2032643862', 'W2033227089', 'W2034100527', 'W2037312364', 'W2037711932', 'W2037960865', 'W2043078269', 'W2044788427', 'W2046607296', 'W2047163848', 'W2047444442', 'W2049247235', 'W2050956531', 'W2051210555', 'W2051476262', 'W2051831487', 'W2051969447', 'W2052002049', 'W2053449148', 'W2057447180', 'W2058336694', 'W2059324948', 'W2059385902', 'W2060259273', 'W2060809301', 'W2062327179', 'W2062693912', 'W2062957518', 'W2065283382', 'W2066162606', 'W2067098868', 'W2069454751', 'W2071486470', 'W2073758233', 'W2074231493', 'W2074534136', 'W2076694118', 'W2077648918', 'W2079093876', 'W2082667898', 'W2084466582', 'W2087064593', 'W2088933990', 'W2089035513', 'W2095036253', 'W2097225375', 'W2097546936', 'W2097767833', 'W2099075703', 'W2099946731', 'W2100444467', 'W2101220662', 'W2102122585', 'W2102245393', 'W2102470476', 'W2102993309', 'W2103273582', 'W2104095072', 'W2105801262', 'W2105844209', 'W2106066024', 'W2106154817', 'W2106241755', 'W2106648157', 'W2106882534', 'W2107644675', 'W2108376320', 'W2108551329', 'W2109402847', 'W2110890402', 'W2114520383', 'W2115451817', 'W2115854254', 'W2118200955', 'W2118387386', 'W2118619377', 'W2119899406', 'W2123085274', 'W2125677968', 'W2125725297', 'W2125793518', 'W2125898812', 'W2125977924', 'W2126016150', 'W2129153475', 'W2129169382', 'W2130060890', 'W2130479394', 'W2131736388', 'W2132428379', 'W2132926880', 'W2133549024', 'W2134815007', 'W2135484509', 'W2136995436', 'W2138741360', 'W2140244239', 'W2140508442', 'W2141120048', 'W2141885858', 'W2142131535', 'W2142529984', 'W2144357990', 'W2144362290', 'W2144483796', 'W2145268834', 'W2147059525', 'W2147770194', 'W2148557238', 'W2148830266', 'W2149480059', 'W2149758239', 'W2150072721', 'W2150627302', 'W2151631782', 'W2151792730', 'W2151831732', 'W2152301430', 'W2152844725', 'W2154733115', 'W2156563976', 'W2157925921', 'W2157975034', 'W2158605937', 'W2158714788', 'W2158999434', 'W2159614853', 'W2160378127', 'W2160672912', 'W2161062388', 'W2161899160', 'W2162751934', 'W2163312887', 'W2164750034', 'W2166281885', 'W2167612644', 'W2168107543', 'W2168508791', 'W2168770860', 'W2168885089', 'W2170471837', 'W2411804977', 'W2917125726', 'W3047916294', 'W3200464911', 'W4210290733', 'W4210400672', 'W4240401512', 'W4242729757', 'W4246356731', 'W4295888250'], 'abstract': 'Comparative protein structure modeling predicts the three-dimensional structure of a given protein sequence (target) based primarily on its alignment to one or more proteins of known structure (templates). The prediction process consists of fold assignment, target-template alignment, model building, and model evaluation. This unit describes how to calculate comparative models using the program MODELLER and how to use the ModBase database of such models, and discusses all four steps of comparative modeling, frequently observed errors, and some applications. Modeling lactate dehydrogenase from Trichomonas vaginalis (TvLDH) is described as an example. The download and installation of the MODELLER software is also described. © 2016 by John Wiley & Sons, Inc.', 'counts_by_year': [[2022, 318], [2021, 467], [2020, 306], [2019, 153], [2018, 75], [2017, 55], [2016, 6]]}, {'id': 'W2962902328', 'doi': 'https://doi.org/10.18653/v1/p16-1101', 'title': 'End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF', 'type': 'proceedings-article', 'publication_date': '2016-03-04', 'host_venue': 'V4306420508', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2250429641', ['I74973139']], ['A2046588481', ['I74973139']]], 'cited_by_count': 1376, 'concepts': [['C206134035', '0.9368003'], ['C41008148', '0.86368644'], ['C35639132', '0.7341838'], ['C2778827112', '0.69834673'], ['C154945302', '0.6938343']], 'referenced_works': ['W6908809', 'W581956982', 'W1515847863', 'W1533861849', 'W1632114991', 'W1677182931', 'W1815076433', 'W1899794420', 'W1940872118', 'W2004763266', 'W2008830554', 'W2045993505', 'W2056451646', 'W2064675550', 'W2095705004', 'W2096953947', 'W2101609803', 'W2104518905', 'W2111023066', 'W2114609248', 'W2116410915', 'W2119035792', 'W2130848543', 'W2130903752', 'W2136848157', 'W2137845336', 'W2139885235', 'W2143612262', 'W2144578941', 'W2147568880', 'W2147800946', 'W2147880316', 'W2153579005', 'W2156876426', 'W2158899491', 'W2168596788', 'W2170986599', 'W2249612659', 'W2250539671', 'W2250628419', 'W2250709962', 'W2250861254', 'W2251559320', 'W2251664617', 'W2251715934', 'W2296283641', 'W2308486447', 'W2399720833', 'W2949952998', 'W2963254740', 'W2963338481', 'W2963625095', 'W2963682821', 'W2963687836', 'W2964121744', 'W2964199361', 'W2964266863'], 'abstract': 'State-of-the-art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks — Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both datasets — 97.55% accuracy for POS tagging and 91.21% F1 for NER.', 'counts_by_year': [[2022, 100], [2021, 263], [2020, 326], [2019, 344], [2018, 237], [2017, 84], [2016, 21]]}, {'id': 'W2950985821', 'doi': 'https://doi.org/10.1371/journal.pcbi.1005752', 'title': 'mixOmics: An R package for ‘omics feature selection and multiple data integration', 'type': 'journal-article', 'publication_date': '2017-11-03', 'host_venue': 'V86033158', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2288576738', ['I165143802']], ['A2287760605', ['I165143802']], ['A3163022818', ['I141945490']], ['A2103009465', ['I165779595', 'I165143802']]], 'cited_by_count': 1371, 'concepts': [['C199163554', '0.6922599'], ['C157585117', '0.6001547'], ['C41008148', '0.569454'], ['C72634772', '0.53517574'], ['C148483581', '0.5283295']], 'referenced_works': ['W20683381', 'W1727290854', 'W1968206427', 'W1969098730', 'W1969729036', 'W1982866666', 'W1992549770', 'W2015961513', 'W2032230795', 'W2039514112', 'W2061997283', 'W2096315641', 'W2097057782', 'W2097360283', 'W2116662982', 'W2133199783', 'W2133554582', 'W2135046866', 'W2137211388', 'W2140652752', 'W2168117806', 'W2270456711', 'W2270970094', 'W2279408814', 'W2307267906', 'W2311949915', 'W2323165957', 'W2340210804', 'W2340637042', 'W2342543340', 'W2777005960', 'W2949244194', 'W2950445893', 'W4291745724', 'W4292053589', 'W4294541781'], 'abstract': "The advent of high throughput technologies has led to a wealth of publicly available 'omics data coming from different sources, such as transcriptomics, proteomics, metabolomics. Combining such large-scale biological data sets can lead to the discovery of important biological insights, provided that relevant information can be extracted in a holistic manner. Current statistical approaches have been focusing on identifying small subsets of molecules (a 'molecular signature') to explain or predict biological conditions, but mainly for a single type of 'omics. In addition, commonly used methods are univariate and consider each biological feature independently. We introduce mixOmics, an R package dedicated to the multivariate analysis of biological data sets with a specific focus on data exploration, dimension reduction and visualisation. By adopting a systems biology approach, the toolkit provides a wide range of methods that statistically integrate several data sets at once to probe relationships between heterogeneous 'omics data sets. Our recent methods extend Projection to Latent Structure (PLS) models for discriminant analysis, for data integration across multiple 'omics data or across independent studies, and for the identification of molecular signatures. We illustrate our latest mixOmics integrative frameworks for the multivariate analyses of 'omics data available from the package.", 'counts_by_year': [[2022, 331], [2021, 425], [2020, 319], [2019, 210], [2018, 72], [2017, 8], [2016, 1]]}, {'id': 'W2175094144', 'doi': 'https://doi.org/10.1093/bioinformatics/btv681', 'title': 'JSpeciesWS: a web server for prokaryotic species circumscription based on pairwise genome comparison', 'type': 'journal-article', 'publication_date': '2016-03-15', 'host_venue': 'V4210188263', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2100497465', ['I4210105988']], ['A681782119', ['I4210116941']], ['A1536378997', ['I193619901', 'I4210103014']], ['A2085026782', ['I4210105988']]], 'cited_by_count': 1369, 'concepts': [['C141231307', '0.7473321'], ['C62360110', '0.72880995'], ['C184898388', '0.5490322'], ['C71901391', '0.4911096'], ['C86803240', '0.45990795']], 'referenced_works': ['W2028513472', 'W2036562529', 'W2076027587', 'W2079222081', 'W2095513921', 'W2097477547', 'W2098969756', 'W2107282968', 'W2116206245', 'W2119888547', 'W2127036970', 'W2142678478', 'W2158714788'], 'abstract': 'Abstract Summary: JSpecies Web Server (JSpeciesWS) is a user-friendly online service for in silico calculating the extent of identity between two genomes, a parameter routinely used in the process of polyphasic microbial species circumscription. The service measures the average nucleotide identity (ANI) based on BLAST+ (ANIb) and MUMmer (ANIm), as well as correlation indexes of tetra-nucleotide signatures (Tetra). In addition, it provides a Tetra Correlation Search function, which allows to rapidly compare selected genomes against a continuously updated reference database with currently about 32 000 published whole and draft genome sequences. For comparison, own genomes can be uploaded and references can be selected from the JSpeciesWS reference database. The service indicates whether two genomes share genomic identities above or below the species embracing thresholds, and serves as a fast way to allocate unknown genomes in the frame of the hitherto sequenced species. Availability and implementation: JSpeciesWS is available at http://jspecies.ribohost.com/jspeciesws. Supplementary information: Supplementary data are available at Bioinformatics online. Contact: mrichter@ribocon.com', 'counts_by_year': [[2022, 268], [2021, 324], [2020, 315], [2019, 243], [2018, 142], [2017, 58], [2016, 18]]}, {'id': 'W2598457882', 'doi': 'https://doi.org/10.1111/mice.12263', 'title': 'Deep Learning-Based Crack Damage Detection Using Convolutional Neural Networks', 'type': 'journal-article', 'publication_date': '2017-05-01', 'host_venue': 'V206927758', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2492190690', ['I46247651']], ['A2359611365', ['I46247651']], ['A2004936390', ['I63966007']]], 'cited_by_count': 1367, 'concepts': [['C81363708', '0.81315917'], ['C108583219', '0.6806836'], ['C154945302', '0.57759655'], ['C41008148', '0.5624356'], ['C119857082', '0.36165977']], 'referenced_works': ['W1505943987', 'W1513670756', 'W1523493493', 'W1596173341', 'W1806891645', 'W1920794317', 'W1941370076', 'W1955857676', 'W1963882359', 'W1975831815', 'W1981066713', 'W1986969925', 'W2005029343', 'W2020797024', 'W2021086697', 'W2022909534', 'W2023920199', 'W2028660383', 'W2048152057', 'W2055122019', 'W2071905184', 'W2098180043', 'W2101632303', 'W2103559027', 'W2106238010', 'W2112796928', 'W2122240896', 'W2126052502', 'W2156163116', 'W2160918136', 'W2169303706', 'W2282462020', 'W2478677992', 'W2549940417', 'W2919115771', 'W4255949318'], 'abstract': 'A number of image processing techniques IPTs have been implemented for detecting civil infrastructure defects to partially replace human-conducted onsite inspections. These IPTs are primarily used to manipulate images to extract defect features, such as cracks in concrete and steel surfaces. However, the extensively varying real-world situations e.g., lighting and shadow changes can lead to challenges to the wide adoption of IPTs. To overcome these challenges, this article proposes a vision-based method using a deep architecture of convolutional neural networks CNNs for detecting concrete cracks without calculating the defect features. As CNNs are capable of learning image features automatically, the proposed method works without the conjugation of IPTs for extracting features. The designed CNN is trained on 40 K images of 256 × 256 pixel resolutions and, consequently, records with about 98% accuracy. The trained CNN is combined with a sliding window technique to scan any image size larger than 256 × 256 pixel resolutions. The robustness and adaptability of the proposed approach are tested on 55 images of 5,888 × 3,584 pixel resolutions taken from a different structure which is not used for training and validation processes under various conditions e.g., strong light spot, shadows, and very thin cracks. Comparative studies are conducted to examine the performance of the proposed CNN using traditional Canny and Sobel edge detection methods. The results show that the proposed method shows quite better performances and can indeed find concrete cracks in realistic situations.', 'counts_by_year': [[2022, 312], [2021, 355], [2020, 331], [2019, 224], [2018, 127], [2017, 15]]}, {'id': 'W2256362396', 'doi': 'https://doi.org/10.1109/tip.2016.2598681', 'title': 'DehazeNet: An End-to-End System for Single Image Haze Removal', 'type': 'journal-article', 'publication_date': '2016-11-01', 'host_venue': 'V4210173141', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2224194168', ['I90610280']], ['A2293405630', ['I90610280']], ['A1983094374', ['I90610280']], ['A2158968104', ['I90610280']], ['A2104129307', ['I114017466']]], 'cited_by_count': 1361, 'concepts': [['C41008148', '0.78589034'], ['C185798385', '0.6993633'], ['C81363708', '0.6783442'], ['C79974267', '0.6718676'], ['C154945302', '0.67179716']], 'referenced_works': ['W54257720', 'W1494869093', 'W1885185971', 'W1951319388', 'W1973567017', 'W1980939919', 'W2002299629', 'W2003709967', 'W2007692451', 'W2028990532', 'W2035773017', 'W2065002911', 'W2077862666', 'W2081418206', 'W2084464972', 'W2097117768', 'W2097900287', 'W2099712288', 'W2100495367', 'W2102605133', 'W2104974755', 'W2110644821', 'W2112796928', 'W2114867966', 'W2117539524', 'W2125188192', 'W2128254161', 'W2133665775', 'W2147318913', 'W2150796457', 'W2154815154', 'W2155479981', 'W2155893237', 'W2156936307', 'W2161126623', 'W2163337806', 'W2168078737', 'W2536722097', 'W2912104034', 'W3102431071'], 'abstract': 'Single image haze removal is a challenging ill-posed problem. Existing methods use various constraints/priors to get plausible dehazing solutions. The key to achieve haze removal is to estimate a medium transmission map for an input hazy image. In this paper, we propose a trainable end-to-end system called DehazeNet, for medium transmission estimation. DehazeNet takes a hazy image as input, and outputs its medium transmission map that is subsequently used to recover a haze-free image via atmospheric scattering model. DehazeNet adopts Convolutional Neural Networks (CNN) based deep architecture, whose layers are specially designed to embody the established assumptions/priors in image dehazing. Specifically, layers of Maxout units are used for feature extraction, which can generate almost all haze-relevant features. We also propose a novel nonlinear activation function in DehazeNet, called Bilateral Rectified Linear Unit (BReLU), which is able to improve the quality of recovered haze-free image. We establish connections between components of the proposed DehazeNet and those used in existing methods. Experiments on benchmark images show that DehazeNet achieves superior performance over existing methods, yet keeps efficient and easy to use.', 'counts_by_year': [[2022, 222], [2021, 358], [2020, 301], [2019, 235], [2018, 175], [2017, 63], [2016, 7]]}, {'id': 'W2990747873', 'doi': 'https://doi.org/10.1109/mcom.001.1900107', 'title': 'Towards Smart and Reconfigurable Environment: Intelligent Reflecting Surface Aided Wireless Network', 'type': 'journal-article', 'publication_date': '2020-01-01', 'host_venue': 'V158797327', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2311797224', ['I165932596']], ['A2413204075', ['I165932596']]], 'cited_by_count': 1347, 'concepts': [['C41008148', '0.82561636'], ['C555944384', '0.56791604'], ['C108037233', '0.48737532'], ['C31258907', '0.46000555'], ['C149635348', '0.40295365']], 'referenced_works': ['W2023535408', 'W2082432933', 'W2103972037', 'W2118588134', 'W2160593627', 'W2552158593', 'W2735938380', 'W2794344156', 'W2795167939', 'W2805166665', 'W2891354184', 'W2963460596', 'W2964129085', 'W2969424089'], 'abstract': 'IRS is a new and revolutionizing technology that is able to significantly improve the performance of wireless communication networks, by smartly reconfiguring the wireless propagation environment with the use of massive low-cost passive reflecting elements integrated on a planar surface. Specifically, different elements of an IRS can independently reflect the incident signal by controlling its amplitude and/or phase and thereby collaboratively achieve fine-grained 3D passive beamforming for directional signal enhancement or nulling. In this article, we first provide an overview of the IRS technology, including its main applications in wireless communication, competitive advantages over existing technologies, hardware architecture as well as the corresponding new signal model. We then address the key challenges in designing and implementing the new IRS-aided hybrid (with both active and passive components) wireless network, as compared to the traditional network comprising active components only. Finally, numerical results are provided to show the great performance enhancement with the use of IRS in typical wireless networks.', 'counts_by_year': [[2022, 404], [2021, 579], [2020, 317], [2019, 45], [2018, 1]]}, {'id': 'W2884001105', 'doi': 'https://doi.org/10.1109/mci.2018.2840738', 'title': 'Recent Trends in Deep Learning Based Natural Language Processing [Review Article]', 'type': 'journal-article', 'publication_date': '2018-07-20', 'host_venue': 'V104797584', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2607330899', ['I125839683']], ['A2543255112', ['I165932596']], ['A1992239148', ['I172675005']], ['A2806062742', ['I172675005']]], 'cited_by_count': 1345, 'concepts': [['C41008148', '0.7725005'], ['C154945302', '0.6243069'], ['C108583219', '0.55191904'], ['C195324797', '0.5081117'], ['C204321447', '0.46346536']], 'referenced_works': ['W1518951372', 'W1662133657', 'W1832693441', 'W1895577753', 'W1905882502', 'W1975244201', 'W1985258458', 'W1995562189', 'W2028140375', 'W2064675550', 'W2087946919', 'W2101105183', 'W2114590102', 'W2115101920', 'W2117130368', 'W2117352571', 'W2117671523', 'W2119717200', 'W2120615054', 'W2131876387', 'W2134036914', 'W2136016850', 'W2142192571', 'W2143612262', 'W2152311353', 'W2155893237', 'W2157331557', 'W2161222299', 'W2163455955', 'W2171928131', 'W2250539671', 'W2250644439', 'W2250861254', 'W2250879510', 'W2250966211', 'W2250999640', 'W2251103205', 'W2251143283', 'W2251287417', 'W2251599843', 'W2251805006', 'W2296283641', 'W2427312199', 'W2556418146', 'W2561368124', 'W2562607067', 'W2581637843', 'W2605078375', 'W2618530766', 'W2734205292', 'W2734389934', 'W2739862888', 'W2740550900', 'W2740765036', 'W2741613777', 'W2760392765', 'W2766736793', 'W2786411768', 'W2919115771', 'W2962854379', 'W2962965405', 'W2963042536', 'W2963064439', 'W2963167310', 'W2963355447', 'W2963421945', 'W2963572611', 'W2963583512', 'W2963599948', 'W2963682821', 'W2963956670', 'W2964010806', 'W2964266863', 'W2964275331', 'W2964352131', 'W4205896512', 'W4241645538', 'W4254816979'], 'abstract': 'Deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.', 'counts_by_year': [[2022, 349], [2021, 406], [2020, 375], [2019, 198], [2018, 15]]}, {'id': 'W2963363373', 'doi': 'https://doi.org/10.1109/iccv.2017.155', 'title': 'Channel Pruning for Accelerating Very Deep Neural Networks', 'type': 'proceedings-article', 'publication_date': '2017-10-01', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2980528185', ['I74973139']], ['A2499063207', ['I87445476']], ['A2200192130', ['I87445476']]], 'cited_by_count': 1345, 'concepts': [['C41008148', '0.7913401'], ['C108010975', '0.74297357'], ['C81363708', '0.66005385'], ['C11413529', '0.565822'], ['C127162648', '0.53379524']], 'referenced_works': ['W1536680647', 'W2070094080', 'W2097117768', 'W2104636679', 'W2108598243', 'W2112796928', 'W2123469553', 'W2135046866', 'W2285660444', 'W3138798301'], 'abstract': 'In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks.Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by 5x speed-up along with only 0.3% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4%, 1.0% accuracy loss under 2x speed-up respectively, which is significant. Code has been made publicly available.', 'counts_by_year': [[2022, 161], [2021, 407], [2020, 399], [2019, 280], [2018, 95], [2017, 2], [2016, 1]]}, {'id': 'W2963918968', 'doi': 'https://doi.org/10.1109/cvpr.2019.00293', 'title': 'MnasNet: Platform-Aware Neural Architecture Search for Mobile', 'type': 'proceedings-article', 'publication_date': '2019-06-01', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2231381296', ['I1291425158']], ['A2601901235', ['I1291425158']], ['A2056797953', ['I1291425158']], ['A2089062156', ['I1291425158']], ['A1973675100', ['I1291425158']], ['A2612173839', ['I1291425158']], ['A2911865271', ['I1291425158']]], 'cited_by_count': 1345, 'concepts': [['C41008148', '0.85125947'], ['C81363708', '0.61993474'], ['C82876162', '0.5853433'], ['C2776214188', '0.56825346'], ['C186967261', '0.52771443']], 'referenced_works': ['W2117539524', 'W2194775991', 'W2198098822', 'W2570343428', 'W2752782242', 'W2808938483', 'W2886851211', 'W2962697884', 'W2962861284', 'W2963122961', 'W2963125010', 'W2963163009', 'W2963993763', 'W2964081807', 'W2964217527', 'W2965658867'], 'abstract': 'Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x faster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than NASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet', 'counts_by_year': [[2022, 166], [2021, 549], [2020, 465], [2019, 143], [2018, 20], [2017, 1]]}, {'id': 'W2585392941', 'doi': 'https://doi.org/10.1016/j.advengsoft.2017.01.004', 'title': 'Grasshopper Optimisation Algorithm: Theory and application', 'type': 'journal-article', 'publication_date': '2017-03-01', 'host_venue': 'V16540516', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2114835760', ['I2188195594']], ['A3006208000', ['I2188195594']], ['A2166930145', ['I11701301']]], 'cited_by_count': 1341, 'concepts': [['C2778575915', '0.66985154'], ['C11413529', '0.54261005'], ['C41008148', '0.48755258'], ['C126255220', '0.40985546'], ['C33923547', '0.3332563']], 'referenced_works': ['W1570180898', 'W1595159159', 'W1968964720', 'W1974336701', 'W1983362686', 'W1984130668', 'W1990966828', 'W1995668158', 'W1997188340', 'W2000621750', 'W2001979953', 'W2002713790', 'W2003890325', 'W2006694777', 'W2007357498', 'W2012333217', 'W2020320008', 'W2024060531', 'W2050927287', 'W2061001542', 'W2061438946', 'W2065401134', 'W2065682543', 'W2072955302', 'W2076608183', 'W2083000360', 'W2091638274', 'W2103065366', 'W2118044993', 'W2122621647', 'W2140796089', 'W2143055330', 'W2147271386', 'W2151554678', 'W2154756201', 'W2154943049', 'W2161494499', 'W2167580870', 'W2209380745', 'W4246598646', 'W4250503569', 'W4251612499'], 'abstract': 'The Grasshopper Optimisation Algorithm inspired by grasshopper swarms is proposed.The GOA algorithm is benchmarked on challenging test functions.The results on the unimodal functions show the superior exploitation of GOA.The exploration ability of GOA is confirmed by the results on multimodal and composite functions.The results on structural design problems confirm the performance of GOA in practice. This paper proposes an optimisation algorithm called Grasshopper Optimisation Algorithm (GOA) and applies it to challenging problems in structural optimisation. The proposed algorithm mathematically models and mimics the behaviour of grasshopper swarms in nature for solving optimisation problems. The GOA algorithm is first benchmarked on a set of test problems including CEC2005 to test and verify its performance qualitatively and quantitatively. It is then employed to find the optimal shape for a 52-bar truss, 3-bar truss, and cantilever beam to demonstrate its applicability. The results show that the proposed algorithm is able to provide superior results compared to well-known and recent algorithms in the literature. The results of the real applications also prove the merits of GOA in solving real problems with unknown search spaces.', 'counts_by_year': [[2022, 352], [2021, 368], [2020, 332], [2019, 186], [2018, 80], [2017, 15]]}, {'id': 'W2963839617', 'doi': 'https://doi.org/10.1109/fg.2018.00020', 'title': 'VGGFace2: A Dataset for Recognising Faces across Pose and Age', 'type': 'proceedings-article', 'publication_date': '2018-05-15', 'host_venue': 'V4306418650', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2493979725', ['I40120149']], ['A2236987923', ['I40120149']], ['A2413558386', ['I40120149']], ['A2066024570', ['I40120149']], ['A2469405535', ['I40120149']]], 'cited_by_count': 1341, 'concepts': [['C41008148', '0.76521146'], ['C81363708', '0.67996264'], ['C154945302', '0.6709465'], ['C31510193', '0.5992742'], ['C153180895', '0.5464226']], 'referenced_works': ['W1921147789', 'W1949778830', 'W1984309565', 'W1998808035', 'W2019464758', 'W2024922353', 'W2194775991', 'W2239239723', 'W2325939864', 'W2736633948', 'W2963559058', 'W2963570419', 'W2963671154', 'W2964228922', 'W3099206234', 'W3101998545'], 'abstract': 'In this paper, we introduce a new large-scale face dataset named VGGFace2. The dataset contains 3.31 million images of 9131 subjects, with an average of 362.6 images for each subject. Images are downloaded from Google Image Search and have large variations in pose, age, illumination, ethnicity and profession (e.g. actors, athletes, politicians). The dataset was collected with three goals in mind: (i) to have both a large number of identities and also a large number of images for each identity; (ii) to cover a large range of pose, age and ethnicity; and (iii) to minimize the label noise. We describe how the dataset was collected, in particular the automated and manual filtering stages to ensure a high accuracy for the images of each identity. To assess face recognition performance using the new dataset, we train ResNet-50 (with and without Squeeze-and-Excitation blocks) Convolutional Neural Networks on VGGFace2, on MS- Celeb-1M, and on their union, and show that training on VGGFace2 leads to improved recognition performance over pose and age. Finally, using the models trained on these datasets, we demonstrate state-of-the-art performance on all the IARPA Janus face recognition benchmarks, e.g. IJB-A, IJB-B and IJB-C, exceeding the previous state-of-the-art by a large margin. Datasets and models are publicly available.', 'counts_by_year': [[2022, 182], [2021, 428], [2020, 410], [2019, 256], [2018, 62], [2017, 2], [2012, 1]]}, {'id': 'W2933138175', 'doi': 'https://doi.org/10.18653/v1/n19-4009', 'title': 'fairseq: A Fast, Extensible Toolkit for Sequence Modeling', 'type': 'proceedings-article', 'publication_date': '2019-04-01', 'host_venue': 'V4306420633', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2166285282', ['I2252078561']], ['A2768388130', ['I2252078561']], ['A2893542027', ['I2252078561']], ['A2722765719', ['I2252078561']], ['A2295347290', ['I2252078561']], ['A2927220563', []], ['A2959607770', ['I1291425158']], ['A2139710560', ['I2252078561']]], 'cited_by_count': 1340, 'concepts': [['C41008148', '0.8872668'], ['C170858558', '0.831215'], ['C199360897', '0.5668017'], ['C32833848', '0.5472153'], ['C2776214188', '0.5125456']], 'referenced_works': ['W1544827683', 'W1902237438', 'W1938755728', 'W2154652894', 'W2259472270', 'W2606974598', 'W2612675303', 'W2633911929', 'W2767989436', 'W2778814079', 'W2792376130', 'W2795285343', 'W2797328513', 'W2804704270', 'W2807964741', 'W2888482885', 'W2889326796', 'W2889518897', 'W2890914727', 'W2914855263', 'W2949555952', 'W2951672049', 'W2952339051', 'W2962784628', 'W2962805889', 'W2962974452', 'W2963034893', 'W2963096510', 'W2963112338', 'W2963206679', 'W2963212250', 'W2963263347', 'W2963366552', 'W2963403868', 'W2963418779', 'W2963475460', 'W2963532001', 'W2963631907', 'W2963773505', 'W2963807318', 'W2963925437', 'W2963929190', 'W2963970792', 'W2964190861', 'W2964258094', 'W2964265128'], 'abstract': 'fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto', 'counts_by_year': [[2022, 100], [2021, 627], [2020, 486], [2019, 125], [2018, 1]]}, {'id': 'W2963155035', 'doi': 'https://doi.org/10.1109/cvpr.2018.00675', 'title': 'A Closer Look at Spatiotemporal Convolutions for Action Recognition', 'type': 'proceedings-article', 'publication_date': '2018-04-12', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2810588857', ['I2252078561']], ['A2679504641', ['I2252078561']], ['A200106589', ['I2252078561']], ['A2747852184', ['I107672454', 'I2252078561']], ['A2053214915', ['I2252078561']], ['A975267881', ['I2252078561']]], 'cited_by_count': 1340, 'concepts': [['C2987834672', '0.8838781'], ['C41008148', '0.723737'], ['C155512373', '0.7146251'], ['C2777210771', '0.61715037'], ['C154945302', '0.60683864']], 'referenced_works': ['W1522734439', 'W1573040851', 'W1578985305', 'W1923404803', 'W1947481528', 'W1950136256', 'W1983364832', 'W1999192586', 'W2016053056', 'W2024868105', 'W2063153269', 'W2097117768', 'W2105101328', 'W2108333036', 'W2119799051', 'W2126579184', 'W2194775991', 'W2342662179', 'W2471695703', 'W2533739470', 'W2563717578', 'W2608988379', 'W2963446712', 'W2963524571', 'W2963820951', 'W2964191259', 'W2964214371'], 'abstract': 'In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly gains in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block R(2+1)D which produces CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101, and HMDB51.', 'counts_by_year': [[2022, 204], [2021, 518], [2020, 364], [2019, 208], [2018, 42], [2017, 3], [2012, 1]]}, {'id': 'W2230652337', 'doi': 'https://doi.org/10.1109/tie.2015.2478397', 'title': 'Disturbance-Observer-Based Control and Related Methods—An Overview', 'type': 'journal-article', 'publication_date': '2016-01-01', 'host_venue': 'V58031724', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2136014268', ['I143804889']], ['A2465609154', ['I76569877']], ['A2268258939', ['I82880672']], ['A2129015881', ['I76569877']]], 'cited_by_count': 1334, 'concepts': [['C47446073', '0.6430942'], ['C2777601987', '0.50037193'], ['C41008148', '0.48008257'], ['C133731056', '0.45161864'], ['C2775924081', '0.38056108']], 'referenced_works': ['W633814648', 'W1595055308', 'W1603314374', 'W1965585748', 'W1970194488', 'W1973102996', 'W1978555529', 'W1982182342', 'W1983687325', 'W1993709370', 'W1998230610', 'W2006107734', 'W2008047298', 'W2008217681', 'W2014638583', 'W2026676067', 'W2027057139', 'W2036099961', 'W2040345264', 'W2047035041', 'W2051194902', 'W2053406538', 'W2054938976', 'W2057759400', 'W2062244411', 'W2067863383', 'W2069919210', 'W2071745233', 'W2075133506', 'W2077258597', 'W2080238122', 'W2080630593', 'W2083514989', 'W2087050671', 'W2091206412', 'W2103345351', 'W2106352892', 'W2107122393', 'W2109276551', 'W2110047814', 'W2111055830', 'W2111566172', 'W2117596346', 'W2118706662', 'W2126613641', 'W2135493101', 'W2136791643', 'W2137048068', 'W2138327849', 'W2142205210', 'W2144860711', 'W2145887311', 'W2147164650', 'W2147374670', 'W2147578878', 'W2149459291', 'W2155721335', 'W2158675660', 'W2161649955', 'W2163114046', 'W2165624689', 'W2167808181', 'W2167875635', 'W2169344394', 'W2169917987', 'W2172286211', 'W2212491487', 'W3196167895', 'W4255626245', 'W4292872153'], 'abstract': 'Disturbance-observer-based control (DOBC) and related methods have been researched and applied in various industrial sectors in the last four decades. This survey, at first time, gives a systematic and comprehensive tutorial and summary on the existing disturbance/uncertainty estimation and attenuation techniques, most notably, DOBC, active disturbance rejection control, disturbance accommodation control, and composite hierarchical antidisturbance control. In all of these methods, disturbance and uncertainty are, in general, lumped together, and an observation mechanism is employed to estimate the total disturbance. This paper first reviews a number of widely used linear and nonlinear disturbance/uncertainty estimation techniques and then discusses and compares various compensation techniques and the procedures of integrating disturbance/uncertainty compensation with a (predesigned) linear/nonlinear controller. It also provides concise tutorials of the main methods in this area with clear descriptions of their features. The application of this group of methods in various industrial sections is reviewed, with emphasis on the commercialization of some algorithms. The survey is ended with the discussion of future directions.', 'counts_by_year': [[2022, 185], [2021, 304], [2020, 253], [2019, 226], [2018, 182], [2017, 114], [2016, 62], [2015, 1]]}, {'id': 'W2944727022', 'doi': 'https://doi.org/10.1038/s41586-019-1186-3', 'title': 'Next-generation characterization of the Cancer Cell Line Encyclopedia', 'type': 'journal-article', 'publication_date': '2019-05-08', 'host_venue': 'V137773608', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2066826771', ['I107606265']], ['A2201471881', ['I107606265']], ['A70017314', ['I107606265']], ['A1990462511', ['I107606265']], ['A2794089548', ['I107606265']], ['A2109270128', ['I1283582996']], ['A406210710', ['I1283582996']], ['A2469218141', ['I107606265']], ['A2184761863', ['I107606265']], ['A2886964938', ['I107606265']], ['A3015418401', ['I107606265']], ['A2944360925', ['I107606265']], ['A2125149125', ['I107606265']], ['A2158396693', ['I107606265']], ['A2404020724', ['I107606265']], ['A2061081861', ['I107606265']], ['A2162600841', ['I107606265']], ['A2944487928', ['I107606265']], ['A1999595909', ['I107606265']], ['A2141598061', []], ['A1227741941', ['I1343551460']], ['A2121421940', ['I1343551460']], ['A2599420692', []], ['A2135172034', []], ['A2320029069', ['I1283582996']], ['A2944800764', ['I107606265']], ['A2339782197', ['I107606265']], ['A3093284208', ['I107606265']], ['A2983593642', ['I205783295']], ['A2943995681', ['I107606265']], ['A2170293292', ['I107606265']], ['A2493247980', ['I1283582996']], ['A2154223817', ['I1283582996']], ['A1995741108', ['I1283582996']], ['A2109001467', ['I1283582996']], ['A2761499030', ['I1283582996']], ['A2244685270', ['I1283582996']], ['A2952181309', ['I1283582996']], ['A2149666325', ['I1283582996']], ['A2522861075', ['I1283582996']], ['A2084280152', ['I1283582996']], ['A2154786688', ['I107606265']], ['A2752216145', ['I107606265']], ['A1994507919', ['I107606265']], ['A2943938592', ['I107606265']], ['A2735646687', ['I107606265']], ['A2186935295', ['I107606265']], ['A2013598389', ['I1283582996']], ['A1977842172', ['I107606265']], ['A1921601128', ['I205783295']], ['A2425263847', ['I107606265']], ['A180898208', ['I107606265']], ['A320370931', ['I107606265']], ['A2191548731', ['I107606265']], ['A2097611485', ['I107606265']], ['A2163448164', ['I136199984']], ['A2152116782', ['I136199984']], ['A2593873559', ['I107606265']], ['A2372461298', ['I1283582996']], ['A1592760610', ['I1283582996']], ['A2306370627', ['I1283582996']], ['A2126498025', ['I107606265']], ['A2160087672', []], ['A2108643002', ['I1343551460']], ['A2421021785', ['I107606265']], ['A1997993096', ['I107606265']], ['A256401212', ['I107606265']], ['A2148635917', ['I107606265']]], 'cited_by_count': 1334, 'concepts': [['C148863701', '0.69234955'], ['C2780841128', '0.49982572'], ['C121608353', '0.441396'], ['C70721500', '0.3855633'], ['C161191863', '0.3035221']], 'referenced_works': ['W1812256879', 'W1848939527', 'W1883773314', 'W1964959571', 'W1975277357', 'W1980462742', 'W1993391636', 'W2010522003', 'W2011474181', 'W2025113307', 'W2036480824', 'W2043398720', 'W2046585589', 'W2070569107', 'W2081098333', 'W2096173332', 'W2104500293', 'W2114843025', 'W2115875921', 'W2130979840', 'W2131374955', 'W2133046153', 'W2135760976', 'W2138601221', 'W2146512944', 'W2149441684', 'W2150162153', 'W2150194502', 'W2159220884', 'W2160443244', 'W2163327652', 'W2166468632', 'W2170490448', 'W2174441123', 'W2221710132', 'W2256016639', 'W2294814189', 'W2294990791', 'W2318806991', 'W2343368549', 'W2407075689', 'W2461427403', 'W2468923062', 'W2557547609', 'W2558475804', 'W2559515797', 'W2567964444', 'W2588627862', 'W2600889385', 'W2607938959', 'W2735132087', 'W2739511077', 'W2740246399', 'W2741508285', 'W2756358680', 'W2761275051', 'W2765468373', 'W2799563655', 'W2886855629', 'W2944723276', 'W2950833487', 'W4294541781'], 'abstract': 'Large panels of comprehensively characterized human cancer models, including the Cancer Cell Line Encyclopedia (CCLE), have provided a rigorous framework with which to study genetic variants, candidate targets, and small-molecule and biological therapeutics and to identify new marker-driven cancer dependencies. To improve our understanding of the molecular features that contribute to cancer phenotypes, including drug responses, here we have expanded the characterizations of cancer cell lines to include genetic, RNA splicing, DNA methylation, histone H3 modification, microRNA expression and reverse-phase protein array data for 1,072 cell lines from individuals of various lineages and ethnicities. Integration of these data with functional characterizations such as drug-sensitivity, short hairpin RNA knockdown and CRISPR-Cas9 knockout data reveals potential targets for cancer drugs and associated biomarkers. Together, this dataset and an accompanying public data portal provide a resource for the acceleration of cancer research using model cancer cell lines.', 'counts_by_year': [[2022, 382], [2021, 566], [2020, 311], [2019, 73], [2018, 1]]}, {'id': 'W3012789146', 'doi': 'https://doi.org/10.1016/s2468-2667(20)30073-6', 'title': 'The effect of control strategies to reduce social mixing on outcomes of the COVID-19 epidemic in Wuhan, China: a modelling study', 'type': 'journal-article', 'publication_date': '2020-03-25', 'host_venue': 'V2764808104', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2652325360', ['I4210089966']], ['A3007838758', ['I4210089966']], ['A3005524366', ['I4210089966']], ['A2094073093', ['I4210089966']], ['A1784943301', ['I4210089966']], ['A2767609261', ['I4210089966']], ['A2231526214', []], ['A1966109781', ['I4210089966']], ['A2530399487', ['I4210089966']], ['A2889493969', []], ['A2835261579', []], ['A3009321980', []], ['A2796631224', []], ['A3004279427', []], ['A11549829', []], ['A3009512101', []], ['A3005493534', []], ['A3006242220', []], ['A2889721592', []], ['A3001207621', []], ['A2644929985', []], ['A2239392000', []], ['A3006236200', []], ['A2765635069', []], ['A2231526214', ['I4210089966']], ['A2777073030', ['I4210089966']]], 'cited_by_count': 1333, 'concepts': [['C116675565', '0.79691553'], ['C172656115', '0.7059632'], ['C191935318', '0.5290317'], ['C3008058167', '0.5281299'], ['C761482', '0.5048688']], 'referenced_works': ['W1964326221', 'W1965499304', 'W1981274325', 'W2016577729', 'W2032559946', 'W2034051093', 'W2096145431', 'W2101311852', 'W2128788856', 'W2150053999', 'W2154416797', 'W2156158169', 'W2159301256', 'W2161944342', 'W2755849266', 'W2804572859', 'W2965852633', 'W3001897055', 'W3002539152', 'W3003573988', 'W3003668884', 'W3004026249', 'W3004313240', 'W3004542065', 'W3004912618', 'W3005151538', 'W3006659024', 'W3009003996', 'W3009468976', 'W3009577418', 'W3009909874'], 'abstract': '<h2>Summary</h2><h3>Background</h3> In December, 2019, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), a novel coronavirus, emerged in Wuhan, China. Since then, the city of Wuhan has taken unprecedented measures in response to the outbreak, including extended school and workplace closures. We aimed to estimate the effects of physical distancing measures on the progression of the COVID-19 epidemic, hoping to provide some insights for the rest of the world. <h3>Methods</h3> To examine how changes in population mixing have affected outbreak progression in Wuhan, we used synthetic location-specific contact patterns in Wuhan and adapted these in the presence of school closures, extended workplace closures, and a reduction in mixing in the general community. Using these matrices and the latest estimates of the epidemiological parameters of the Wuhan outbreak, we simulated the ongoing trajectory of an outbreak in Wuhan using an age-structured susceptible-exposed-infected-removed (SEIR) model for several physical distancing measures. We fitted the latest estimates of epidemic parameters from a transmission model to data on local and internationally exported cases from Wuhan in an age-structured epidemic framework and investigated the age distribution of cases. We also simulated lifting of the control measures by allowing people to return to work in a phased-in way and looked at the effects of returning to work at different stages of the underlying outbreak (at the beginning of March or April). <h3>Findings</h3> Our projections show that physical distancing measures were most effective if the staggered return to work was at the beginning of April; this reduced the median number of infections by more than 92% (IQR 66–97) and 24% (13–90) in mid-2020 and end-2020, respectively. There are benefits to sustaining these measures until April in terms of delaying and reducing the height of the peak, median epidemic size at end-2020, and affording health-care systems more time to expand and respond. However, the modelled effects of physical distancing measures vary by the duration of infectiousness and the role school children have in the epidemic. <h3>Interpretation</h3> Restrictions on activities in Wuhan, if maintained until April, would probably help to delay the epidemic peak. Our projections suggest that premature and sudden lifting of interventions could lead to an earlier secondary peak, which could be flattened by relaxing the interventions gradually. However, there are limitations to our analysis, including large uncertainties around estimates of <i>R</i><sub>0</sub> and the duration of infectiousness. <h3>Funding</h3> Bill & Melinda Gates Foundation, National Institute for Health Research, Wellcome Trust, and Health Data Research UK.', 'counts_by_year': [[2022, 187], [2021, 541], [2020, 602], [2019, 1]]}, {'id': 'W2984472267', 'doi': 'https://doi.org/10.1038/s41592-019-0619-0', 'title': 'Fast, sensitive and accurate integration of single-cell data with Harmony', 'type': 'journal-article', 'publication_date': '2019-11-18', 'host_venue': 'V127827428', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2603015496', ['I1283280774']], ['A2947484615', ['I1283280774']], ['A2613994738', ['I136199984']], ['A1827129742', ['I1283280774']], ['A2647312634', ['I1283280774']], ['A2170901014', ['I1283280774']], ['A1722124649', ['I1283280774']], ['A2116003597', ['I1283280774']], ['A2164552791', ['I1283280774', 'I136199984', 'I107606265']], ['A2119223456', ['I1283280774']]], 'cited_by_count': 1332, 'concepts': [['C2776453491', '0.7992069'], ['C86803240', '0.5166962'], ['C41008148', '0.50212574'], ['C70721500', '0.47997046'], ['C33099171', '0.45223233']], 'referenced_works': ['W1790954942', 'W1977968768', 'W1986911601', 'W2007091385', 'W2016753357', 'W2025653905', 'W2050332440', 'W2070050178', 'W2082572780', 'W2093774827', 'W2103017472', 'W2123491442', 'W2137499573', 'W2146512944', 'W2151820006', 'W2166542012', 'W2169456326', 'W2345356016', 'W2472063172', 'W2523369352', 'W2551194178', 'W2604808360', 'W2753908609', 'W2766437506', 'W2770178180', 'W2789497201', 'W2791033272', 'W2794480084', 'W2794521141', 'W2794891507', 'W2810097927', 'W2889326414', 'W2899224111', 'W2902652978', 'W2905317377', 'W2912272542', 'W2942880491', 'W2944872709', 'W2949631571', 'W2951506174', 'W2967177832'], 'abstract': 'The emerging diversity of single-cell RNA-seq datasets allows for the full transcriptional characterization of cell types across a wide variety of biological and clinical conditions. However, it is challenging to analyze them together, particularly when datasets are assayed with different technologies, because biological and technical differences are interspersed. We present Harmony (https://github.com/immunogenomics/harmony), an algorithm that projects cells into a shared embedding in which cells group by cell type rather than dataset-specific conditions. Harmony simultaneously accounts for multiple experimental and biological factors. In six analyses, we demonstrate the superior performance of Harmony to previously published algorithms while requiring fewer computational resources. Harmony enables the integration of ~106 cells on a personal computer. We apply Harmony to peripheral blood mononuclear cells from datasets with large experimental differences, five studies of pancreatic islet cells, mouse embryogenesis datasets and the integration of scRNA-seq with spatial transcriptomics data.', 'counts_by_year': [[2022, 592], [2021, 507], [2020, 207], [2019, 21]]}, {'id': 'W2474281075', 'doi': 'https://doi.org/10.1109/tpami.2017.2658577', 'title': 'Direct Sparse Odometry', 'type': 'journal-article', 'publication_date': '2018-03-01', 'host_venue': 'V199944782', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2163642868', ['I1297596422']], ['A1809196549', ['I1343180700']], ['A2153496801', ['I62916508']]], 'cited_by_count': 1329, 'concepts': [['C14740026', '0.8500035'], ['C154945302', '0.80305904'], ['C31972630', '0.7009829'], ['C5799516', '0.660772'], ['C41008148', '0.64281815']], 'referenced_works': ['W1514909517', 'W1968315983', 'W1970504153', 'W2013503831', 'W2019321876', 'W2037050532', 'W2041139939', 'W2058535340', 'W2091790851', 'W2093659073', 'W2108134361', 'W2118428504', 'W2135988303', 'W2151290401', 'W2152671441', 'W2154280780', 'W2200993670', 'W2202251471', 'W2218842719', 'W2396274919', 'W2427448504', 'W3103648783', 'W3108793894'], 'abstract': 'Direct Sparse Odometry (DSO) is a visual odometry method based on a novel, highly accurate sparse and direct structure and motion formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry-represented as inverse depth in a reference frame-and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on essentially featureless walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.', 'counts_by_year': [[2022, 171], [2021, 319], [2020, 320], [2019, 318], [2018, 147], [2017, 52]]}, {'id': 'W2884436604', 'doi': 'https://doi.org/10.1007/978-3-030-00889-5_1', 'title': 'UNet++: A Nested U-Net Architecture for Medical Image Segmentation', 'type': 'book-chapter', 'publication_date': '2018-09-20', 'host_venue': 'V4306463941', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2750079513', ['I55732556']], ['A2883582199', ['I55732556']], ['A1924660746', ['I55732556']], ['A2251339526', ['I55732556']]], 'cited_by_count': 1325, 'concepts': [['C41008148', '0.83292407'], ['C89600930', '0.7879675'], ['C118505674', '0.7393324'], ['C154945302', '0.6681953'], ['C124504099', '0.5792']], 'referenced_works': ['W1901129140', 'W1903029394', 'W1986649315', 'W2346062110', 'W2517954747', 'W2750023899', 'W2963150697', 'W2963446712', 'W2963741402', 'W2964227007'], 'abstract': 'In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.', 'counts_by_year': [[2022, 535], [2021, 479], [2020, 237], [2019, 70]]}, {'id': 'W2896872272', 'doi': 'https://doi.org/10.1101/gr.239244.118', 'title': 'Maftools: efficient and comprehensive analysis of somatic variants in cancer', 'type': 'journal-article', 'publication_date': '2018-10-19', 'host_venue': 'V43092948', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2254161975', ['I165932596', 'I87890613']], ['A2602938496', ['I1282927834']], ['A40272620', ['I87890613', 'I4210156450']], ['A2805042187', ['I87890613', 'I4210156450']], ['A391635857', ['I165932596', 'I1282927834']]], 'cited_by_count': 1323, 'concepts': [['C2779694297', '0.74754757'], ['C86803240', '0.7287962'], ['C70721500', '0.63318586'], ['C134305767', '0.54479975'], ['C2776321320', '0.5306389']], 'referenced_works': ['W1255676896', 'W1602954224', 'W1793984240', 'W1842792112', 'W1911343030', 'W1965151932', 'W1967940883', 'W1970633390', 'W1976492799', 'W1979521298', 'W1987709906', 'W1999207703', 'W2003665515', 'W2008255210', 'W2009720434', 'W2020534880', 'W2032734987', 'W2035563427', 'W2044433401', 'W2044702943', 'W2066556833', 'W2072655253', 'W2080948706', 'W2096283457', 'W2101836667', 'W2114023704', 'W2114031931', 'W2121443461', 'W2123397870', 'W2126172213', 'W2129136620', 'W2129973040', 'W2130741439', 'W2131478115', 'W2133174470', 'W2136787567', 'W2140929811', 'W2149031002', 'W2149082178', 'W2149441684', 'W2150045600', 'W2152061559', 'W2153220158', 'W2157852151', 'W2158065648', 'W2159987716', 'W2160697532', 'W2163151214', 'W2262414037', 'W2276335691', 'W2343368549', 'W2406250479', 'W2466657174', 'W2490318515', 'W2537167162', 'W2560367415', 'W2570401594', 'W2740910083', 'W2753687033', 'W2763859406', 'W2794804602', 'W4210424573'], 'abstract': 'Numerous large-scale genomic studies of matched tumor-normal samples have established the somatic landscapes of most cancer types. However, the downstream analysis of data from somatic mutations entails a number of computational and statistical approaches, requiring usage of independent software and numerous tools. Here, we describe an R Bioconductor package, Maftools, which offers a multitude of analysis and visualization modules that are commonly used in cancer genomic studies, including driver gene identification, pathway, signature, enrichment, and association analyses. Maftools only requires somatic variants in Mutation Annotation Format (MAF) and is independent of larger alignment files. With the implementation of well-established statistical and computational methods, Maftools facilitates data-driven research and comparative analysis to discover novel results from publicly available data sets. In the present study, using three of the well-annotated cohorts from The Cancer Genome Atlas (TCGA), we describe the application of Maftools to reproduce known results. More importantly, we show that Maftools can also be used to uncover novel findings through integrative analysis.', 'counts_by_year': [[2022, 629], [2021, 426], [2020, 217], [2019, 49], [2018, 2]]}, {'id': 'W2118301480', 'doi': 'https://doi.org/10.1007/jhep08(2016)106', 'title': 'A bound on chaos', 'type': 'journal-article', 'publication_date': '2016-08-17', 'host_venue': 'V187585107', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2306152478', ['I4210137766']], ['A2299899939', ['I97018004']], ['A2127198779', ['I4210137766']]], 'cited_by_count': 1322, 'concepts': [['C121332964', '0.79263484'], ['C145620117', '0.7909603'], ['C2780990831', '0.76151943'], ['C191544260', '0.6483411'], ['C77553402', '0.56114763']], 'referenced_works': ['W1603380880', 'W1744159570', 'W1848366941', 'W1946358044', 'W1973061550', 'W2011290825', 'W2012812032', 'W2027008864', 'W2048607668', 'W2053781383', 'W2055793058', 'W2059045419', 'W2070375169', 'W2089191553', 'W2097909025', 'W2110669415', 'W2121607258', 'W2123621088', 'W2129872856', 'W2137001571', 'W3037042650', 'W3037318174', 'W3037748802', 'W3100708525', 'W3100762698', 'W3101868850', 'W3102681075', 'W3106454759'], 'abstract': 'We conjecture a sharp bound on the rate of growth of chaos in thermal quantum systems with a large number of degrees of freedom. Chaos can be diagnosed using an out-of-time-order correlation function closely related to the commutator of operators separated in time. We conjecture that the influence of chaos on this correlator can develop no faster than exponentially, with Lyapunov exponent $\\lambda_L \\le 2 \\pi k_B T/\\hbar$. We give a precise mathematical argument, based on plausible physical assumptions, establishing this conjecture.', 'counts_by_year': [[2022, 184], [2021, 291], [2020, 220], [2019, 231], [2018, 213], [2017, 143], [2016, 34], [2015, 5]]}, {'id': 'W2557641257', 'doi': 'https://doi.org/10.1109/cvpr.2017.733', 'title': 'ECO: Efficient Convolution Operators for Tracking', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A260882311', ['I102134673']], ['A2557978175', ['I102134673']], ['A2110150181', ['I102134673']], ['A26172899', ['I102134673']]], 'cited_by_count': 1322, 'concepts': [['C63479239', '0.8030945'], ['C68339613', '0.77882004'], ['C97931131', '0.6958606'], ['C41008148', '0.68194336'], ['C45347329', '0.6344104']], 'referenced_works': ['W1857884451', 'W1892578678', 'W1904671147', 'W1915785815', 'W1955514522', 'W1964846093', 'W2032576830', 'W2044986361', 'W2089961441', 'W2098941887', 'W2147533695', 'W2149829493', 'W2154889144', 'W2158592639', 'W2161969291', 'W2162349892', 'W2207983105', 'W2214352687', 'W2244956674', 'W2469582947', 'W2520438127', 'W2963249584', 'W2964111344', 'W3102624093'], 'abstract': 'In recent years, Discriminative Correlation Filter (DCF) based methods have significantly advanced the state-of-the-art in tracking. However, in the pursuit of ever increasing tracking performance, their characteristic speed and real-time capability have gradually faded. Further, the increasingly complex models, with massive number of trainable parameters, have introduced the risk of severe over-fitting. In this work, we tackle the key causes behind the problems of computational complexity and over-fitting, with the aim of simultaneously improving both speed and performance. We revisit the core DCF formulation and introduce: (i) a factorized convolution operator, which drastically reduces the number of parameters in the model, (ii) a compact generative model of the training sample distribution, that significantly reduces memory and time complexity, while providing better diversity of samples, (iii) a conservative model update strategy with improved robustness and reduced complexity. We perform comprehensive experiments on four benchmarks: VOT2016, UAV123, OTB-2015, and TempleColor. When using expensive deep features, our tracker provides a 20-fold speedup and achieves a 13.0% relative gain in Expected Average Overlap compared to the top ranked method [12] in the VOT2016 challenge. Moreover, our fast variant, using hand-crafted features, operates at 60 Hz on a single CPU, while obtaining 65.0% AUC on OTB-2015.', 'counts_by_year': [[2022, 211], [2021, 344], [2020, 322], [2019, 303], [2018, 133], [2017, 9]]}, {'id': 'W2999615587', 'doi': 'https://doi.org/10.1038/s42256-019-0138-9', 'title': 'From local explanations to global understanding with explainable AI for trees', 'type': 'journal-article', 'publication_date': '2020-01-17', 'host_venue': 'V2912241403', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2118551440', ['I1290206253', 'I201448701']], ['A2771412981', ['I201448701']], ['A2763359630', ['I201448701']], ['A2898429367', ['I201448701']], ['A1721427750', ['I201448701']], ['A2098679358', ['I4210126198']], ['A2097352813', ['I201448701']], ['A2099487737', ['I201448701']], ['A2601897814', ['I201448701']], ['A2111600720', ['I201448701']]], 'cited_by_count': 1322, 'concepts': [['C2781067378', '0.95867974'], ['C119857082', '0.6972128'], ['C2780586970', '0.6785165'], ['C41008148', '0.6505388'], ['C113174947', '0.6174047']], 'referenced_works': ['W199682464', 'W1539593569', 'W2011402106', 'W2038127015', 'W2059958079', 'W2101350555', 'W2102099143', 'W2129888542', 'W2157840751', 'W2167299318', 'W2282821441', 'W2510508396', 'W2601152064', 'W2622118130', 'W2788403449', 'W2892741787', 'W2899876413', 'W2963749936', 'W2964089344', 'W3100711616', 'W3102476541', 'W3150595609', 'W4235011515'], 'abstract': "Tree-based machine learning models such as random forests, decision trees, and gradient boosted trees are popular non-linear predictive models, yet comparatively little attention has been paid to explaining their predictions. Here, we improve the interpretability of tree-based models through three main contributions: 1) The first polynomial time algorithm to compute optimal explanations based on game theory. 2) A new type of explanation that directly measures local feature interaction effects. 3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to i) identify high magnitude but low frequency non-linear mortality risk factors in the US population, ii) highlight distinct population sub-groups with shared risk characteristics, iii) identify non-linear interaction effects among risk factors for chronic kidney disease, and iv) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model's performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.", 'counts_by_year': [[2022, 578], [2021, 539], [2020, 189], [2019, 5], [2018, 1]]}, {'id': 'W2963954913', 'doi': 'https://doi.org/10.1109/cvpr.2016.10', 'title': 'Stacked Attention Networks for Image Question Answering', 'type': 'proceedings-article', 'publication_date': '2016-06-27', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2596307046', ['I74973139']], ['A2122755126', ['I1290206253']], ['A2104437897', ['I1290206253']], ['A2101552792', ['I1290206253']], ['A1972291593', ['I74973139']]], 'cited_by_count': 1321, 'concepts': [['C44291984', '0.85683876'], ['C41008148', '0.77623785'], ['C36464697', '0.6675639'], ['C115961682', '0.657529'], ['C2776359362', '0.57369405']], 'referenced_works': ['W1832693441', 'W1861492603', 'W1894439495', 'W1933349210', 'W2064675550', 'W2112796928', 'W2131876387', 'W2136480620', 'W2142192571', 'W2250225488', 'W2251079237', 'W2251143283'], 'abstract': 'This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.', 'counts_by_year': [[2022, 88], [2021, 250], [2020, 266], [2019, 337], [2018, 224], [2017, 113], [2016, 40], [2015, 2]]}, {'id': 'W2621222172', 'doi': 'https://doi.org/10.1038/ncomms15684', 'title': 'One-Year stable perovskite solar cells by 2D/3D interface engineering', 'type': 'journal-article', 'publication_date': '2017-06-01', 'host_venue': 'V64187185', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2036675489', ['I5124864']], ['A2090170979', ['I5124864']], ['A2536609197', ['I5124864']], ['A2029847491', ['I30771326']], ['A2621124487', ['I5124864']], ['A2646121931', ['I4210086915']], ['A2078326816', ['I4210086915']], ['A2161116066', ['I4210086915']], ['A2118394222', ['I30771326']], ['A2012626572', ['I5124864']], ['A2004633034', ['I5124864']]], 'cited_by_count': 1319, 'concepts': [['C155011858', '0.85237145'], ['C41291067', '0.6680236'], ['C206991015', '0.64650464'], ['C2780625559', '0.62612593'], ['C82776694', '0.53130674']], 'referenced_works': ['W1766531778', 'W1802734970', 'W1849148947', 'W1981368803', 'W2011472471', 'W2035869425', 'W2044423188', 'W2045369867', 'W2049901029', 'W2055652426', 'W2067910821', 'W2071858459', 'W2072966713', 'W2076208415', 'W2087585288', 'W2092646979', 'W2100526315', 'W2120145199', 'W2150690597', 'W2151010124', 'W2152034665', 'W2197620748', 'W2206784480', 'W2229814172', 'W2230715130', 'W2230864058', 'W2231517473', 'W2265691491', 'W2301656337', 'W2317851040', 'W2318514767', 'W2327721819', 'W2331511205', 'W2336068196', 'W2398920238', 'W2474894284', 'W2510705912', 'W2525986166', 'W2527042386', 'W2549206003', 'W2557553337', 'W2562577626', 'W4233815961'], 'abstract': 'Despite the impressive photovoltaic performances with power conversion efficiency beyond 22%, perovskite solar cells are poorly stable under operation, failing by far the market requirements. Various technological approaches have been proposed to overcome the instability problem, which, while delivering appreciable incremental improvements, are still far from a market-proof solution. Here we show one-year stable perovskite devices by engineering an ultra-stable 2D/3D (HOOC(CH2)4NH3)2PbI4/CH3NH3PbI3 perovskite junction. The 2D/3D forms an exceptional gradually-organized multi-dimensional interface that yields up to 12.9% efficiency in a carbon-based architecture, and 14.6% in standard mesoporous solar cells. To demonstrate the up-scale potential of our technology, we fabricate 10 × 10 cm2 solar modules by a fully printable industrial-scale process, delivering 11.2% efficiency stable for >10,000 h with zero loss in performances measured under controlled standard conditions. This innovative stable and low-cost architecture will enable the timely commercialization of perovskite solar cells.', 'counts_by_year': [[2022, 177], [2021, 321], [2020, 276], [2019, 271], [2018, 232], [2017, 40]]}, {'id': 'W2194187530', 'doi': 'https://doi.org/10.1109/tpami.2016.2646371', 'title': 'An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition', 'type': 'journal-article', 'publication_date': '2017-11-01', 'host_venue': 'V199944782', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2785766916', ['I47720641']], ['A2168860362', ['I47720641']], ['A2130302435', ['I47720641']]], 'cited_by_count': 1316, 'concepts': [['C41008148', '0.79865694'], ['C154945302', '0.7099465'], ['C153180895', '0.5752604'], ['C2778121359', '0.57311666'], ['C52622490', '0.51404274']], 'referenced_works': ['W654550266', 'W1895191496', 'W1922126009', 'W1922658220', 'W1975453746', 'W1981283549', 'W1988461287', 'W1990550880', 'W1994529543', 'W2008806374', 'W2043075591', 'W2045220951', 'W2053317383', 'W2064675550', 'W2099247484', 'W2102605133', 'W2107878631', 'W2112796928', 'W2122221966', 'W2122585011', 'W2127141656', 'W2135231474', 'W2143612262', 'W2153182373', 'W2341629100', 'W2618530766'], 'abstract': 'Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.', 'counts_by_year': [[2022, 171], [2021, 379], [2020, 295], [2019, 294], [2018, 124], [2017, 44], [2016, 7], [2015, 1]]}, {'id': 'W2574952845', 'doi': 'https://doi.org/10.1109/tip.2017.2713099', 'title': 'Deep Convolutional Neural Network for Inverse Problems in Imaging', 'type': 'journal-article', 'publication_date': '2017-06-15', 'host_venue': 'V4210173141', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2163161566', ['I5124864']], ['A3004386559', ['I5124864']], ['A2223729097', ['I5124864']], ['A1996518183', ['I5124864']]], 'cited_by_count': 1315, 'concepts': [['C81363708', '0.7766512'], ['C135252773', '0.7093505'], ['C41008148', '0.66577923'], ['C141379421', '0.6608742'], ['C45347329', '0.592081']], 'referenced_works': ['W1005811612', 'W1849277567', 'W1885185971', 'W1902903569', 'W1903029394', 'W1915360731', 'W1963882359', 'W1972150100', 'W1972868011', 'W1999320095', 'W2005089986', 'W2011380718', 'W2037521346', 'W2037642501', 'W2094366314', 'W2098239148', 'W2100556411', 'W2100705753', 'W2101675075', 'W2102605133', 'W2103559027', 'W2103913786', 'W2106002835', 'W2107844156', 'W2107882641', 'W2115706991', 'W2117539524', 'W2119290843', 'W2125527601', 'W2130975789', 'W2132122471', 'W2142058898', 'W2142683286', 'W2145096794', 'W2148267424', 'W2194775991', 'W2242218935', 'W2273561594', 'W2295936755', 'W2328247767', 'W2463302795', 'W2489529491', 'W2494890720', 'W2500060407', 'W2508457857', 'W2519021537', 'W2570202822', 'W2593226005', 'W2919115771', 'W3100774664', 'W4249760698', 'W4292363360'], 'abstract': 'In this paper, we propose a novel deep convolutional neural network (CNN)-based algorithm for solving ill-posed inverse problems. Regularized iterative algorithms have emerged as the standard approach to ill-posed inverse problems in the past few decades. These methods produce excellent results, but can be challenging to deploy in practice due to factors including the high computational cost of the forward and adjoint operators and the difficulty of hyper parameter selection. The starting point of our work is the observation that unrolled iterative methods have the form of a CNN (filtering followed by point-wise non-linearity) when the normal operator (H*H, the adjoint of H times H) of the forward model is a convolution. Based on this observation, we propose using direct inversion followed by a CNN to solve normal-convolutional inverse problems. The direct inversion encapsulates the physical model of the system, but leads to artifacts when the problem is ill-posed; the CNN combines multiresolution decomposition and residual learning in order to learn to remove these artifacts while preserving image structure. We demonstrate the performance of the proposed network in sparse-view reconstruction (down to 50 views) on parallel beam X-ray computed tomography in synthetic phantoms as well as in real experimental sinograms. The proposed network outperforms total variation-regularized iterative reconstruction for the more realistic phantoms and requires less than a second to reconstruct a 512 x 512 image on GPU.', 'counts_by_year': [[2022, 189], [2021, 375], [2020, 345], [2019, 271], [2018, 109], [2017, 23], [2016, 1]]}, {'id': 'W2951257985', 'doi': 'https://doi.org/10.1093/molbev/msx319', 'title': 'BUSCO Applications from Quality Assessments to Gene Prediction and Phylogenomics', 'type': 'journal-article', 'publication_date': '2018-03-01', 'host_venue': 'V57552105', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2096443021', ['I12708293']], ['A2567648508', ['I12708293']], ['A1978421822', ['I12708293']], ['A2116062619', ['I12708293']], ['A2011246032', ['I12708293']], ['A2748150692', ['I12708293']], ['A581455859', ['I12708293']], ['A2208245595', ['I12708293']]], 'cited_by_count': 1311, 'concepts': [['C70343354', '0.83101976'], ['C86803240', '0.80850744'], ['C189206191', '0.69813955'], ['C141231307', '0.5095689'], ['C15151743', '0.45270938']], 'referenced_works': ['W1484347389', 'W1977730494', 'W1981838417', 'W2006895104', 'W2038184529', 'W2059359550', 'W2068600671', 'W2101786683', 'W2112814626', 'W2115888213', 'W2119837484', 'W2124927293', 'W2131400772', 'W2138122982', 'W2138181932', 'W2141052558', 'W2142678478', 'W2155628349', 'W2158592738', 'W2160378127', 'W2254795425', 'W2312959168', 'W2410941586', 'W2487644634', 'W2528315574', 'W2552622217', 'W2559011563', 'W2572068809', 'W2581699331', 'W2602840959'], 'abstract': 'Genomics promises comprehensive surveying of genomes and metagenomes, but rapidly changing technologies and expanding data volumes make evaluation of completeness a challenging task. Technical sequencing quality metrics can be complemented by quantifying completeness of genomic data sets in terms of the expected gene content of Benchmarking Universal Single-Copy Orthologs (BUSCO, http://busco.ezlab.org). The latest software release implements a complete refactoring of the code to make it more flexible and extendable to facilitate high-throughput assessments. The original six lineage assessment data sets have been updated with improved species sampling, 34 new subsets have been built for vertebrates, arthropods, fungi, and prokaryotes that greatly enhance resolution, and data sets are now also available for nematodes, protists, and plants. Here, we present BUSCO v3 with example analyses that highlight the wide-ranging utility of BUSCO assessments, which extend beyond quality control of genomics data sets to applications in comparative genomics analyses, gene predictor training, metagenomics, and phylogenomics.', 'counts_by_year': [[2022, 198], [2021, 343], [2020, 402], [2019, 291], [2018, 76], [2017, 1]]}, {'id': 'W2604308930', 'doi': 'https://doi.org/10.1093/molbev/msx116', 'title': 'TimeTree: A Resource for Timelines, Timetrees, and Divergence Times', 'type': 'journal-article', 'publication_date': '2017-07-01', 'host_venue': 'V57552105', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2149898553', ['I84392919']], ['A2307130762', ['I84392919']], ['A2951628622', ['I84392919']], ['A2117680553', ['I84392919']]], 'cited_by_count': 1309, 'concepts': [['C4438859', '0.9370018'], ['C86803240', '0.81207764'], ['C206345919', '0.64825696'], ['C207390915', '0.6437557'], ['C130217890', '0.60622096']], 'referenced_works': ['W1981281557', 'W1981435137', 'W2025696977', 'W2089746913', 'W2096230176', 'W2116885418', 'W2117260150', 'W2153452535', 'W2161480748'], 'abstract': "Evolutionary information on species divergence times is fundamental to studies of biodiversity, development, and disease. Molecular dating has enhanced our understanding of the temporal patterns of species divergences over the last five decades, and the number of studies is increasing quickly due to an exponential growth in the available collection of molecular sequences from diverse species and large number of genes. Our TimeTree resource is a public knowledge-base with the primary focus to make available all species divergence times derived using molecular sequence data to scientists, educators, and the general public in a consistent and accessible format. Here, we report a major expansion of the TimeTree resource, which more than triples the number of species (>97,000) and more than triples the number of studies assembled (>3,000). Furthermore, scientists can access not only the divergence time between two species or higher taxa, but also a timetree of a group of species and a timeline that traces a species' evolution through time. The new timetree and timeline visualizations are integrated with display of events on earth and environmental history over geological time, which will lead to broader and better understanding of the interplay of the change in the biosphere with the diversity of species on Earth. The next generation TimeTree resource is publicly available online at http://www.timetree.org.", 'counts_by_year': [[2022, 310], [2021, 426], [2020, 281], [2019, 178], [2018, 95], [2017, 18]]}, {'id': 'W2866634454', 'doi': 'https://doi.org/10.1007/978-3-030-01234-2_18', 'title': 'Image Super-Resolution Using Very Deep Residual Channel Attention Networks', 'type': 'book-chapter', 'publication_date': '2018-09-08', 'host_venue': 'V4306463941', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2297316014', ['I87182695']], ['A2740633562', ['I87182695']], ['A2096856256', ['I87182695']], ['A2788366970', ['I87182695']], ['A2102375896', ['I87182695']], ['A2123131494', ['I87182695']]], 'cited_by_count': 1309, 'concepts': [['C41008148', '0.83961225'], ['C155512373', '0.67264843'], ['C154945302', '0.5568569'], ['C127162648', '0.45928574'], ['C141239990', '0.45396364']], 'referenced_works': ['W54257720', 'W134193804', 'W1885185971', 'W1919542679', 'W1930824406', 'W2047920195', 'W2149760002', 'W2192954843', 'W2194775991', 'W2214802144', 'W2221625691', 'W2242218935', 'W2263468737', 'W2331128040', 'W2462651488', 'W2476548250', 'W2503339013', 'W2607041014', 'W2613155248', 'W2747898905', 'W2752782242', 'W2963037581', 'W2963372104', 'W2963470893', 'W2963495494', 'W2963606198', 'W2963729050', 'W2964101377', 'W2964125708', 'W2964277374'], 'abstract': 'Convolutional neural network (CNN) depth is of crucial importance for image super-resolution (SR). However, we observe that deeper networks for image SR are more difficult to train. The low-resolution inputs and features contain abundant low-frequency information, which is treated equally across channels, hence hindering the representational ability of CNNs. To solve these problems, we propose the very deep residual channel attention networks (RCAN). Specifically, we propose a residual in residual (RIR) structure to form very deep network, which consists of several residual groups with long skip connections. Each residual group contains some residual blocks with short skip connections. Meanwhile, RIR allows abundant low-frequency information to be bypassed through multiple skip connections, making the main network focus on learning high-frequency information. Furthermore, we propose a channel attention mechanism to adaptively rescale channel-wise features by considering interdependencies among channels. Extensive experiments show that our RCAN achieves better accuracy and visual improvements against state-of-the-art methods.', 'counts_by_year': [[2022, 252], [2021, 491], [2020, 402], [2019, 147], [2018, 13]]}, {'id': 'W2946657092', 'doi': 'https://doi.org/10.1093/nar/gkz430', 'title': 'GEPIA2: an enhanced web server for large-scale expression profiling and interactive analysis', 'type': 'journal-article', 'publication_date': '2019-07-02', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2723801666', ['I20231570']], ['A2673676330', ['I20231570']], ['A2630712196', ['I4210160507']], ['A2946127919', ['I20231570']], ['A2183341720', ['I20231570']]], 'cited_by_count': 1309, 'concepts': [['C71901391', '0.7079338'], ['C86803240', '0.70243657'], ['C11392498', '0.5530289'], ['C70721500', '0.52366227'], ['C18431079', '0.519836']], 'referenced_works': ['W1533942137', 'W2018838463', 'W2120865735', 'W2158485828', 'W2254034232', 'W2287918349', 'W2513371636', 'W2600132724', 'W2607129810', 'W2607211817', 'W2624572223', 'W2624831627', 'W2749613778', 'W2796207838', 'W2799562872', 'W2800502807', 'W2808837841', 'W2898120081', 'W2904776046', 'W2912721163', 'W2914323984'], 'abstract': 'Abstract Introduced in 2017, the GEPIA (Gene Expression Profiling Interactive Analysis) web server has been a valuable and highly cited resource for gene expression analysis based on tumor and normal samples from the TCGA and the GTEx databases. Here, we present GEPIA2, an updated and enhanced version to provide insights with higher resolution and more functionalities. Featuring 198 619 isoforms and 84 cancer subtypes, GEPIA2 has extended gene expression quantification from the gene level to the transcript level, and supports analysis of a specific cancer subtype, and comparison between subtypes. In addition, GEPIA2 has adopted new analysis techniques of gene signature quantification inspired by single-cell sequencing studies, and provides customized analysis where users can upload their own RNA-seq data and compare them with TCGA and GTEx samples. We also offer an API for batch process and easy retrieval of the analysis results. The updated web server is publicly accessible at http://gepia2.cancer-pku.cn/.', 'counts_by_year': [[2022, 600], [2021, 504], [2020, 180], [2019, 24]]}, {'id': 'W2897450989', 'doi': 'https://doi.org/10.1504/ijwgs.2018.095647', 'title': 'Blockchain challenges and opportunities: a survey', 'type': 'journal-article', 'publication_date': '2018-10-17', 'host_venue': 'V200047447', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A3164421333', ['I157773358']], ['A2756284913', ['I157773358']], ['A2161461259', ['I111950717']], ['A2304902609', ['I157773358']], ['A2131230917', ['I170215575']]], 'cited_by_count': 1308, 'concepts': [['C2779687700', '0.94998634'], ['C41008148', '0.5003207'], ['C2522767166', '0.33667472'], ['C38652104', '0.30515814']], 'referenced_works': ['W18125873', 'W69553326', 'W1559136758', 'W1589846406', 'W1663109347', 'W1819513546', 'W1972614536', 'W1988600100', 'W1990126021', 'W2013613544', 'W2013686672', 'W2022932831', 'W2043007983', 'W2092068344', 'W2100522554', 'W2105103777', 'W2110471885', 'W2119046642', 'W2126087831', 'W2127612127', 'W2176225732', 'W2183126124', 'W2189446046', 'W2221133380', 'W2225278338', 'W2245534414', 'W2288515487', 'W2293877007', 'W2294180185', 'W2295506186', 'W2311567330', 'W2335755599', 'W2392113277', 'W2397575926', 'W2397661077', 'W2405518766', 'W2410226093', 'W2483525606', 'W2486460265', 'W2517744317', 'W2553353335', 'W2553854225', 'W2563947283', 'W2754529701', 'W2886706106', 'W2951095688', 'W2952926927', 'W2964262836', 'W3030392202', 'W3105959039', 'W3123838901', 'W3151748982'], 'abstract': 'Blockchain has numerous benefits such as decentralisation, persistency, anonymity and auditability. There is a wide spectrum of blockchain applications ranging from cryptocurrency, financial services, risk management, internet of things (IoT) to public and social services. Although a number of studies focus on using the blockchain technology in various application aspects, there is no comprehensive survey on the blockchain technology in both technological and application perspectives. To fill this gap, we conduct a comprehensive survey on the blockchain technology. In particular, this paper gives the blockchain taxonomy, introduces typical blockchain consensus algorithms, reviews blockchain applications and discusses technical challenges as well as recent advances in tackling the challenges. Moreover, this paper also points out the future directions in the blockchain technology.', 'counts_by_year': [[2022, 268], [2021, 418], [2020, 352], [2019, 219], [2018, 45], [2017, 4]]}, {'id': 'W2745859992', 'doi': 'https://doi.org/10.1109/tro.2018.2853729', 'title': 'VINS-Mono: A Robust and Versatile Monocular Visual-Inertial State Estimator', 'type': 'journal-article', 'publication_date': '2018-08-01', 'host_venue': 'V144620930', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2747041432', ['I200769079']], ['A2746957544', ['I200769079']], ['A2686590597', ['I200769079']]], 'cited_by_count': 1300, 'concepts': [['C79061980', '0.84040296'], ['C41008148', '0.66876745'], ['C114466953', '0.61534315'], ['C154945302', '0.59384876'], ['C49441653', '0.59197617']], 'referenced_works': ['W612478963', 'W1498041548', 'W1562603105', 'W1968315983', 'W1969726450', 'W1970504153', 'W1981699551', 'W1989484209', 'W1991544872', 'W2021851106', 'W2046033161', 'W2048249748', 'W2049750053', 'W2056358962', 'W2091790851', 'W2109635530', 'W2118223742', 'W2124313187', 'W2127752689', 'W2144042618', 'W2150066425', 'W2151290401', 'W2214788824', 'W2274359774', 'W2276677361', 'W2314596443', 'W2342437324', 'W2396274919', 'W2411412724', 'W2474281075', 'W2482726005', 'W2538522345', 'W2732510496', 'W2736926039', 'W2768130891', 'W2773447888', 'W3103648783', 'W4236769309', 'W4246483956', 'W4246614213'], 'abstract': 'A monocular visual-inertial system (VINS), consisting of a camera and a low-cost inertial measurement unit (IMU), forms the minimum sensor suite for metric six degrees-of-freedom (DOF) state estimation. However, the lack of direct distance measurement poses significant challenges in terms of IMU processing, estimator initialization, extrinsic calibration, and nonlinear optimization. In this work, we present VINS-Mono: a robust and versatile monocular visual-inertial state estimator.Our approach starts with a robust procedure for estimator initialization and failure recovery. A tightly-coupled, nonlinear optimization-based method is used to obtain high accuracy visual-inertial odometry by fusing pre-integrated IMU measurements and feature observations. A loop detection module, in combination with our tightly-coupled formulation, enables relocalization with minimum computation overhead.We additionally perform four degrees-of-freedom pose graph optimization to enforce global consistency. We validate the performance of our system on public datasets and real-world experiments and compare against other state-of-the-art algorithms. We also perform onboard closed-loop autonomous flight on the MAV platform and port the algorithm to an iOS-based demonstration. We highlight that the proposed work is a reliable, complete, and versatile system that is applicable for different applications that require high accuracy localization. We open source our implementations for both PCs and iOS mobile devices.', 'counts_by_year': [[2022, 278], [2021, 383], [2020, 356], [2019, 236], [2018, 42], [2017, 1], [2016, 1]]}, {'id': 'W2537679995', 'doi': 'https://doi.org/10.1093/nar/gkw943', 'title': 'DisGeNET: a comprehensive platform integrating information on human disease-associated genes and variants', 'type': 'journal-article', 'publication_date': '2017-01-04', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2144413363', ['I170486558']], ['A2100411629', ['I170486558']], ['A376002515', ['I170486558']], ['A318587656', ['I170486558']], ['A2090807123', ['I170486558']], ['A2560313182', ['I170486558']], ['A2250452851', ['I170486558']], ['A2146221463', ['I170486558']], ['A2014703636', ['I170486558']]], 'cited_by_count': 1299, 'concepts': [['C41009113', '0.62257653'], ['C86803240', '0.54514503'], ['C147497476', '0.45758358'], ['C41008148', '0.428356'], ['C70721500', '0.42094475']], 'referenced_works': ['W1505770165', 'W1770847181', 'W1858189377', 'W1859359226', 'W1971192989', 'W1976546232', 'W1983335031', 'W1983766967', 'W1989277387', 'W1992442866', 'W2012896813', 'W2019515024', 'W2036935277', 'W2048201932', 'W2048296798', 'W2074370114', 'W2090234932', 'W2104549677', 'W2113100102', 'W2113310608', 'W2116868464', 'W2118822739', 'W2122732537', 'W2124045889', 'W2125004669', 'W2126997165', 'W2136437513', 'W2136672520', 'W2144086069', 'W2144427809', 'W2146749911', 'W2147418473', 'W2148315196', 'W2151491827', 'W2159583324', 'W2162151166', 'W2174602966', 'W2174694627', 'W2223549218', 'W2234406841', 'W2256016639', 'W2271071948', 'W2297837852', 'W2302501749', 'W2320983896', 'W2341884156', 'W2341975812', 'W2370481231', 'W2396557861', 'W2417483443', 'W2460955802', 'W2465629559', 'W2512727383', 'W2739999456'], 'abstract': 'The information about the genetic basis of human diseases lies at the heart of precision medicine and drug discovery. However, to realize its full potential to support these goals, several problems, such as fragmentation, heterogeneity, availability and different conceptualization of the data must be overcome. To provide the community with a resource free of these hurdles, we have developed DisGeNET (http://www.disgenet.org), one of the largest available collections of genes and variants involved in human diseases. DisGeNET integrates data from expert curated repositories, GWAS catalogues, animal models and the scientific literature. DisGeNET data are homogeneously annotated with controlled vocabularies and community-driven ontologies. Additionally, several original metrics are provided to assist the prioritization of genotype-phenotype relationships. The information is accessible through a web interface, a Cytoscape App, an RDF SPARQL endpoint, scripts in several programming languages and an R package. DisGeNET is a versatile platform that can be used for different research purposes including the investigation of the molecular underpinnings of specific human diseases and their comorbidities, the analysis of the properties of disease genes, the generation of hypothesis on drug therapeutic action and drug adverse effects, the validation of computationally predicted disease genes and the evaluation of text-mining methods performance.', 'counts_by_year': [[2022, 261], [2021, 306], [2020, 292], [2019, 231], [2018, 144], [2017, 62], [2016, 2]]}, {'id': 'W1767272795', 'doi': 'https://doi.org/10.1007/s11192-015-1765-5', 'title': 'The journal coverage of Web of Science and Scopus: a comparative analysis', 'type': 'journal-article', 'publication_date': '2016-01-01', 'host_venue': 'V148561398', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A1923114979', ['I70931966']], ['A2312138317', ['I70931966']]], 'cited_by_count': 1294, 'concepts': [['C83867959', '0.91193604'], ['C3020774429', '0.7515701'], ['C2777683733', '0.65623564'], ['C178315738', '0.50317234'], ['C525823164', '0.48778254']], 'referenced_works': ['W1655306493', 'W1964117853', 'W1970211697', 'W1991526941', 'W2034085520', 'W2061922306', 'W2065157469', 'W2070593969', 'W2071257110', 'W2071707531', 'W2080222100', 'W2108440411', 'W2113847349', 'W2132615533', 'W2147002372', 'W2164534115', 'W2165122403', 'W2952984634', 'W4255356361'], 'abstract': "Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters' Web of Science (WoS) and Elsevier's Scopus. This research compares the journal coverage of both databases in terms of fields, countries and languages, using Ulrich's extensive periodical directory as a base for comparison. Results indicate that the use of either WoS or Scopus for research evaluation may introduces biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used.", 'counts_by_year': [[2022, 326], [2021, 352], [2020, 262], [2019, 177], [2018, 96], [2017, 51], [2016, 24], [2015, 2]]}, {'id': 'W2519132385', 'doi': 'https://doi.org/10.32614/rj-2016-021', 'title': 'mclust 5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models', 'type': 'journal-article', 'publication_date': '2016-08-01', 'host_venue': 'V2489169438', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A277340435', ['I27483092']], ['A2522671255', ['I100930933']], ['A2168993385', ['I100930933']], ['A2052065285', ['I201448701']]], 'cited_by_count': 1293, 'concepts': [['C61224824', '0.7627959'], ['C73555534', '0.731777'], ['C178650346', '0.63817954'], ['C41008148', '0.5978716'], ['C70518039', '0.56059825']], 'referenced_works': ['W1858253732', 'W1986420798', 'W2083682919', 'W2089395261', 'W2096476924', 'W2117446015', 'W2146646206'], 'abstract': 'Finite mixture models are being used increasingly to model a wide variety of random phenomena for clustering, classification and density estimation. mclust is a powerful and popular package which allows modelling of data as a Gaussian finite mixture with different covariance structures and different numbers of mixture components, for a variety of purposes of analysis. Recently, version 5 of the package has been made available on CRAN. This updated version adds new covariance structures, dimension reduction capabilities for visualisation, model selection criteria, initialisation strategies for the EM algorithm, and bootstrap-based inference, making it a full-featured R package for data analysis via finite mixture modelling.', 'counts_by_year': [[2022, 293], [2021, 356], [2020, 298], [2019, 214], [2018, 104], [2017, 21], [2016, 3], [2015, 2]]}, {'id': 'W2738588019', 'doi': 'https://doi.org/10.1145/3072959.3073659', 'title': 'Globally and locally consistent image completion', 'type': 'journal-article', 'publication_date': '2017-07-20', 'host_venue': 'V185367456', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2667084142', ['I150744194']], ['A1720417961', ['I150744194']], ['A3173423655', ['I150744194']]], 'cited_by_count': 1290, 'concepts': [['C115961682', '0.55168504'], ['C41008148', '0.53405654'], ['C31972630', '0.48944584'], ['C154945302', '0.44643486'], ['C121684516', '0.43588278']], 'referenced_works': ['W66427752', 'W140273596', 'W345598540', 'W1763426478', 'W1834627138', 'W1903029394', 'W1967577110', 'W1987010331', 'W1997903019', 'W2012875423', 'W2062618418', 'W2068470708', 'W2093212899', 'W2100415658', 'W2105038642', 'W2108598243', 'W2115273023', 'W2116013899', 'W2147800946', 'W2147901574', 'W2152516042', 'W2159693038', 'W2164490837', 'W2557414982', 'W2963420272', 'W2997095758', 'W3001217199', 'W3144890709', 'W4236692733', 'W4240726888'], 'abstract': 'We present a novel approach for image completion that results in images that are both locally and globally consistent. With a fully-convolutional neural network, we can complete images of arbitrary resolutions by filling-in missing regions of any shape. To train this image completion network to be consistent, we use global and local context discriminators that are trained to distinguish real images from completed ones. The global discriminator looks at the entire image to assess if it is coherent as a whole, while the local discriminator looks only at a small area centered at the completed region to ensure the local consistency of the generated patches. The image completion network is then trained to fool the both context discriminator networks, which requires it to generate images that are indistinguishable from real ones with regard to overall consistency as well as in details. We show that our approach can be used to complete a wide variety of scenes. Furthermore, in contrast with the patch-based approaches such as PatchMatch, our approach can generate fragments that do not appear elsewhere in the image, which allows us to naturally complete the images of objects with familiar and highly specific structures, such as faces.', 'counts_by_year': [[2022, 167], [2021, 323], [2020, 354], [2019, 287], [2018, 138], [2017, 15], [2016, 1]]}, {'id': 'W2607307034', 'doi': 'https://doi.org/10.1016/j.watres.2017.04.014', 'title': 'Mistakes and inconsistencies regarding adsorption of contaminants from aqueous solutions: A critical review', 'type': 'journal-article', 'publication_date': '2017-09-01', 'host_venue': 'V52566953', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2807445882', ['I151221077']], ['A3177917382', ['I151221077']], ['A1506660978', ['I4210166348']], ['A2778976791', ['I151221077']]], 'cited_by_count': 1287, 'concepts': [['C150394285', '0.8134423'], ['C2777310092', '0.7758243'], ['C9652623', '0.4781243'], ['C41008148', '0.45097324'], ['C184651966', '0.43736723']], 'referenced_works': ['W1890657190', 'W1919779920', 'W1968687271', 'W1969828902', 'W1971314282', 'W1972112507', 'W1972684360', 'W1973327099', 'W1973687238', 'W1975078736', 'W1978214351', 'W1978640119', 'W1979380543', 'W1979614281', 'W1981149420', 'W1981194112', 'W1983740774', 'W1984953206', 'W1986113254', 'W1988067617', 'W1988875621', 'W1991568324', 'W1992800763', 'W1992982227', 'W1993432520', 'W1993612803', 'W1995435180', 'W1996145503', 'W1996816692', 'W1996822951', 'W1997130984', 'W1997711425', 'W1998185917', 'W1998502673', 'W1999321735', 'W1999527696', 'W2001039849', 'W2001094610', 'W2003015013', 'W2003689563', 'W2003810107', 'W2004464872', 'W2004668044', 'W2005494158', 'W2006319148', 'W2008310830', 'W2009516392', 'W2010051414', 'W2010080818', 'W2010215642', 'W2011244933', 'W2012731965', 'W2016409770', 'W2016832682', 'W2018950007', 'W2020181012', 'W2020954811', 'W2022255840', 'W2023233323', 'W2024287959', 'W2026159702', 'W2028744523', 'W2029326621', 'W2030352617', 'W2032124032', 'W2032283635', 'W2033705851', 'W2035600072', 'W2035746317', 'W2035749031', 'W2036405018', 'W2036606490', 'W2037227386', 'W2038539215', 'W2038673661', 'W2040153588', 'W2042953363', 'W2044621747', 'W2046145149', 'W2048319840', 'W2048504787', 'W2049829804', 'W2052008448', 'W2052599976', 'W2054665873', 'W2055119556', 'W2055612057', 'W2056688910', 'W2060829709', 'W2060857867', 'W2062761807', 'W2065669130', 'W2066274648', 'W2066644206', 'W2071121520', 'W2072152237', 'W2073408443', 'W2073430993', 'W2074481963', 'W2079209529', 'W2082593310', 'W2083097028', 'W2083565363', 'W2083619273', 'W2084950453', 'W2085520409', 'W2086422325', 'W2087791308', 'W2088227395', 'W2089062653', 'W2089631849', 'W2089960817', 'W2091997912', 'W2092103619', 'W2093445341', 'W2093518556', 'W2094706797', 'W2096348730', 'W2097178144', 'W2097525189', 'W2111104483', 'W2117899149', 'W2122828946', 'W2130469967', 'W2131602139', 'W2135292116', 'W2136983589', 'W2141837911', 'W2146813843', 'W2157460471', 'W2162570309', 'W2163529355', 'W2173402607', 'W2173409788', 'W2187199564', 'W2197483714', 'W2207827760', 'W2274309816', 'W2277537480', 'W2290037907', 'W2300474821', 'W2320882040', 'W2322766914', 'W2323385024', 'W2333744648', 'W2338858199', 'W2340249546', 'W2340787998', 'W2347032618', 'W2347121088', 'W2403903894', 'W2469151508', 'W2470939982', 'W2521259950', 'W2530509048', 'W2534722109', 'W2537388762', 'W2547395427', 'W2555472932', 'W2557048044', 'W2564151498', 'W2581487417', 'W2590347101', 'W2591820131', 'W2592238808', 'W2611138748', 'W2979484942', 'W3140931271', 'W3149001505', 'W4231554351', 'W4233442784', 'W4236023481', 'W4248783688'], 'abstract': 'In recent years, adsorption science and technology for water and wastewater treatment has attracted substantial attention from the scientific community. However, the number of publications containing inconsistent concepts is increasing. Many publications either reiterate previously discussed mistakes or create new mistakes. The inconsistencies are reflected by the increasing publication of certain types of article in this field, including "short communications", "discussions", "critical reviews", "comments", "letters to the editor", and "correspondence (comment/rebuttal)". This article aims to discuss (1) the inaccurate use of technical terms, (2) the problem associated with quantities for measuring adsorption performance, (3) the important roles of the adsorbate and adsorbent pKa, (4) mistakes related to the study of adsorption kinetics, isotherms, and thermodynamics, (5) several problems related to adsorption mechanisms, (6) inconsistent data points in experimental data and model fitting, (7) mistakes in measuring the specific surface area of an adsorbent, and (8) other mistakes found in the literature. Furthermore, correct expressions and original citations of the relevant models (i.e., adsorption kinetics and isotherms) are provided. The authors hope that this work will be helpful for readers, researchers, reviewers, and editors who are interested in the field of adsorption studies.', 'counts_by_year': [[2022, 307], [2021, 358], [2020, 277], [2019, 213], [2018, 106], [2017, 17]]}, {'id': 'W2260960995', 'doi': 'https://doi.org/10.1002/adma.201504244', 'title': 'Flexible and Stretchable Physical Sensor Integrated Platforms for Wearable Human-Activity Monitoringand Personal Healthcare', 'type': 'journal-article', 'publication_date': '2016-06-01', 'host_venue': 'V99352657', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2172176892', ['I848706']], ['A2296376829', ['I848706']]], 'cited_by_count': 1280, 'concepts': [['C150594956', '0.8599691'], ['C2780598303', '0.80848205'], ['C54290928', '0.5902181'], ['C41325743', '0.5595155'], ['C2781196758', '0.4997425']], 'referenced_works': ['W21784219', 'W1139244977', 'W1573844195', 'W1661573186', 'W1772021317', 'W1871236778', 'W1900911187', 'W1926387934', 'W1943785911', 'W1964191741', 'W1965085030', 'W1965871852', 'W1969392009', 'W1970308461', 'W1970438428', 'W1970961037', 'W1971844777', 'W1972055561', 'W1973693657', 'W1975813261', 'W1976780894', 'W1977018044', 'W1977366816', 'W1977859733', 'W1979369614', 'W1979632448', 'W1979938295', 'W1982085124', 'W1982276321', 'W1983102280', 'W1985392057', 'W1986883928', 'W1986993716', 'W1988552153', 'W1988770798', 'W1989172022', 'W1989248068', 'W1993070126', 'W1993110089', 'W1995042675', 'W1998333175', 'W1999339091', 'W1999533146', 'W1999585120', 'W1999653735', 'W1999933062', 'W1999959880', 'W2000129545', 'W2000634863', 'W2001650137', 'W2001776513', 'W2002284451', 'W2002590494', 'W2002608843', 'W2002656537', 'W2002962459', 'W2004072722', 'W2004835256', 'W2007763899', 'W2010387091', 'W2010971702', 'W2011225964', 'W2011367371', 'W2011513227', 'W2011667905', 'W2012577889', 'W2016461817', 'W2016822963', 'W2017832059', 'W2019040493', 'W2020277470', 'W2020440059', 'W2021417561', 'W2022182826', 'W2022758665', 'W2023415963', 'W2025487006', 'W2027262352', 'W2028483415', 'W2029004044', 'W2030280774', 'W2032300755', 'W2032423464', 'W2033444367', 'W2033601397', 'W2033607182', 'W2034615315', 'W2035559809', 'W2036484700', 'W2037289764', 'W2038486771', 'W2038857141', 'W2039191438', 'W2039330441', 'W2039519879', 'W2040992870', 'W2044330180', 'W2045530696', 'W2046468431', 'W2046544546', 'W2050984075', 'W2051740096', 'W2055542322', 'W2055548486', 'W2057865119', 'W2059099067', 'W2059687579', 'W2059772445', 'W2061230903', 'W2061453480', 'W2062160114', 'W2065900416', 'W2066275639', 'W2067070990', 'W2068090639', 'W2069137854', 'W2071837176', 'W2072132220', 'W2072730328', 'W2076131235', 'W2078344108', 'W2078529521', 'W2078835847', 'W2080409247', 'W2081009679', 'W2081919843', 'W2085834391', 'W2087481902', 'W2088330671', 'W2088680052', 'W2089546492', 'W2090915280', 'W2092156403', 'W2093779999', 'W2095904636', 'W2096050373', 'W2097880175', 'W2098329693', 'W2099353039', 'W2100561453', 'W2100563097', 'W2101573662', 'W2103559710', 'W2103561623', 'W2105225099', 'W2105733071', 'W2106361199', 'W2107857447', 'W2111931721', 'W2112889810', 'W2113012396', 'W2114682092', 'W2114692928', 'W2114721754', 'W2115467748', 'W2118962721', 'W2122788387', 'W2122977338', 'W2124823640', 'W2129547735', 'W2129716057', 'W2132717567', 'W2136868307', 'W2138649957', 'W2141655728', 'W2146191673', 'W2146703650', 'W2147619583', 'W2149032140', 'W2154707832', 'W2155310328', 'W2155605920', 'W2157216919', 'W2158417770', 'W2159505440', 'W2161098413', 'W2161619292', 'W2161875271', 'W2164025253', 'W2165388742', 'W2167020753', 'W2167582037', 'W2168136601', 'W2169112538', 'W2169336864', 'W2170118506', 'W2171632855', 'W2313024814', 'W2314459593', 'W2315563763', 'W2319826792', 'W2322535512', 'W2323521842', 'W2323734260', 'W2324503552', 'W2327040532', 'W2327160382', 'W2330718237', 'W2332608106', 'W2334354910', 'W2405129552', 'W2488164446', 'W2768866949', 'W4213360791', 'W4230148542', 'W4241320250'], 'abstract': 'Flexible and stretchable physical sensors that can measure and quantify electrical signals generated by human activities are attracting a great deal of attention as they have unique characteristics, such as ultrathinness, low modulus, light weight, high flexibility, and stretchability. These flexible and stretchable physical sensors conformally attached on the surface of organs or skin can provide a new opportunity for human-activity monitoring and personal healthcare. Consequently, in recent years there has been considerable research effort devoted to the development of flexible and stretchable physical sensors to fulfill the requirements of future technology, and much progress has been achieved. Here, the most recent developments of flexible and stretchable physical sensors are described, including temperature, pressure, and strain sensors, and flexible and stretchable sensor-integrated platforms. The latest successful examples of flexible and stretchable physical sensors for the detection of temperature, pressure, and strain, as well as their novel structures, technological innovations, and challenges, are reviewed first. In the next section, recent progress regarding sensor-integrated wearable platforms is overviewed in detail. Some of the latest achievements regarding self-powered sensor-integrated wearable platform technologies are also reviewed. Further research direction and challenges are also proposed to develop a fully sensor-integrated wearable platform for monitoring human activity and personal healthcare in the near future.', 'counts_by_year': [[2022, 156], [2021, 247], [2020, 259], [2019, 273], [2018, 186], [2017, 133], [2016, 25]]}, {'id': 'W2788228074', 'doi': 'https://doi.org/10.1038/s41587-019-0072-8', 'title': 'Assembly of long, error-prone reads using repeat graphs', 'type': 'journal-article', 'publication_date': '2019-04-01', 'host_venue': 'V106963461', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A67813783', ['I36258959']], ['A2309808327', ['I36258959']], ['A2240457458', ['I118347636']], ['A2016289770', ['I36258959']]], 'cited_by_count': 1279, 'concepts': [['C18949551', '0.77600276'], ['C68767595', '0.63843954'], ['C41008148', '0.5985528'], ['C113425843', '0.52932185'], ['C132525143', '0.52355593']], 'referenced_works': ['W1556105006', 'W1579534339', 'W1982855075', 'W1983403167', 'W2002438422', 'W2003979986', 'W2013798014', 'W2064284095', 'W2099809434', 'W2120902911', 'W2151899848', 'W2153325555', 'W2157888653', 'W2179507357', 'W2194172909', 'W2538143681', 'W2562683361', 'W2580040938', 'W2591367881', 'W2602637791', 'W2607669908', 'W2621385661', 'W2625594140', 'W2784936077', 'W2802224661', 'W2808352813', 'W2950214184', 'W2950354111', 'W2952020649', 'W2953100295', 'W2953147862', 'W4233756358'], 'abstract': 'Accurate genome assembly is hampered by repetitive regions. Although long single molecule sequencing reads are better able to resolve genomic repeats than short-read data, most long-read assembly algorithms do not provide the repeat characterization necessary for producing optimal assemblies. Here, we present Flye, a long-read assembly algorithm that generates arbitrary paths in an unknown repeat graph, called disjointigs, and constructs an accurate repeat graph from these error-riddled disjointigs. We benchmark Flye against five state-of-the-art assemblers and show that it generates better or comparable assemblies, while being an order of magnitude faster. Flye nearly doubled the contiguity of the human genome assembly (as measured by the NGA50 assembly quality metric) compared with existing assemblers.', 'counts_by_year': [[2022, 449], [2021, 497], [2020, 271], [2019, 56], [2018, 2], [2017, 1]]}, {'id': 'W2471898040', 'doi': 'https://doi.org/10.1016/j.ultsonch.2016.06.035', 'title': 'Ultrasound assisted extraction of food and natural products. Mechanisms, techniques, combinations, protocols and applications. A review', 'type': 'journal-article', 'publication_date': '2017-01-01', 'host_venue': 'V130447213', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A1373557297', ['I198415970']], ['A1967810082', ['I198415970']], ['A332595743', ['I198415970']], ['A1687612524', ['I198415970']], ['A1921743233', ['I198415970']], ['A2115190518', ['I198415970']]], 'cited_by_count': 1276, 'concepts': [['C4725764', '0.63801855'], ['C183696295', '0.58219737'], ['C41008148', '0.53567725'], ['C143753070', '0.48493245'], ['C21880701', '0.46236613']], 'referenced_works': ['W210920136', 'W596920516', 'W985214746', 'W1543133300', 'W1611792871', 'W1955209431', 'W1964972086', 'W1965155103', 'W1967523238', 'W1967601894', 'W1967969586', 'W1967975287', 'W1968703014', 'W1968823102', 'W1972155313', 'W1974051833', 'W1974673092', 'W1975031905', 'W1975783767', 'W1975884593', 'W1976017237', 'W1982947882', 'W1984169245', 'W1985100640', 'W1985955602', 'W1986523767', 'W1988022564', 'W1990084472', 'W1992591582', 'W1993075237', 'W1995653174', 'W1996276246', 'W1998318114', 'W2000962924', 'W2003589909', 'W2004587319', 'W2005127689', 'W2005549038', 'W2005592034', 'W2007737186', 'W2008070225', 'W2010249706', 'W2013781398', 'W2017837734', 'W2019365034', 'W2020533101', 'W2025088645', 'W2026661310', 'W2026841870', 'W2027426020', 'W2028759296', 'W2031094331', 'W2033131420', 'W2035079118', 'W2035206254', 'W2036839875', 'W2039588983', 'W2041171075', 'W2041233816', 'W2043290687', 'W2045736540', 'W2047449842', 'W2051048916', 'W2052120911', 'W2054784597', 'W2057679661', 'W2058967038', 'W2059362270', 'W2060227369', 'W2060315917', 'W2064859762', 'W2065140801', 'W2067827600', 'W2072435855', 'W2072656918', 'W2073848363', 'W2077793491', 'W2078135174', 'W2078923649', 'W2083758760', 'W2084043161', 'W2086414578', 'W2087387713', 'W2087604631', 'W2090940239', 'W2091459598', 'W2091877338', 'W2094139212', 'W2095534151', 'W2096957460', 'W2100757875', 'W2106653086', 'W2114439125', 'W2128302122', 'W2132954073', 'W2133017055', 'W2139856732', 'W2140206850', 'W2148283430', 'W2155416931', 'W2168526937', 'W2170257079', 'W2219428855', 'W2300552860', 'W2301437343', 'W2334023175', 'W2340459819', 'W3021566866'], 'abstract': 'This review presents a complete picture of current knowledge on ultrasound-assisted extraction (UAE) in food ingredients and products, nutraceutics, cosmetic, pharmaceutical and bioenergy applications. It provides the necessary theoretical background and some details about extraction by ultrasound, the techniques and their combinations, the mechanisms (fragmentation, erosion, capillarity, detexturation, and sonoporation), applications from laboratory to industry, security, and environmental impacts. In addition, the ultrasound extraction procedures and the important parameters influencing its performance are also included, together with the advantages and the drawbacks of each UAE techniques. Ultrasound-assisted extraction is a research topic, which affects several fields of modern plant-based chemistry. All the reported applications have shown that ultrasound-assisted extraction is a green and economically viable alternative to conventional techniques for food and natural products. The main benefits are decrease of extraction and processing time, the amount of energy and solvents used, unit operations, and CO2 emissions.', 'counts_by_year': [[2022, 278], [2021, 304], [2020, 264], [2019, 209], [2018, 132], [2017, 77], [2016, 8]]}, {'id': 'W2963709863', 'doi': 'https://doi.org/10.1109/cvpr.2017.241', 'title': 'Learning from Simulated and Unsupervised Images through Adversarial Training', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2183241032', ['I1311269955']], ['A2798413642', ['I1311269955']], ['A1967568329', ['I1311269955']], ['A669260807', ['I1311269955']], ['A2565739635', ['I1311269955']], ['A3091056286', ['I1311269955']]], 'cited_by_count': 1275, 'concepts': [['C41008148', '0.8212265'], ['C2779803651', '0.78661597'], ['C154945302', '0.7347759'], ['C2776135515', '0.5855701'], ['C119857082', '0.5454635']], 'referenced_works': ['W1922126009', 'W1950149599', 'W1995694455', 'W2001252859', 'W2036196300', 'W2073246097', 'W2075156252', 'W2077532029', 'W2087806427', 'W2101032778', 'W2123576187', 'W2134557905', 'W2214145768', 'W2299591120', 'W2343052201', 'W2431874326', 'W2475287302', 'W2962690307', 'W2962759496', 'W3106262690'], 'abstract': 'With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulators output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a self-regularization term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.', 'counts_by_year': [[2022, 70], [2021, 242], [2020, 301], [2019, 356], [2018, 251], [2017, 55]]}, {'id': 'W2526646482', 'doi': 'https://doi.org/10.1038/nmat4756', 'title': 'Memristors with diffusive dynamics as synaptic emulators for neuromorphic computing', 'type': 'journal-article', 'publication_date': '2017-01-01', 'host_venue': 'V103895331', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2103116688', ['I24603500']], ['A2133618369', ['I24603500']], ['A2068343323', ['I143804889']], ['A2666036124', ['I24603500']], ['A2525371084', ['I24603500']], ['A2307678290', ['I24603500']], ['A2121048098', ['I1324840837']], ['A2101212584', ['I1324840837']], ['A1995234748', ['I1324840837']], ['A2304382102', ['I1324840837']], ['A2941929813', ['I1280414376']], ['A2223498537', ['I1280414376']], ['A2583753130', ['I24603500']], ['A1971852604', ['I111187474']], ['A2144852142', ['I1324840837']], ['A2135258759', ['I24603500']], ['A2250927587', ['I24603500']]], 'cited_by_count': 1269, 'concepts': [['C151927369', '0.93853986'], ['C150072547', '0.6727116'], ['C149810388', '0.6169336'], ['C192562407', '0.5718341'], ['C197341189', '0.4766615']], 'referenced_works': ['W1542981317', 'W1627694432', 'W1926340497', 'W1966010951', 'W1972384396', 'W1974734977', 'W1980490877', 'W1990915007', 'W2012543994', 'W2015088745', 'W2016922062', 'W2021656562', 'W2027354053', 'W2031781678', 'W2032832574', 'W2032916735', 'W2034368898', 'W2036145275', 'W2041308596', 'W2043707274', 'W2045197688', 'W2046491231', 'W2048500784', 'W2050066016', 'W2069164217', 'W2070856049', 'W2071947829', 'W2092268242', 'W2107433900', 'W2111892130', 'W2112927743', 'W2118980095', 'W2120557145', 'W2121917196', 'W2122702230', 'W2126194538', 'W2136401714', 'W2141071915', 'W2147101007', 'W2162341456', 'W2162651880', 'W2336611641', 'W2336669647', 'W2345186205', 'W2626876670', 'W2799167959', 'W3100847490', 'W4255760236'], 'abstract': 'The accumulation and extrusion of Ca2+ in the pre- and postsynaptic compartments play a critical role in initiating plastic changes in biological synapses. To emulate this fundamental process in electronic devices, we developed diffusive Ag-in-oxide memristors with a temporal response during and after stimulation similar to that of the synaptic Ca2+ dynamics. In situ high-resolution transmission electron microscopy and nanoparticle dynamics simulations both demonstrate that Ag atoms disperse under electrical bias and regroup spontaneously under zero bias because of interfacial energy minimization, closely resembling synaptic influx and extrusion of Ca2+, respectively. The diffusive memristor and its dynamics enable a direct emulation of both short- and long-term plasticity of biological synapses, representing an advance in hardware implementation of neuromorphic functionalities.', 'counts_by_year': [[2022, 203], [2021, 272], [2020, 271], [2019, 244], [2018, 198], [2017, 80], [2015, 1]]}, {'id': 'W2789444712', 'doi': 'https://doi.org/10.1080/00207543.2018.1444806', 'title': 'Industry 4.0: state of the art and future trends', 'type': 'journal-article', 'publication_date': '2018-03-09', 'host_venue': 'V65690446', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2389027000', ['I81365321']], ['A2501116981', ['I130238516']], ['A2488556515', ['I81365321']]], 'cited_by_count': 1269, 'concepts': [['C48103436', '0.5064496'], ['C2777986313', '0.43395266'], ['C144133560', '0.4086547'], ['C127413603', '0.37453666'], ['C40700', '0.3457142']], 'referenced_works': ['W1911193768', 'W1963489908', 'W1966100307', 'W1971665536', 'W1972012147', 'W1972223635', 'W1977777780', 'W1979519462', 'W1984571257', 'W1989457024', 'W1990911977', 'W1996746465', 'W1997298031', 'W2002014610', 'W2003641262', 'W2009715706', 'W2012296875', 'W2013306276', 'W2014631670', 'W2015606516', 'W2015908582', 'W2020443422', 'W2020851875', 'W2022709631', 'W2029608738', 'W2031922597', 'W2034055240', 'W2036149162', 'W2038194220', 'W2040980020', 'W2041100749', 'W2042001389', 'W2051593024', 'W2057401209', 'W2062908348', 'W2070665593', 'W2070944660', 'W2077658321', 'W2078185497', 'W2079977831', 'W2080363292', 'W2081754384', 'W2082266344', 'W2082433370', 'W2085495361', 'W2092623288', 'W2092910425', 'W2094636500', 'W2094951638', 'W2095258561', 'W2118849499', 'W2119046642', 'W2121120741', 'W2122827939', 'W2126112613', 'W2129450113', 'W2129466958', 'W2133720971', 'W2134747449', 'W2135138790', 'W2138341809', 'W2139531852', 'W2141050228', 'W2142332462', 'W2152589772', 'W2158397222', 'W2165238979', 'W2165887572', 'W2172214231', 'W2204466726', 'W2205445074', 'W2315851432', 'W2317016330', 'W2323775583', 'W2323950732', 'W2324704545', 'W2339884137', 'W2342655591', 'W2376897184', 'W2408328578', 'W2409571956', 'W2430898800', 'W2464780158', 'W2465639267', 'W2470546636', 'W2473678888', 'W2479367333', 'W2484890567', 'W2486296320', 'W2499976535', 'W2501162690', 'W2509409510', 'W2509600987', 'W2510606728', 'W2511504997', 'W2513195681', 'W2516274974', 'W2519166994', 'W2527920150', 'W2528033708', 'W2528223815', 'W2552831038', 'W2557892820', 'W2559164450', 'W2560147852', 'W2562329584', 'W2564588653', 'W2566728633', 'W2579022458', 'W2587290317', 'W2594125487', 'W2603008685', 'W2606213502', 'W2606416873', 'W2607327939', 'W2626730145', 'W2736038727', 'W2737940375', 'W2739295537', 'W2742910676', 'W2753007331', 'W2753155801', 'W2754682862', 'W2765533193', 'W2765905728', 'W2766199893', 'W2767828640', 'W2769228326', 'W2770733137', 'W2915361282', 'W2915547444', 'W3021457534', 'W4236312073', 'W4252045606', 'W4256613398', 'W4286276003'], 'abstract': 'Rapid advances in industrialisation and informatisation methods have spurred tremendous progress in developing the next generation of manufacturing technology. Today, we are on the cusp of the Fourth Industrial Revolution. In 2013, amongst one of 10 ‘Future Projects’ identified by the German government as part of its High-Tech Strategy 2020 Action Plan, the Industry 4.0 project is considered to be a major endeavour for Germany to establish itself as a leader of integrated industry. In 2014, China’s State Council unveiled their ten-year national plan, Made-in-China 2025, which was designed to transform China from the world’s workshop into a world manufacturing power. Made-in-China 2025 is an initiative to comprehensively upgrade China’s industry including the manufacturing sector. In Industry 4.0 and Made-in-China 2025, many applications require a combination of recently emerging new technologies, which is giving rise to the emergence of Industry 4.0. Such technologies originate from different disciplines ...', 'counts_by_year': [[2022, 276], [2021, 453], [2020, 341], [2019, 167], [2018, 30], [2012, 1]]}, {'id': 'W2572080171', 'doi': 'https://doi.org/10.1038/nprot.2016.169', 'title': 'The ClusPro web server for protein–protein docking', 'type': 'journal-article', 'publication_date': '2017-02-01', 'host_venue': 'V109387254', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A1966578480', ['I59553526']], ['A2324847765', ['I4210119794']], ['A2435344188', ['I111088046']], ['A2554882342', ['I111088046']], ['A2554839044', ['I59553526']], ['A2170965571', ['I111088046']], ['A1907627525', ['I111088046']], ['A2031281642', ['I111088046']]], 'cited_by_count': 1266, 'concepts': [['C119145174', '0.8211938'], ['C11392498', '0.7256002'], ['C41685203', '0.7255721'], ['C65556437', '0.69196486'], ['C41008148', '0.6396253']], 'referenced_works': ['W1595347157', 'W1651271348', 'W1875868233', 'W1899503782', 'W1951398569', 'W1965078663', 'W1965410913', 'W1966090426', 'W1971350955', 'W1973119987', 'W1973382479', 'W1975153518', 'W1975315212', 'W1976646890', 'W1977198374', 'W1978376428', 'W1983954282', 'W1984106100', 'W1988122006', 'W1988315750', 'W1997639796', 'W1998520443', 'W1999218203', 'W2000383223', 'W2005303195', 'W2008195298', 'W2008937207', 'W2012194103', 'W2013792107', 'W2014744153', 'W2018410079', 'W2019611548', 'W2027403155', 'W2031746097', 'W2032778333', 'W2037036397', 'W2037974914', 'W2039063819', 'W2043659227', 'W2047467443', 'W2048643046', 'W2049451881', 'W2053538195', 'W2054099123', 'W2054394841', 'W2055905702', 'W2058015311', 'W2058369753', 'W2060417147', 'W2061394683', 'W2064660701', 'W2068456988', 'W2073530208', 'W2074104861', 'W2074169396', 'W2074844635', 'W2077472794', 'W2077794219', 'W2078171377', 'W2081180630', 'W2084619201', 'W2085942874', 'W2091734240', 'W2092002923', 'W2092214022', 'W2097031841', 'W2098150321', 'W2099286799', 'W2099367604', 'W2100587026', 'W2101304778', 'W2101395824', 'W2102312699', 'W2104299396', 'W2105438341', 'W2107300380', 'W2107602721', 'W2108689162', 'W2108852701', 'W2111387452', 'W2112743424', 'W2116457925', 'W2118542302', 'W2123883108', 'W2126265982', 'W2128036243', 'W2128039232', 'W2128326332', 'W2128484254', 'W2128971907', 'W2129358707', 'W2129414710', 'W2132378522', 'W2133712572', 'W2133842820', 'W2137167381', 'W2143549808', 'W2143881057', 'W2144689095', 'W2145154167', 'W2145369603', 'W2147860818', 'W2150399099', 'W2151460035', 'W2152872596', 'W2153255725', 'W2153731681', 'W2153983456', 'W2159817444', 'W2159820766', 'W2160868144', 'W2162166182', 'W2162682281', 'W2163462520', 'W2165531467', 'W2166876734', 'W2166988813', 'W2167300514', 'W2167869241', 'W2169829977', 'W2170421654', 'W2171249995', 'W2304188484', 'W2343238565', 'W2345139079', 'W2413736521', 'W2419381003', 'W2423377342', 'W2465190302', 'W2514674691', 'W2915344963', 'W3156843434', 'W4238813159'], 'abstract': 'The ClusPro server (https://cluspro.org) is a widely used tool for protein-protein docking. The server provides a simple home page for basic use, requiring only two files in Protein Data Bank (PDB) format. However, ClusPro also offers a number of advanced options to modify the search; these include the removal of unstructured protein regions, application of attraction or repulsion, accounting for pairwise distance restraints, construction of homo-multimers, consideration of small-angle X-ray scattering (SAXS) data, and location of heparin-binding sites. Six different energy functions can be used, depending on the type of protein. Docking with each energy parameter set results in ten models defined by centers of highly populated clusters of low-energy docked structures. This protocol describes the use of the various options, the construction of auxiliary restraints files, the selection of the energy parameters, and the analysis of the results. Although the server is heavily used, runs are generally completed in <4 h.', 'counts_by_year': [[2022, 289], [2021, 365], [2020, 282], [2019, 188], [2018, 112], [2017, 29]]}, {'id': 'W2607041014', 'doi': 'https://doi.org/10.1109/cvpr.2017.618', 'title': 'Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution', 'type': 'proceedings-article', 'publication_date': '2017-04-12', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2724218609', ['I156087764']], ['A2129541952', ['I859038795']], ['A2129011837', ['I157725225']], ['A2112462370', ['I156087764']]], 'cited_by_count': 1266, 'concepts': [['C110384440', '0.90146065'], ['C41008148', '0.7607293'], ['C49608258', '0.67577106'], ['C154945302', '0.658006'], ['C185798385', '0.65206766']], 'referenced_works': ['W7682646', 'W845365781', 'W1677182931', 'W1885185971', 'W1919542679', 'W1930824406', 'W1950594372', 'W1963882359', 'W1976416062', 'W2047920195', 'W2097074225', 'W2098506229', 'W2103504761', 'W2110158442', 'W2118963448', 'W2142884912', 'W2149669120', 'W2210480155', 'W2214802144', 'W2242218935', 'W2476548250', 'W2503339013', 'W2534320940', 'W2963470893', 'W3138063419', 'W4249022109'], 'abstract': 'Convolutional neural networks have recently demonstrated high-quality reconstruction for single-image super-resolution. In this paper, we propose the Laplacian Pyramid Super-Resolution Network (LapSRN) to progressively reconstruct the sub-band residuals of high-resolution images. At each pyramid level, our model takes coarse-resolution feature maps as input, predicts the high-frequency residuals, and uses transposed convolutions for upsampling to the finer level. Our method does not require the bicubic interpolation as the pre-processing step and thus dramatically reduces the computational complexity. We train the proposed LapSRN with deep supervision using a robust Charbonnier loss function and achieve high-quality reconstruction. Furthermore, our network generates multi-scale predictions in one feed-forward pass through the progressive reconstruction, thereby facilitates resource-aware applications. Extensive quantitative and qualitative evaluations on benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of speed and accuracy.', 'counts_by_year': [[2022, 183], [2021, 348], [2020, 339], [2019, 243], [2018, 136], [2017, 15]]}, {'id': 'W2963250244', 'doi': 'https://doi.org/10.18653/v1/d18-2012', 'title': 'SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing', 'type': 'proceedings-article', 'publication_date': '2018-08-19', 'host_venue': 'V4306418267', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2141901792', ['I1291425158']], ['A2250344025', ['I22299242']]], 'cited_by_count': 1265, 'concepts': [['C41008148', '0.88028514'], ['C203005215', '0.75618106'], ['C519991488', '0.70132923'], ['C204321447', '0.6615982'], ['C154945302', '0.6043403']], 'referenced_works': ['W1591706642', 'W1902237438', 'W2101105183', 'W2525778437', 'W2550821151', 'W2759088880', 'W2962784628', 'W2962824887', 'W2962965405', 'W2963011474', 'W2963403868', 'W2963532001', 'W2963979492', 'W2964308564'], 'abstract': 'This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.', 'counts_by_year': [[2022, 102], [2021, 563], [2020, 453], [2019, 145], [2018, 2]]}, {'id': 'W2321533354', 'doi': 'https://doi.org/10.1007/978-3-319-46466-4_5', 'title': 'Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles', 'type': 'book-chapter', 'publication_date': '2016-10-08', 'host_venue': 'V106296714', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2567524684', ['I118564535']], ['A2019402600', ['I118564535']]], 'cited_by_count': 1263, 'concepts': [['C2779405079', '0.98854595'], ['C41008148', '0.8292138'], ['C154945302', '0.53111744'], ['C8038995', '0.49137893'], ['C119857082', '0.34758383']], 'referenced_works': ['W219040644', 'W343636949', 'W1520997877', 'W1536680647', 'W1849277567', 'W1915485278', 'W1976015505', 'W1982931780', 'W1987488988', 'W2008508961', 'W2017257315', 'W2037227137', 'W2053186076', 'W2097308346', 'W2102605133', 'W2105464873', 'W2108598243', 'W2120480077', 'W2154889144', 'W2155893237', 'W2160921898', 'W2163922914', 'W2963420272', 'W4240035482'], 'abstract': 'We propose a novel unsupervised learning approach to build features suitable for object detection and classification. The features are pre-trained on a large dataset without human annotation and later transferred via fine-tuning on a different, smaller and labeled dataset. The pre-training consists of solving jigsaw puzzles of natural images. To facilitate the transfer of features to other tasks, we introduce the context-free network (CFN), a siamese-ennead convolutional neural network. The features correspond to the columns of the CFN and they process image tiles independently (i.e., free of context). The later layers of the CFN then use the features to identify their geometric arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. We pre-train the CFN on the training set of the ILSVRC2012 dataset and transfer the features on the combined training and validation set of Pascal VOC 2007 for object detection (via fast RCNN) and classification. These features outperform all current unsupervised features with \\(51.8\\,\\%\\) for detection and \\(68.6\\,\\%\\) for classification, and reduce the gap with supervised learning (\\(56.5\\,\\%\\) and \\(78.2\\,\\%\\) respectively).', 'counts_by_year': [[2022, 140], [2021, 479], [2020, 395], [2019, 143], [2018, 65], [2017, 30], [2016, 10]]}, {'id': 'W2905810301', 'doi': 'https://doi.org/10.1038/s41591-018-0316-z', 'title': 'A guide to deep learning in healthcare', 'type': 'journal-article', 'publication_date': '2019-01-01', 'host_venue': 'V203256638', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2116717815', ['I97018004']], ['A2274990849', ['I97018004']], ['A1704656907', ['I97018004']], ['A2058266270', ['I97018004']], ['A164686048', ['I1291425158']], ['A2346477986', ['I1291425158']], ['A2277345763', ['I1291425158']], ['A1994222016', ['I1291425158']], ['A2075956027', ['I97018004']], ['A2429370538', ['I1291425158']]], 'cited_by_count': 1263, 'concepts': [['C108583219', '0.6861064'], ['C41008148', '0.6567933'], ['C154945302', '0.63502175'], ['C2779343474', '0.625151'], ['C97541855', '0.5921335']], 'referenced_works': ['W22040386', 'W1019830208', 'W1663984431', 'W1895577753', 'W1901129140', 'W1964502429', 'W1966775465', 'W1967021867', 'W1975463331', 'W1999874108', 'W2011582941', 'W2014347464', 'W2024692616', 'W2025468671', 'W2079718713', 'W2088338354', 'W2110243528', 'W2120869188', 'W2160815625', 'W2160995259', 'W2221443338', 'W2257979135', 'W2262715205', 'W2264017649', 'W2373570000', 'W2396881363', 'W2460266206', 'W2493683088', 'W2525106365', 'W2557738935', 'W2559794190', 'W2581082771', 'W2592929672', 'W2618245351', 'W2625625371', 'W2786635213', 'W2788633781', 'W2886281300', 'W2897705349', 'W2919115771', 'W2951560283', 'W2951572348', 'W2952935243', 'W2963647178', 'W3098949126'], 'abstract': 'Here we present deep-learning techniques for healthcare, centering our discussion on deep learning in computer vision, natural language processing, reinforcement learning, and generalized methods. We describe how these computational techniques can impact a few key areas of medicine and explore how to build end-to-end systems. Our discussion of computer vision focuses largely on medical imaging, and we describe the application of natural language processing to domains such as electronic health record data. Similarly, reinforcement learning is discussed in the context of robotic-assisted surgery, and generalized deep-learning methods for genomics are reviewed.', 'counts_by_year': [[2022, 321], [2021, 458], [2020, 358], [2019, 121], [2018, 2]]}, {'id': 'W2913323966', 'doi': 'https://doi.org/10.1038/s41586-019-0912-1', 'title': 'Deep learning and process understanding for data-driven Earth system science', 'type': 'journal-article', 'publication_date': '2019-02-13', 'host_venue': 'V137773608', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A1966552824', ['I4210154168']], ['A2174097060', ['I16097986']], ['A2135560984', ['I4210163131']], ['A2157031145', ['I4210154168']], ['A1988407101', ['I76198965']], ['A1943359396', ['I83558840', 'I4210154168']], ['A208362897', ['I148283060']]], 'cited_by_count': 1259, 'concepts': [['C9770341', '0.70819545'], ['C41008148', '0.6864571'], ['C98045186', '0.6457487'], ['C80368990', '0.6248671'], ['C154945302', '0.6081605']], 'referenced_works': ['W59495185', 'W1019830208', 'W1494192115', 'W1520204840', 'W1541028045', 'W1584663654', 'W1789155650', 'W1896386811', 'W1965256357', 'W1979723077', 'W1980901385', 'W1981213426', 'W1990653740', 'W2008847349', 'W2025540709', 'W2026058014', 'W2026647979', 'W2033904036', 'W2036197592', 'W2048217351', 'W2050648603', 'W2052648234', 'W2055308682', 'W2059111237', 'W2076063813', 'W2077968790', 'W2081340599', 'W2102892532', 'W2104043570', 'W2108564811', 'W2108598243', 'W2125621954', 'W2126479957', 'W2130694448', 'W2134844037', 'W2140351252', 'W2144841545', 'W2148556297', 'W2152550446', 'W2165811161', 'W2167881994', 'W2167894638', 'W2176178328', 'W2255002787', 'W2267317359', 'W2280849556', 'W2298779432', 'W2301696792', 'W2338949170', 'W2412588858', 'W2443985281', 'W2516419770', 'W2554311679', 'W2560167313', 'W2572951874', 'W2587023990', 'W2588003345', 'W2601923741', 'W2614292202', 'W2620513537', 'W2620779710', 'W2625876696', 'W2657631929', 'W2734256217', 'W2792309568', 'W2804943168', 'W2808400960', 'W2869444156', 'W2901801216', 'W2919115771', 'W2963689043', 'W2964128214'], 'abstract': 'Machine learning approaches are increasingly used to extract patterns and insights from the ever-increasing stream of geospatial data, but current approaches may not be optimal when system behaviour is dominated by spatial or temporal context. Here, rather than amending classical machine learning, we argue that these contextual cues should be used as part of deep learning (an approach that is able to extract spatio-temporal features automatically) to gain further process understanding of Earth system science problems, improving the predictive ability of seasonal forecasting and modelling of long-range spatial connections across multiple timescales, for example. The next step will be a hybrid modelling approach, coupling physical process models with the versatility of data-driven machine learning.', 'counts_by_year': [[2022, 419], [2021, 467], [2020, 286], [2019, 82], [2018, 1]]}, {'id': 'W1912456745', 'doi': 'https://doi.org/10.4135/9781506326139.n729', 'title': 'Utilization-Focused Evaluation', 'type': 'reference-entry', 'publication_date': '2018-01-01', 'host_venue': 'V66810094', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2105298251', ['I204669301']]], 'cited_by_count': 1256, 'concepts': [['C2778023277', '0.8038629'], ['C112930515', '0.57261145'], ['C98045186', '0.5598179'], ['C3018395757', '0.535264'], ['C41008148', '0.5190016']], 'referenced_works': ['W1964808568', 'W2003030682', 'W2031714407', 'W2047926687', 'W2065065287', 'W2133485235', 'W2146835964', 'W2316848901', 'W4233301761'], 'abstract': 'Utilization-focused evaluation begins with the premise that evaluations should be judged by their utility and actual use; therefore, evaluators should facilitate the evaluation process and design any evaluation with careful consideration of how everything that is done, from beginning to end, will affect use. This is consistent with standards developed by the Joint Committee on Standards for Evaluation and adopted by the American Evaluation Association that evaluations should be judged by their utility, feasibility, propriety, and accuracy. (See chapter on standards and principles for evaluations.)', 'counts_by_year': [[2022, 4], [2021, 21], [2020, 29], [2019, 24], [2018, 39], [2017, 27], [2016, 58], [2015, 76], [2014, 106], [2013, 57], [2012, 53]]}, {'id': 'W2657631929', 'doi': 'https://doi.org/10.1016/j.dsp.2017.10.011', 'title': 'Methods for interpreting and understanding deep neural networks', 'type': 'journal-article', 'publication_date': '2018-02-01', 'host_venue': 'V64117906', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2037010747', ['I4577782']], ['A2786373057', ['I2800274787']], ['A2012736320', ['I197347611', 'I4577782', 'I4210109712']]], 'cited_by_count': 1256, 'concepts': [['C41008148', '0.7837231'], ['C527412718', '0.6571496'], ['C2984842247', '0.61542714'], ['C154945302', '0.614321'], ['C50644808', '0.5645763']], 'referenced_works': ['W1019830208', 'W1498436455', 'W1531674615', 'W1727290854', 'W1787224781', 'W2011402106', 'W2020786104', 'W2021191744', 'W2061627538', 'W2072735345', 'W2083844448', 'W2084680372', 'W2095350947', 'W2096192437', 'W2099509424', 'W2118022153', 'W2144869872', 'W2152171700', 'W2195388612', 'W2240067561', 'W2527189750', 'W2561412020', 'W2585152223', 'W2602556590', 'W2963287333', 'W3102449990'], 'abstract': 'This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most efficient use of these techniques on real data. It also discusses a number of practical applications.', 'counts_by_year': [[2022, 204], [2021, 359], [2020, 331], [2019, 247], [2018, 99], [2017, 15], [2012, 1]]}, {'id': 'W2768696376', 'doi': 'https://doi.org/10.1016/j.future.2017.11.022', 'title': 'IoT security: Review, blockchain solutions, and open challenges', 'type': 'journal-article', 'publication_date': '2017-11-01', 'host_venue': 'V186357190', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2429178468', ['I127670440']], ['A2293999425', ['I176601375']]], 'cited_by_count': 1255, 'concepts': [['C2779687700', '0.9635962'], ['C41008148', '0.8991653'], ['C81860439', '0.62437403'], ['C38652104', '0.58970827']], 'referenced_works': ['W1639305476', 'W1656678770', 'W1904483788', 'W1909266099', 'W1971673042', 'W1980035202', 'W1986755722', 'W2011778831', 'W2028956415', 'W2041374101', 'W2052938009', 'W2054439668', 'W2056973119', 'W2060409434', 'W2077921734', 'W2087903610', 'W2093678351', 'W2093798919', 'W2095788961', 'W2101890615', 'W2104149471', 'W2104250954', 'W2104927807', 'W2105103777', 'W2105442001', 'W2110221314', 'W2121720842', 'W2127305011', 'W2142411122', 'W2150411054', 'W2157283388', 'W2206531137', 'W2234639732', 'W2253635105', 'W2254691854', 'W2280971486', 'W2293247967', 'W2392113277', 'W2476563174', 'W2540162589', 'W2541307573', 'W2546016631', 'W2576765166', 'W2604813584', 'W2624953603', 'W2690196614', 'W2752019525', 'W2963264685', 'W3122025391', 'W4230622395', 'W4232836212', 'W4236546623', 'W4290473017', 'W4292117754'], 'abstract': 'Abstract   With the advent of smart homes, smart cities, and smart everything, the Internet of Things (IoT) has emerged as an area of incredible impact, potential, and growth, with Cisco Inc. predicting to have 50 billion connected devices by 2020. However, most of these IoT devices are easy to hack and compromise. Typically, these IoT devices are limited in compute, storage, and network capacity, and therefore they are more vulnerable to attacks than other endpoint devices such as smartphones, tablets, or computers.  In this paper, we present and survey major security issues for IoT. We review and categorize popular security issues with regard to the IoT layered architecture, in addition to protocols used for networking, communication, and management. We outline security requirements for IoT along with the existing attacks, threats, and state-of-the-art solutions. Furthermore, we tabulate and map IoT security problems against existing solutions found in the literature. More importantly, we discuss, how blockchain, which is the underlying technology for bitcoin, can be a key enabler to solve many IoT security problems. The paper also identifies open research problems and challenges for IoT security.', 'counts_by_year': [[2022, 259], [2021, 362], [2020, 330], [2019, 248], [2018, 56]]}, {'id': 'W3101450547', 'doi': 'https://doi.org/10.1109/comst.2016.2621116', 'title': 'Power-Domain Non-Orthogonal Multiple Access (NOMA) in 5G Systems: Potentials and Challenges', 'type': 'journal-article', 'publication_date': '2017-01-22', 'host_venue': 'V23688054', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2949944473', ['I191879574']], ['A1578369453', ['I191879574']], ['A125495501', ['I130438778']], ['A2093473277', ['I191879574']]], 'cited_by_count': 1253, 'concepts': [['C2775918612', '0.929783'], ['C41008148', '0.80608237'], ['C83204339', '0.6113808'], ['C16021271', '0.5412012'], ['C47798520', '0.5000253']], 'referenced_works': ['W1500900870', 'W1503128632', 'W1505688808', 'W1513760385', 'W1668816244', 'W1690504809', 'W1905762937', 'W1922937245', 'W1958281780', 'W1974978002', 'W1980246112', 'W1987011617', 'W1994828487', 'W1997834106', 'W2014050847', 'W2016550442', 'W2034768305', 'W2042519026', 'W2045262861', 'W2045299395', 'W2051943016', 'W2066452435', 'W2075829594', 'W2077793016', 'W2083213425', 'W2084498548', 'W2084670088', 'W2084872270', 'W2086559523', 'W2091815971', 'W2102241149', 'W2107080958', 'W2108284059', 'W2113129849', 'W2115973658', 'W2116334496', 'W2124894634', 'W2127531738', 'W2131164410', 'W2138020668', 'W2152332139', 'W2157386333', 'W2162436073', 'W2166782072', 'W2168284980', 'W2170531625', 'W2171498754', 'W2182489736', 'W2184092541', 'W2185882774', 'W2186171661', 'W2187903755', 'W2187915581', 'W2188659504', 'W2188857870', 'W2204409796', 'W2204482951', 'W2211498766', 'W2273854459', 'W2296441616', 'W2465293689', 'W2485002209', 'W2497369044', 'W2509696551', 'W2528407593', 'W2535070481', 'W2584493856', 'W3098197514', 'W3099378150', 'W3103423272', 'W3103537411', 'W3103737847', 'W3124127183', 'W4229922087'], 'abstract': 'Non-orthogonal multiple access (NOMA) is one of the promising radio access techniques for performance enhancement in next-generation cellular communications. Compared to orthogonal frequency division multiple access (OFDMA), which is a well-known high-capacity orthogonal multiple access (OMA) technique, NOMA offers a set of desirable benefits, including greater spectrum efficiency. There are different types of NOMA techniques, including power-domain and code-domain. This paper primarily focuses on power-domain NOMA that utilizes superposition coding (SC) at the transmitter and successive interference cancellation (SIC) at the receiver. Various researchers have demonstrated that NOMA can be used effectively to meet both network-level and user-experienced data rate requirements of fifth-generation (5G) technologies. From that perspective, this paper comprehensively surveys the recent progress of NOMA in 5G systems, reviewing the state-of-the-art capacity analysis, power allocation strategies, user fairness, and user-pairing schemes in NOMA. In addition, this paper discusses how NOMA performs when it is integrated with various proven wireless communications techniques, such as cooperative communications, multiple input multiple output (MIMO), beamforming, space time coding, and network coding, among others. Furthermore, this paper discusses several important issues on NOMA implementation and provides some avenues for future research.', 'counts_by_year': [[2022, 163], [2021, 271], [2020, 318], [2019, 302], [2018, 155], [2017, 44]]}, {'id': 'W2036595722', 'doi': 'https://doi.org/10.1002/9781119618232.ch1', 'title': 'Introduction to Quantum Mechanics', 'type': 'other', 'publication_date': '2019-10-16', 'host_venue': 'V137773608', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2150096637', ['I55486353']]], 'cited_by_count': 1249, 'concepts': [['C96469262', '0.44367716'], ['C33332235', '0.44038558'], ['C41008148', '0.3777276'], ['C121332964', '0.3596301'], ['C33923547', '0.24919808']], 'referenced_works': ['W4235672057'], 'abstract': 'Changes and additions to the new edition of this classic textbook include a new chapter on symmetries, new problems and examples, improved explanations, more numerical problems to be worked on a computer, new applications to solid state physics, and consolidated treatment of time-dependent potentials.', 'counts_by_year': [[2022, 2], [2021, 82], [2020, 73], [2019, 75], [2018, 44], [2017, 61], [2016, 68], [2015, 95], [2014, 84], [2013, 97], [2012, 75]]}, {'id': 'W1802734970', 'doi': 'https://doi.org/10.1039/c5ee02733k', 'title': 'Organometal halide perovskite solar cells: degradation and stability', 'type': 'journal-article', 'publication_date': '2016-02-09', 'host_venue': 'V117082959', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2342421321', ['I154864474']], ['A2171811638', ['I154864474']], ['A2617508039', ['I154864474']], ['A2142764006', ['I154864474']], ['A2164770921', ['I154864474']], ['A2655010647', ['I154864474']], ['A2117071740', ['I154864474']], ['A2166790823', ['I154864474']], ['A1439113960', ['I154864474']], ['A2113281132', ['I154864474']]], 'cited_by_count': 1247, 'concepts': [['C171560689', '0.91472936'], ['C155011858', '0.85299635'], ['C2779679103', '0.82495'], ['C192562407', '0.48632604'], ['C42360764', '0.4620065']], 'referenced_works': ['W1766513776', 'W1889636542', 'W1896289247', 'W1931132907', 'W1963891503', 'W1965464747', 'W1965921951', 'W1968571822', 'W1968636500', 'W1968683287', 'W1969732071', 'W1969815211', 'W1969831726', 'W1970810735', 'W1971231641', 'W1972590175', 'W1972633320', 'W1974713017', 'W1976139237', 'W1976254827', 'W1978249658', 'W1979086238', 'W1980609462', 'W1980893518', 'W1981420304', 'W1983239290', 'W1983978653', 'W1984159322', 'W1985363521', 'W1987412006', 'W1988182882', 'W1988567331', 'W1989767748', 'W1990489012', 'W1990582950', 'W1991833576', 'W1991889838', 'W1991981401', 'W1993691574', 'W1994663208', 'W1998211376', 'W1998343415', 'W1998961727', 'W1999228708', 'W1999523901', 'W1999556083', 'W1999717631', 'W2000114161', 'W2000237547', 'W2000841635', 'W2001192581', 'W2001574526', 'W2002774102', 'W2003945084', 'W2004069199', 'W2004873235', 'W2005193722', 'W2005631912', 'W2005868847', 'W2006151188', 'W2006304022', 'W2006619554', 'W2007452336', 'W2007866565', 'W2008106023', 'W2008130017', 'W2009398088', 'W2009851838', 'W2010053129', 'W2011117704', 'W2012143220', 'W2012327696', 'W2012394813', 'W2013334120', 'W2013495179', 'W2014034164', 'W2015620390', 'W2017367538', 'W2018326691', 'W2018416757', 'W2019162166', 'W2019358059', 'W2019557326', 'W2020073730', 'W2020683240', 'W2022879044', 'W2022968784', 'W2023831526', 'W2024069373', 'W2025962660', 'W2025986565', 'W2026663178', 'W2027583341', 'W2028410080', 'W2028616464', 'W2028782049', 'W2029738921', 'W2031186092', 'W2032361405', 'W2033656168', 'W2035920177', 'W2040394151', 'W2041391801', 'W2041863854', 'W2042468043', 'W2044157182', 'W2044270989', 'W2044423188', 'W2045287719', 'W2045699336', 'W2046025314', 'W2046484968', 'W2048126895', 'W2050011196', 'W2050079556', 'W2050416247', 'W2051021209', 'W2051196502', 'W2053703206', 'W2054093403', 'W2054128950', 'W2055279278', 'W2055944332', 'W2057327293', 'W2057514692', 'W2057588945', 'W2059166959', 'W2059735475', 'W2065204918', 'W2066151171', 'W2066644325', 'W2066938000', 'W2067910821', 'W2068085953', 'W2069492492', 'W2069766621', 'W2070589101', 'W2070866370', 'W2071503210', 'W2071858459', 'W2073126198', 'W2073292260', 'W2073726416', 'W2075511306', 'W2076723780', 'W2077477317', 'W2078807245', 'W2079941673', 'W2081043854', 'W2081052871', 'W2081274994', 'W2081640113', 'W2081879098', 'W2081909864', 'W2082052057', 'W2082202171', 'W2082367686', 'W2083117871', 'W2083256613', 'W2084208871', 'W2086361343', 'W2086776294', 'W2088032404', 'W2091279781', 'W2091471276', 'W2091871541', 'W2091960457', 'W2093596699', 'W2093689405', 'W2095062688', 'W2095477500', 'W2095995071', 'W2098446738', 'W2098911987', 'W2100436521', 'W2100438467', 'W2100716359', 'W2102969048', 'W2102969622', 'W2103763354', 'W2105143151', 'W2106064741', 'W2106658818', 'W2108339468', 'W2110905820', 'W2111861485', 'W2112517274', 'W2114118829', 'W2115374632', 'W2116004146', 'W2116053603', 'W2116992651', 'W2121304740', 'W2122291807', 'W2123478102', 'W2125469086', 'W2129970907', 'W2131232103', 'W2133130045', 'W2134161979', 'W2134324115', 'W2135803814', 'W2136291721', 'W2140042874', 'W2143296232', 'W2143338675', 'W2144574847', 'W2147193038', 'W2147269441', 'W2151579308', 'W2151915808', 'W2152034665', 'W2152772359', 'W2153273832', 'W2153278281', 'W2156019254', 'W2156846970', 'W2156925746', 'W2160391726', 'W2161922874', 'W2162504912', 'W2167590372', 'W2167945761', 'W2167982574', 'W2169328609', 'W2171105325', 'W2256398192', 'W2271355543', 'W2281513767', 'W2313540142', 'W2314462761', 'W2314579407', 'W2314692321', 'W2316199401', 'W2317208862', 'W2318514767', 'W2320528970', 'W2320884563', 'W2321732853', 'W2322428624', 'W2323066956', 'W2324370679', 'W2324528558', 'W2325412154', 'W2327499546', 'W2328576186', 'W2329908301', 'W2332570151', 'W2332980192', 'W2334620001', 'W2507995640', 'W2952456644', 'W3105103439', 'W4237048808', 'W4239309783', 'W4292053748'], 'abstract': 'What are the bottlenecks for organometal halide perovskite solar cells to achieve the stability required for commercialization?', 'counts_by_year': [[2022, 131], [2021, 194], [2020, 193], [2019, 207], [2018, 215], [2017, 207], [2016, 98]]}, {'id': 'W2964110616', 'doi': 'https://doi.org/10.18653/v1/p19-1285', 'title': 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context', 'type': 'proceedings-article', 'publication_date': '2019-01-09', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2616911546', ['I74973139']], ['A2489788272', ['I74973139']], ['A2159253281', ['I74973139']], ['A2100444261', ['I74973139']], ['A2148448995', ['I74973139']], ['A2031945151', ['I1291425158']]], 'cited_by_count': 1247, 'concepts': [['C100279451', '0.7936512'], ['C41008148', '0.7464629'], ['C137293760', '0.7055388'], ['C66322947', '0.6549191'], ['C206134035', '0.58990186']], 'referenced_works': ['W179875071', 'W1525783482', 'W1800356822', 'W1810943226', 'W1999965501', 'W2064675550', 'W2132339004', 'W2145543707', 'W2170973209', 'W2197913429', 'W2259472270', 'W2510842514', 'W2525246036', 'W2525332836', 'W2553303224', 'W2605203995', 'W2743945814', 'W2792376130', 'W2792764867', 'W2795285343', 'W2804845563', 'W2810075754', 'W2900096133', 'W2950527759', 'W2951672049', 'W2951714314', 'W2952339051', 'W2952723479', 'W2962739339', 'W2962746461', 'W2962754271', 'W2962964385', 'W2963034893', 'W2963088785', 'W2963266340', 'W2963304263', 'W2963341956', 'W2963403868', 'W2963537482', 'W2963573053', 'W2963631907', 'W2963925437', 'W2963951265', 'W2963970792', 'W2963983719', 'W2964059481', 'W2964269252', 'W2964308564', 'W2964347220', 'W2964348070'], 'abstract': 'Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.', 'counts_by_year': [[2022, 171], [2021, 534], [2020, 406], [2019, 134], [2018, 1]]}, {'id': 'W2963212250', 'doi': 'https://doi.org/10.18653/v1/p17-4012', 'title': 'OpenNMT: Open-Source Toolkit for Neural Machine Translation', 'type': 'proceedings-article', 'publication_date': '2017-01-10', 'host_venue': 'V4306420508', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2664061733', ['I68947357']], ['A2516971438', ['I136199984']], ['A2123267846', ['I74973139']], ['A280727668', []], ['A2701382563', ['I47508984']]], 'cited_by_count': 1244, 'concepts': [['C41008148', '0.83649075'], ['C2779478453', '0.8258414'], ['C56666940', '0.75233734'], ['C203005215', '0.69713295'], ['C32833848', '0.6849176']], 'referenced_works': ['W1514535095', 'W1591706642', 'W1855892484', 'W1902237438', 'W1924770834', 'W2124807415', 'W2130942839', 'W2155607551', 'W2157331557', 'W2168231600', 'W2250548645', 'W2467173223', 'W2470673105', 'W2521665229', 'W2525778437', 'W2537667581', 'W2550821151', 'W2962784628', 'W2963123301', 'W2963223306', 'W2963876447', 'W2964308564'], 'abstract': 'We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.', 'counts_by_year': [[2022, 61], [2021, 265], [2020, 345], [2019, 345], [2018, 184], [2017, 43], [2016, 1]]}, {'id': 'W2809254203', 'doi': 'https://doi.org/10.1007/s13244-018-0639-9', 'title': 'Convolutional neural networks: an overview and application in radiology', 'type': 'journal-article', 'publication_date': '2018-06-22', 'host_venue': 'V44632665', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2675897136', ['I22299242', 'I1334819555']], ['A2168685740', ['I4210150196']], ['A2152710113', ['I1334819555']], ['A2107199607', ['I22299242']]], 'cited_by_count': 1243, 'concepts': [['C81363708', '0.8416605'], ['C41008148', '0.7694124'], ['C154945302', '0.72782564'], ['C153083717', '0.63620037'], ['C70437156', '0.62522066']], 'referenced_works': ['W130099911', 'W639708223', 'W1901129140', 'W1980287119', 'W1986649315', 'W2080448710', 'W2083927153', 'W2097117768', 'W2101926813', 'W2103004421', 'W2117539524', 'W2117731089', 'W2128739912', 'W2143516773', 'W2164160732', 'W2194775991', 'W2250539671', 'W2295107390', 'W2322371438', 'W2322406257', 'W2357815549', 'W2493683088', 'W2526009326', 'W2557738935', 'W2570202822', 'W2574952845', 'W2581082771', 'W2608231518', 'W2753887715', 'W2754132686', 'W2765571304', 'W2768567289', 'W2770853452', 'W2772723798', 'W2779664341', 'W2783687327', 'W2919115771', 'W2962858109', 'W2962914239', 'W2963446712', 'W3101156210'], 'abstract': 'Convolutional neural network (CNN), a class of artificial neural networks that has become dominant in various computer vision tasks, is attracting interest across a variety of domains, including radiology. CNN is designed to automatically and adaptively learn spatial hierarchies of features through backpropagation by using multiple building blocks, such as convolution layers, pooling layers, and fully connected layers. This review article offers a perspective on the basic concepts of CNN and its application to various radiological tasks, and discusses its challenges and future directions in the field of radiology. Two challenges in applying CNN to radiological tasks, small dataset and overfitting, will also be covered in this article, as well as techniques to minimize them. Being familiar with the concepts and advantages, as well as limitations, of CNN is essential to leverage its potential in diagnostic radiology, with the goal of augmenting the performance of radiologists and improving patient care. KEY POINTS: • Convolutional neural network is a class of deep learning methods which has become dominant in various computer vision tasks and is attracting interest across a variety of domains, including radiology. • Convolutional neural network is composed of multiple building blocks, such as convolution layers, pooling layers, and fully connected layers, and is designed to automatically and adaptively learn spatial hierarchies of features through a backpropagation algorithm. • Familiarity with the concepts and advantages, as well as limitations, of convolutional neural network is essential to leverage its potential to improve radiologist performance and, eventually, patient care.', 'counts_by_year': [[2022, 425], [2021, 466], [2020, 265], [2019, 80], [2018, 5]]}, {'id': 'W2975424845', 'doi': 'https://doi.org/10.1136/ebmental-2019-300117', 'title': 'How to perform a meta-analysis with R: a practical tutorial', 'type': 'journal-article', 'publication_date': '2019-11-01', 'host_venue': 'V160517820', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A2016095707', ['I161046081']], ['A2158789975', ['I161046081']], ['A2017622541', ['I161046081']]], 'cited_by_count': 1243, 'concepts': [['C82605166', '0.91711783'], ['C95190672', '0.8063703'], ['C2780439572', '0.70320725'], ['C148220186', '0.6200967'], ['C41008148', '0.6000762']], 'referenced_works': ['W204046373', 'W1590137713', 'W1598602811', 'W1784175517', 'W1965385672', 'W2025091395', 'W2039418122', 'W2069224123', 'W2097744101', 'W2103410254', 'W2108116635', 'W2116951140', 'W2119605658', 'W2126602143', 'W2128640268', 'W2129183432', 'W2156665896', 'W2168718800', 'W2169629898', 'W2332470030', 'W2532414435', 'W4211010295', 'W4251211582', 'W4297918764'], 'abstract': 'Objective Meta-analysis is of fundamental importance to obtain an unbiased assessment of the available evidence. In general, the use of meta-analysis has been increasing over the last three decades with mental health as a major research topic. It is then essential to well understand its methodology and interpret its results. In this publication, we describe how to perform a meta-analysis with the freely available statistical software environment R, using a working example taken from the field of mental health. Methods R package meta is used to conduct standard meta-analysis. Sensitivity analyses for missing binary outcome data and potential selection bias are conducted with R package metasens. All essential R commands are provided and clearly described to conduct and report analyses. Results The working example considers a binary outcome: we show how to conduct a fixed effect and random effects meta-analysis and subgroup analysis, produce a forest and funnel plot and to test and adjust for funnel plot asymmetry. All these steps work similar for other outcome types. Conclusions R represents a powerful and flexible tool to conduct meta-analyses. This publication gives a brief glimpse into the topic and provides directions to more advanced meta-analysis methods available in R.', 'counts_by_year': [[2022, 594], [2021, 500], [2020, 143]]}, {'id': 'W2755255888', 'doi': 'https://doi.org/10.1038/nature23879', 'title': 'Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets', 'type': 'journal-article', 'publication_date': '2017-09-14', 'host_venue': 'V137773608', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A1963557412', ['I1341412227']], ['A2922295397', ['I1341412227']], ['A2951254601', ['I1341412227']], ['A1983840058', ['I1341412227']], ['A2951574028', ['I1341412227']], ['A2065588448', ['I1341412227']], ['A1988511495', ['I1341412227']]], 'cited_by_count': 1242, 'concepts': [['C58053490', '0.70554733'], ['C110340908', '0.65242916'], ['C84114770', '0.5956935'], ['C161166931', '0.59401363'], ['C130787639', '0.53576714']], 'referenced_works': ['W649366798', 'W2026154948', 'W2027003240', 'W2027780909', 'W2052891002', 'W2089867976', 'W2104399080', 'W2117980155', 'W2124289529', 'W2157120748', 'W2161685427', 'W2254754114', 'W2257937122', 'W2297918601', 'W2298410160', 'W2338360705', 'W2560309622', 'W2562526363', 'W2565654137', 'W2592769978', 'W2895657145', 'W3103810096', 'W3104202037'], 'abstract': 'Quantum computers can be used to address molecular structure, materials science and condensed matter physics problems, which currently stretch the limits of existing high-performance computing resources. Finding exact numerical solutions to these interacting fermion problems has exponential cost, while Monte Carlo methods are plagued by the fermionic sign problem. These limitations of classical computational methods have made even few-atom molecular structures problems of practical interest for medium-sized quantum computers. Yet, thus far experimental implementations have been restricted to molecules involving only Period I elements. Here, we demonstrate the experimental optimization of up to six-qubit Hamiltonian problems with over a hundred Pauli terms, determining the ground state energy for molecules of increasing size, up to BeH2. This is enabled by a hardware-efficient variational quantum eigensolver with trial states specifically tailored to the available interactions in our quantum processor, combined with a compact encoding of fermionic Hamiltonians and a robust stochastic optimization routine. We further demonstrate the flexibility of our approach by applying the technique to a problem of quantum magnetism. Across all studied problems, we find agreement between experiment and numerical simulations with a noisy model of the device. These results help elucidate the requirements for scaling the method to larger systems, and aim at bridging the gap between problems at the forefront of high-performance computing and their implementation on quantum hardware.', 'counts_by_year': [[2022, 254], [2021, 424], [2020, 283], [2019, 169], [2018, 93], [2017, 19]]}, {'id': 'W2981869278', 'doi': 'https://doi.org/10.1126/science.aax2342', 'title': 'Dissecting racial bias in an algorithm used to manage the health of populations', 'type': 'journal-article', 'publication_date': '2019-10-25', 'host_venue': 'V3880285', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A847559205', ['I1283280774', 'I95457486']], ['A2190439512', ['I1283280774']], ['A2015321523', ['I4210087915']], ['A109652375', ['I40347166']]], 'cited_by_count': 1241, 'concepts': [['C2780148112', '0.69546187'], ['C160735492', '0.54633045'], ['C11413529', '0.5132704'], ['C56273599', '0.47110087'], ['C12713177', '0.45830894']], 'referenced_works': ['W1648303880', 'W1969564727', 'W1978642336', 'W1987043226', 'W2033609349', 'W2042572308', 'W2044998035', 'W2053339749', 'W2080433173', 'W2087018232', 'W2123200029', 'W2133808321', 'W2139766084', 'W2140452679', 'W2141344358', 'W2203167467', 'W2210543184', 'W2318723339', 'W2431687448', 'W2465758073', 'W2529251251', 'W2544063074', 'W2611119674', 'W2615995104', 'W2765284031', 'W2893425640', 'W2920906063', 'W2939028443', 'W2964031043', 'W3083206818', 'W3101243562', 'W3105536512', 'W3123508566'], 'abstract': 'Racial bias in health algorithms The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care. Science , this issue p. 447 ; see also p. 421', 'counts_by_year': [[2022, 400], [2021, 506], [2020, 321], [2019, 13]]}, {'id': 'W2741137940', 'doi': 'https://doi.org/10.1109/cvprw.2017.150', 'title': 'NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study', 'type': 'proceedings-article', 'publication_date': '2017-07-21', 'host_venue': 'V4306417987', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2595043282', ['I35440088']], ['A2058433139', ['I35440088']]], 'cited_by_count': 1240, 'concepts': [['C41008148', '0.76001126'], ['C115961682', '0.673513'], ['C177264268', '0.5504344'], ['C138268822', '0.5355458'], ['C154945302', '0.5083088']], 'referenced_works': ['W1885185971', 'W1919542679', 'W1930824406', 'W1949096787', 'W1950594372', 'W1964571510', 'W1974013408', 'W2011181254', 'W2047920195', 'W2087380704', 'W2097074225', 'W2110505738', 'W2112796928', 'W2114122776', 'W2115548755', 'W2117539524', 'W2118963448', 'W2121927366', 'W2133665775', 'W2140257560', 'W2141983208', 'W2142884912', 'W2149669120', 'W2150060382', 'W2150081556', 'W2160547390', 'W2161907179', 'W2165939075', 'W2170608748', 'W2194775991', 'W2214802144', 'W2242218935', 'W2263468737', 'W2417716951', 'W2462651488', 'W2476548250', 'W2534320940', 'W2565312867', 'W2608989529', 'W2739757502', 'W2963372104', 'W3104720471'], 'abstract': 'This paper introduces a novel large dataset for example-based single image super-resolution and studies the state-of-the-art as emerged from the NTIRE 2017 challenge. The challenge is the first challenge of its kind, with 6 competitions, hundreds of participants and tens of proposed solutions. Our newly collected DIVerse 2K resolution image dataset (DIV2K) was employed by the challenge. In our study we compare the solutions from the challenge to a set of representative methods from the literature and evaluate them using diverse measures on our proposed DIV2K dataset. Moreover, we conduct a number of experiments and draw conclusions on several topics of interest. We conclude that the NTIRE 2017 challenge pushes the state-of-the-art in single-image super-resolution, reaching the best results to date on the popular Set5, Set14, B100, Urban100 datasets and on our newly proposed DIV2K.', 'counts_by_year': [[2022, 207], [2021, 435], [2020, 344], [2019, 186], [2018, 59], [2017, 5], [2012, 1]]}, {'id': 'W2558999090', 'doi': 'https://doi.org/10.1093/nar/gkw1074', 'title': 'The ChEMBL database in 2017', 'type': 'journal-article', 'publication_date': '2017-01-04', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2013623313', ['I1303153112']], ['A2010579728', ['I1303153112']], ['A2428252930', ['I1303153112']], ['A2135972990', ['I1303153112']], ['A2753061352', ['I1303153112']], ['A2568873244', ['I1303153112']], ['A2467659763', ['I1303153112']], ['A2126137628', ['I1303153112']], ['A1983883118', ['I1303153112']], ['A2580016008', ['I1303153112']], ['A2141160170', ['I1303153112']], ['A1970556020', ['I1303153112']], ['A2580625044', ['I1303153112']], ['A3183630668', ['I1303153112']], ['A282012227', ['I1303153112']], ['A2117867372', ['I1303153112']], ['A2571155078', ['I1303153112']], ['A1989637924', ['I1303153112']]], 'cited_by_count': 1237, 'concepts': [['C63222358', '0.9603338'], ['C86803240', '0.5139944'], ['C77088390', '0.49617514'], ['C41008148', '0.4352537'], ['C60644358', '0.33634895']], 'referenced_works': ['W959778778', 'W1539403009', 'W1663154925', 'W1895993106', 'W1966456689', 'W1967105697', 'W1968779484', 'W1975743735', 'W1975875968', 'W1976902852', 'W1989821915', 'W1997362383', 'W2000388030', 'W2016736023', 'W2016976521', 'W2030952396', 'W2030992393', 'W2039468377', 'W2043509228', 'W2048080607', 'W2058259168', 'W2058298068', 'W2061623393', 'W2085587362', 'W2096541451', 'W2096777607', 'W2098334118', 'W2105836514', 'W2113364934', 'W2114347076', 'W2116034137', 'W2116238734', 'W2117819606', 'W2119676748', 'W2120534674', 'W2128386749', 'W2133614244', 'W2142280055', 'W2147018385', 'W2147670638', 'W2152657402', 'W2155478691', 'W2160097714', 'W2163089452', 'W2169678694', 'W2176758925', 'W2194756172', 'W2204695023', 'W2205755822', 'W2320207251', 'W2410476179', 'W2437823009', 'W2527110257', 'W2557530941', 'W2558366645', 'W2916534270'], 'abstract': 'ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 and 2014 Nucleic Acids Research Database Issues. Since then, alongside the continued extraction of data from the medicinal chemistry literature, new sources of bioactivity data have also been added to the database. These include: deposited data sets from neglected disease screening; crop protection data; drug metabolism and disposition data and bioactivity data from patents. A number of improvements and new features have also been incorporated. These include the annotation of assays and targets using ontologies, the inclusion of targets and indications for clinical candidates, addition of metabolic pathways for drugs and calculation of structural alerts. The ChEMBL data can be accessed via a web-interface, RDF distribution, data downloads and RESTful web-services.', 'counts_by_year': [[2022, 217], [2021, 300], [2020, 309], [2019, 252], [2018, 118], [2017, 41]]}, {'id': 'W3035574324', 'doi': 'https://doi.org/10.1109/cvpr42600.2020.00813', 'title': 'Analyzing and Improving the Image Quality of StyleGAN', 'type': 'proceedings-article', 'publication_date': '2020-06-14', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2153729192', ['I1304085615']], ['A2158163072', ['I1304085615']], ['A2230368550', ['I1304085615']], ['A2995724757', ['I1304085615']], ['A2779040656', ['I9927081']], ['A1975563680', ['I1304085615']]], 'cited_by_count': 1236, 'concepts': [['C136886441', '0.7785076'], ['C41008148', '0.7569956'], ['C2780992000', '0.71701247'], ['C154945302', '0.5837682'], ['C55020928', '0.55195063']], 'referenced_works': ['W1677182931', 'W2603777577', 'W2879390606', 'W2921230249', 'W2962770929', 'W2963525668', 'W3012472557', 'W3035231706', 'W4244659560'], 'abstract': 'The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.', 'counts_by_year': [[2022, 316], [2021, 686], [2020, 222], [2019, 7]]}, {'id': 'W3105081694', 'doi': 'https://doi.org/10.1038/s41598-020-76550-z', 'title': 'COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images', 'type': 'journal-article', 'publication_date': '2020-11-11', 'host_venue': 'V196734849', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2905717005', ['I151746483']], ['A2896708105', ['I151746483']], ['A2126481209', ['I151746483']]], 'cited_by_count': 1234, 'concepts': [['C3008058167', '0.76293135'], ['C81363708', '0.70127755'], ['C41008148', '0.5918518'], ['C185798385', '0.56294227'], ['C36454342', '0.511891']], 'referenced_works': ['W2108598243', 'W2194775991', 'W2919115771', 'W2963446712', 'W3001118548', 'W3005827208', 'W3006082171', 'W3007497549', 'W3008985036', 'W3011149445', 'W3013601031', 'W3014113618', 'W3014561994', 'W3014640306', 'W3015292413', 'W3015883707', 'W3015984951', 'W3017364693', 'W3019531985', 'W3023614822', 'W3024468951', 'W3025550508', 'W3045460727', 'W3086039674', 'W3101156210', 'W3121339497', 'W3126084544', 'W3129576291'], 'abstract': 'Abstract The Coronavirus Disease 2019 (COVID-19) pandemic continues to have a devastating effect on the health and well-being of the global population. A critical step in the fight against COVID-19 is effective screening of infected patients, with one of the key screening approaches being radiology examination using chest radiography. It was found in early studies that patients present abnormalities in chest radiography images that are characteristic of those infected with COVID-19. Motivated by this and inspired by the open source efforts of the research community, in this study we introduce COVID-Net, a deep convolutional neural network design tailored for the detection of COVID-19 cases from chest X-ray (CXR) images that is open source and available to the general public. To the best of the authors’ knowledge, COVID-Net is one of the first open source network designs for COVID-19 detection from CXR images at the time of initial release. We also introduce COVIDx, an open access benchmark dataset that we generated comprising of 13,975 CXR images across 13,870 patient patient cases, with the largest number of publicly available COVID-19 positive cases to the best of the authors’ knowledge. Furthermore, we investigate how COVID-Net makes predictions using an explainability method in an attempt to not only gain deeper insights into critical factors associated with COVID cases, which can aid clinicians in improved screening, but also audit COVID-Net in a responsible and transparent manner to validate that it is making decisions based on relevant information from the CXR images. By no means a production-ready solution, the hope is that the open access COVID-Net, along with the description on constructing the open source COVIDx dataset, will be leveraged and build upon by both researchers and citizen data scientists alike to accelerate the development of highly accurate yet practical deep learning solutions for detecting COVID-19 cases and accelerate treatment of those who need it the most.', 'counts_by_year': [[2022, 363], [2021, 602], [2020, 269]]}, {'id': 'W2752849906', 'doi': 'https://doi.org/10.1038/nphoton.2017.93', 'title': 'Deep learning with coherent nanophotonic circuits', 'type': 'journal-article', 'publication_date': '2017-07-01', 'host_venue': 'V120392215', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2132165925', ['I63966007']], ['A2156379272', ['I63966007']], ['A1070600132', ['I63966007']], ['A2170814258', ['I63966007']], ['A40544600', ['I63966007']]], 'cited_by_count': 1233, 'concepts': [['C41008148', '0.7634815'], ['C80469333', '0.76115346'], ['C50644808', '0.68797666'], ['C27289702', '0.54870206'], ['C108583219', '0.5164838']], 'referenced_works': ['W1598990735', 'W1838686065', 'W1958979682', 'W1977664984', 'W1978426240', 'W1983033020', 'W1984906252', 'W2019582343', 'W2022248835', 'W2028166238', 'W2029939668', 'W2036594235', 'W2037996680', 'W2051270432', 'W2057087137', 'W2057707608', 'W2062294208', 'W2069556503', 'W2075665712', 'W2076063813', 'W2076476236', 'W2083936311', 'W2085364762', 'W2100495367', 'W2125964296', 'W2145339207', 'W2146747949', 'W2155893237', 'W2160837761', 'W2163630896', 'W2164369030', 'W2200176946', 'W2220870060', 'W2238099481', 'W2257979135', 'W2289252105', 'W2314470091', 'W2401577158', 'W2530887700', 'W2919115771', 'W2952285760', 'W3101465594', 'W4251247379'], 'abstract': "Artificial Neural Networks are computational network models inspired by signal processing in the brain. These models have dramatically improved the performance of many learning tasks, including speech and object recognition. However, today's computing hardware is inefficient at implementing neural networks, in large part because much of it was designed for von Neumann computing schemes. Significant effort has been made to develop electronic architectures tuned to implement artificial neural networks that improve upon both computational speed and energy efficiency. Here, we propose a new architecture for a fully-optical neural network that, using unique advantages of optics, promises a computational speed enhancement of at least two orders of magnitude over the state-of-the-art and three orders of magnitude in power efficiency for conventional learning tasks. We experimentally demonstrate essential parts of our architecture using a programmable nanophotonic processor.", 'counts_by_year': [[2022, 284], [2021, 343], [2020, 286], [2019, 200], [2018, 100], [2017, 15], [2015, 1]]}, {'id': 'W3126033509', 'doi': 'https://doi.org/10.1016/j.physrep.2016.09.002', 'title': 'Community detection in networks: A user guide', 'type': 'journal-article', 'publication_date': '2016-11-11', 'host_venue': 'V48633963', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2122189410', ['I592451', 'I9927081']], ['A2484113080', ['I9927081']]], 'cited_by_count': 1233, 'concepts': [['C28719098', '0.6137736'], ['C63882131', '0.59918606'], ['C121332964', '0.5982716'], ['C9652623', '0.5631539'], ['C2522767166', '0.56297064']], 'referenced_works': ['W143174683', 'W745973578', 'W1535365835', 'W1617641394', 'W1743429370', 'W1821277955', 'W1862701203', 'W1871641673', 'W1912249326', 'W1930572491', 'W1931149371', 'W1970301364', 'W1970871468', 'W1971421925', 'W1971729008', 'W1972675431', 'W1976224488', 'W1983345514', 'W1987173300', 'W1987643017', 'W1988074911', 'W1991408655', 'W1992419399', 'W1994321911', 'W1995996823', 'W2000087119', 'W2004779842', 'W2009827286', 'W2012213783', 'W2012662151', 'W2014259951', 'W2016776056', 'W2017109153', 'W2017987256', 'W2020547732', 'W2023655578', 'W2023708440', 'W2024128809', 'W2024514015', 'W2025543856', 'W2029130073', 'W2032273054', 'W2033403400', 'W2033456101', 'W2033947830', 'W2038920443', 'W2039750798', 'W2041713562', 'W2042276255', 'W2044881936', 'W2044988896', 'W2047940964', 'W2049294562', 'W2054152820', 'W2054658115', 'W2061099285', 'W2061849527', 'W2066636486', 'W2074617510', 'W2075557508', 'W2086327058', 'W2089458547', 'W2090372231', 'W2091476183', 'W2092124750', 'W2094674364', 'W2095072199', 'W2095189226', 'W2095293504', 'W2098303869', 'W2100240966', 'W2100978151', 'W2101580881', 'W2102907934', 'W2103269698', 'W2103887734', 'W2109726592', 'W2110620844', 'W2111642621', 'W2112461976', 'W2112745879', 'W2114244473', 'W2115346774', 'W2116007667', 'W2118608338', 'W2119998616', 'W2124536061', 'W2125050594', 'W2126630970', 'W2126641717', 'W2127042504', 'W2127048411', 'W2128366083', 'W2129043771', 'W2132202037', 'W2132958441', 'W2133095386', 'W2136088806', 'W2139818818', 'W2146591355', 'W2146815027', 'W2148781711', 'W2150256170', 'W2151936673', 'W2152971731', 'W2153107964', 'W2155369095', 'W2156028561', 'W2157305458', 'W2157547297', 'W2157762631', 'W2162450625', 'W2164245295', 'W2164928285', 'W2164998314', 'W2327250108', 'W2375070860', 'W2470861207', 'W2769133055', 'W2963621728', 'W3037819556', 'W3037826211', 'W3098742898', 'W3098765153', 'W3099209941', 'W3101413764', 'W3101685104', 'W3102191718', 'W3103589660', 'W3123041312', 'W3124682511', 'W4235169531', 'W4292081303'], 'abstract': 'Community detection in networks is one of the most popular topics of modern network science. Communities, or clusters, are usually groups of vertices having higher probability of being connected to each other than to members of other groups, though other patterns are possible. Identifying communities is an ill-defined problem. There are no universal protocols on the fundamental ingredients, like the definition of community itself, nor on other crucial issues, like the validation of algorithms and the comparison of their performances. This has generated a number of confusions and misconceptions, which undermine the progress in the field. We offer a guided tour through the main aspects of the problem. We also point out strengths and weaknesses of popular methods, and give directions to their use.', 'counts_by_year': [[2022, 150], [2021, 233], [2020, 282], [2019, 280], [2018, 185], [2017, 93], [2016, 7], [2015, 1]]}, {'id': 'W2295939521', 'doi': 'https://doi.org/10.1109/hicss.2016.488', 'title': 'Design Principles for Industrie 4.0 Scenarios', 'type': 'proceedings-article', 'publication_date': '2016-01-05', 'host_venue': 'V4306418516', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2294778969', ['I200332995']], ['A2293756537', ['I202963720']], ['A2103815220', ['I200332995']]], 'cited_by_count': 1232, 'concepts': [['C110875604', '0.50040126'], ['C2780966255', '0.49524036'], ['C2776291640', '0.45504442'], ['C81860439', '0.44756004'], ['C517468935', '0.44524944']], 'referenced_works': ['W3577882', 'W95081507', 'W114276366', 'W147699954', 'W203466237', 'W495022269', 'W1486449559', 'W1599488776', 'W1892891676', 'W1966404428', 'W2017234320', 'W2019356162', 'W2031820504', 'W2032614950', 'W2034960640', 'W2035922091', 'W2037124552', 'W2037395587', 'W2054275824', 'W2070665593', 'W2074147596', 'W2075366908', 'W2080646064', 'W2095655043', 'W2108721666', 'W2115571999', 'W2123288626', 'W2143253416', 'W2150845579', 'W2169239645', 'W2505553280', 'W2542622692', 'W3210265966', 'W4231892747', 'W4232676207'], 'abstract': 'The increasing integration of the Internet of Everything into the industrial value chain has built the foundation for the next industrial revolution called Industrie 4.0. Although Industrie 4.0 is currently a top priority for many companies, research centers, and universities, a generally accepted understanding of the term does not exist. As a result, discussing the topic on an academic level is difficult, and so is implementing Industrie 4.0 scenarios. Based on a quantitative text analysis and a qualitative literature review, the paper identifies design principles of Industrie 4.0. Taking into account these principles, academics may be enabled to further investigate on the topic, while practitioners may find assistance in identifying appropriate scenarios. A case study illustrates how the identified design principles support practitioners in identifying Industrie 4.0 scenarios.', 'counts_by_year': [[2022, 131], [2021, 255], [2020, 223], [2019, 262], [2018, 184], [2017, 129], [2016, 45], [2015, 2]]}, {'id': 'W2340005653', 'doi': 'https://doi.org/10.1126/science.aad4424', 'title': 'Photovoltaic materials: Present efficiencies and future challenges', 'type': 'journal-article', 'publication_date': '2016-04-15', 'host_venue': 'V3880285', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2518447353', ['I2803035111']], ['A2336136286', ['I2803035111']], ['A2077145110', ['I2803035111']], ['A2047374661', ['I2803035111']], ['A2865916493', ['I2803035111', 'I2802259095']]], 'cited_by_count': 1232, 'concepts': [['C41291067', '0.8876724'], ['C105339364', '0.7273222'], ['C26517878', '0.5935361'], ['C208052160', '0.5339398'], ['C41008148', '0.4822915']], 'referenced_works': ['W1590304507', 'W1965452494', 'W1966368446', 'W1968836471', 'W1975074493', 'W1979583260', 'W1987880468', 'W1999617030', 'W2013943268', 'W2017009702', 'W2017940247', 'W2027426921', 'W2028808594', 'W2029637177', 'W2042448437', 'W2047227381', 'W2052041578', 'W2052432882', 'W2061689131', 'W2073593109', 'W2075511306', 'W2075741401', 'W2076340632', 'W2080086353', 'W2088448952', 'W2094087345', 'W2098006793', 'W2099200340', 'W2123590280', 'W2123857657', 'W2140042874', 'W2172511679', 'W2199718429', 'W2316933386', 'W2318210909', 'W2341739539'], 'abstract': 'Surveying the solar cell landscape The rate of development and deployment of large-scale photovoltaic systems over recent years has been unprecedented. Because the cost of photovoltaic systems is only partly determined by the cost of the solar cells, efficiency is a key driver to reduce the cost of solar energy. There are several materials systems being explored to achieve high efficiency at low cost. Polman et al. comprehensively and systematically review the leading candidate materials, present the limitations of each system, and analyze how these limitations can be overcome and overall cell performance improved. Science , this issue p. 10.1126/science.aad4424', 'counts_by_year': [[2022, 142], [2021, 226], [2020, 199], [2019, 239], [2018, 224], [2017, 154], [2016, 47]]}, {'id': 'W2611604995', 'doi': 'https://doi.org/10.1093/nar/gkx382', 'title': 'agriGO v2.0: a GO analysis toolkit for the agricultural community, 2017 update', 'type': 'journal-article', 'publication_date': '2017-07-03', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2477274386', ['I52158045']], ['A2641564894', ['I52158045']], ['A2495988402', ['I52158045']], ['A2220316886', ['I52158045']], ['A2128657245', ['I52158045']], ['A2103858192', ['I52158045']], ['A2151912543', ['I52158045']], ['A2115729623', ['I52158045']]], 'cited_by_count': 1232, 'concepts': [['C86803240', '0.76070964'], ['C2987395477', '0.5543268'], ['C25810664', '0.5223397'], ['C118518473', '0.50852764'], ['C36464697', '0.4877575']], 'referenced_works': ['W1527927437', 'W1596515083', 'W1921522604', 'W1970892042', 'W1979368453', 'W1992512552', 'W2001191731', 'W2002420618', 'W2005137303', 'W2018925823', 'W2036714085', 'W2036775722', 'W2045729458', 'W2053551770', 'W2059102372', 'W2070050178', 'W2077355527', 'W2088387316', 'W2091981172', 'W2096585111', 'W2100761975', 'W2102074241', 'W2106780701', 'W2109579035', 'W2115012618', 'W2125905600', 'W2127119915', 'W2128049108', 'W2129003604', 'W2133465414', 'W2138098955', 'W2140628921', 'W2141292707', 'W2158217645', 'W2164154943', 'W2224056471', 'W2300677196', 'W2310808048', 'W2346290259', 'W2403944210', 'W2418870819', 'W2509475426', 'W2534942962', 'W2557496587', 'W2594541022', 'W2918086501', 'W4210702584'], 'abstract': 'The agriGO platform, which has been serving the scientific community for >10 years, specifically focuses on gene ontology (GO) enrichment analyses of plant and agricultural species. We continuously maintain and update the databases and accommodate the various requests of our global users. Here, we present our updated agriGO that has a largely expanded number of supporting species (394) and datatypes (865). In addition, a larger number of species have been classified into groups covering crops, vegetables, fish, birds and insects closely related to the agricultural community. We further improved the computational efficiency, including the batch analysis and P-value distribution (PVD), and the user-friendliness of the web pages. More visualization features were added to the platform, including SEACOMPARE (cross comparison of singular enrichment analysis), direct acyclic graph (DAG) and Scatter Plots, which can be merged by choosing any significant GO term. The updated platform agriGO v2.0 is now publicly accessible at http://systemsbiology.cau.edu.cn/agriGOv2/.', 'counts_by_year': [[2022, 251], [2021, 343], [2020, 324], [2019, 206], [2018, 92], [2017, 14]]}, {'id': 'W2886281300', 'doi': 'https://doi.org/10.1038/s41591-018-0107-6', 'title': 'Clinically applicable deep learning for diagnosis and referral in retinal disease', 'type': 'journal-article', 'publication_date': '2018-08-13', 'host_venue': 'V203256638', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2484064519', ['I4210090411']], ['A2605474872', ['I4210090411']], ['A1921173107', ['I4210090411']], ['A2906757897', ['I4210090411']], ['A2311756312', ['I4210090411']], ['A2919774455', ['I4210090411']], ['A2911321354', ['I4210090411']], ['A295353625', ['I4210090411']], ['A2257068296', ['I4210090411']], ['A3012575134', ['I4210090411']], ['A2600316320', ['I4210090411']], ['A1843466435', ['I4210090411']], ['A2808484806', ['I4210090411']], ['A2887881007', ['I4210090411']], ['A2887427580', ['I4210090411']], ['A2975642107', ['I4210090411']], ['A2293227892', ['I177914224']], ['A2113303599', ['I4210090411']], ['A2305091355', ['I4210090411']], ['A2145521977', ['I45129253']], ['A2199645189', ['I45129253']], ['A2887075609', ['I177914224']], ['A2083544299', ['I177914224']], ['A2156801113', ['I177914224']], ['A1995617974', ['I177914224']], ['A2135186967', ['I45129253']], ['A4302276', ['I4210090411']], ['A1990388081', ['I45129253']], ['A2485788851', ['I4210090411']], ['A2015859460', ['I177914224']], ['A2697786795', ['I4210090411']], ['A2630278389', ['I45129253']], ['A2032769932', ['I177914224']], ['A2044097715', ['I4210090411']]], 'cited_by_count': 1231, 'concepts': [['C2776135927', '0.6729772'], ['C41008148', '0.6181476'], ['C154945302', '0.6095938'], ['C2778818243', '0.6070361'], ['C108583219', '0.5899766']], 'referenced_works': ['W1458919156', 'W1901129140', 'W1991442926', 'W2001080026', 'W2003545017', 'W2013754472', 'W2024615783', 'W2045373862', 'W2050378179', 'W2063967273', 'W2121963763', 'W2136381651', 'W2167279371', 'W2183341477', 'W2289254711', 'W2418802570', 'W2464708700', 'W2528491735', 'W2549799826', 'W2557738935', 'W2570893893', 'W2581082771', 'W2582609956', 'W2589074029', 'W2606534623', 'W2608854843', 'W2616378250', 'W2621101817', 'W2741346289', 'W2743266749', 'W2748657116', 'W2772059204', 'W2949122205', 'W2963446712', 'W3130201740', 'W4240635375', 'W4241116478'], 'abstract': 'The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue segmentations produced by our architecture act as a device-independent representation; referral accuracy is maintained when using tissue segmentations from a different type of device. Our work removes previous barriers to wider clinical use without prohibitive training data requirements across multiple pathologies in a real-world setting.', 'counts_by_year': [[2022, 199], [2021, 360], [2020, 383], [2019, 256], [2018, 30], [2012, 1]]}, {'id': 'W2568772110', 'doi': 'https://doi.org/10.1109/mc.2017.9', 'title': 'The Emergence of Edge Computing', 'type': 'journal-article', 'publication_date': '2017-01-01', 'host_venue': 'V178916657', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A416104820', ['I74973139']]], 'cited_by_count': 1230, 'concepts': [['C41008148', '0.8207613'], ['C2778653333', '0.42931092'], ['C80444323', '0.34892696'], ['C120314980', '0.32262957'], ['C11413529', '0.13488826']], 'referenced_works': ['W1982421724', 'W1988175327', 'W2015597701', 'W2100538831', 'W2109595453', 'W2114623221', 'W2119276484', 'W2121884932', 'W2124074197', 'W2125444986', 'W2135099885', 'W2145695845', 'W2159694746', 'W2259009889', 'W4253902778'], 'abstract': "Industry investment and research interest in edge computing, in which computing and storage nodes are placed at the Internet's edge in close proximity to mobile devices or sensors, have grown dramatically in recent years. This emerging technology promises to deliver highly responsive cloud services for mobile computing, scalability and privacy-policy enforcement for the Internet of Things, and the ability to mask transient cloud outages. The web extra at www.youtube.com/playlist?list=PLmrZVvFtthdP3fwHPy_4d61oDvQY_RBgS includes a five-video playlist demonstrating proof-of-concept implementations for three tasks: assembling 2D Lego models, freehand sketching, and playing Ping-Pong.", 'counts_by_year': [[2022, 178], [2021, 295], [2020, 278], [2019, 263], [2018, 172], [2017, 41], [2012, 1]]}, {'id': 'W3017855299', 'doi': 'https://doi.org/10.1016/j.compbiomed.2020.103792', 'title': 'Automated detection of COVID-19 cases using deep neural networks with X-ray images', 'type': 'journal-article', 'publication_date': '2020-06-01', 'host_venue': 'V44278595', 'open_access_is_oa': True, 'open_access_oa_status': 'bronze', 'authorships': [['A1983321978', ['I1319772077']], ['A2883101955', ['I143396566']], ['A3017499235', ['I1303077703']], ['A2800789459', ['I36234482']], ['A2099228199', ['I4210166533']], ['A2145174184', ['I58236372']]], 'cited_by_count': 1228, 'concepts': [['C41008148', '0.72464883'], ['C3008058167', '0.70743114'], ['C154945302', '0.70027345'], ['C81363708', '0.6883776'], ['C66905080', '0.6113008']], 'referenced_works': ['W2533800772', 'W2581082771', 'W2592929672', 'W2748902594', 'W2749049648', 'W2777186991', 'W2797694788', 'W2889838428', 'W2902644322', 'W2919115771', 'W2952855260', 'W2963059730', 'W2979487364', 'W3001118548', 'W3002539152', 'W3003217347', 'W3003465021', 'W3003951199', 'W3006110666', 'W3006643024', 'W3006792870', 'W3006882119', 'W3007273493', 'W3007355693', 'W3007764760', 'W3008028633', 'W3008116551', 'W3008627141', 'W3009821127', 'W3009875419', 'W3009928129', 'W3010699833', 'W3012751338', 'W3013966144'], 'abstract': 'The novel coronavirus 2019 (COVID-2019), which first appeared in Wuhan city of China in December 2019, spread rapidly around the world and became a pandemic. It has caused a devastating effect on both daily lives, public health, and the global economy. It is critical to detect the positive cases as early as possible so as to prevent the further spread of this epidemic and to quickly treat affected patients. The need for auxiliary diagnostic tools has increased as there are no accurate automated toolkits available. Recent findings obtained using radiology imaging techniques suggest that such images contain salient information about the COVID-19 virus. Application of advanced artificial intelligence (AI) techniques coupled with radiological imaging can be helpful for the accurate detection of this disease, and can also be assistive to overcome the problem of a lack of specialized physicians in remote villages. In this study, a new model for automatic COVID-19 detection using raw chest X-ray images is presented. The proposed model is developed to provide accurate diagnostics for binary classification (COVID vs. No-Findings) and multi-class classification (COVID vs. No-Findings vs. Pneumonia). Our model produced a classification accuracy of 98.08% for binary classes and 87.02% for multi-class cases. The DarkNet model was used in our study as a classifier for the you only look once (YOLO) real time object detection system. We implemented 17 convolutional layers and introduced different filtering on each layer. Our model (available at ( https://github.com/muhammedtalo/COVID-19 )) can be employed to assist radiologists in validating their initial screening, and can also be employed via cloud to immediately screen patients. • Proposed deep model for early detection of COVID-19 cases using X-ray images. • Obtained accuracy of 98.08% and 87.02% for binary and multi-classes. • Proposed heatmaps can help the radiologists to locate the affected regions on chest X-rays. • DarkCovidNet model can assist the clinicians to make faster and accurate diagnosis.', 'counts_by_year': [[2022, 381], [2021, 620], [2020, 224]]}, {'id': 'W2534737939', 'doi': 'https://doi.org/10.1016/j.cej.2016.10.064', 'title': 'Application of peroxymonosulfate and its activation methods for degradation of environmental organic pollutants: Review', 'type': 'journal-article', 'publication_date': '2017-02-15', 'host_venue': 'V149937609', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2361775706', ['I119939603']], ['A2164126639', ['I58048189']]], 'cited_by_count': 1227, 'concepts': [['C82685317', '0.77536905'], ['C2779679103', '0.77329296'], ['C107872376', '0.6316979'], ['C185592680', '0.5646162'], ['C39432304', '0.4083194']], 'referenced_works': ['W214926136', 'W256457453', 'W327195071', 'W380829632', 'W624074743', 'W886589509', 'W1127470191', 'W1127712305', 'W1136756498', 'W1138025054', 'W1421189506', 'W1553418891', 'W1610334416', 'W1765836652', 'W1776328638', 'W1792053144', 'W1824788921', 'W1845442996', 'W1860002421', 'W1886832367', 'W1961453377', 'W1962407424', 'W1963708063', 'W1964975932', 'W1965620979', 'W1967026758', 'W1967932039', 'W1971694940', 'W1972119071', 'W1974066606', 'W1974435265', 'W1974855989', 'W1978412154', 'W1978417585', 'W1981281465', 'W1982187838', 'W1982687253', 'W1982757851', 'W1983215067', 'W1983228601', 'W1984074384', 'W1986727388', 'W1987061787', 'W1987103411', 'W1987952875', 'W1988975355', 'W1990670192', 'W1991821893', 'W1992283811', 'W1992386997', 'W1992411968', 'W1994564578', 'W1997240523', 'W1997510797', 'W1997854946', 'W1999028149', 'W1999336879', 'W1999414626', 'W2000082462', 'W2000410867', 'W2001350703', 'W2003515398', 'W2004521164', 'W2006639923', 'W2006840347', 'W2008195384', 'W2008308368', 'W2009671188', 'W2010479638', 'W2010532473', 'W2012998347', 'W2013897997', 'W2014723389', 'W2014808889', 'W2014945627', 'W2015265053', 'W2015576710', 'W2018345237', 'W2018735512', 'W2019157029', 'W2019479774', 'W2019635083', 'W2019738563', 'W2019783449', 'W2021562578', 'W2024303543', 'W2024417295', 'W2024860397', 'W2025946634', 'W2026583102', 'W2026623563', 'W2027585298', 'W2027923342', 'W2031149317', 'W2031806648', 'W2032448751', 'W2032550278', 'W2032652071', 'W2032807843', 'W2033446780', 'W2035361249', 'W2035429631', 'W2036139564', 'W2036169956', 'W2036699846', 'W2038173949', 'W2039168291', 'W2039940707', 'W2040456488', 'W2040581404', 'W2041294340', 'W2041319750', 'W2042091725', 'W2042709024', 'W2045342647', 'W2046670028', 'W2046723375', 'W2047063025', 'W2050755687', 'W2054759372', 'W2056424411', 'W2056444436', 'W2058016171', 'W2060044938', 'W2060184651', 'W2060480206', 'W2061110916', 'W2061400071', 'W2062212173', 'W2062618008', 'W2062796523', 'W2063473628', 'W2065589530', 'W2069916097', 'W2071880505', 'W2073356400', 'W2073564719', 'W2076456080', 'W2076557324', 'W2078003267', 'W2080704741', 'W2082495188', 'W2082608269', 'W2082650752', 'W2085350707', 'W2087112532', 'W2088083245', 'W2088398848', 'W2089138607', 'W2089153062', 'W2091372087', 'W2092590444', 'W2092645630', 'W2093076236', 'W2093131958', 'W2093175754', 'W2094190414', 'W2094731216', 'W2094868944', 'W2094982872', 'W2095104341', 'W2095708811', 'W2096571017', 'W2097370534', 'W2105149620', 'W2108033212', 'W2113589815', 'W2116492037', 'W2119244953', 'W2140304859', 'W2140750116', 'W2142320165', 'W2142637560', 'W2145950951', 'W2146015864', 'W2149439116', 'W2153404268', 'W2155287970', 'W2158242209', 'W2161567034', 'W2170265639', 'W2172868803', 'W2173450593', 'W2184689531', 'W2185118848', 'W2186348980', 'W2193455037', 'W2193965084', 'W2208719136', 'W2224184671', 'W2236020494', 'W2239636067', 'W2244578615', 'W2277482974', 'W2277814118', 'W2280351861', 'W2280545431', 'W2281600796', 'W2286206949', 'W2290596494', 'W2291000936', 'W2294690355', 'W2297802339', 'W2300158998', 'W2300214231', 'W2312250990', 'W2313927412', 'W2315193584', 'W2315776822', 'W2328967289', 'W2335221168', 'W2336262713', 'W2337685078', 'W2337940487', 'W2338071762', 'W2338186405', 'W2342991161', 'W2343218888', 'W2345687145', 'W2345717141', 'W2345880405', 'W2345998342', 'W2346905400', 'W2347123726', 'W2348689834', 'W2394722219', 'W2406361769', 'W2408898971', 'W2413402613', 'W2417932691', 'W2430964849', 'W2460517998', 'W2461099527', 'W2462074182', 'W2463360277', 'W2465461534', 'W2466617984', 'W2467444326', 'W2472852820', 'W2473132646', 'W2486559881', 'W2498744255', 'W2505746704', 'W2511057559', 'W2511264310', 'W2513088319', 'W2514132730', 'W2514380859', 'W2516359589', 'W2516724817', 'W2517770735', 'W2519718304', 'W2520000022', 'W2525597145', 'W2526052368', 'W2950673817'], 'abstract': 'Abstract   The degradation of refractory organic compounds to harmless matters is one of the major concerns of environmentalists. Advanced oxidation processes (AOPs) are promising technologies producing the hydroxyl and sulfate radicals for pollutant degradation. Recently, much attention has been paid to producing sulfate radicals through activation of peroxymonosulfate (PMS). Nowadays, the use of PMS has acquired popularity thanks to its high reactivity and also to its high potential in generating sulfate radical. Actually it is becoming an alternative for hydrogen peroxide and persulfate. PMS is an unsymmetrical oxidant which can be activated to produce both hydroxyl and sulfate radicals. Various methods of PMS activation have been reported in literature including transition metals (homogenous and heterogeneous), ultraviolet, ultrasound, conduction electron, carbon catalysts and so on. PMS activation has been broadly applied for a wide range of pollutants mostly in aqueous solution. A literature review is carried out on environmental application of PMS in degradation of contaminants to clarify the performance of PMS. This review in detail describes the PMS usage in remediation of environmental pollutants with focus on the different methods of activation and the effect of main operational parameters on PMS-based processes. Moreover, the identification of contribution of each radical is discussed based on quenching experiments and electron spin resonance method. Finally, an overview on applying PMS in real wastewater and other matrixes (air, soil and sludge) is conducted and some recommendations are proposed for future studies.', 'counts_by_year': [[2022, 348], [2021, 298], [2020, 239], [2019, 178], [2018, 102], [2017, 52]]}, {'id': 'W2963516811', 'doi': 'https://doi.org/10.1109/cvpr.2016.89', 'title': 'Training Region-Based Object Detectors with Online Hard Example Mining', 'type': 'proceedings-article', 'publication_date': '2016-06-01', 'host_venue': 'V4306417987', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2116029713', ['I74973139']], ['A2099263982', ['I74973139']], ['A2473549963', ['I2252078561']]], 'cited_by_count': 1226, 'concepts': [['C75608658', '0.91887647'], ['C127705205', '0.89274883'], ['C8642999', '0.8643944'], ['C41008148', '0.76907396'], ['C2776151529', '0.6941826']], 'referenced_works': ['W219040644', 'W1536680647', 'W1932624639', 'W1989684337', 'W1991367009', 'W2010181071', 'W2017691720', 'W2031489346', 'W2066624635', 'W2088049833', 'W2102605133', 'W2128715914', 'W2147800946', 'W2159386181', 'W2161969291', 'W2168356304', 'W2217896605'], 'abstract': 'The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always been – detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more effective and efficient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on PASCAL VOC 2007 and 2012 respectively.', 'counts_by_year': [[2022, 136], [2021, 305], [2020, 306], [2019, 251], [2018, 167], [2017, 52], [2016, 7], [2015, 1]]}, {'id': 'W2963929190', 'doi': 'https://doi.org/10.18653/v1/k16-1028', 'title': 'Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond', 'type': 'proceedings-article', 'publication_date': '2016-02-19', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A93460222', ['I1341412227']], ['A2142227750', ['I1341412227']], ['A2158019794', ['I1341412227']], ['A152899538', ['I70931966']], ['A2168100440', ['I1341412227']]], 'cited_by_count': 1223, 'concepts': [['C170858558', '0.93196917'], ['C41008148', '0.86674976'], ['C2777530160', '0.694034'], ['C147168706', '0.67315894'], ['C204321447', '0.6645367']], 'referenced_works': ['W6908809', 'W23242996', 'W1843891098', 'W1890230307', 'W1924770834', 'W2028339364', 'W2043004216', 'W2083451366', 'W2090773135', 'W2102269292', 'W2103164118', 'W2115613106', 'W2118434577', 'W2139501017', 'W2251656952', 'W2267186426', 'W2293941196', 'W2307381258', 'W2467173223', 'W2507756961', 'W2949615363', 'W2950133940', 'W2950344723', 'W2952230511', 'W2962826786', 'W2962944953', 'W2962996600', 'W2964308564'], 'abstract': 'In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.', 'counts_by_year': [[2022, 82], [2021, 320], [2020, 300], [2019, 319], [2018, 159], [2017, 41], [2016, 2]]}, {'id': 'W2471536144', 'doi': 'https://doi.org/10.1126/science.aaf2403', 'title': 'Visualization and analysis of gene expression in tissue sections by spatial transcriptomics', 'type': 'journal-article', 'publication_date': '2016-07-01', 'host_venue': 'V3880285', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2061107792', ['I28166907', 'I86987016']], ['A2270613969', ['I86987016']], ['A1932003521', ['I86987016']], ['A2466656131', ['I28166907', 'I86987016']], ['A2608097817', ['I28166907', 'I86987016']], ['A2680767883', ['I28166907']], ['A2081870406', ['I86987016']], ['A2182253404', ['I86987016']], ['A251421662', ['I2800139495']], ['A2276344709', ['I2800139495']], ['A2468236757', ['I86987016']], ['A814294502', ['I28166907']], ['A2236833834', ['I28166907']], ['A2151504428', ['I187531555']], ['A2096165246', ['I123387679']], ['A2061886714', ['I86987016']], ['A2259285936', ['I86987016']], ['A2237187824', ['I2800139495']], ['A1969387195', ['I28166907']], ['A2060093159', ['I86987016']], ['A77240292', ['I28166907']]], 'cited_by_count': 1222, 'concepts': [['C70721500', '0.62196815'], ['C104317684', '0.6084486'], ['C162317418', '0.59571695'], ['C150194340', '0.5743246'], ['C86803240', '0.55805933']], 'referenced_works': ['W1593845303', 'W1977544680', 'W1980697049', 'W1981509058', 'W1997179616', 'W2003445065', 'W2008118471', 'W2018246047', 'W2038596550', 'W2069089843', 'W2083092470', 'W2086527601', 'W2099851046', 'W2121777852', 'W2123273050', 'W2123491442', 'W2134526812', 'W2136101247', 'W2137586531', 'W2139838862', 'W2170551349', 'W2179438025'], 'abstract': 'Spatial structure of RNA expression RNA-seq and similar methods can record gene expression within and among cells. Current methods typically lose positional information and many require arduous single-cell isolation and sequencing. Ståhl et al. have developed a way of measuring the spatial distribution of transcripts by annealing fixed brain or cancer tissue samples directly to bar-coded reverse transcriptase primers, performing reverse transcription followed by sequencing and computational reconstruction, and they can do so for multiple genes. Science , this issue p. 78', 'counts_by_year': [[2022, 322], [2021, 376], [2020, 250], [2019, 124], [2018, 90], [2017, 44], [2016, 11]]}, {'id': 'W2766085136', 'doi': 'https://doi.org/10.1021/acs.jchemed.7b00361', 'title': 'A Practical Beginner’s Guide to Cyclic Voltammetry', 'type': 'journal-article', 'publication_date': '2018-02-13', 'host_venue': 'V105208853', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2580818025', ['I114027177']], ['A2765862332', ['I114027177']], ['A2501150308', ['I114027177']], ['A1997270625', ['I114027177']], ['A2330902154', ['I114027177']], ['A2144818350', ['I114027177']]], 'cited_by_count': 1222, 'concepts': [['C2777869211', '0.6203171'], ['C113196181', '0.3799698'], ['C185592680', '0.37267017'], ['C41008148', '0.34246784'], ['C171250308', '0.333197']], 'referenced_works': ['W1598955959', 'W1967244854', 'W1969300761', 'W1983749634', 'W2024630314', 'W2029291565', 'W2041301556', 'W2044560300', 'W2058577292', 'W2061977671', 'W2063074480', 'W2067492429', 'W2079490844', 'W2084797580', 'W2170909829', 'W2320625878', 'W2326181770', 'W2346021612', 'W2519743225', 'W2566716795', 'W2588935881', 'W2614048962', 'W4234148912', 'W4253339442', 'W4254720078'], 'abstract': 'Despite the growing popularity of cyclic voltammetry, many students do not receive formalized training in this technique as part of their coursework. Confronted with self-instruction, students can be left wondering where to start. Here, a short introduction to cyclic voltammetry is provided to help the reader with data acquisition and interpretation. Tips and common pitfalls are provided, and the reader is encouraged to apply what is learned in short, simple training modules provided in the Supporting Information. Armed with the basics, the motivated aspiring electrochemist will find existing resources more accessible and will progress much faster in the understanding of cyclic voltammetry.', 'counts_by_year': [[2022, 333], [2021, 408], [2020, 267], [2019, 166], [2018, 40]]}, {'id': 'W2997280954', 'doi': 'https://doi.org/10.1186/s13059-019-1874-1', 'title': 'Normalization and variance stabilization of single-cell RNA-seq data using regularized negative binomial regression', 'type': 'journal-article', 'publication_date': '2019-12-23', 'host_venue': 'V81160022', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A347231821', ['I4210151462']], ['A2026615643', ['I57206974']]], 'cited_by_count': 1222, 'concepts': [['C22019652', '0.7564563'], ['C136886441', '0.6811436'], ['C70437156', '0.66992366'], ['C119043178', '0.66822326'], ['C199335787', '0.64348155']], 'referenced_works': ['W839653734', 'W1602702667', 'W1967327758', 'W1981641790', 'W1988391018', 'W2010653277', 'W2069089843', 'W2081098333', 'W2114104545', 'W2130116522', 'W2137526110', 'W2152239989', 'W2179438025', 'W2190545194', 'W2233603983', 'W2343956310', 'W2510746232', 'W2516455020', 'W2540833380', 'W2582033455', 'W2591733518', 'W2605611918', 'W2612502013', 'W2620721366', 'W2739492614', 'W2783644856', 'W2785264903', 'W2794480084', 'W2794891507', 'W2800392236', 'W2889326414', 'W2889435420', 'W2901677030', 'W2949177718', 'W2951381561'], 'abstract': 'Single-cell RNA-seq (scRNA-seq) data exhibits significant cell-to-cell variation due to technical factors, including the number of molecules detected in each cell, which can confound biological heterogeneity with technical effects. To address this, we present a modeling framework for the normalization and variance stabilization of molecular count data from scRNA-seq experiments. We propose that the Pearson residuals from "regularized negative binomial regression," where cellular sequencing depth is utilized as a covariate in a generalized linear model, successfully remove the influence of technical characteristics from downstream analyses while preserving biological heterogeneity. Importantly, we show that an unconstrained negative binomial model may overfit scRNA-seq data, and overcome this by pooling information across genes with similar abundances to obtain stable parameter estimates. Our procedure omits the need for heuristic steps including pseudocount addition or log-transformation and improves common downstream analytical tasks such as variable gene selection, dimensional reduction, and differential expression. Our approach can be applied to any UMI-based scRNA-seq dataset and is freely available as part of the R package sctransform, with a direct interface to our single-cell toolkit Seurat.', 'counts_by_year': [[2022, 462], [2021, 506], [2020, 242], [2019, 10], [2018, 1]]}, {'id': 'W2890964092', 'doi': 'https://doi.org/10.1109/icassp.2018.8461375', 'title': 'X-Vectors: Robust DNN Embeddings for Speaker Recognition', 'type': 'proceedings-article', 'publication_date': '2018-04-15', 'host_venue': 'V4306419087', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2254062345', ['I145311948']], ['A2154831416', ['I145311948']], ['A2308828484', ['I145311948']], ['A2779265140', ['I145311948']], ['A1996119148', ['I145311948']]], 'cited_by_count': 1221, 'concepts': [['C41008148', '0.7363708'], ['C111219384', '0.6919722'], ['C63479239', '0.67414415'], ['C28490314', '0.5996232'], ['C153083717', '0.58048165']], 'referenced_works': ['W1528954144', 'W1589137271', 'W2004497042', 'W2039057510', 'W2114925438', 'W2129066450', 'W2129379984', 'W2138547132', 'W2150769028', 'W2183016404', 'W2290689761', 'W2395750323', 'W2584329820', 'W2586741466', 'W2587150483', 'W2696967604', 'W2726515241', 'W2747238065', 'W2748488820', 'W2964228006'], 'abstract': 'In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.', 'counts_by_year': [[2022, 143], [2021, 392], [2020, 415], [2019, 229], [2018, 41]]}, {'id': 'W2968810373', 'doi': 'https://doi.org/10.1109/access.2019.2935192', 'title': 'Wireless Communications Through Reconfigurable Intelligent Surfaces', 'type': 'journal-article', 'publication_date': '2019-08-13', 'host_venue': 'V2485537415', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2101464133', ['I1351752']], ['A2142852865', ['I1300957399']], ['A2608975988', ['I149758196']], ['A2952783946', ['I4210107720']], ['A1976698169', ['I71920554']], ['A2413204075', ['I165932596']]], 'cited_by_count': 1214, 'concepts': [['C555944384', '0.70099556'], ['C41008148', '0.66015846'], ['C108037233', '0.57822955'], ['C47798520', '0.53129154'], ['C41971633', '0.47958675']], 'referenced_works': ['W595252221', 'W1766242247', 'W1971549192', 'W1994232933', 'W2029262884', 'W2037866776', 'W2065105631', 'W2067605736', 'W2082432933', 'W2121607567', 'W2149164057', 'W2154841070', 'W2160593627', 'W2164028232', 'W2256638885', 'W2344487751', 'W2384823607', 'W2404663615', 'W2409048281', 'W2479310799', 'W2524701924', 'W2536076834', 'W2562197722', 'W2592240905', 'W2594990926', 'W2617211273', 'W2735938380', 'W2744485539', 'W2765606903', 'W2769583887', 'W2775035220', 'W2783168053', 'W2790634998', 'W2792966440', 'W2794344156', 'W2795167939', 'W2797556357', 'W2801689992', 'W2805166665', 'W2818739605', 'W2891191577', 'W2891354184', 'W2896507689', 'W2898925204', 'W2901450205', 'W2904015002', 'W2916238596', 'W2937693915', 'W2945534522', 'W2949670636', 'W2950077417', 'W2963121727', 'W2963187309', 'W2963238659', 'W2963281988', 'W2963322519', 'W2963458581', 'W2963460596', 'W2963504849', 'W2963973401', 'W2964076076', 'W2964204680', 'W2964278319', 'W2969001189', 'W2969072158', 'W2969509661', 'W2970375713', 'W3100891096', 'W3103820901', 'W3104964198', 'W4241214195', 'W4246762037', 'W4256217385'], 'abstract': 'The future of mobile communications looks exciting with the potential new use cases and challenging requirements of future 6th generation (6G) and beyond wireless networks. Since the beginning of the modern era of wireless communications, the propagation medium has been perceived as a randomly behaving entity between the transmitter and the receiver, which degrades the quality of the received signal due to the uncontrollable interactions of the transmitted radio waves with the surrounding objects. The recent advent of reconfigurable intelligent surfaces in wireless communications enables, on the other hand, network operators to control the scattering, reflection, and refraction characteristics of the radio waves, by overcoming the negative effects of natural wireless propagation. Recent results have revealed that reconfigurable intelligent surfaces can effectively control the wavefront, e.g., the phase, amplitude, frequency, and even polarization, of the impinging signals without the need of complex decoding, encoding, and radio frequency processing operations. Motivated by the potential of this emerging technology, the present article is aimed to provide the readers with a detailed overview and historical perspective on state-of-the-art solutions, and to elaborate on the fundamental differences with other technologies, the most important open research issues to tackle, and the reasons why the use of reconfigurable intelligent surfaces necessitates to rethink the communication-theoretic models currently employed in wireless networks. This article also explores theoretical performance limits of reconfigurable intelligent surface-assisted communication systems using mathematical techniques and elaborates on the potential use cases of intelligent surfaces in 6G and beyond wireless networks.', 'counts_by_year': [[2022, 325], [2021, 513], [2020, 329], [2019, 46]]}, {'id': 'W2412588858', 'doi': 'https://doi.org/10.1109/mgrs.2016.2540798', 'title': 'Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art', 'type': 'journal-article', 'publication_date': '2016-06-07', 'host_venue': 'V2491948244', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2099831346', ['I4210118728']], ['A2104063197', ['I4210118728']], ['A2157883762', ['I37461747']]], 'cited_by_count': 1212, 'concepts': [['C41008148', '0.5322981'], ['C48103436', '0.5009208'], ['C62649853', '0.4285489'], ['C154945302', '0.38914'], ['C2522767166', '0.32151535']], 'referenced_works': ['W1491705651', 'W1491912111', 'W1521436688', 'W1526295910', 'W1535289548', 'W1560922506', 'W1593057674', 'W1624980979', 'W1843514792', 'W1843779453', 'W1912954554', 'W1914401667', 'W1920235975', 'W1931125522', 'W1950365613', 'W1958291604', 'W1963985200', 'W1965838475', 'W1966580635', 'W1977177161', 'W1980038761', 'W1984792953', 'W1985133440', 'W1992102478', 'W1993821172', 'W1996663388', 'W1998489088', 'W2000952059', 'W2001298023', 'W2003059629', 'W2005106632', 'W2006603039', 'W2008847349', 'W2013882339', 'W2015386604', 'W2015861736', 'W2018257962', 'W2019338222', 'W2027413748', 'W2027668293', 'W2027922120', 'W2028104478', 'W2029316659', 'W2034344316', 'W2037850804', 'W2039077039', 'W2041972612', 'W2044439250', 'W2058537106', 'W2061240006', 'W2062326663', 'W2063102607', 'W2066916495', 'W2078433039', 'W2084465266', 'W2085625911', 'W2085665642', 'W2086866337', 'W2087263574', 'W2090424610', 'W2093679105', 'W2094455438', 'W2098676252', 'W2105032938', 'W2105536892', 'W2106051978', 'W2106777458', 'W2108597246', 'W2112306707', 'W2112796928', 'W2114819256', 'W2117130368', 'W2121915926', 'W2123046940', 'W2124537004', 'W2127152713', 'W2132424367', 'W2136251662', 'W2136922672', 'W2149194912', 'W2149980531', 'W2150990614', 'W2151103935', 'W2154789478', 'W2161969291', 'W2162915993', 'W2163922914', 'W2166923144', 'W2179290474', 'W2212194823', 'W2219715551', 'W2241675565', 'W2248723555', 'W2267317359', 'W2273297058', 'W2275061906', 'W2282131058', 'W2291068538', 'W2294802479', 'W2325982591', 'W2417947228', 'W2546302380', 'W2919115771', 'W4231109964', 'W4248936881', 'W4250837326'], 'abstract': 'Deep-learning (DL) algorithms, which learn the representative and discriminative features in a hierarchical manner from the data, have recently become a hotspot in the machine-learning area and have been introduced into the geoscience and remote sensing (RS) community for RS big data analysis. Considering the low-level features (e.g., spectral and texture) as the bottom level, the output feature representation from the top level of the network can be directly fed into a subsequent classifier for pixel-based classification. As a matter of fact, by carefully addressing the practical demands in RS applications and designing the input?output levels of the whole network, we have found that DL is actually everywhere in RS data analysis: from the traditional topics of image preprocessing, pixel-based classification, and target recognition, to the recent challenging tasks of high-level semantic feature extraction and RS scene understanding.', 'counts_by_year': [[2022, 175], [2021, 279], [2020, 251], [2019, 247], [2018, 172], [2017, 81], [2016, 7]]}, {'id': 'W2327622231', 'doi': 'https://doi.org/10.1109/comst.2015.2495297', 'title': 'Survey of Important Issues in UAV Communication Networks', 'type': 'journal-article', 'publication_date': '2016-01-22', 'host_venue': 'V23688054', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2228122089', ['I204465549']], ['A2157313770', ['I204465549']], ['A2548790549', ['I4210131589']]], 'cited_by_count': 1207, 'concepts': [['C41008148', '0.8319014'], ['C31258907', '0.6525243'], ['C94523657', '0.5412031'], ['C91280400', '0.534934'], ['C62611344', '0.502198']], 'referenced_works': ['W66768925', 'W67092002', 'W1448121968', 'W1792893069', 'W1964917131', 'W1965396227', 'W1965703925', 'W1969455159', 'W1985692170', 'W1996673595', 'W1997522524', 'W1998667180', 'W1998900707', 'W1999521261', 'W2008979299', 'W2010634640', 'W2011482408', 'W2012031862', 'W2017333190', 'W2020199432', 'W2026859718', 'W2027414550', 'W2034301681', 'W2035268479', 'W2036586979', 'W2043850694', 'W2044959725', 'W2049874629', 'W2052525433', 'W2061891825', 'W2064902583', 'W2066204203', 'W2067453331', 'W2070449754', 'W2071738571', 'W2073185086', 'W2073909425', 'W2073966433', 'W2075092260', 'W2081534709', 'W2084156584', 'W2084430118', 'W2084718899', 'W2088606702', 'W2090434188', 'W2090837088', 'W2091706496', 'W2100684961', 'W2100873863', 'W2102258543', 'W2103902040', 'W2104705174', 'W2106252042', 'W2106335692', 'W2110429345', 'W2110623752', 'W2111430407', 'W2113443014', 'W2118577620', 'W2122181843', 'W2125957038', 'W2128361500', 'W2130834448', 'W2136420233', 'W2140250995', 'W2142522527', 'W2145198931', 'W2146218804', 'W2147118406', 'W2150825860', 'W2151714868', 'W2155490220', 'W2155993893', 'W2156077208', 'W2156160382', 'W2156803816', 'W2157776896', 'W2158090495', 'W2159013099', 'W2159661467', 'W2159924146', 'W2160580939', 'W2164900367', 'W2166406195', 'W2166576909', 'W2169753080', 'W2171970784', 'W2181717997', 'W2314623158', 'W2324466850', 'W2545197492', 'W2962826974', 'W3162828249', 'W3162914541', 'W4234048677', 'W4249186852'], 'abstract': 'Unmanned Aerial Vehicles (UAVs) have enormous potential in the public and civil domains. These are particularly useful in applications where human lives would otherwise be endangered. Multi-UAV systems can collaboratively complete missions more efficiently and economically as compared to single UAV systems. However, there are many issues to be resolved before effective use of UAVs can be made to provide stable and reliable context-specific networks. Much of the work carried out in the areas of Mobile Ad Hoc Networks (MANETs), and Vehicular Ad Hoc Networks (VANETs) does not address the unique characteristics of the UAV networks. UAV networks may vary from slow dynamic to dynamic; have intermittent links and fluid topology. While it is believed that ad hoc mesh network would be most suitable for UAV networks yet the architecture of multi-UAV networks has been an understudied area. Software Defined Networking (SDN) could facilitate flexible deployment and management of new services and help reduce cost, increase security and availability in networks. Routing demands of UAV networks go beyond the needs of MANETS and VANETS. Protocols are required that would adapt to high mobility, dynamic topology, intermittent links, power constraints and changing link quality. UAVs may fail and the network may get partitioned making delay and disruption tolerance an important design consideration. Limited life of the node and dynamicity of the network leads to the requirement of seamless handovers where researchers are looking at the work done in the areas of MANETs and VANETs, but the jury is still out. As energy supply on UAVs is limited, protocols in various layers should contribute towards greening of the network. This article surveys the work done towards all of these outstanding issues, relating to this new class of networks, so as to spur further research in these areas.', 'counts_by_year': [[2022, 164], [2021, 243], [2020, 265], [2019, 288], [2018, 164], [2017, 67], [2016, 16]]}, {'id': 'W2999309192', 'doi': 'https://doi.org/10.1186/s12864-019-6413-7', 'title': 'The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation', 'type': 'journal-article', 'publication_date': '2020-01-02', 'host_venue': 'V28614077', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2397237946', ['I4210131630']], ['A125273320', ['I2277624104']]], 'cited_by_count': 1207, 'concepts': [['C66905080', '0.7143434'], ['C64869954', '0.70609957'], ['C48372109', '0.63410753'], ['C112789634', '0.6076186'], ['C117220453', '0.5776466']], 'referenced_works': ['W61851215', 'W79139011', 'W1540240814', 'W1556310958', 'W1672197616', 'W1758661201', 'W1966716734', 'W1966976587', 'W1975730959', 'W1986515506', 'W1987869189', 'W1988301066', 'W1992223383', 'W2004902747', 'W2008056655', 'W2008525336', 'W2021560851', 'W2028962062', 'W2053154970', 'W2059185913', 'W2064236088', 'W2064397275', 'W2070493638', 'W2087309216', 'W2087684630', 'W2088252378', 'W2090091537', 'W2093213907', 'W2096451472', 'W2097057782', 'W2099715980', 'W2101807845', 'W2107432340', 'W2109553965', 'W2109826612', 'W2116825089', 'W2122814955', 'W2131046966', 'W2132513126', 'W2137377746', 'W2137959503', 'W2138290126', 'W2141014056', 'W2145845610', 'W2146096861', 'W2146841526', 'W2154039449', 'W2155653793', 'W2157825442', 'W2158698691', 'W2164777277', 'W2170505850', 'W2171558600', 'W2187848264', 'W2333286569', 'W2490420619', 'W2512288612', 'W2532349966', 'W2606460416', 'W2620760558', 'W2755779374', 'W2771169143', 'W2773275559', 'W2791704581', 'W2796029694', 'W2802494476', 'W2888728157', 'W2904016702', 'W2910437673', 'W2910814369', 'W2911964244', 'W2918408501', 'W2919115771', 'W2949423586', 'W2963776453', 'W2975256032', 'W3102476541', 'W4245055982', 'W4294214983'], 'abstract': 'Abstract Background To evaluate binary classifications and their confusion matrices, scientific researchers can employ several statistical rates, accordingly to the goal of the experiment they are investigating. Despite being a crucial issue in machine learning, no widespread consensus has been reached on a unified elective chosen measure yet. Accuracy and F 1 score computed on confusion matrices have been (and still are) among the most popular adopted metrics in binary classification tasks. However, these statistical measures can dangerously show overoptimistic inflated results, especially on imbalanced datasets. Results The Matthews correlation coefficient (MCC), instead, is a more reliable statistical rate which produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives), proportionally both to the size of positive elements and the size of negative elements in the dataset. Conclusions In this article, we show how MCC produces a more informative and truthful score in evaluating binary classifications than accuracy and F 1 score, by first explaining the mathematical properties, and then the asset of MCC in six synthetic use cases and in a real genomics scenario. We believe that the Matthews correlation coefficient should be preferred to accuracy and F 1 score in evaluating binary classification tasks by all scientific communities.', 'counts_by_year': [[2022, 497], [2021, 529], [2020, 174]]}, {'id': 'W2503339013', 'doi': 'https://doi.org/10.1007/978-3-319-46475-6_25', 'title': 'Accelerating the Super-Resolution Convolutional Neural Network', 'type': 'book-chapter', 'publication_date': '2016-10-08', 'host_venue': 'V4306463941', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2609354386', ['I177725633']], ['A2123758277', ['I177725633']], ['A2166284823', ['I177725633']]], 'cited_by_count': 1204, 'concepts': [['C41008148', '0.8740543'], ['C81363708', '0.76754844'], ['C138268822', '0.52200705'], ['C154945302', '0.4823498']], 'referenced_works': ['W7682646', 'W135113724', 'W935139217', 'W1677182931', 'W1791560514', 'W1849277567', 'W1885185971', 'W1893585201', 'W1903029394', 'W1930824406', 'W1950594372', 'W2047920195', 'W2098506229', 'W2149669120', 'W2150081556', 'W2155893237', 'W2214802144', 'W2242218935', 'W2489661638', 'W2507235960', 'W2520322935'], 'abstract': 'As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quality. However, the high computational cost still hinders it from practical usage that demands real-time performance (24 fps). In this paper, we aim at accelerating the current SRCNN, and propose a compact hourglass-shape CNN structure for faster and better SR. We re-design the SRCNN structure mainly in three aspects. First, we introduce a deconvolution layer at the end of the network, then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate the mapping layer by shrinking the input feature dimension before mapping and expanding back afterwards. Third, we adopt smaller filter sizes but more mapping layers. The proposed model achieves a speed up of more than 40 times with even superior restoration quality. Further, we present the parameter settings that can achieve real-time performance on a generic CPU while still maintaining good performance. A corresponding transfer strategy is also proposed for fast training and testing across different upscaling factors.', 'counts_by_year': [[2022, 128], [2021, 311], [2020, 331], [2019, 234], [2018, 135], [2017, 57], [2016, 7]]}, {'id': 'W2963591054', 'doi': 'https://doi.org/10.1109/3dv.2016.32', 'title': 'Deeper Depth Prediction with Fully Convolutional Residual Networks', 'type': 'proceedings-article', 'publication_date': '2016-10-01', 'host_venue': 'V4306419358', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2530842385', ['I62916508']], ['A2251807875', ['I62916508']], ['A1467282660', ['I40120149']], ['A2013077172', ['I62916508']], ['A572302453', ['I62916508']]], 'cited_by_count': 1204, 'concepts': [['C41008148', '0.82508516'], ['C155512373', '0.71083295'], ['C154945302', '0.7044217'], ['C81363708', '0.64292216'], ['C65909025', '0.57359606']], 'referenced_works': ['W87843015', 'W1604693428', 'W1893585201', 'W1903029394', 'W1905829557', 'W1915250530', 'W1923779427', 'W1992178727', 'W2021930164', 'W2026203852', 'W2065906272', 'W2083047701', 'W2090518410', 'W2116626343', 'W2117539524', 'W2118246710', 'W2118304946', 'W2123043875', 'W2125416623', 'W2132947399', 'W2141200610', 'W2155871590', 'W2156094778', 'W2161969291', 'W2436453945', 'W4211015534'], 'abstract': 'This paper addresses the problem of estimating the depth map of a scene given a single RGB image. We propose a fully convolutional architecture, encompassing residual learning, to model the ambiguous mapping between monocular images and depth maps. In order to improve the output resolution, we present a novel way to efficiently learn feature map up-sampling within the network. For optimization, we introduce the reverse Huber loss that is particularly suited for the task at hand and driven by the value distributions commonly present in depth maps. Our model is composed of a single architecture that is trained end-to-end and does not rely on post-processing techniques, such as CRFs or other additional refinement steps. As a result, it runs in real-time on images or videos. In the evaluation, we show that the proposed model contains fewer parameters and requires fewer training data than the current state of the art, while outperforming all approaches on depth estimation. Code and models are publicly available.', 'counts_by_year': [[2022, 103], [2021, 260], [2020, 290], [2019, 313], [2018, 190], [2017, 43], [2016, 4]]}, {'id': 'W3027286012', 'doi': 'https://doi.org/10.1093/nar/gkaa407', 'title': 'TIMER2.0 for analysis of tumor-infiltrating immune cells', 'type': 'journal-article', 'publication_date': '2020-07-02', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2699360546', ['I24185976']], ['A2887286394', ['I4210127126', 'I4210117453']], ['A2639898157', ['I4210117453']], ['A2095673424', ['I4210117453']], ['A2618312554', ['I24185976']], ['A2156432672', ['I24185976']], ['A3173271316', ['I867280407']], ['A2108629376', ['I4210117453']]], 'cited_by_count': 1204, 'concepts': [['C86803240', '0.81144965'], ['C8891405', '0.8026844'], ['C2776107976', '0.5750036'], ['C162317418', '0.5644758'], ['C2777701055', '0.53354603']], 'referenced_works': ['W1985987855', 'W1999042146', 'W1999574084', 'W2037900813', 'W2061164905', 'W2077813812', 'W2114104545', 'W2142236901', 'W2142300779', 'W2153056550', 'W2157852151', 'W2158485828', 'W2170137783', 'W2279794760', 'W2507880739', 'W2533508881', 'W2586629973', 'W2600132724', 'W2765224588', 'W2771978163', 'W2789878273', 'W2791040477', 'W2791512052', 'W2796408191', 'W2796531277', 'W2886498337', 'W2889237222', 'W2907564726', 'W2914270121', 'W2934069627', 'W2947554843', 'W2952001873', 'W2953757801', 'W2973051889', 'W3010745413', 'W3012668557'], 'abstract': 'Abstract Tumor progression and the efficacy of immunotherapy are strongly influenced by the composition and abundance of immune cells in the tumor microenvironment. Due to the limitations of direct measurement methods, computational algorithms are often used to infer immune cell composition from bulk tumor transcriptome profiles. These estimated tumor immune infiltrate populations have been associated with genomic and transcriptomic changes in the tumors, providing insight into tumor–immune interactions. However, such investigations on large-scale public data remain challenging. To lower the barriers for the analysis of complex tumor–immune interactions, we significantly improved our previous web platform TIMER. Instead of just using one algorithm, TIMER2.0 (http://timer.cistrome.org/) provides more robust estimation of immune infiltration levels for The Cancer Genome Atlas (TCGA) or user-provided tumor profiles using six state-of-the-art algorithms. TIMER2.0 provides four modules for investigating the associations between immune infiltrates and genetic or clinical features, and four modules for exploring cancer-related associations in the TCGA cohorts. Each module can generate a functional heatmap table, enabling the user to easily identify significant associations in multiple cancer types simultaneously. Overall, the TIMER2.0 web server provides comprehensive analysis and visualization functions of tumor infiltrating immune cells.', 'counts_by_year': [[2022, 743], [2021, 426], [2020, 31], [2019, 1]]}, {'id': 'W3098824823', 'doi': 'https://doi.org/10.5281/zenodo.5347031', 'title': 'Transformers: State-of-the-Art Natural Language Processing', 'type': 'proceedings-article', 'publication_date': '2020-10-01', 'host_venue': 'V4306418267', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2151891926', ['I198945027']], ['A2978660204', []], ['A2901920096', []], ['A2794942406', []], ['A2795365971', []], ['A2980009562', []], ['A2979782832', []], ['A3209621609', []], ['A148301988', ['I57206974']], ['A1885034522', ['I1902872']], ['A2970865618', ['I37461747']], ['A3099917620', []], ['A2970805034', []], ['A3099575957', []], ['A3105155681', []], ['A2701382563', ['I205783295']]], 'cited_by_count': 1204, 'concepts': [['C66322947', '0.8284201'], ['C41008148', '0.67436236'], ['C123657996', '0.65768033'], ['C44291984', '0.41134456'], ['C115903868', '0.35497397']], 'referenced_works': ['W2123442489', 'W2752194699', 'W2787560479', 'W2793978524', 'W2896457183', 'W2911109671', 'W2946119234', 'W2946417913', 'W2949251082', 'W2950339735', 'W2952509486', 'W2953320089', 'W2963026768', 'W2963212250', 'W2963310665', 'W2963403868', 'W2963756346', 'W2965373594', 'W2970597249', 'W2970771982', 'W2972119347', 'W2973727699', 'W2975059944', 'W2978017171', 'W2979949198', 'W2981852735', 'W2990704537', 'W3000514857', 'W3011411500', 'W3011573503', 'W3015468748', 'W3032532958', 'W3037191812'], 'abstract': 'Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.', 'counts_by_year': [[2022, 47], [2021, 1108], [2020, 47], [2019, 2]]}, {'id': 'W2328393125', 'doi': 'https://doi.org/10.1038/ng.3538', 'title': 'Integration of summary data from GWAS and eQTL studies predicts complex trait gene targets', 'type': 'journal-article', 'publication_date': '2016-05-01', 'host_venue': 'V137905309', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2615385315', ['I165143802']], ['A2345303091', ['I165143802']], ['A2661161961', ['I76130692']], ['A2152478446', ['I165143802']], ['A2981364557', ['I165143802']], ['A2141486400', ['I165143802']], ['A2118051784', ['I217937681']], ['A2064391365', ['I165779595']], ['A2101768980', ['I165143802']], ['A2150449257', ['I165143802']], ['A2603303308', ['I165143802']]], 'cited_by_count': 1203, 'concepts': [['C168393362', '0.88995165'], ['C106208931', '0.84754634'], ['C86803240', '0.77764213'], ['C81941488', '0.63778216'], ['C54355233', '0.577258']], 'referenced_works': ['W72392028', 'W278413204', 'W1504914687', 'W1533942137', 'W1636205509', 'W1844405070', 'W1972696558', 'W1972698844', 'W1980664123', 'W1981693548', 'W1983259675', 'W1989590412', 'W2008774738', 'W2012766110', 'W2013167069', 'W2033151532', 'W2039153962', 'W2047783882', 'W2052236587', 'W2058401000', 'W2073836327', 'W2075199575', 'W2082818899', 'W2091434498', 'W2094379190', 'W2101357408', 'W2103783427', 'W2104278634', 'W2106036991', 'W2115783720', 'W2116831158', 'W2116868464', 'W2118008714', 'W2133915764', 'W2135199713', 'W2139852278', 'W2141704631', 'W2148860875', 'W2151872224', 'W2152885121', 'W2153662687', 'W2156714600', 'W2160435411', 'W2162276444', 'W2164021954', 'W2164780420', 'W2171777347', 'W2209106767', 'W2264585211'], 'abstract': 'Genome-wide association studies (GWAS) have identified thousands of genetic variants associated with human complex traits. However, the genes or functional DNA elements through which these variants exert their effects on the traits are often unknown. We propose a method (called SMR) that integrates summary-level data from GWAS with data from expression quantitative trait locus (eQTL) studies to identify genes whose expression levels are associated with a complex trait because of pleiotropy. We apply the method to five human complex traits using GWAS data on up to 339,224 individuals and eQTL data on 5,311 individuals, and we prioritize 126 genes (for example, TRAF1 and ANKRD55 for rheumatoid arthritis and SNX19 and NMRAL1 for schizophrenia), of which 25 genes are new candidates; 77 genes are not the nearest annotated gene to the top associated GWAS SNP. These genes provide important leads to design future functional studies to understand the mechanism whereby DNA variation leads to complex trait variation.', 'counts_by_year': [[2022, 194], [2021, 242], [2020, 233], [2019, 212], [2018, 173], [2017, 122], [2016, 27]]}, {'id': 'W2949867654', 'doi': 'https://doi.org/10.1093/bioinformatics/btz305', 'title': 'RAxML-NG: a fast, scalable and user-friendly tool for maximum likelihood phylogenetic inference', 'type': 'journal-article', 'publication_date': '2019-11-01', 'host_venue': 'V52395412', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2211632775', ['I71030271']], ['A2047695532', ['I71030271']], ['A2950693761', ['I71030271']], ['A2787536255', ['I71030271']], ['A2289379467', ['I71030271', 'I102335020']]], 'cited_by_count': 1202, 'concepts': [['C41008148', '0.7551561'], ['C48044578', '0.71062046'], ['C170130773', '0.61471474'], ['C2776214188', '0.59205055'], ['C113174947', '0.5828189']], 'referenced_works': ['W1957469265', 'W2016171324', 'W2031611770', 'W2095523744', 'W2100030044', 'W2111211467', 'W2111485477', 'W2111647009', 'W2141052558', 'W2168736508', 'W2801900613', 'W2949367299', 'W2949751825', 'W2950001245', 'W2951358594'], 'abstract': 'Abstract Motivation Phylogenies are important for fundamental biological research, but also have numerous applications in biotechnology, agriculture and medicine. Finding the optimal tree under the popular maximum likelihood (ML) criterion is known to be NP-hard. Thus, highly optimized and scalable codes are needed to analyze constantly growing empirical datasets. Results We present RAxML-NG, a from-scratch re-implementation of the established greedy tree search algorithm of RAxML/ExaML. RAxML-NG offers improved accuracy, flexibility, speed, scalability, and usability compared with RAxML/ExaML. On taxon-rich datasets, RAxML-NG typically finds higher-scoring trees than IQTree, an increasingly popular recent tool for ML-based phylogenetic inference (although IQ-Tree shows better stability). Finally, RAxML-NG introduces several new features, such as the detection of terraces in tree space and the recently introduced transfer bootstrap support metric. Availability and implementation The code is available under GNU GPL at https://github.com/amkozlov/raxml-ng. RAxML-NG web service (maintained by Vital-IT) is available at https://raxml-ng.vital-it.ch/. Supplementary information Supplementary data are available at Bioinformatics online.', 'counts_by_year': [[2022, 419], [2021, 468], [2020, 250], [2019, 64], [2018, 1]]}, {'id': 'W2322358787', 'doi': 'https://doi.org/10.1093/ve/vew007', 'title': 'Exploring the temporal structure of heterochronous sequences using TempEst (formerly Path-O-Gen)', 'type': 'journal-article', 'publication_date': '2016-01-01', 'host_venue': 'V2734951655', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2109961709', ['I4210129072']], ['A2123201679', ['I889458895']], ['A2511416390', ['I4210135169']], ['A2232962782', ['I40120149']]], 'cited_by_count': 1199, 'concepts': [['C2776971686', '0.8734511'], ['C207390915', '0.61574715'], ['C193252679', '0.5954048'], ['C140779682', '0.5932522'], ['C2778112365', '0.5262938']], 'referenced_works': ['W1903589011', 'W1944637642', 'W1964350046', 'W1987423081', 'W1993210385', 'W2000870362', 'W2001750285', 'W2020315722', 'W2022600772', 'W2045517599', 'W2052708825', 'W2055809429', 'W2079027243', 'W2081949838', 'W2094939159', 'W2096690315', 'W2100806916', 'W2110335151', 'W2110481174', 'W2111207219', 'W2113439848', 'W2130362098', 'W2131202846', 'W2134213231', 'W2154331465', 'W2159368494', 'W2167740275'], 'abstract': 'Gene sequences sampled at different points in time can be used to infer molecular phylogenies on a natural timescale of months or years, provided that the sequences in question undergo measurable amounts of evolutionary change between sampling times. Data sets with this property are termed heterochronous and have become increasingly common in several fields of biology, most notably the molecular epidemiology of rapidly evolving viruses. Here we introduce the cross-platform software tool, TempEst (formerly known as Path-O-Gen), for the visualization and analysis of temporally sampled sequence data. Given a molecular phylogeny and the dates of sampling for each sequence, TempEst uses an interactive regression approach to explore the association between genetic divergence through time and sampling dates. TempEst can be used to (1) assess whether there is sufficient temporal signal in the data to proceed with phylogenetic molecular clock analysis, and (2) identify sequences whose genetic divergence and sampling date are incongruent. Examination of the latter can help identify data quality problems, including errors in data annotation, sample contamination, sequence recombination, or alignment error. We recommend that all users of the molecular clock models implemented in BEAST first check their data using TempEst prior to analysis.', 'counts_by_year': [[2022, 211], [2021, 314], [2020, 260], [2019, 159], [2018, 123], [2017, 102], [2016, 30]]}, {'id': 'W2751904527', 'doi': 'https://doi.org/10.1109/jiot.2017.2750180', 'title': 'Mobile Edge Computing: A Survey', 'type': 'journal-article', 'publication_date': '2018-02-01', 'host_venue': 'V2480266640', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2109101836', ['I184942183']], ['A2256822898', ['I184942183']], ['A2000060892', ['I184942183']], ['A2025742053', ['I184942183']]], 'cited_by_count': 1199, 'concepts': [['C41008148', '0.77074516'], ['C144543869', '0.5153701'], ['C2776061582', '0.5007851'], ['C2778456923', '0.4294775'], ['C162307627', '0.39797932']], 'referenced_works': ['W1513379583', 'W1557386445', 'W1564901575', 'W1595535199', 'W1638163870', 'W1790074971', 'W1904504745', 'W1966736232', 'W1971901219', 'W1973454958', 'W1974737884', 'W1977512233', 'W1988774297', 'W1990136478', 'W1992469725', 'W1999458275', 'W2012423440', 'W2013507536', 'W2014699205', 'W2035203720', 'W2045371716', 'W2051074457', 'W2055817410', 'W2056450656', 'W2056959789', 'W2065228210', 'W2065873941', 'W2071637194', 'W2076766986', 'W2091846236', 'W2094922928', 'W2099138627', 'W2104237724', 'W2114676075', 'W2118768326', 'W2124754476', 'W2126462381', 'W2128810582', 'W2130329024', 'W2130483595', 'W2132257180', 'W2133294820', 'W2134295053', 'W2135099885', 'W2140202771', 'W2149003305', 'W2152539964', 'W2162840341', 'W2195423816', 'W2240887455', 'W2245189809', 'W2275530856', 'W2285762523', 'W2285924575', 'W2293247967', 'W2295019192', 'W2317585507', 'W2318393221', 'W2335223849', 'W2340239395', 'W2343908699', 'W2345301610', 'W2372534467', 'W2401387098', 'W2407223074', 'W2443832241', 'W2460675431', 'W2472413865', 'W2472738043', 'W2495013813', 'W2497369044', 'W2508938868', 'W2513210860', 'W2513931985', 'W2520611857', 'W2528462506', 'W2531437790', 'W2543892322', 'W2547727199', 'W2606461524', 'W2621360686', 'W2962804709'], 'abstract': 'Mobile edge computing (MEC) is an emergent architecture where cloud computing services are extended to the edge of networks leveraging mobile base stations. As a promising edge technology, it can be applied to mobile, wireless, and wireline scenarios, using software and hardware platforms, located at the network edge in the vicinity of end-users. MEC provides seamless integration of multiple application service providers and vendors toward mobile subscribers, enterprises, and other vertical segments. It is an important component in the 5G architecture which supports variety of innovative applications and services where ultralow latency is required. This paper is aimed to present a comprehensive survey of relevant research and technological developments in the area of MEC. It provides the definition of MEC, its advantages, architectures, and application areas; where we in particular highlight related research and future directions. Finally, security and privacy issues and related existing solutions are also discussed.', 'counts_by_year': [[2022, 261], [2021, 339], [2020, 314], [2019, 206], [2018, 74], [2017, 4]]}, {'id': 'W2963206148', 'doi': 'https://doi.org/10.18653/v1/n16-1014', 'title': 'A Diversity-Promoting Objective Function for Neural Conversation Models', 'type': 'proceedings-article', 'publication_date': '2016-03-01', 'host_venue': 'V4306420633', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2294693014', ['I97018004']], ['A2509907281', ['I74973139']], ['A2095316779', ['I1290206253']], ['A2104437897', ['I1290206253']], ['A2165659695', ['I1290206253']]], 'cited_by_count': 1199, 'concepts': [['C2777200299', '0.7936355'], ['C41008148', '0.70207125'], ['C14036430', '0.6796336'], ['C2778112365', '0.6783825'], ['C50644808', '0.6643738']], 'referenced_works': ['W10957333', 'W194577561', 'W319421170', 'W635530177', 'W1518951372', 'W1519256446', 'W1538748823', 'W1578856370', 'W1591706642', 'W1604513301', 'W1623245510', 'W1847211030', 'W1877570817', 'W1948566616', 'W1970207841', 'W2004637830', 'W2064675550', 'W2083305840', 'W2101105183', 'W2115101920', 'W2118434577', 'W2124807415', 'W2130119531', 'W2130942839', 'W2132997613', 'W2140054881', 'W2146574666', 'W2250445771', 'W2252065493', 'W2296712013', 'W2950178297', 'W2962706528', 'W2962883855', 'W2963069010', 'W2963963856', 'W2964308564'], 'abstract': 'Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., I don’t know) regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as the objective function in neural models. Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets and in human evaluations.', 'counts_by_year': [[2022, 64], [2021, 292], [2020, 330], [2019, 277], [2018, 136], [2017, 71], [2016, 27], [2015, 2]]}, {'id': 'W2356997838', 'doi': 'https://doi.org/10.1126/science.aaf5251', 'title': 'Photochemical route for synthesizing atomically dispersed palladium catalysts', 'type': 'journal-article', 'publication_date': '2016-05-13', 'host_venue': 'V3880285', 'open_access_is_oa': False, 'open_access_oa_status': 'closed', 'authorships': [['A2715812379', ['I191208505']], ['A2608785895', ['I191208505']], ['A2475473766', ['I191208505']], ['A2659961665', ['I191208505']], ['A2096114242', ['I191208505']], ['A2207046724', ['I4210159876']], ['A2048587179', ['I129902397']], ['A2524574448', ['I129902397']], ['A2718577685', ['I191208505']], ['A2490626415', ['I191208505']], ['A2124556652', ['I191208505']], ['A2952853795', ['I191208505']], ['A2131362166', ['I191208505']]], 'cited_by_count': 1197, 'concepts': [['C502130503', '0.8941531'], ['C161790260', '0.8496704'], ['C2779851234', '0.7005224'], ['C544153396', '0.65097684'], ['C2777516009', '0.63044035']], 'referenced_works': ['W1853374276', 'W1968426044', 'W1968806874', 'W1970127494', 'W1974987791', 'W1979544533', 'W1980897511', 'W1981014033', 'W1981368803', 'W1982301706', 'W1983711689', 'W1989224009', 'W1990469294', 'W1992277893', 'W1993998340', 'W1997867361', 'W2001692072', 'W2007395042', 'W2010016085', 'W2010940699', 'W2012715800', 'W2014824706', 'W2018222553', 'W2018366372', 'W2020135974', 'W2022317403', 'W2032430846', 'W2033270806', 'W2035548551', 'W2035745357', 'W2036113194', 'W2042997794', 'W2054591468', 'W2066545307', 'W2068489863', 'W2076376028', 'W2083222334', 'W2087570091', 'W2087698390', 'W2094504822', 'W2105761236', 'W2109694766', 'W2110540540', 'W2111631030', 'W2134716155', 'W2135845615', 'W2151747661', 'W2163488899', 'W2164555007', 'W2186035915', 'W2314274336', 'W2340423559', 'W2952056866'], 'abstract': 'Lightly dispersed palladium Catalysts made from atomically dispersed metal atoms on oxide supports can exhibit very high per atom activity. However, the low loadings needed to prevent metal particle formation can limit overall performance. Liu et al. stably decorated titanium oxide nanosheets with relatively high loadings of single palladium atoms by reducing the ions with ultraviolet light and ethylene glycol. These catalysts cleaved H 2 into atoms and were highly effective for hydrogenating alkenes and aldehydes. Science , this issue p. 797', 'counts_by_year': [[2022, 171], [2021, 230], [2020, 249], [2019, 213], [2018, 200], [2017, 114], [2016, 18]]}, {'id': 'W2897805491', 'doi': 'https://doi.org/10.1093/nar/gky962', 'title': 'New approach for understanding genome variations in KEGG', 'type': 'journal-article', 'publication_date': '2019-01-08', 'host_venue': 'V134668137', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2151300008', ['I22299242']], ['A2310244469', ['I2252096349']], ['A143568440', ['I22299242']], ['A2328909099', ['I22299242']], ['A2168774161', ['I22299242']]], 'cited_by_count': 1197, 'concepts': [['C152724338', '0.9819027'], ['C141231307', '0.7350693'], ['C148863701', '0.69970965'], ['C86803240', '0.62463415'], ['C70721500', '0.59386253']], 'referenced_works': ['W1981295860', 'W2032936332', 'W2034269086', 'W2110256992', 'W2117692326', 'W2122732537', 'W2171437346', 'W2174602966', 'W2178043251', 'W2320983896', 'W2558715006', 'W2559588208', 'W2609516818', 'W2883432403'], 'abstract': "KEGG (Kyoto Encyclopedia of Genes and Genomes; https://www.kegg.jp/ or https://www.genome.jp/kegg/) is a reference knowledge base for biological interpretation of genome sequences and other high-throughput data. It is an integrated database consisting of three generic categories of systems information, genomic information and chemical information, and an additional human-specific category of health information. KEGG pathway maps, BRITE hierarchies and KEGG modules have been developed as generic molecular networks with KEGG Orthology nodes of functional orthologs so that KEGG pathway mapping and other procedures can be applied to any cellular organism. Unfortunately, however, this generic approach was inadequate for knowledge representation in the health information category, where variations of human genomes, especially disease-related variations, had to be considered. Thus, we have introduced a new approach where human gene variants are explicitly incorporated into what we call 'network variants' in the recently released KEGG NETWORK database. This allows accumulation of knowledge about disease-related perturbed molecular networks caused not only by gene variants, but also by viruses and other pathogens, environmental factors and drugs. We expect that KEGG NETWORK will become another reference knowledge base for the basic understanding of disease mechanisms and practical use in clinical sequencing and drug development.", 'counts_by_year': [[2022, 175], [2021, 332], [2020, 464], [2019, 220], [2018, 5]]}, {'id': 'W2963782415', 'doi': 'https://doi.org/10.1109/cvpr.2018.00931', 'title': 'PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume', 'type': 'proceedings-article', 'publication_date': '2018-06-01', 'host_venue': 'V4306400194', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2114933504', ['I1304085615']], ['A2569806212', ['I1304085615']], ['A2167546878', ['I1304085615']], ['A1987057275', ['I1304085615']]], 'cited_by_count': 1197, 'concepts': [['C157202957', '0.91265357'], ['C142575187', '0.73183787'], ['C155542232', '0.70914185'], ['C20556612', '0.70286876'], ['C41008148', '0.6945277']], 'referenced_works': ['W764651262', 'W1578285471', 'W1904063580', 'W1921093919', 'W1951289974', 'W1997409415', 'W2019661178', 'W2025768430', 'W2100315781', 'W2104974755', 'W2110140398', 'W2113221323', 'W2120190345', 'W2131747574', 'W2147253850', 'W2147800946', 'W2148534289', 'W2150066425', 'W2152231592', 'W2155302366', 'W2155893237', 'W2294238219', 'W2441099548', 'W2548527721', 'W2558923462', 'W2559597482', 'W2560474170', 'W2779333428', 'W2963210183', 'W2963219046', 'W2963305757', 'W2963317244', 'W2963403249', 'W2963446712', 'W2964140947', 'W3003662786', 'W3020339995', 'W3100388886', 'W4249022109'], 'abstract': 'We present a compact but effective CNN model for optical flow, called PWC-Net. PWC-Net has been designed according to simple and well-established principles: pyramidal processing, warping, and the use of a cost volume. Cast in a learnable feature pyramid, PWC-Net uses the cur- rent optical flow estimate to warp the CNN features of the second image. It then uses the warped features and features of the first image to construct a cost volume, which is processed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller in size and easier to train than the recent FlowNet2 model. Moreover, it outperforms all published optical flow methods on the MPI Sintel final pass and KITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024x436) images. Our models are available on https://github.com/NVlabs/PWC-Net.', 'counts_by_year': [[2022, 137], [2021, 408], [2020, 388], [2019, 227], [2018, 36]]}, {'id': 'W2971763914', 'doi': 'https://doi.org/10.1021/acsnano.9b04224', 'title': 'Present and Future of Surface-Enhanced Raman Scattering', 'type': 'journal-article', 'publication_date': '2020-01-28', 'host_venue': 'V145476921', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A2617277435', []], ['A1979965364', []], ['A2127463333', ['I47686490']], ['A274660962', ['I55952717']], ['A579135234', ['I41156924']], ['A2293776551', ['I241749']], ['A2098079168', ['I154570441']], ['A2115934730', ['I126231945']], ['A2035805593', ['I96673099']], ['A2162026173', ['I212119943']], ['A1762200379', ['I67900169']], ['A1580786970', ['I76198965']], ['A2343797982', ['I76198965']], ['A2131302827', ['I102322142']], ['A2146041805', ['I181647926']], ['A686985919', []], ['A2134338194', ['I146655781']], ['A2123808317', ['I181647926']], ['A1264111355', ['I126307644']], ['A2140062619', ['I130238516']], ['A2107616811', ['I223822909']], ['A2096105785', ['I73613424']], ['A2135375972', ['I66862912']], ['A2020378998', ['I39343248']], ['A1238411706', ['I27837315']], ['A2161618220', ['I111599522']], ['A1515858300', ['I41156924']], ['A2155653207', ['I97018004']], ['A2131589960', ['I191208505']], ['A1863451368', []], ['A2136117519', ['I8204097']], ['A695158005', ['I76198965']], ['A435581889', ['I154570441']], ['A2559022559', ['I205349734']], ['A2583323567', ['I139264467']], ['A2157788512', ['I157725225']], ['A2128289914', ['I206011266']], ['A340212904', ['I6289922']], ['A139602267', ['I6289922']], ['A2464991386', ['I76198965']], ['A2113779504', ['I223822909']], ['A1985712128', ['I75951250']], ['A2126156997', ['I191208505']], ['A2114238882', ['I111979921']], ['A2220653449', ['I66862912']], ['A119539660', ['I62318514']], ['A2111481465', ['I197604219']], ['A2112773396', ['I50557253']], ['A2120318955', ['I191208505']], ['A80458470', ['I111979921']], ['A2327970182', ['I170897317']], ['A2972156233', ['I9224756']], ['A1948764250', ['I84392919']], ['A2108757406', ['I37461747']], ['A2303442392', ['I126231945']], ['A2163355969', ['I177738480']], ['A2165531613', ['I194450716']], ['A334858327', ['I110594554']]], 'cited_by_count': 1196, 'concepts': [['C169573571', '0.85925657'], ['C171250308', '0.7678238'], ['C192562407', '0.48320615'], ['C136197465', '0.4724099'], ['C191486275', '0.4427332']], 'referenced_works': ['W9181081', 'W1213546971', 'W1266157635', 'W1278483824', 'W1450920268', 'W1489292692', 'W1496521667', 'W1521131431', 'W1546843241', 'W1564122896', 'W1576095949', 'W1619968654', 'W1636616099', 'W1690138441', 'W1770977891', 'W1795625810', 'W1820964590', 'W1836369406', 'W1850806008', 'W1876088485', 'W1915307707', 'W1915516683', 'W1922109947', 'W1928613405', 'W1937480654', 'W1939221954', 'W1963860368', 'W1963891260', 'W1963913631', 'W1964501678', 'W1964548759', 'W1964965441', 'W1965333676', 'W1965681180', 'W1966514906', 'W1967196907', 'W1967765688', 'W1968368552', 'W1968871412', 'W1969645755', 'W1970216303', 'W1970592466', 'W1972010202', 'W1972031874', 'W1972082742', 'W1972318722', 'W1972502260', 'W1972938543', 'W1973010157', 'W1973452921', 'W1973609130', 'W1973724012', 'W1975039694', 'W1975909492', 'W1976226355', 'W1976659711', 'W1976950483', 'W1977097025', 'W1977374593', 'W1978062677', 'W1978102817', 'W1978482781', 'W1979650884', 'W1979800732', 'W1980501859', 'W1981692041', 'W1981864358', 'W1982055252', 'W1982146974', 'W1982376326', 'W1982603622', 'W1983243445', 'W1985230130', 'W1985728278', 'W1986413241', 'W1986704497', 'W1987309696', 'W1987511091', 'W1988018135', 'W1988127719', 'W1988285035', 'W1988668502', 'W1989152680', 'W1989315684', 'W1990325478', 'W1990748481', 'W1991017022', 'W1991062282', 'W1991068361', 'W1991100166', 'W1991197480', 'W1991205426', 'W1992822871', 'W1993198696', 'W1993331221', 'W1993343079', 'W1993375813', 'W1994029476', 'W1995529953', 'W1996399590', 'W1996661495', 'W1997868652', 'W1998093075', 'W1998893440', 'W1999173254', 'W2000109557', 'W2000731965', 'W2000942019', 'W2002801807', 'W2003422288', 'W2003515234', 'W2003771178', 'W2004513302', 'W2004812896', 'W2004913474', 'W2005284180', 'W2005425942', 'W2005534285', 'W2005875999', 'W2005908748', 'W2006273975', 'W2006335565', 'W2006874122', 'W2007476652', 'W2007628177', 'W2007631334', 'W2007788631', 'W2008455886', 'W2008590513', 'W2008655860', 'W2008701738', 'W2008738865', 'W2009658139', 'W2010062682', 'W2010950228', 'W2011166834', 'W2011295917', 'W2011911415', 'W2012099076', 'W2012821798', 'W2013068587', 'W2013085398', 'W2013518440', 'W2013630184', 'W2013648089', 'W2013680983', 'W2014133708', 'W2014487114', 'W2014555232', 'W2015101659', 'W2015180722', 'W2015476284', 'W2015951011', 'W2016099912', 'W2016617677', 'W2016752301', 'W2016852573', 'W2017371967', 'W2017466199', 'W2018998388', 'W2020093054', 'W2020653839', 'W2021352330', 'W2022524943', 'W2023095522', 'W2023822005', 'W2024073153', 'W2024236859', 'W2024499712', 'W2024586682', 'W2024594740', 'W2025341110', 'W2025820080', 'W2026840979', 'W2027918727', 'W2028736267', 'W2028927766', 'W2029082348', 'W2029626347', 'W2029810894', 'W2029899740', 'W2030814280', 'W2030943904', 'W2031108968', 'W2031324139', 'W2031686597', 'W2031795179', 'W2032207820', 'W2032347877', 'W2033239757', 'W2033372200', 'W2033428140', 'W2034233243', 'W2034399639', 'W2034446591', 'W2034561752', 'W2035227761', 'W2035293473', 'W2035864062', 'W2036362318', 'W2036379321', 'W2038170516', 'W2038614206', 'W2038784801', 'W2039559580', 'W2039671438', 'W2039968117', 'W2041321047', 'W2042203965', 'W2042375980', 'W2042378760', 'W2042605749', 'W2043009173', 'W2043242157', 'W2043391059', 'W2043582554', 'W2043852629', 'W2044024547', 'W2044256901', 'W2044279302', 'W2044857855', 'W2045178532', 'W2045830497', 'W2046109662', 'W2046555752', 'W2047087058', 'W2047461514', 'W2047723150', 'W2048134161', 'W2048881713', 'W2049062069', 'W2049092431', 'W2049502333', 'W2049759353', 'W2050197030', 'W2050896599', 'W2051115823', 'W2051129881', 'W2051361054', 'W2052174531', 'W2052269384', 'W2052430083', 'W2054066467', 'W2054085096', 'W2054388987', 'W2054431801', 'W2054568006', 'W2055099199', 'W2055158065', 'W2056163357', 'W2056483398', 'W2056866405', 'W2056908636', 'W2057086358', 'W2057179163', 'W2057428057', 'W2057725451', 'W2058220293', 'W2058541903', 'W2058859566', 'W2058895063', 'W2059335162', 'W2059729590', 'W2059981519', 'W2060989292', 'W2061239482', 'W2061581555', 'W2061593511', 'W2064235730', 'W2064336192', 'W2064494070', 'W2064796543', 'W2066895338', 'W2067712937', 'W2069015156', 'W2069690374', 'W2070242211', 'W2070642086', 'W2070892998', 'W2070968266', 'W2071893949', 'W2073394315', 'W2074823645', 'W2076440513', 'W2076898558', 'W2077149255', 'W2077177042', 'W2077325585', 'W2077816672', 'W2077853325', 'W2079494318', 'W2079954478', 'W2080049995', 'W2080742998', 'W2080840327', 'W2080847952', 'W2081233786', 'W2082564318', 'W2082980381', 'W2083289375', 'W2083710951', 'W2083764438', 'W2084165492', 'W2085082168', 'W2085241772', 'W2085287693', 'W2085914647', 'W2086403974', 'W2086869547', 'W2087424332', 'W2087439980', 'W2088852881', 'W2088909164', 'W2088921269', 'W2089181989', 'W2089851463', 'W2089864796', 'W2089906568', 'W2090169650', 'W2090667122', 'W2090964142', 'W2091273808', 'W2091547792', 'W2091820804', 'W2092049851', 'W2092145621', 'W2092459136', 'W2092575834', 'W2092968254', 'W2093168178', 'W2093468996', 'W2093678239', 'W2093710297', 'W2094107947', 'W2094146713', 'W2094273643', 'W2094720260', 'W2097378387', 'W2099277030', 'W2100235363', 'W2100298741', 'W2101085174', 'W2101165679', 'W2101282262', 'W2101785706', 'W2102195653', 'W2102197282', 'W2102304835', 'W2103113521', 'W2104160815', 'W2104285173', 'W2104791047', 'W2105245195', 'W2105443512', 'W2106170147', 'W2106209114', 'W2106317934', 'W2106635129', 'W2108038055', 'W2108061380', 'W2108603769', 'W2110180898', 'W2110955731', 'W2113370570', 'W2113458047', 'W2114870709', 'W2115797520', 'W2116959539', 'W2117635999', 'W2119261212', 'W2119767565', 'W2119979113', 'W2120068382', 'W2120332222', 'W2120905553', 'W2121341113', 'W2121551036', 'W2121809188', 'W2122028423', 'W2122296714', 'W2122347920', 'W2122715606', 'W2122764002', 'W2122796663', 'W2123425325', 'W2123852381', 'W2124158735', 'W2125123975', 'W2125492639', 'W2125965180', 'W2126408127', 'W2127808169', 'W2133242737', 'W2134503542', 'W2137037774', 'W2138428688', 'W2139002972', 'W2139587679', 'W2141501403', 'W2141742048', 'W2142647038', 'W2143476473', 'W2143971456', 'W2144653973', 'W2145922222', 'W2147374653', 'W2148457909', 'W2148744629', 'W2149063273', 'W2149481847', 'W2149910855', 'W2150294514', 'W2150829140', 'W2150853706', 'W2152732343', 'W2153340700', 'W2153515784', 'W2154596527', 'W2155271344', 'W2155462488', 'W2155497668', 'W2156624962', 'W2157055089', 'W2157930111', 'W2158335575', 'W2158435804', 'W2159220300', 'W2159656505', 'W2159981054', 'W2160154617', 'W2160698148', 'W2160791684', 'W2161502718', 'W2161610527', 'W2161667587', 'W2162192640', 'W2162303488', 'W2164733772', 'W2165075822', 'W2165353311', 'W2166224345', 'W2166985514', 'W2169590766', 'W2169776589', 'W2169842284', 'W2170755932', 'W2170944163', 'W2171733899', 'W2171864173', 'W2173666432', 'W2180192327', 'W2187797706', 'W2192174432', 'W2193363642', 'W2194646219', 'W2227038956', 'W2227560517', 'W2227974312', 'W2229213858', 'W2230620782', 'W2232199395', 'W2234648718', 'W2234692913', 'W2235357140', 'W2241553685', 'W2253102388', 'W2254159412', 'W2256769347', 'W2260011327', 'W2261969346', 'W2267203883', 'W2271649788', 'W2273537787', 'W2276367670', 'W2276669315', 'W2277086306', 'W2286593000', 'W2286754526', 'W2289073284', 'W2289523892', 'W2294913083', 'W2294970873', 'W2297600515', 'W2298636456', 'W2299480139', 'W2301954252', 'W2302215001', 'W2312687167', 'W2313176109', 'W2313251397', 'W2313529616', 'W2314027892', 'W2314379510', 'W2315034636', 'W2315203095', 'W2315267969', 'W2315578403', 'W2316180345', 'W2316467102', 'W2316765574', 'W2316914931', 'W2317552399', 'W2318045848', 'W2318083399', 'W2318386614', 'W2318527211', 'W2318779261', 'W2319045601', 'W2319288524', 'W2319530976', 'W2319868797', 'W2319891357', 'W2320023068', 'W2320608472', 'W2320812022', 'W2321406324', 'W2321586013', 'W2322105224', 'W2322463543', 'W2322511324', 'W2323258162', 'W2323506142', 'W2323585116', 'W2323658098', 'W2323769997', 'W2324002133', 'W2324196670', 'W2324399039', 'W2324468711', 'W2324597909', 'W2324797010', 'W2324911857', 'W2325316312', 'W2325505722', 'W2325562515', 'W2325770519', 'W2325794174', 'W2325934769', 'W2326537845', 'W2327776224', 'W2327974999', 'W2328713564', 'W2329397967', 'W2329456468', 'W2329469982', 'W2330405457', 'W2330562742', 'W2330672744', 'W2331316671', 'W2331391572', 'W2331773060', 'W2332526522', 'W2332905688', 'W2333456874', 'W2333951576', 'W2334252442', 'W2334401928', 'W2335077566', 'W2335428125', 'W2335731761', 'W2336876935', 'W2337838872', 'W2338255793', 'W2339023730', 'W2339446211', 'W2339572033', 'W2339952160', 'W2340277620', 'W2341089887', 'W2342845453', 'W2343907567', 'W2344982785', 'W2346207302', 'W2347127586', 'W2356210405', 'W2365466025', 'W2367659986', 'W2369250039', 'W2384159072', 'W2387983861', 'W2395300225', 'W2395330015', 'W2397553609', 'W2397622414', 'W2400739184', 'W2408018068', 'W2408986451', 'W2409984413', 'W2413557359', 'W2413644903', 'W2414374850', 'W2415712484', 'W2416021823', 'W2416094463', 'W2416748122', 'W2416806992', 'W2418097529', 'W2418622137', 'W2428308551', 'W2432631089', 'W2434602933', 'W2465387921', 'W2466460847', 'W2466516562', 'W2467399775', 'W2468826163', 'W2470309863', 'W2472109776', 'W2472474602', 'W2472854251', 'W2481053322', 'W2489803733', 'W2490548652', 'W2494960904', 'W2500290533', 'W2503178535', 'W2505676970', 'W2508105077', 'W2509143451', 'W2509469366', 'W2511041920', 'W2511574995', 'W2512815348', 'W2512815903', 'W2512982980', 'W2513536758', 'W2513761671', 'W2514641483', 'W2514955246', 'W2516048010', 'W2516811479', 'W2516895384', 'W2517340107', 'W2518214728', 'W2520206962', 'W2521204505', 'W2521754266', 'W2522439080', 'W2525305627', 'W2529159498', 'W2529951980', 'W2530188073', 'W2531003645', 'W2531660345', 'W2533571958', 'W2533581290', 'W2533725972', 'W2536639227', 'W2537649838', 'W2539910658', 'W2543172877', 'W2546027427', 'W2547047447', 'W2547935001', 'W2550114781', 'W2550307340', 'W2552717798', 'W2552835525', 'W2553503884', 'W2554569057', 'W2554672369', 'W2555186747', 'W2555370621', 'W2560055168', 'W2560630804', 'W2562676312', 'W2562959833', 'W2564173213', 'W2566274594', 'W2566360472', 'W2566416434', 'W2566911252', 'W2568387487', 'W2570536352', 'W2572185142', 'W2584916314', 'W2587795192', 'W2588698844', 'W2588918392', 'W2589620274', 'W2590060078', 'W2590401950', 'W2591276097', 'W2591382255', 'W2591840504', 'W2591877025', 'W2592422834', 'W2593619230', 'W2594682827', 'W2595463699', 'W2597515552', 'W2597734467', 'W2600967500', 'W2601846563', 'W2602194882', 'W2602205506', 'W2602556976', 'W2602600633', 'W2605393158', 'W2606223814', 'W2606933979', 'W2607025989', 'W2608064079', 'W2609161865', 'W2609961284', 'W2610861761', 'W2611172562', 'W2611223374', 'W2613187506', 'W2613672780', 'W2615492215', 'W2616789045', 'W2617057812', 'W2617402555', 'W2617670767', 'W2617871863', 'W2618468622', 'W2619025018', 'W2619609863', 'W2620308334', 'W2621602701', 'W2621747531', 'W2622466909', 'W2623448786', 'W2624439114', 'W2625689675', 'W2625725862', 'W2626362756', 'W2627068782', 'W2632690061', 'W2660185678', 'W2682229993', 'W2694085773', 'W2698076822', 'W2713064966', 'W2717065228', 'W2724473782', 'W2729392074', 'W2730627939', 'W2731059670', 'W2731797000', 'W2733302551', 'W2734655451', 'W2735746319', 'W2736319063', 'W2736403125', 'W2737285585', 'W2738395006', 'W2740337666', 'W2742803722', 'W2742951485', 'W2744183462', 'W2745992063', 'W2746722373', 'W2748718637', 'W2749488004', 'W2750402737', 'W2751965304', 'W2753036493', 'W2754054227', 'W2754733628', 'W2755382325', 'W2758481841', 'W2759433020', 'W2759907460', 'W2760147053', 'W2760983585', 'W2761173066', 'W2761503646', 'W2761746302', 'W2763387022', 'W2763614898', 'W2764144455', 'W2765809264', 'W2765825620', 'W2765843192', 'W2765846400', 'W2766228063', 'W2766404322', 'W2766570871', 'W2767185672', 'W2768026100', 'W2769493952', 'W2769529515', 'W2769707585', 'W2769728972', 'W2770978717', 'W2771353688', 'W2771383427', 'W2772958364', 'W2773530531', 'W2773580742', 'W2774576305', 'W2774576568', 'W2775218961', 'W2779716391', 'W2781746491', 'W2781990371', 'W2782725406', 'W2782735281', 'W2784268921', 'W2784546822', 'W2785956082', 'W2787601138', 'W2788256195', 'W2788795282', 'W2788884019', 'W2789145551', 'W2790268062', 'W2790876513', 'W2791860513', 'W2791954955', 'W2792835296', 'W2793416400', 'W2793936121', 'W2794104564', 'W2794319958', 'W2796910558', 'W2797445881', 'W2797778915', 'W2799377162', 'W2799460382', 'W2799842393', 'W2800188881', 'W2800916479', 'W2801190244', 'W2801204835', 'W2801447215', 'W2801701774', 'W2803334056', 'W2803346212', 'W2803365154', 'W2803574407', 'W2803745503', 'W2803929346', 'W2804995621', 'W2805636964', 'W2806336438', 'W2807288497', 'W2807344079', 'W2807464890', 'W2808175301', 'W2808228964', 'W2808263590', 'W2808266063', 'W2808743080', 'W2808925555', 'W2810534239', 'W2811173466', 'W2813336664', 'W2852139651', 'W2863846037', 'W2870458810', 'W2876810382', 'W2883267057', 'W2883296134', 'W2884096448', 'W2885741661', 'W2886258632', 'W2886373236', 'W2886531226', 'W2886579776', 'W2887306454', 'W2887397759', 'W2887938422', 'W2888225502', 'W2888303409', 'W2888337651', 'W2888623383', 'W2888956738', 'W2889471144', 'W2890133858', 'W2890236305', 'W2890967653', 'W2891451125', 'W2891933205', 'W2893748735', 'W2894390495', 'W2894768827', 'W2894905332', 'W2895261396', 'W2895279544', 'W2895356586', 'W2895552223', 'W2896861889', 'W2897034108', 'W2897126608', 'W2897590679', 'W2897649654', 'W2897787157', 'W2898064466', 'W2898260012', 'W2898362703', 'W2898687314', 'W2898900405', 'W2899368188', 'W2899722733', 'W2899773067', 'W2900522382', 'W2900877093', 'W2901123157', 'W2901462587', 'W2901760167', 'W2902019754', 'W2902454076', 'W2903045975', 'W2903314701', 'W2903478533', 'W2904703066', 'W2904724367', 'W2904840918', 'W2906116449', 'W2906286569', 'W2908051142', 'W2908225547', 'W2909089127', 'W2909756679', 'W2910086278', 'W2910480319', 'W2921037220', 'W2921742325', 'W2933064643', 'W2935271152', 'W2936210773', 'W2945581304', 'W3102366353', 'W3105550380', 'W3213474653', 'W4234368842', 'W4237936560', 'W4240868444', 'W4241057039', 'W4247358698', 'W4247432230', 'W4253002754', 'W4253132507', 'W4253769532', 'W4290051497'], 'abstract': 'The discovery of the enhancement of Raman scattering by molecules adsorbed on nanostructured metal surfaces is a landmark in the history of spectroscopic and analytical techniques. Significant experimental and theoretical effort has been directed toward understanding the surface-enhanced Raman scattering (SERS) effect and demonstrating its potential in various types of ultrasensitive sensing applications in a wide variety of fields. In the 45 years since its discovery, SERS has blossomed into a rich area of research and technology, but additional efforts are still needed before it can be routinely used analytically and in commercial products. In this Review, prominent authors from around the world joined together to summarize the state of the art in understanding and using SERS and to predict what can be expected in the near future in terms of research, applications, and technological development. This Review is dedicated to SERS pioneer and our coauthor, the late Prof. Richard Van Duyne, whom we lost during the preparation of this article.', 'counts_by_year': [[2022, 449], [2021, 492], [2020, 235], [2019, 6]]}, {'id': 'W3011720761', 'doi': 'https://doi.org/10.1371/journal.pone.0231924', 'title': 'Mental health problems and social media exposure during COVID-19 outbreak', 'type': 'journal-article', 'publication_date': '2020-04-16', 'host_venue': 'V202381698', 'open_access_is_oa': True, 'open_access_oa_status': 'gold', 'authorships': [['A2619146681', ['I24943067']], ['A3012461023', ['I24943067']], ['A2128324607', ['I24943067']], ['A2965471627', ['I24943067']], ['A3012326268', ['I24943067']], ['A3010804109', ['I24943067']], ['A2296817949', ['I24943067']], ['A2109090600', ['I24943067']], ['A2489857502', ['I24943067']]], 'cited_by_count': 1196, 'concepts': [['C3008058167', '0.78357065'], ['C116675565', '0.7719077'], ['C3006700255', '0.6346161'], ['C3007834351', '0.5995066'], ['C99454951', '0.53848934']], 'referenced_works': ['W1968380849', 'W1989377012', 'W2036395105', 'W2043705607', 'W2076537644', 'W2567841985', 'W2579200804', 'W2591886901', 'W2916226351', 'W2947270929'], 'abstract': 'Huge citizens expose to social media during a novel coronavirus disease (COVID-19) outbroke in Wuhan, China. We assess the prevalence of mental health problems and examine their association with social media exposure. A cross-sectional study among Chinese citizens aged≥18 years old was conducted during Jan 31 to Feb 2, 2020. Online survey was used to do rapid assessment. Total of 4872 participants from 31 provinces and autonomous regions were involved in the current study. Besides demographics and social media exposure (SME), depression was assessed by The Chinese version of WHO-Five Well-Being Index (WHO-5) and anxiety was assessed by Chinese version of generalized anxiety disorder scale (GAD-7). multivariable logistic regressions were used to identify associations between social media exposure with mental health problems after controlling for covariates. The prevalence of depression, anxiety and combination of depression and anxiety (CDA) was 48.3% (95%CI: 46.9%-49.7%), 22.6% (95%CI: 21.4%-23.8%) and 19.4% (95%CI: 18.3%-20.6%) during COVID-19 outbroke in Wuhan, China. More than 80% (95%CI:80.9%-83.1%) of participants reported frequently exposed to social media. After controlling for covariates, frequently SME was positively associated with high odds of anxiety (OR = 1.72, 95%CI: 1.31-2.26) and CDA (OR = 1.91, 95%CI: 1.52-2.41) compared with less SME. Our findings show there are high prevalence of mental health problems, which positively associated with frequently SME during the COVID-19 outbreak. These findings implicated the government need pay more attention to mental health problems, especially depression and anxiety among general population and combating with infodemic while combating during public health emergency.', 'counts_by_year': [[2022, 323], [2021, 577], [2020, 295]]}, {'id': 'W2760946358', 'doi': 'https://doi.org/10.1038/s41591-018-0177-5', 'title': 'Classification and mutation prediction from non–small cell lung cancer histopathology images using deep learning', 'type': 'journal-article', 'publication_date': '2018-09-17', 'host_venue': 'V203256638', 'open_access_is_oa': True, 'open_access_oa_status': 'green', 'authorships': [['A2580934298', ['I57206974']], ['A2156865458', ['I57206974']], ['A1990514653', ['I174458059']], ['A2655563459', ['I57206974']], ['A2950164936', ['I57206974']], ['A2015158303', ['I57206974']], ['A2608666126', ['I57206974']], ['A2078741054', ['I57206974']], ['A1523331842', ['I57206974']]], 'cited_by_count': 1194, 'concepts': [['C544855455', '0.74579996'], ['C2776256026', '0.65436125'], ['C2781182431', '0.6303805'], ['C2781187634', '0.58435804'], ['C142724271', '0.52177316']], 'referenced_works': ['W1484784716', 'W1849277567', 'W1971176522', 'W1973414121', 'W1980497534', 'W1988033732', 'W1995945562', 'W1996910715', 'W2011192758', 'W2016489352', 'W2019629912', 'W2022495797', 'W2049484790', 'W2049674541', 'W2053154970', 'W2076063813', 'W2097117768', 'W2114784462', 'W2120649999', 'W2120669034', 'W2129590156', 'W2132000016', 'W2134640949', 'W2135617839', 'W2139556177', 'W2141744105', 'W2142312761', 'W2153345506', 'W2157825442', 'W2163933632', 'W2171966132', 'W2180481128', 'W2183341477', 'W2275877493', 'W2280351290', 'W2312404985', 'W2323929895', 'W2325271624', 'W2327529106', 'W2329365547', 'W2329625383', 'W2341106171', 'W2345010043', 'W2355329030', 'W2504220184', 'W2514628397', 'W2521492299', 'W2526050071', 'W2532104363', 'W2533800772', 'W2548615865', 'W2557738935', 'W2581082771', 'W2607075141', 'W2614808277', 'W2619902663', 'W2620771841', 'W2789277655', 'W2791475466', 'W2949648625', 'W2963896025', 'W4242460399', 'W4294214983'], 'abstract': 'Visual inspection of histopathology slides is one of the main methods used by pathologists to assess the stage, type and subtype of lung tumors. Adenocarcinoma (LUAD) and squamous cell carcinoma (LUSC) are the most prevalent subtypes of lung cancer, and their distinction requires visual inspection by an experienced pathologist. In this study, we trained a deep convolutional neural network (inception v3) on whole-slide images obtained from The Cancer Genome Atlas to accurately and automatically classify them into LUAD, LUSC or normal lung tissue. The performance of our method is comparable to that of pathologists, with an average area under the curve (AUC) of 0.97. Our model was validated on independent datasets of frozen tissues, formalin-fixed paraffin-embedded tissues and biopsies. Furthermore, we trained the network to predict the ten most commonly mutated genes in LUAD. We found that six of them-STK11, EGFR, FAT1, SETBP1, KRAS and TP53-can be predicted from pathology images, with AUCs from 0.733 to 0.856 as measured on a held-out population. These findings suggest that deep-learning models can assist pathologists in the detection of cancer subtype or gene mutations. Our approach can be applied to any cancer type, and the code is available at https://github.com/ncoudray/DeepPATH .', 'counts_by_year': [[2022, 312], [2021, 386], [2020, 310], [2019, 177], [2018, 8]]}, {'id': 'W2963223306', 'doi': 'https://doi.org/10.18653/v1/k16-1002', 'title': 'Generating Sentences from a Continuous Space', 'type': 'proceedings-article', 'publication_date': '2016-01-01', 'host_venue': 'V4306418031', 'open_access_is_oa': True, 'open_access_oa_status': 'hybrid', 'authorships': [['A1967404238', ['I97018004']], ['A56040624', ['I2802841742']], ['A2634174050', ['I4210128969']], ['A2083455184', ['I4210128969']], ['A2241764882', ['I4210128969']], ['A2016539005', ['I4210128969']]], 'cited_by_count': 1193, 'concepts': [['C41008148', '0.81352854'], ['C101738243', '0.7310552'], ['C2777530160', '0.6722661'], ['C154945302', '0.65616184'], ['C137293760', '0.61397934']], 'referenced_works': ['W592244745', 'W648143168', 'W1026270304', 'W1486649854', 'W1487641199', 'W1554544485', 'W1632114991', 'W1681397005', 'W1832693441', 'W1884859883', 'W1895577753', 'W1933340426', 'W1947481528', 'W1980776243', 'W2064675550', 'W2070246124', 'W2095705004', 'W2097039814', 'W2099471712', 'W2103305545', 'W2115613106', 'W2130942839', 'W2131241448', 'W2131744502', 'W2171928131', 'W2212660284', 'W2250473257', 'W2570840460', 'W2962706528', 'W2962741254', 'W2962897886', 'W2963090522', 'W2963619462', 'W2963773425', 'W2964091467', 'W2964308564'], 'abstract': "The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.", 'counts_by_year': [[2022, 72], [2021, 304], [2020, 321], [2019, 288], [2018, 140], [2017, 59], [2016, 9]]}]
